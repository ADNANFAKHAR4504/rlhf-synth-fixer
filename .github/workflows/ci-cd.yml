name: CI/CD Pipeline

# Special keyword [skip-jobs] can be added to commit messages to skip most jobs
# The upload-task-to-s3 job will still run when a PR is merged, regardless of this tag
# The cleanup-pr job will still run when a PR is closed, regardless of this tag

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
    types: [opened, synchronize, reopened, closed]
  workflow_dispatch:
  

permissions:
  contents: read

env:
  NODE_VERSION: '22.17.0'
  # Use PR number for resource isolation, fallback to 'main' for main branch
  # Prefix with 'pr' to ensure AWS resource names start with a letter
  ENVIRONMENT_SUFFIX: ${{ github.event.number && format('pr{0}', github.event.number) || 'dev' }}
  S3_RELEASE_BUCKET_NAME: 'iac-rlhf-aws-release'
  S3_PRODUCTION_BUCKET_NAME: 'iac-rlhf-production'

jobs:
  detect-metadata:
    name: Detect Project Files
    runs-on: ubuntu-24.04
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    outputs:
      platform: ${{ steps.metadata.outputs.platform }}
      language: ${{ steps.metadata.outputs.language }}
      po_id: ${{ steps.metadata.outputs.po_id }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Read metadata.json
        id: metadata
        run: |
          if [ ! -f "metadata.json" ]; then
            echo "‚ùå metadata.json not found, exiting with failure"
            exit 1
          fi

          PLATFORM=$(jq -r '.platform // "unknown"' metadata.json)
          LANGUAGE=$(jq -r '.language // "unknown"' metadata.json)
          PO_ID=$(jq -r '.po_id // empty' metadata.json)

          echo "Detected platform: $PLATFORM, language: $LANGUAGE"

          echo "platform=$PLATFORM" >> $GITHUB_OUTPUT
          echo "language=$LANGUAGE" >> $GITHUB_OUTPUT
          echo "po_id=$PO_ID" >> $GITHUB_OUTPUT

      - name: Check for required documentation files
        id: docs-check
        run: |
          if [ ! -f "lib/PROMPT.md" ]; then
            echo "‚ùå lib/PROMPT.md not found, exiting with failure"
            exit 1
          fi

          if [ ! -f "lib/MODEL_RESPONSE.md" ]; then
            echo "‚ùå lib/PROMPT.md not found, exiting with failure"
            exit 1
          fi

  version-check:
    name: Check Runtime Versions
    runs-on: ubuntu-24.04
    needs: detect-metadata
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js 22.17.0
        uses: actions/setup-node@v4
        with:
          node-version: '22.17.0'
          node-version-file: '.nvmrc'
          cache: 'npm'

      - name: Setup Python 3.12.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.12.11'
          check-latest: true

      - name: Install pipenv
        run: |
          python -m pip install --upgrade pip
          pip install pipenv==2025.0.4

      - name: Verify Node.js version
        run: |
          node --version
          npm --version
          if [ "$(node --version)" != "v22.17.0" ]; then
            echo "‚ùå Node.js version mismatch! Expected: v22.17.0, Got: $(node --version)"
            exit 1
          fi
          echo "‚úÖ Node.js version is correct: $(node --version)"

      - name: Verify Python version
        run: |
          python --version
          if [ "$(python --version)" != "Python 3.12.11" ]; then
            echo "‚ùå Python version mismatch! Expected: Python 3.12.11, Got: $(python --version)"
            exit 1
          fi
          echo "‚úÖ Python version is correct: $(python --version)"

      - name: Verify pipenv version
        run: |
          pipenv --version
          PIPENV_VERSION=$(pipenv --version | grep -oE '[0-9]+\.[0-9]+\.[0-9]+')
          if [ "$PIPENV_VERSION" != "2025.0.4" ]; then
            echo "‚ùå Pipenv version mismatch! Expected: 2025.0.4, Got: $PIPENV_VERSION"
            exit 1
          fi
          echo "‚úÖ Pipenv version is correct: $PIPENV_VERSION"

  build:
    name: Build
    runs-on: ubuntu-24.04
    needs: version-check
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    environment: dev

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Python 3.12.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.12.11'
          check-latest: true

      - name: Install pipenv
        run: |
          python -m pip install --upgrade pip
          pip install pipenv==2025.0.4

      - name: Install dependencies
        run: npm ci

      - name: Build
        run: npm run build

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-artifacts
          include-hidden-files: true # <-- This one!
          path: |
            bin/
            lib/
            package*.json
            tsconfig.json
            cdk.json
            metadata.json

  synth:
    name: Synth
    runs-on: ubuntu-24.04
    needs: [detect-metadata, build]
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    environment: dev
    permissions:
      contents: read
    outputs:
      platform: ${{ needs.detect-metadata.outputs.platform }}
      language: ${{ needs.detect-metadata.outputs.language }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'true'

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Synth
        run: |
          echo "Project: platform=${{ needs.detect-metadata.outputs.platform }}, language=${{ needs.detect-metadata.outputs.language }}"

          PLATFORM="${{ needs.detect-metadata.outputs.platform }}"
          LANGUAGE="${{ needs.detect-metadata.outputs.language }}"

          if [ "$PLATFORM" = "cdk" ]; then
            echo "‚úÖ CDK project detected, running CDK synth..."
            npm run cdk:synth
          else
            echo "‚ÑπÔ∏è Not a CDK project, skipping CDK synth"
            echo "This is expected for non-CDK projects like CloudFormation templates"
            # Create empty cdk.out directory to satisfy artifact upload
            mkdir -p cdk.out
            echo "# No CDK artifacts generated for non-CDK projects" > cdk.out/README.md
          fi
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}

      - name: Upload Build Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: cdk-artifacts
          path: cdk.out/

  deploy:
    name: Deploy
    runs-on: ubuntu-24.04
    needs: synth
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    environment: dev

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'true'
          download-cdk-artifacts: 'true'

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Bootstrap
        run: |
          PLATFORM="${{ needs.synth.outputs.platform }}"
          LANGUAGE="${{ needs.synth.outputs.language }}"

          if [ "$PLATFORM" = "cdk" ]; then
            echo "‚úÖ CDK project detected, running CDK bootstrap..."
            npm run cdk:bootstrap
          else
            echo "‚ÑπÔ∏è Not a CDK project, skipping CDK bootstrap"
          fi
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
          REPOSITORY: ${{ github.repository }}
          COMMIT_AUTHOR: ${{ github.event.head_commit.author.name || github.actor }}

      - name: Deploy
        run: |
          PLATFORM="${{ needs.synth.outputs.platform }}"
          LANGUAGE="${{ needs.synth.outputs.language }}"

          if [ "$PLATFORM" = "cdk" ]; then
            echo "‚úÖ CDK project detected, running CDK deploy..."
            npm run cdk:deploy
          elif [ "$PLATFORM" = "cfn" ] && [ "$LANGUAGE" = "yaml" ]; then
            echo "‚úÖ CloudFormation YAML project detected, deploying with AWS CLI..."
            npm run cfn:deploy-yaml
          elif [ "$PLATFORM" = "cfn" ] && [ "$LANGUAGE" = "json" ]; then
            echo "‚úÖ CloudFormation JSON project detected, deploying with AWS CLI..."
            npm run cfn:deploy-json
          else
            echo "‚ÑπÔ∏è Unknown deployment method for platform: $PLATFORM, language: $LANGUAGE"
            echo "üí° Supported combinations: cdk+typescript, cdk+python, cfn+yaml, cfn+json"
          fi
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
          REPOSITORY: ${{ github.repository }}
          COMMIT_AUTHOR: ${{ github.event.head_commit.author.name || github.actor }}

      - name: Get Deployment Outputs
        run: |
          PLATFORM="${{ needs.synth.outputs.platform }}"
          LANGUAGE="${{ needs.synth.outputs.language }}"

          if [ "$PLATFORM" = "cdk" ]; then
            echo "‚úÖ CDK project detected, getting CDK outputs..."
            npx cdk list --json > cdk-stacks.json
            mkdir -p cfn-outputs
            echo "Getting all CloudFormation stacks..."
            aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE --query "StackSummaries[?contains(StackName, \`TapStack${{ env.ENVIRONMENT_SUFFIX }}\`)].StackName" --output text > cf-stacks.txt
            echo "{}" > cfn-outputs/all-outputs.json
            if [ -s cf-stacks.txt ]; then
              for stack in $(cat cf-stacks.txt); do
                echo "Getting outputs for CloudFormation stack: $stack"
                aws cloudformation describe-stacks --stack-name "$stack" --query 'Stacks[0].Outputs' --output json > "temp-${stack}-outputs.json" 2>/dev/null || echo "No outputs for $stack"
                if [ -f "temp-${stack}-outputs.json" ]; then
                  output_count=$(jq 'length' "temp-${stack}-outputs.json" 2>/dev/null || echo "0")
                  if [ "$output_count" != "0" ] && [ "$output_count" != "null" ]; then
                    jq -n --arg stack "$stack" --slurpfile outputs "temp-${stack}-outputs.json" '{($stack): $outputs[0]}' > "temp-stack.json"
                    jq -s '.[0] * .[1]' cfn-outputs/all-outputs.json temp-stack.json > temp-merged.json
                    mv temp-merged.json cfn-outputs/all-outputs.json
                    if [ ! -f "cfn-outputs/flat-outputs.json" ]; then
                      echo "{}" > cfn-outputs/flat-outputs.json
                    fi
                    jq -r '.[] | "\(.OutputKey)=\(.OutputValue)"' "temp-${stack}-outputs.json" | while IFS='=' read -r key value; do
                      jq --arg key "$key" --arg value "$value" '. + {($key): $value}' cfn-outputs/flat-outputs.json > temp-flat.json
                      mv temp-flat.json cfn-outputs/flat-outputs.json
                    done
                  fi
                  rm -f "temp-${stack}-outputs.json"
                fi
              done
              rm -f temp-stack.json temp-merged.json temp-flat.json
            else
              echo "No TapStack CloudFormation stacks found"
            fi
            echo "Consolidated outputs:"
            cat cfn-outputs/all-outputs.json || echo "No consolidated outputs"
            echo "Flat outputs:"
            cat cfn-outputs/flat-outputs.json || echo "No flat outputs"
          elif [ "${{ needs.synth.outputs.platform }}" = "cfn" ]; then
            echo "‚úÖ CloudFormation project detected, getting stack outputs..."
            mkdir -p cfn-outputs
            # Try to find the stack name (assuming TapStack<ENVIRONMENT_SUFFIX>)
            STACK_NAME="TapStack${{ env.ENVIRONMENT_SUFFIX }}"
            echo "Getting outputs for CloudFormation stack: $STACK_NAME"
            aws cloudformation describe-stacks --stack-name "$STACK_NAME" --query 'Stacks[0].Outputs' --output json > temp-${STACK_NAME}-outputs.json 2>/dev/null || echo "No outputs for $STACK_NAME"
            echo "{}" > cfn-outputs/all-outputs.json
            if [ -f "temp-${STACK_NAME}-outputs.json" ]; then
              output_count=$(jq 'length' "temp-${STACK_NAME}-outputs.json" 2>/dev/null || echo "0")
              if [ "$output_count" != "0" ] && [ "$output_count" != "null" ]; then
                jq -n --arg stack "$STACK_NAME" --slurpfile outputs "temp-${STACK_NAME}-outputs.json" '{($stack): $outputs[0]}' > temp-stack.json
                jq -s '.[0] * .[1]' cfn-outputs/all-outputs.json temp-stack.json > temp-merged.json
                mv temp-merged.json cfn-outputs/all-outputs.json
                if [ ! -f "cfn-outputs/flat-outputs.json" ]; then
                  echo "{}" > cfn-outputs/flat-outputs.json
                fi
                jq -r '.[] | "\(.OutputKey)=\(.OutputValue)"' "temp-${STACK_NAME}-outputs.json" | while IFS='=' read -r key value; do
                  jq --arg key "$key" --arg value "$value" '. + {($key): $value}' cfn-outputs/flat-outputs.json > temp-flat.json
                  mv temp-flat.json cfn-outputs/flat-outputs.json
                done
              fi
              rm -f "temp-${STACK_NAME}-outputs.json"
            fi
            rm -f temp-stack.json temp-merged.json temp-flat.json
            echo "Consolidated outputs:"
            cat cfn-outputs/all-outputs.json || echo "No consolidated outputs"
            echo "Flat outputs:"
            cat cfn-outputs/flat-outputs.json || echo "No flat outputs"
          else
            echo "‚ÑπÔ∏è Not a CDK TypeScript or CloudFormation project, creating empty outputs for consistency"
            mkdir -p cfn-outputs
            echo "{}" > cfn-outputs/all-outputs.json
            echo "{}" > cfn-outputs/flat-outputs.json
            echo "# No CDK outputs for non-CDK projects" > cdk-stacks.json
          fi

      - name: Upload Deployment Outputs
        uses: actions/upload-artifact@v4
        with:
          name: cfn-outputs
          path: |
            cfn-outputs/
            cdk-stacks.json

  lint:
    name: Lint
    runs-on: ubuntu-24.04
    needs: [detect-metadata, build]
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    environment: qa
    outputs:
      platform: ${{ needs.detect-metadata.outputs.platform }}
      language: ${{ needs.detect-metadata.outputs.language }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'true'

      # Python setup is handled by the setup-environment action

      - name: Run Linting
        run: |
          # Enable shell debugging - prints each command before execution
          set -x
          PLATFORM="${{ needs.detect-metadata.outputs.platform }}"
          LANGUAGE="${{ needs.detect-metadata.outputs.language }}"

          echo "Running linting for platform: $PLATFORM, language: $LANGUAGE"

          if [ "$LANGUAGE" = "ts" ]; then
            echo "‚úÖ TypeScript project detected, running ESLint..."
            npm run lint
          elif [ "$LANGUAGE" = "py" ]; then
            # --- NEW: Install Python dev dependencies within this step ---
            echo "Installing Python dev dependencies via pipenv..."
            pipenv install --dev --deploy --system || { echo "‚ùå pipenv install failed!"; exit 1; }
            echo "Python dev dependencies installed."
            echo "Executing pipenv run lint..."
            LINT_OUTPUT=$(pipenv run lint 2>&1 || true)
            LINT_EXIT_CODE=$?
            echo "--- START PYLINT OUTPUT (Raw) ---"
            echo "$LINT_OUTPUT"
            echo "--- END PYLINT OUTPUT (Raw) ---"
            echo "Pylint command raw exit code: $LINT_EXIT_CODE"

            if [ "$LINT_EXIT_CODE" -ne 0 ]; then
              echo "‚ö†Ô∏è Pylint command exited with non-zero status code: $LINT_EXIT_CODE."
              # We will still attempt to parse the score, but this is a warning.
              # If score parsing also fails, the next check will catch it.
            fi
            # Step 3: Extract the score from the Pylint output.
            # This `sed` command looks for lines containing "rated at X/10" and extracts X.
            # It handles both integer (e.g., 7) and float (e.g., 7.50) scores.
            SCORE=$(echo "$LINT_OUTPUT" | sed -n 's/.*rated at \([0-9.]*\)\/10.*/\1/p')
            if [[ -z "$SCORE" || ! "$SCORE" =~ ^[0-9.]+$ ]]; then
              echo "‚ùå ERROR: Could not extract linting score from Pylint output."
              echo "Please verify Pylint's output format or if it ran successfully enough to produce a score."
              exit 1 # Fail the step if the score cannot be determined
            fi
            echo "Detected Pylint Score: $SCORE/10"
            # Step 5: Define the minimum acceptable score
            MIN_SCORE=7.0
            # Step 6: Compare the extracted score with the minimum threshold.
            # `bc -l` is used for floating-point arithmetic comparison in shell scripts.
            if (( $(echo "$SCORE >= $MIN_SCORE" | bc -l) )); then
              echo "‚úÖ Linting score $SCORE/10 is greater than or equal to $MIN_SCORE. Linting passed."
              exit 0 # Explicitly exit with 0 to indicate success for the step
            else
              echo "‚ùå Linting score $SCORE/10 is less than $MIN_SCORE. Linting failed."
              exit 1 # Explicitly exit with 1 to indicate failure for the step
            fi
          elif [ "$PLATFORM" = "cfn" ]; then
            echo "‚úÖ CloudFormation project detected, running CloudFormation validation..."
            if [ "$LANGUAGE" = "json" ]; then
              pipenv run cfn-validate-json -D
            elif [ "$LANGUAGE" = "yaml" ]; then
              pipenv run cfn-validate-yaml -D
            fi
          else
            echo "‚ÑπÔ∏è Unknown platform/language combination: $PLATFORM/$LANGUAGE"
            echo "üí° Running default ESLint only"
            npm run lint
          fi

  unit-tests:
    name: Unit Testing
    runs-on: ubuntu-24.04
    needs: lint
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    environment: qa

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'true'

      - name: Convert YAML to JSON for CloudFormation projects
        if: ${{ needs.lint.outputs.platform == 'cfn' && needs.lint.outputs.language == 'yaml' }}
        run: |
          echo "‚úÖ CloudFormation YAML project detected, converting YAML to JSON for unit tests..."
          pipenv run cfn-flip-to-json > lib/TapStack.json

      - name: Run unit tests
        run: |
          if [ "${{ needs.lint.outputs.language }}" = "ts" ]; then
            echo "‚úÖ TypeScript project detected, running unit tests..."
            npm run test:unit
          elif [ "${{ needs.lint.outputs.language }}" = "py" ]; then
            echo "‚úÖ Python project detected, running pytest unit tests..."
            pipenv run test-py-unit
          else
            echo "‚úÖ Running default unit tests..."
            npm run test:unit
          fi

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: |
            coverage/
            cov.json

  # mocked-integration-tests:
  #   name: Mocked Integration Tests
  #   runs-on: ubuntu-24.04
  #   needs: unit-tests

  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@v4

  #     - name: Setup Environment
  #       uses: ./.github/actions/setup-environment
  #       with:
  #         node-version: ${{ env.NODE_VERSION }}
  #         download-artifacts: "true"

  #     - name: Run mocked integration tests
  #       run: npm run test:integration
  #       env:
  #         # Mock environment variables for testing
  #         API_GATEWAY_ENDPOINT: "https://mock-api.example.com/prod"
  #         READ_ONLY_API_KEY: "mock-readonly-key"
  #         ADMIN_API_KEY: "mock-admin-key"

  integration-tests-live:
    name: Integration Tests (Live)
    runs-on: ubuntu-24.04
    needs: [lint, unit-tests, deploy]
    environment: qa
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'true'

      - name: Download Deployment Outputs
        uses: actions/download-artifact@v4
        with:
          name: cfn-outputs
          path: .

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Run integration tests against live environment
        run: |
          if [ "${{ needs.lint.outputs.language }}" = "py" ]; then
            echo "‚úÖ CDK project detected, running integration tests..."
            pipenv run test-py-integration
          else
            echo "‚úÖ Running default integration tests..."
            npm run test:integration
          fi
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
          CI: "1"           # Set CI for the entire step
          LOCAL_TESTING: "1" # Set LOCAL_TESTING for the entire step

  cleanup:
    name: Cleanup (Destroy Resources)
    runs-on: ubuntu-24.04
    needs: [detect-metadata, integration-tests-live]
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    environment: stage

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'false'

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Destroy Resources (cleanup any leftover resources)
        run: |
          PLATFORM="${{ needs.detect-metadata.outputs.platform }}"
          LANGUAGE="${{ needs.detect-metadata.outputs.language }}"

          if [ "$PLATFORM" = "cdk" ]; then
            echo "‚úÖ CDK project detected, running CDK destroy..."
            # Try to destroy any leftover resources from PRs, but don't fail if they don't exist
            npm run cdk:destroy || echo "No resources to destroy or destruction failed"
          elif [ "$PLATFORM" = "cfn" ]; then
            echo "‚úÖ CloudFormation project detected, running CloudFormation destroy..."
            # Try to destroy any leftover resources from PRs, but don't fail if they don't exist
            npm run cfn:destroy || echo "No resources to destroy or destruction failed"
          else
            echo "‚ÑπÔ∏è Platform '$PLATFORM' with language '$LANGUAGE' not supported for destruction, skipping destroy"
            echo "üí° Consider adding cleanup logic for $PLATFORM/$LANGUAGE projects here"
          fi
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}

  archive-folders:
    name: Archive Folders and Reset Repository
    runs-on: ubuntu-24.04
    needs: [detect-metadata, cleanup]
    environment: release
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    permissions:
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          repository: ${{ github.repository }}
          ref: ${{ github.head_ref }}

      - name: Download coverage reports
        uses: actions/download-artifact@v4
        with:
          name: coverage-reports
          path: coverage/

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Build Docker Image
        run: |
          # Extract po_id from metadata.json for Docker image tagging
          PO_ID=$(jq -r '.po_id // "pr${{ github.event.number }}"' metadata.json)
          echo "Building Docker image for po_id: $PO_ID"

          # Build Docker image with po_id tag
          docker build -t tap-app:${PO_ID} .

          # Export Docker image to tar file
          docker save tap-app:${PO_ID} -o docker-image-${PO_ID}.tar

          # Check file size
          ls -lh docker-image-${PO_ID}.tar
          echo "Docker image built and exported successfully with po_id: $PO_ID"

          # Use metadata from detect-metadata job
          PLATFORM="${{ needs.detect-metadata.outputs.platform }}"
          LANGUAGE="${{ needs.detect-metadata.outputs.language }}"

          # Upload Docker image directly to S3
          S3_PREFIX="docker_images/Pr${{ github.event.number }}"
          echo "Uploading Docker image to S3 bucket: ${{ env.S3_PRODUCTION_BUCKET_NAME }}"
          echo "S3 key: ${S3_PREFIX}/docker-image-${PO_ID}.tar"

          # Upload Docker image to S3
          aws s3 cp docker-image-${PO_ID}.tar \
            "s3://${{ env.S3_PRODUCTION_BUCKET_NAME }}/${S3_PREFIX}/docker-image-${PO_ID}.tar"

          echo "Docker image uploaded successfully"

          # Store the S3 location for reference
          S3_LOCATION="s3://${{ env.S3_PRODUCTION_BUCKET_NAME }}/${S3_PREFIX}/docker-image-${PO_ID}.tar"
          echo "Docker image S3 location: $S3_LOCATION"

          # Save the S3 location for the next step
          echo "DOCKER_S3_LOCATION=$S3_LOCATION" >> $GITHUB_ENV

          # Remove Docker image locally to save space
          rm -f docker-image-${PO_ID}.tar
          echo "Docker image removed locally to save space"

      - name: Archive folders and reset repository
        run: |
          # Configure Git
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"

          # Use metadata from detect-metadata job
          PLATFORM="${{ needs.detect-metadata.outputs.platform }}"
          LANGUAGE="${{ needs.detect-metadata.outputs.language }}"
          echo "Using platform: $PLATFORM, language: $LANGUAGE"

          # Update metadata.json with coverage information and commit author
          # Get commit author from GitHub context
          COMMIT_AUTHOR="${{ github.event.pull_request.user.login || github.actor }}"
          echo "Commit author: $COMMIT_AUTHOR"

          # Extract coverage percentages based on language
          if [ "$LANGUAGE" = "py" ]; then
            echo "Extracting Python coverage from cov.json..."
            if [ -f "cov.json" ]; then
              LINES_COVERAGE=$(jq -r '.totals.percent_covered' cov.json)
              # Calculate branch coverage percentage
              BRANCHES_NUM=$(jq -r '.totals.num_branches' cov.json)
              if [ "$BRANCHES_NUM" = "0" ] || [ "$BRANCHES_NUM" = "null" ]; then
                # No branches to cover, set to 100%
                BRANCHES_COVERAGE=100
              else
                COVERED_BRANCHES=$(jq -r '.totals.covered_branches' cov.json)
                # Calculate percentage: (covered_branches / num_branches) * 100
                BRANCHES_COVERAGE=$(awk "BEGIN {print ($COVERED_BRANCHES / $BRANCHES_NUM) * 100}")
              fi
            else
              echo "Warning: cov.json not found, setting default coverage values"
              LINES_COVERAGE=100
              BRANCHES_COVERAGE=100
            fi
          else
            echo "Extracting TypeScript/JavaScript coverage from coverage-summary.json..."
            if [ -f "coverage/coverage-summary.json" ]; then
              LINES_COVERAGE=$(jq -r '.total.lines.pct' coverage/coverage-summary.json)
              BRANCHES_COVERAGE=$(jq -r '.total.branches.pct' coverage/coverage-summary.json)
            else
              echo "Warning: coverage-summary.json not found, setting default coverage values"
              LINES_COVERAGE=100
              BRANCHES_COVERAGE=100
            fi
          fi

          # If coverage is "Unknown", set to 100
          if [ "$LINES_COVERAGE" = "Unknown" ]; then
            LINES_COVERAGE=100
          fi
          if [ "$BRANCHES_COVERAGE" = "Unknown" ]; then
            BRANCHES_COVERAGE=100
          fi

          echo "Lines coverage: $LINES_COVERAGE%"
          echo "Branches coverage: $BRANCHES_COVERAGE%"

          # Update metadata.json with coverage information and commit author (assuming it always exists)
          jq --arg lines "$LINES_COVERAGE" --arg branches "$BRANCHES_COVERAGE" --arg author "$COMMIT_AUTHOR" '. + {coverage: {lines: ($lines|tonumber), branches: ($branches|tonumber)}, author: $author}' metadata.json > metadata.json.tmp
          mv metadata.json.tmp metadata.json
          echo "Updated metadata.json with coverage information and commit author"

          cat metadata.json

          echo "Current directory contents before archiving:"
          ls -la

          # Get Docker S3 location from the previous step and add it to metadata.json
          DOCKER_S3_LOCATION="${DOCKER_S3_LOCATION}"
          echo "Docker S3 location from previous step: $DOCKER_S3_LOCATION"

          # Add the dockerS3Location to metadata.json
          jq --arg location "$DOCKER_S3_LOCATION" '. + {dockerS3Location: $location}' metadata.json > metadata.json.tmp
          mv metadata.json.tmp metadata.json
          echo "Updated metadata.json with dockerS3Location: $DOCKER_S3_LOCATION"

          # Create archive directory with platform, language, and PR number
          ARCHIVE_DIR="archive/${PLATFORM}-${LANGUAGE}/Pr${{ github.event.number }}"
          mkdir -p "$ARCHIVE_DIR"
          echo "Created archive directory: $ARCHIVE_DIR"

            # Define list of paths to move to archive
            PATHS_TO_ARCHIVE=(
            "lib"
            "bin" 
            "test"
            "tests"
            "cdk.json"
            "metadata.json"
            "tap.py"
            )

          # Move paths to archive
          for path in "${PATHS_TO_ARCHIVE[@]}"; do
          if [[ -d "$path" || -f "$path" ]]; then
            mv "$path" "$ARCHIVE_DIR"/
            echo "Moved $path to archive"
          else
            echo "Path $path not found, skipping"
          fi
          done

          # Check if there are changes to commit
          if git diff --quiet && git diff --cached --quiet; then
            echo "No changes to commit - no folders found to archive"
          else
            git add -A
            git commit -m "Archive build artifacts for PR ${{ github.event.number }} [skip-jobs]"
            git push origin HEAD
            echo "Archive committed to current branch"
          fi

  upload-task-to-s3:
    name: Upload Task to S3
    runs-on: ubuntu-24.04
    environment: release
    if: ${{ github.event_name == 'pull_request' && github.event.pull_request.merged == true }}
    permissions:
      contents: read

    steps:
      - name: Checkout merged code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          repository: ${{ github.repository }}
          ref: ${{ github.event.pull_request.merge_commit_sha }}

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Extract platform and language for S3 upload
        id: extract-metadata
        run: |
          # We need to find the specific archive folder for this PR
          # Since we don't know platform/language yet, we'll search for the PR-specific folder
          PR_FOLDER_PATTERN="archive/*/Pr${{ github.event.number }}"

          # Find the archive folder for this specific PR
          ARCHIVE_FOLDER=""
          for folder in $PR_FOLDER_PATTERN; do
            if [ -d "$folder" ]; then
              ARCHIVE_FOLDER="$folder"
              break
            fi
          done

          if [ -n "$ARCHIVE_FOLDER" ] && [ -f "$ARCHIVE_FOLDER/metadata.json" ]; then
            echo "Found archive folder: $ARCHIVE_FOLDER"
            echo "Found metadata.json at: $ARCHIVE_FOLDER/metadata.json"
            PLATFORM=$(jq -r '.platform // "unknown"' "$ARCHIVE_FOLDER/metadata.json")
            LANGUAGE=$(jq -r '.language // "unknown"' "$ARCHIVE_FOLDER/metadata.json")
          else
            echo "Warning: Could not find archive folder or metadata.json for PR ${{ github.event.number }}"
            echo "Available archive folders:"
            ls -la archive/ || echo "No archive directory found"
            PLATFORM="unknown"
            LANGUAGE="unknown"
          fi
          echo "platform=$PLATFORM" >> $GITHUB_OUTPUT
          echo "language=$LANGUAGE" >> $GITHUB_OUTPUT
          echo "Extracted platform: $PLATFORM, language: $LANGUAGE"

      - name: Upload archive folder to S3
        run: |
          PLATFORM="${{ steps.extract-metadata.outputs.platform }}"
          LANGUAGE="${{ steps.extract-metadata.outputs.language }}"
          S3_PREFIX="${PLATFORM}-${LANGUAGE}/Pr${{ github.event.number }}"

          echo "Uploading archive folder to S3 bucket: ${{ env.S3_RELEASE_BUCKET_NAME }}"
          echo "S3 prefix: $S3_PREFIX"

          # Check if archive folder exists
          if [ -d "archive/${S3_PREFIX}" ]; then
            # Upload the archive folder contents to S3, preserving folder structure
            aws s3 sync "archive/${S3_PREFIX}/" "s3://${{ env.S3_RELEASE_BUCKET_NAME }}/${S3_PREFIX}/" \
              --delete \
              --exclude "*.git*" \
              --exclude "node_modules/*"
            echo "Successfully uploaded archive to S3"
          else
            echo "Archive folder archive/${S3_PREFIX} not found, nothing to upload"
            exit 1
          fi
      - name: Verify S3 upload
        run: |
          PLATFORM="${{ steps.extract-metadata.outputs.platform }}"
          LANGUAGE="${{ steps.extract-metadata.outputs.language }}"
          S3_PREFIX="${PLATFORM}-${LANGUAGE}/Pr${{ github.event.number }}"

          echo "Verifying S3 upload for prefix: $S3_PREFIX"
          aws s3 ls "s3://${{ env.S3_RELEASE_BUCKET_NAME }}/${S3_PREFIX}/" --recursive || echo "No files found or access denied"

  cleanup-pr:
    name: Cleanup (PR Closed)
    runs-on: ubuntu-24.04
    # This job should run regardless of the [skip-jobs] tag since it's for cleanup
    if: github.event_name == 'pull_request' && github.event.action == 'closed' && !github.event.pull_request.merged
    environment: qa

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'false'

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Destroy Resources (if resources exist)
        run: |
          # Read metadata directly since this job runs independently
          if [ ! -f "metadata.json" ]; then
            echo "‚ö†Ô∏è metadata.json not found, skipping resource destruction"
            exit 0
          fi

          PLATFORM=$(jq -r '.platform // "unknown"' metadata.json)
          LANGUAGE=$(jq -r '.language // "unknown"' metadata.json)

          if [ "$PLATFORM" = "cdk" ]; then
            echo "‚úÖ CDK TypeScript project detected, running CDK destroy..."
            # Try to destroy resources, but don't fail if they don't exist
            npm run cdk:destroy || echo "No resources to destroy or destruction failed"
          elif [ "$PLATFORM" = "cfn" ]; then
            echo "‚úÖ CloudFormation project detected, running CloudFormation destroy..."
            # Try to destroy resources, but don't fail if they don't exist
            npm run cfn:destroy || echo "No resources to destroy or destruction failed"
          else
            echo "‚ÑπÔ∏è Platform '$PLATFORM' not supported for destruction, skipping destroy"
            echo "üí° Consider adding cleanup logic for $PLATFORM projects here"
          fi
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
