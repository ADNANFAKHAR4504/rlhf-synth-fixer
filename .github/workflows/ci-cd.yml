name: CI/CD Pipeline

# Special keyword [skip-jobs] can be added to commit messages to skip most jobs
# The upload-task-to-s3 job will still run when a PR is merged, regardless of this tag
# The cleanup-pr job will still run when a PR is closed, regardless of this tag
on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
    types: [opened, synchronize, reopened, closed]
  workflow_dispatch:
    inputs:
      environment_suffix:
        description: "Environment suffix for resource naming (e.g., pr123, dev, test)"
        required: false
        default: "manual"
        type: string
      skip_cleanup:
        description: "Skip cleanup/destroy step after deployment"
        required: false
        default: false
        type: boolean

permissions:
  contents: read

env:
  NODE_VERSION: "22.17.0"
  GO_VERSION: "1.23.12"
  # Use PR number for resource isolation, manual input, or fallback to 'dev' for main branch
  # Prefix with 'pr' to ensure AWS resource names start with a letter
  ENVIRONMENT_SUFFIX: ${{ github.event.inputs.environment_suffix || (github.event.number && format('pr{0}', github.event.number)) || 'dev' }}
  S3_RELEASE_BUCKET_NAME: "iac-rlhf-aws-release-342597974367-us-east-1"
  TERRAFORM_STATE_BUCKET: "iac-rlhf-tf-states-342597974367"
  TERRAFORM_STATE_BUCKET_REGION: "us-east-1"
  TERRAFORM_STATE_BUCKET_KEY: ${{ github.event.pull_request.number }}
  PULUMI_STATE_BUCKET: "iac-rlhf-pulumi-states-342597974367"
  PULUMI_BUCKET_REGION: "us-east-1"
  PULUMI_CONFIG_PASSPHRASE: ${{ secrets.PULUMI_CONFIG_PASSPHRASE }}
  PULUMI_ORG: "organization"
  AWS_REGION: ${{ vars.AWS_REGION }}
  CDK_DEFAULT_REGION: ${{ vars.CDK_DEFAULT_REGION || vars.AWS_REGION || 'us-east-1' }}
  CURRENT_ACCOUNT_ID: ${{ vars.ACCOUNT_ID }}
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  MODEL_S3_URI: "s3://social-platform-sagemaker-342597974367-dev/model.tar.gz"
  # Go caching optimization
  GOCACHE: ${{ github.workspace }}/.cache/go-build
  GOMODCACHE: ${{ github.workspace }}/.cache/go-mod
  IAC_ANALYSIS_SUBJECT_LABEL: 'Infrastructure Analysis/Monitoring'
  CICD_PIPELINE_SUBJECT_LABEL: 'CI/CD Pipeline'

jobs:
  detect-metadata:
    name: Detect Project Files
    runs-on: ubuntu-24.04
    if: ${{ (github.event_name == 'pull_request' && github.event.action != 'closed' || github.event_name == 'workflow_dispatch') && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    outputs:
      platform: ${{ steps.metadata.outputs.platform }}
      language: ${{ steps.metadata.outputs.language }}
      po_id: ${{ steps.metadata.outputs.po_id }}
      team: ${{ steps.metadata.outputs.team }}
      subtask: ${{ steps.metadata.outputs.subtask }}
      subject_labels: ${{ steps.metadata.outputs.subject_labels }}
      analysis_subject_label: ${{ env.IAC_ANALYSIS_SUBJECT_LABEL }}
      cicd_pipeline_subject_label: ${{ env.CICD_PIPELINE_SUBJECT_LABEL }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Fetch base branch
        run: git fetch origin main:main

      # - name: Check Project Files
      #   id: check-files
      #   run: ./scripts/check-project-files.sh

      - name: Detect metadata and validate project
        id: metadata
        run: ./scripts/detect-metadata.sh

  validate-commit-message:
    name: Validate Commit Message
    runs-on: ubuntu-24.04
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Required to fetch all history for commitlint

      - name: Setup Node for Commitlint
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install Commitlint locally
        run: |
          npm install --no-save \
            @commitlint/{cli,config-conventional}

      - name: Validate commit message
        run: npx commitlint --last

  build:
    name: Build
    runs-on: ubuntu-24.04
    needs: detect-metadata
    if: ${{ (github.event_name == 'pull_request' && github.event.action != 'closed' || github.event_name == 'workflow_dispatch') && !contains(github.event.head_commit.message, '[skip-jobs]') && !contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.cicd_pipeline_subject_label)}}
    environment: dev

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: "false"
          upload-artifacts: "true"
          platform: ${{ needs.detect-metadata.outputs.platform }}
          language: ${{ needs.detect-metadata.outputs.language }}

      - name: Build
        run: ./scripts/build.sh

      - name: Upload build artifacts (dist/)
        uses: actions/upload-artifact@v4
        with:
          name: build-dist
          path: dist
          if-no-files-found: ignore

  synth:
    name: Synth
    runs-on: ubuntu-24.04
    needs: [detect-metadata, build]
    if: ${{ (github.event_name == 'pull_request' && github.event.action != 'closed' || github.event_name == 'workflow_dispatch') && !contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.analysis_subject_label) && !contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.cicd_pipeline_subject_label) }}
    environment: dev
    permissions:
      contents: read
    outputs:
      platform: ${{ needs.detect-metadata.outputs.platform }}
      language: ${{ needs.detect-metadata.outputs.language }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Skip Synth for non-CDK/CDKTF platforms
        if: ${{ needs.detect-metadata.outputs.platform != 'cdk' && needs.detect-metadata.outputs.platform != 'cdktf' }}
        run: |
          echo "âœ… Synth step not required for platform: ${{ needs.detect-metadata.outputs.platform }}"
          echo "This platform does not require synthesis. Job passes automatically."

      - name: Setup Environment
        if: ${{ needs.detect-metadata.outputs.platform == 'cdk' || needs.detect-metadata.outputs.platform == 'cdktf' }}
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: "false"
          platform: ${{ needs.detect-metadata.outputs.platform }}
          language: ${{ needs.detect-metadata.outputs.language }}

      - name: Configure AWS
        if: ${{ needs.detect-metadata.outputs.platform == 'cdk' || needs.detect-metadata.outputs.platform == 'cdktf' }}
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Synth
        if: ${{ needs.detect-metadata.outputs.platform == 'cdk' || needs.detect-metadata.outputs.platform == 'cdktf' }}
        run: ./scripts/synth.sh
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
          TERRAFORM_STATE_BUCKET: ${{ env.TERRAFORM_STATE_BUCKET }}
          TERRAFORM_STATE_BUCKET_REGION: ${{ env.TERRAFORM_STATE_BUCKET_REGION }}
          PULUMI_BACKEND_URL: s3://${{ env.PULUMI_STATE_BUCKET }}?region=${{ env.PULUMI_BUCKET_REGION }}
          PULUMI_ORG: ${{ env.PULUMI_ORG }}

  infracost:
    name: Infracost (Terraform Cost Estimation)
    needs: [detect-metadata, build, synth]
    runs-on: ubuntu-24.04
    # if: >
    #   ${{
    #   github.event_name == 'pull_request' &&
    #   github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') &&
    #   needs.detect-metadata.outputs.platform == 'tf'}}
    if: false
    environment: dev
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check for Terraform files
        id: tf-check
        run: |
          if ! find lib -maxdepth 1 -name "*.tf" | grep -q .; then
            echo "No Terraform files found in lib/. Skipping Infracost job."
            echo "skip_infracost=true" >> $GITHUB_ENV
            exit 0
          fi

      - name: Setup Terraform
        if: env.skip_infracost != 'true'
        uses: hashicorp/setup-terraform@v3

      - name: Setup Infracost
        if: env.skip_infracost != 'true'
        uses: infracost/actions/setup@v3
        with:
          api-key: ${{ secrets.INFRACOST_API_KEY }}

      - name: Setup Environment (variables)
        if: env.skip_infracost != 'true'
        run: |
          echo "TERRAFORM_STATE_BUCKET=${{ env.TERRAFORM_STATE_BUCKET }}" >> $GITHUB_ENV
          echo "TERRAFORM_STATE_BUCKET_REGION=${{ env.TERRAFORM_STATE_BUCKET_REGION }}" >> $GITHUB_ENV
          echo "ENVIRONMENT_SUFFIX=${{ env.ENVIRONMENT_SUFFIX }}" >> $GITHUB_ENV

      - name: Terraform Init
        if: env.skip_infracost != 'true'
        working-directory: lib
        run: terraform init

      - name: Terraform Plan
        if: env.skip_infracost != 'true'
        working-directory: lib
        run: terraform plan -out=tfplan
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Convert plan to JSON
        if: env.skip_infracost != 'true'
        working-directory: lib
        run: terraform show -json tfplan > plan.json
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Run Infracost breakdown
        if: env.skip_infracost != 'true'
        working-directory: lib
        run: infracost breakdown --path=plan.json --format=json --out-file=../infracost.json
        env:
          INFRACOST_API_KEY: ${{ secrets.INFRACOST_API_KEY }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Post Infracost comment
        if: env.skip_infracost != 'true'
        working-directory: lib
        run: |
          infracost comment github --path=../infracost.json \
                                 --pull-request=${{ github.event.pull_request.number }} \
                                 --repo=${{ github.repository }} \
                                 --github-token=${{ secrets.GITHUB_TOKEN }} \
                                 --behavior=update
        env:
          INFRACOST_API_KEY: ${{ secrets.INFRACOST_API_KEY }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ env.AWS_REGION || 'us-east-1' }}
  analysis:
    name: Analysis
    runs-on: ubuntu-24.04
    needs: [detect-metadata, lint, unit-tests]
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') && contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.analysis_subject_label) && needs.lint.result == 'success' && needs.unit-tests.result == 'success' }}
    environment: dev
    env:
      AWS_ENDPOINT_URL: http://127.0.0.1:5001
      AWS_ACCESS_KEY_ID: test
      AWS_SECRET_ACCESS_KEY: test
      AWS_DEFAULT_REGION: us-east-1
    permissions:
      contents: write
      pull-requests: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          ref: ${{ github.head_ref }}

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'false'
          is-analysis: 'true'

      - name: Run analysis
        run: ./scripts/analysis.sh
        env:
          IAC_ANALYSIS_SUBJECT_LABEL: ${{ needs.detect-metadata.outputs.analysis_subject_label }}
          GITHUB_HEAD_REF: ${{ github.head_ref }}

      - name: Upload analysis results as artifact
        uses: actions/upload-artifact@v4
        with:
          name: analysis-results
          path: lib/analysis-results.txt
          if-no-files-found: warn

  cicd-pipeline-optimization:
    name: CICD Pipeline Optimization
    runs-on: ubuntu-24.04
    needs: detect-metadata
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') && contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.cicd_pipeline_subject_label) }}
    environment: dev
    permissions:
      contents: write
      pull-requests: write
      actions: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          ref: ${{ github.head_ref }}

      - name: Execute CI/CD Pipeline Script
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_HEAD_REF: ${{ github.head_ref }}
        run: ./scripts/cicd-pipeline.sh

      - name: Upload Pipeline Configuration
        uses: actions/upload-artifact@v4
        with:
          name: cicd-pipeline-config
          path: lib/ci-cd.yml
          if-no-files-found: error

  deploy:
    name: Deploy
    runs-on: ubuntu-24.04
    needs: [detect-metadata, synth]
    if: ${{ (github.event_name == 'pull_request' && github.event.action != 'closed' || github.event_name == 'workflow_dispatch') && !contains(github.event.head_commit.message, '[skip-jobs]') && !contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.analysis_subject_label) && !contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.cicd_pipeline_subject_label)}}
    environment: dev

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: "true"
          platform: ${{ needs.detect-metadata.outputs.platform }}
          language: ${{ needs.detect-metadata.outputs.language }}

      - name: Download build artifacts (dist/)
        if: ${{ needs.detect-metadata.outputs.language == 'ts' || needs.detect-metadata.outputs.language == 'js' }}
        uses: actions/download-artifact@v4
        with:
          name: build-dist
          path: dist
      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      # Terraform State Management Setup
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3

      # (Removed DynamoDB lock table step per PR review)

      - name: Bootstrap
        run: ./scripts/bootstrap.sh
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
          REPOSITORY: ${{ github.repository }}
          COMMIT_AUTHOR: ${{ github.event.head_commit.author.name || github.actor }}
          PULUMI_BACKEND_URL: s3://${{ env.PULUMI_STATE_BUCKET }}?region=${{ env.PULUMI_BUCKET_REGION }}
          PULUMI_ORG: ${{ env.PULUMI_ORG }}

      - name: Deploy
        run: ./scripts/deploy.sh
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
          REPOSITORY: ${{ github.repository }}
          COMMIT_AUTHOR: ${{ github.event.head_commit.author.name || github.actor }}
          PR_NUMBER: ${{ github.event.number || 'unknown' }}
          TEAM: ${{ needs.detect-metadata.outputs.team }}
          TERRAFORM_STATE_BUCKET: ${{ env.TERRAFORM_STATE_BUCKET }}
          TERRAFORM_STATE_BUCKET_REGION: ${{ env.TERRAFORM_STATE_BUCKET_REGION }}
          TERRAFORM_STATE_BUCKET_KEY: ${{ env.ENVIRONMENT_SUFFIX }}
          PULUMI_BACKEND_URL: s3://${{ env.PULUMI_STATE_BUCKET }}?region=${{ env.PULUMI_BUCKET_REGION }}
          PULUMI_CONFIG_PASSPHRASE: ${{ secrets.PULUMI_CONFIG_PASSPHRASE }}
          PULUMI_ORG: ${{ env.PULUMI_ORG }}
          CURRENT_ACCOUNT_ID: ${{ env.CURRENT_ACCOUNT_ID }}

      # Note: Deployment outputs are now collected by the deploy.sh script

      - name: Upload Deployment Outputs
        uses: actions/upload-artifact@v4
        with:
          name: cfn-outputs
          path: |
            cfn-outputs/
            cdk-stacks.json

  iac-optimization:
    name: IaC Optimization
    runs-on: ubuntu-24.04
    needs: [detect-metadata, deploy]
    if: ${{ (github.event_name == 'pull_request' && github.event.action != 'closed' || github.event_name == 'workflow_dispatch') && !contains(github.event.head_commit.message, '[skip-jobs]') && contains(needs.detect-metadata.outputs.subject_labels, 'IaC Optimization')}}
    environment: dev

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'true'
          platform: ${{ needs.detect-metadata.outputs.platform }}
          language: ${{ needs.detect-metadata.outputs.language }}

      - name: Download Deployment Outputs
        uses: actions/download-artifact@v4
        with:
          name: cfn-outputs
          path: .

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Check and Run IaC Optimization
        run: ./scripts/optimize.sh
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
          AWS_REGION: ${{ env.AWS_REGION || 'us-east-1' }}

  lint:
    name: Lint
    runs-on: ubuntu-24.04
    needs: [detect-metadata, build]
    if: ${{ (github.event_name == 'pull_request' && github.event.action != 'closed' || github.event_name == 'workflow_dispatch') && !contains(github.event.head_commit.message, '[skip-jobs]') && !contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.cicd_pipeline_subject_label)}}
    environment: qa
    outputs:
      platform: ${{ needs.detect-metadata.outputs.platform }}
      language: ${{ needs.detect-metadata.outputs.language }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: "true"
          platform: ${{ needs.detect-metadata.outputs.platform }}
          language: ${{ needs.detect-metadata.outputs.language }}

      # Python setup is handled by the setup-environment action

      - name: Run Linting
        run: ./scripts/lint.sh

  unit-tests:
    name: Unit Testing
    runs-on: ubuntu-24.04
    needs: [detect-metadata, lint]
    if: ${{ (github.event_name == 'pull_request' && github.event.action != 'closed' || github.event_name == 'workflow_dispatch') && !contains(github.event.head_commit.message, '[skip-jobs]') && !contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.cicd_pipeline_subject_label) }}
    environment: qa

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: "true"
          platform: ${{ needs.detect-metadata.outputs.platform }}
          language: ${{ needs.detect-metadata.outputs.language }}

      - name: Run unit tests
        run: ./scripts/unit-tests.sh

      - name: Prepare coverage reports for Java projects
        if: ${{ needs.detect-metadata.outputs.language == 'java' }}
        run: |
          echo "Preparing Java coverage reports for upload..."
          echo "Debug: Checking current directory structure:"
          ls -la

          echo "Debug: Checking build directory:"
          if [ -d "build" ]; then
            ls -la build/
            if [ -d "build/reports" ]; then
              echo "Reports directory contents:"
              ls -la build/reports/
              if [ -d "build/reports/jacoco" ]; then
                echo "JaCoCo directory contents:"
                find build/reports/jacoco -type f | head -10
              fi
            fi
          else
            echo "No build directory found"
          fi

          mkdir -p coverage

          # Try different possible locations for JaCoCo reports
          if [ -d "build/reports/jacoco/test/" ]; then
            cp -r build/reports/jacoco/test/* coverage/
            echo "âœ… Copied JaCoCo reports from build/reports/jacoco/test/ to coverage/ directory"
          elif [ -d "build/reports/jacoco/" ]; then
            cp -r build/reports/jacoco/* coverage/
            echo "âœ… Copied JaCoCo reports from build/reports/jacoco/ to coverage/ directory"
          else
            echo "âš ï¸ JaCoCo reports not found in expected locations"
            # Create a placeholder to avoid upload failure
            echo "No coverage reports generated" > coverage/no-reports.txt
          fi

          echo "Final coverage directory contents:"
          ls -la coverage/
      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: |
            coverage/
            cov.json

  # mocked-integration-tests:
  #   name: Mocked Integration Tests
  #   runs-on: ubuntu-24.04
  #   needs: unit-tests

  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@v4

  #     - name: Setup Environment
  #       uses: ./.github/actions/setup-environment
  #       with:
  #         node-version: ${{ env.NODE_VERSION }}
  #         download-artifacts: "true"

  #     - name: Run mocked integration tests
  #       run: npm run test:integration
  #       env:
  #         # Mock environment variables for testing
  #         API_GATEWAY_ENDPOINT: "https://mock-api.example.com/prod"
  #         READ_ONLY_API_KEY: "mock-readonly-key"
  #         ADMIN_API_KEY: "mock-admin-key"

  integration-tests-live:
    name: Integration Tests (Live)
    runs-on: ubuntu-24.04
    needs: [detect-metadata, lint, unit-tests, deploy]
    environment: qa
    if: ${{ (github.event_name == 'pull_request' && github.event.action != 'closed' || github.event_name == 'workflow_dispatch') && !contains(github.event.head_commit.message, '[skip-jobs]') && !contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.analysis_subject_label) && !contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.cicd_pipeline_subject_label)}}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: "true"
          platform: ${{ needs.detect-metadata.outputs.platform }}
          language: ${{ needs.detect-metadata.outputs.language }}

      - name: Download Deployment Outputs
        uses: actions/download-artifact@v4
        with:
          name: cfn-outputs
          path: .

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Run integration tests against live environment
        run: ./scripts/integration-tests.sh
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ env.AWS_REGION || 'us-east-1' }}
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
          CI: "1" # Set CI for the entire step
          PULUMI_BACKEND_URL: s3://${{ env.PULUMI_STATE_BUCKET }}?region=${{ env.PULUMI_BUCKET_REGION }}
          PULUMI_CONFIG_PASSPHRASE: ${{ secrets.PULUMI_CONFIG_PASSPHRASE }}
          PULUMI_ORG: ${{ env.PULUMI_ORG }}

  claude-code-action:
    runs-on: ubuntu-24.04
    name: "Claude Review"
    needs:
      [
        detect-metadata,
        integration-tests-live,
        analysis,
        cicd-pipeline-optimization,
      ]
    # For Infrastructure Analysis/Monitoring OR CI/CD Pipeline: Allow int-tests to be skipped
    # For all other subject labels: Require int-tests to succeed
    if: |
      always() && !cancelled() &&
      (needs.analysis.result == 'success' || needs.analysis.result == 'skipped') &&
      (needs.cicd-pipeline-optimization.result == 'success' || needs.cicd-pipeline-optimization.result == 'skipped') &&
      (
        (
          (contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.analysis_subject_label) ||
           contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.cicd_pipeline_subject_label)) &&
          (needs.integration-tests-live.result == 'success' || needs.integration-tests-live.result == 'skipped')
        ) ||
        (
          !contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.analysis_subject_label) &&
          !contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.cicd_pipeline_subject_label) &&
          needs.integration-tests-live.result == 'success'
        )
      )
    permissions:
      contents: write
      pull-requests: write
      issues: write
      id-token: write
      actions: read
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: refs/pull/${{ github.event.pull_request.number }}/head

      - name: Check for required documentation files
        id: docs-check
        run: |
          if [ ! -f "lib/PROMPT.md" ]; then
            echo "âŒ lib/PROMPT.md not found, exiting with failure"
            exit 1
          fi

          if [ ! -f "lib/MODEL_RESPONSE.md" ]; then
            echo "âŒ lib/MODEL_RESPONSE.md not found, exiting with failure"
            exit 1
          fi

          if [ ! -f "lib/IDEAL_RESPONSE.md" ]; then
            echo "âŒ lib/IDEAL_RESPONSE.md not found, exiting with failure"
            exit 1
          fi

          if [ ! -f "lib/MODEL_FAILURES.md" ]; then
            echo "âŒ lib/MODEL_FAILURES.md not found, exiting with failure"
            exit 1
          fi

      - name: Verify IaC Program Optimization script exists
        if: ${{ contains(needs.detect-metadata.outputs.subject_labels, 'Infrastructure Analysis/Monitoring') }}
        run: |
          if [ ! -f "lib/analyse.py" ] && [ ! -f "lib/analyse.sh" ]; then
            echo "âŒ Expected lib/analyse.py or lib/analyse.sh for IaC Program Optimization task"
            exit 1
          fi

      - name: Determine review type
        id: review-type
        run: |
          SUBJECT_LABELS=$(jq -r '.subject_labels[]?' metadata.json 2>/dev/null || echo "")
          if echo "$SUBJECT_LABELS" | grep -q "CI/CD Pipeline"; then
            echo "review_type=cicd-pipeline" >> $GITHUB_OUTPUT
            echo "ðŸ“‹ Detected CI/CD Pipeline subject label - will use specialized review criteria"
          else
            echo "review_type=iac-standard" >> $GITHUB_OUTPUT
            echo "ðŸ“‹ Using standard IaC review criteria"
          fi

      - name: Run Claude PR Review Action
        uses: anthropics/claude-code-action@v1
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          prompt: "Review this pull request's infrastructure code changes, validate file formats and compliance, and provide a comprehensive code review. You MUST post your review findings as a comment on this pull request when complete."
          claude_args: |
            --allowedTools "Bash,Read,Write,Edit,MultiEdit,Glob,Grep,LS,Task,TodoWrite"
            --system-prompt "CRITICAL: You MUST post a GitHub comment on this pull request when your review is complete. This is mandatory and non-negotiable.

            # Step 0: Comprehensive metadata.json Validation (MUST BE FIRST)

            **MANDATORY**: Before proceeding with any review, you MUST thoroughly validate metadata.json and report ALL issues in your PR comment.

            **CRITICAL**: If ANY critical metadata issues are found (missing required fields, invalid values, type mismatches), you MUST:
            1. Post a PR comment with detailed validation results and fix instructions
            2. Exit with code 1 to fail the job
            3. Do NOT continue with code review until metadata is fixed

            ## 0.1: Read and Validate metadata.json Structure

            ```bash
            # Track if critical issues are found
            METADATA_ERRORS=0
            
            # Read metadata.json
            if [ ! -f "metadata.json" ]; then
              echo "âŒ CRITICAL: metadata.json not found"
              METADATA_ERRORS=1
            fi

            # Validate JSON structure
            if [ -f "metadata.json" ] && ! jq empty metadata.json 2>/dev/null; then
              echo "âŒ CRITICAL: metadata.json is not valid JSON"
              METADATA_ERRORS=1
            fi
            ```

            ## 0.2: Validate Required Fields

            Check for ALL required fields and report missing ones:

            **Required Fields**:
            - `platform` (must be: cdk, cdktf, cfn, tf, pulumi, cicd)
            - `language` (must be: ts, py, js, go, java, hcl, yaml, json, yml)
            - `complexity` (must be: medium, hard, expert)
            - `turn_type` (must be: single, multi)
            - `po_id` (non-empty string)
            - `team` (non-empty string, typically 1-6, synth, synth-N, stf)
            - `startedAt` (ISO 8601 timestamp)
            - `subtask` (must match valid subtask from reference file)
            - `subject_labels` (must be array, non-empty, must match valid labels for subtask)

            **For Standard IaC Tasks** (platform != \"cicd\"):
            - `training_quality` (must be present after review, 0-10)
            - `aws_services` (must be array, required for standard IaC)

            **For CI/CD Pipeline Tasks** (platform == \"cicd\" OR subject_labels contains \"CI/CD Pipeline\"):
            - `training_quality` (must be present after review, 0-10)
            - `aws_services` (NOT required for CI/CD tasks)

            **Validation Script**:
            ```bash
            # Validate required fields exist
            REQUIRED_FIELDS=("platform" "language" "complexity" "turn_type" "po_id" "team" "startedAt" "subtask" "subject_labels")
            for field in "${REQUIRED_FIELDS[@]}"; do
              if ! jq -e ".$field" metadata.json > /dev/null 2>&1; then
                echo "âŒ CRITICAL: Missing required field: $field"
                METADATA_ERRORS=1
              fi
            done

            # Validate field values
            PLATFORM=$(jq -r '.platform // empty' metadata.json)
            if [ -n "$PLATFORM" ] && [[ ! "$PLATFORM" =~ ^(cdk|cdktf|cfn|tf|pulumi|cicd)$ ]]; then
              echo "âŒ CRITICAL: Invalid platform: '$PLATFORM' (must be: cdk, cdktf, cfn, tf, pulumi, cicd)"
              METADATA_ERRORS=1
            fi

            LANGUAGE=$(jq -r '.language // empty' metadata.json)
            if [ -n "$LANGUAGE" ] && [[ ! "$LANGUAGE" =~ ^(ts|py|js|go|java|hcl|yaml|json|yml)$ ]]; then
              echo "âŒ CRITICAL: Invalid language: '$LANGUAGE' (must be: ts, py, js, go, java, hcl, yaml, json, yml)"
              METADATA_ERRORS=1
            fi

            COMPLEXITY=$(jq -r '.complexity // empty' metadata.json)
            if [ -n "$COMPLEXITY" ] && [[ ! "$COMPLEXITY" =~ ^(medium|hard|expert)$ ]]; then
              echo "âŒ CRITICAL: Invalid complexity: '$COMPLEXITY' (must be: medium, hard, expert)"
              METADATA_ERRORS=1
            fi

            TURN_TYPE=$(jq -r '.turn_type // empty' metadata.json)
            if [ -n "$TURN_TYPE" ] && [[ ! "$TURN_TYPE" =~ ^(single|multi)$ ]]; then
              echo "âŒ CRITICAL: Invalid turn_type: '$TURN_TYPE' (must be: single, multi)"
              METADATA_ERRORS=1
            fi

            # Validate aws_services requirement based on platform
            PLATFORM=$(jq -r '.platform // empty' metadata.json)
            SUBJECT_LABELS=$(jq -r '.subject_labels // []' metadata.json)
            IS_CICD=false
            if [ "$PLATFORM" == "cicd" ] || echo "$SUBJECT_LABELS" | jq -e '.[] | select(. == "CI/CD Pipeline")' > /dev/null 2>&1; then
              IS_CICD=true
            fi

            if [ "$IS_CICD" == "false" ]; then
              if ! jq -e '.aws_services' metadata.json > /dev/null 2>&1; then
                echo "âŒ CRITICAL: aws_services is required for standard IaC tasks (platform: $PLATFORM)"
                METADATA_ERRORS=1
              fi
            fi
            ```

            ## 0.3: Validate Subtask Against Reference File

            **CRITICAL**: Validate subtask and subject_labels against `.claude/docs/references/iac-subtasks-subject-labels.json`

            ```bash
            # Read reference file
            REFERENCE_FILE=\".claude/docs/references/iac-subtasks-subject-labels.json\"
            if [ ! -f \"$REFERENCE_FILE\" ]; then
              echo \"âš ï¸ WARNING: Reference file not found: $REFERENCE_FILE\"
            else
              # Extract subtask from metadata.json
              METADATA_SUBTASK=$(jq -r '.subtask // empty' metadata.json)
              
              # Extract subject_labels from metadata.json
              METADATA_SUBJECT_LABELS=$(jq -r '.subject_labels // []' metadata.json)
              
              # Validate subtask exists in reference
              SUBTASK_EXISTS=$(jq -r --arg subtask \"$METADATA_SUBTASK\" \"
                .iac_subtasks_and_subject_labels[] | 
                select(.subtask == \\$subtask) | .subtask
              \" \"$REFERENCE_FILE\")
              
              if [ -z \"$SUBTASK_EXISTS\" ]; then
                echo \"âŒ CRITICAL: Invalid subtask: $METADATA_SUBTASK\"
                echo \"Valid subtasks from reference:\"
                jq -r '.iac_subtasks_and_subject_labels[].subtask' \"$REFERENCE_FILE\"
                METADATA_ERRORS=1
              fi
              
              # Validate subject_labels match subtask
              if [ -n \"$SUBTASK_EXISTS\" ]; then
                VALID_LABELS=$(jq -r --arg subtask \"$METADATA_SUBTASK\" \"
                  .iac_subtasks_and_subject_labels[] | 
                  select(.subtask == \\$subtask) | 
                  .subject_labels[]
                \" \"$REFERENCE_FILE\")
                
                # Check each subject_label
                echo \"$METADATA_SUBJECT_LABELS\" | jq -r '.[]' | while read -r label; do
                  if ! echo \"$VALID_LABELS\" | grep -q \"^$label$\"; then
                    echo \"âŒ CRITICAL: Invalid subject_label '$label' for subtask '$METADATA_SUBTASK'\"
                    echo \"Valid labels for this subtask:\"
                    echo \"$VALID_LABELS\"
                    METADATA_ERRORS=1
                  fi
                done
              fi
            fi
            ```

            ## 0.4: Validate Field Types

            Check that arrays are actually arrays (not strings):

            ```bash
            # Check subject_labels is array
            SUBJECT_LABELS_TYPE=$(jq -r '.subject_labels | type' metadata.json)
            if [ \"$SUBJECT_LABELS_TYPE\" != \"array\" ]; then
              echo \"âŒ CRITICAL: subject_labels must be an array, got: $SUBJECT_LABELS_TYPE\"
              METADATA_ERRORS=1
            fi

            # Check aws_services is array (if present)
            if jq -e '.aws_services' metadata.json > /dev/null 2>&1; then
              AWS_SERVICES_TYPE=$(jq -r '.aws_services | type' metadata.json)
              if [ \"$AWS_SERVICES_TYPE\" != \"array\" ]; then
                echo \"âŒ CRITICAL: aws_services must be an array, got: $AWS_SERVICES_TYPE\"
                METADATA_ERRORS=1
              fi
            fi

            # After all validation, check if we should fail
            # Note: Since bash blocks are separate, Claude must track METADATA_ERRORS across all validation steps
            # If any validation step found errors (METADATA_ERRORS=1), Claude MUST post PR comment and exit with code 1
            ```

            ## 0.5: Report metadata.json Issues in PR Comment

            **MANDATORY**: Create a dedicated section in your PR comment for metadata.json validation:

            ```markdown
            ## ðŸ“‹ metadata.json Validation

            ### âœ… Valid Fields
            - platform: {value}
            - language: {value}
            - ... (list all valid fields)

            ### âŒ Critical Issues Found (MUST FIX)
            
            The following metadata issues **MUST be fixed** before this PR can be merged:

            1. **Missing Required Field**: {field_name} is required but not found
               - **Fix**: Add the missing field to metadata.json
               - **Example**: `"field_name": "value"`

            2. **Invalid Subtask**: Found \"{invalid_subtask}\" but valid values are: {list}
               - **Fix**: Update the subtask field to one of the valid values from `.claude/docs/references/iac-subtasks-subject-labels.json`
               - **Example**: `"subtask": "Provisioning of Infrastructure Environments"`

            3. **Invalid Subject Label**: \"{invalid_label}\" is not valid for subtask \"{subtask}\". Valid labels: {list}
               - **Fix**: Update subject_labels array to only include valid labels for the subtask
               - **Example**: `"subject_labels": ["Cloud Environment Setup"]`

            4. **Type Mismatch**: {field_name} should be {expected_type} but got {actual_type}
               - **Fix**: Ensure the field is the correct type (array vs string, etc.)
               - **Example**: `"subject_labels": ["label1", "label2"]` (array) not `"subject_labels": "label1"` (string)

            5. **Invalid Field Value**: {field_name} has invalid value \"{value}\". Valid values: {list}
               - **Fix**: Update the field to one of the valid values
               - **Example**: `"platform": "cdk"` (valid: cdk, cdktf, cfn, tf, pulumi, cicd)

            6. **Missing Training Quality**: training_quality field not found (will be added after review)
               - **Note**: This is expected before review completion and is not a blocking issue

            ### ðŸ“ How to Fix Metadata Issues

            1. **Edit metadata.json** in the root of your repository
            2. **Validate locally** using: `bash .claude/scripts/validate-metadata.sh metadata.json`
            3. **Reference** `.claude/docs/references/metadata-requirements.md` for detailed field requirements
            4. **Reference** `.claude/docs/references/iac-subtasks-subject-labels.json` for valid subtask and subject_label combinations
            5. **Commit and push** the fixed metadata.json
            6. **Re-run** the CI/CD pipeline to verify fixes

            ### âš ï¸ Warnings (Non-blocking)
            - {warning_message}
            ```

            **CRITICAL**: After completing ALL validation steps (0.1 through 0.4), if ANY critical issues were found:
            
            1. **First**: Post the PR comment with the validation results and detailed fix instructions (as shown above)
            2. **Then**: Execute `exit 1` to fail the job immediately
            3. **Do NOT proceed** with code review until metadata is fixed
            4. The job will fail and the PR cannot be merged until metadata issues are resolved
            
            **Important**: You must track METADATA_ERRORS across all validation bash blocks. If any validation step set METADATA_ERRORS=1 or reported CRITICAL errors, you MUST post the PR comment and exit with code 1 before proceeding to any code review.

            ## 0.6: Validate Root Directory Files

            **MANDATORY**: Check for unrequired files in root directory and report in PR comment.

            ```bash
            # Allowed root files
            ALLOWED_ROOT_FILES=(
              \"metadata.json\"
              \"package.json\"
              \"package-lock.json\"
              \"cdk.json\"
              \"cdktf.json\"
              \"Pulumi.yaml\"
              \"tap.py\"
              \"tap.go\"
            )

            # Get all files in root directory
            ROOT_FILES=$(find . -maxdepth 1 -type f -not -path \"./.git/*\" | sed 's|^./||' | sort)

            # Check each file
            UNREQUIRED_FILES=()
            for file in $ROOT_FILES; do
              # Skip if in allowed list
              is_allowed=false
              for allowed in \"${ALLOWED_ROOT_FILES[@]}\"; do
                if [ \"$file\" == \"$allowed\" ]; then
                  is_allowed=true
                  break
                fi
              done
              
              # Skip directories and hidden files (except .git)
              if [[ \"$file\" == .* ]] && [[ \"$file\" != \".git\"* ]]; then
                continue
              fi
              
              if [ \"$is_allowed\" == false ]; then
                UNREQUIRED_FILES+=(\"$file\")
              fi
            done

            # Report unrequired files
            if [ ${#UNREQUIRED_FILES[@]} -gt 0 ]; then
              echo \"âš ï¸ Found ${#UNREQUIRED_FILES[@]} unrequired file(s) in root directory:\"
              for file in \"${UNREQUIRED_FILES[@]}\"; do
                echo \"  - $file\"
              done
            fi
            ```

            **Report in PR Comment**:

            ```markdown
            ## ðŸ“ Root Directory File Validation

            ### âœ… Allowed Root Files Found
            - {list allowed files found}

            ### âŒ Unrequired Files in Root Directory
            The following files should be moved or removed:
            1. **{filename}** - Should be moved to {suggested_location} or removed
            2. **{filename}** - Should be moved to {suggested_location} or removed

            **Impact**: Unrequired files in root directory may cause CI/CD failures.
            ```

            # Determine Review Type

            First, check the subject_labels in metadata.json to determine the review type:
            ```bash
            SUBJECT_LABELS=$(jq -r '.subject_labels[]?' metadata.json 2>/dev/null || echo "")
            if echo "$SUBJECT_LABELS" | grep -q "CI/CD Pipeline"; then
              REVIEW_TYPE="cicd-pipeline"
            else
              REVIEW_TYPE="iac-standard"
            fi
            echo "Review type: $REVIEW_TYPE"
            ```

            # CI/CD Pipeline Review (If REVIEW_TYPE=cicd-pipeline)

            If this is a CI/CD Pipeline task, follow these specialized review criteria:

            ## Step 1: Read CI/CD Pipeline Review Guidelines
            Read and follow `.claude/prompts/cicd-pipeline-review.md` for complete scoring criteria.

            ## Step 2: Validate lib/ci-cd.yml

            The CI/CD pipeline configuration is in `lib/ci-cd.yml`. Review it against these critical criteria:

            ### Security (3 points - CRITICAL)
            1. **Secrets Management (2 points)** - NO hardcoded secrets, all use `${{ secrets.* }}` or `${{ vars.* }}`
            2. **IAM & Authentication (1 point)** - Proper OIDC or GitHub Secrets authentication

            If ANY hardcoded secrets found: **Automatic FAIL (score = 0)**

            ### Architecture (3 points)
            1. **Multi-stage deployment (2 points)** - Dev â†’ Staging â†’ Prod with approval gates
            2. **Job dependencies & artifacts (1 point)** - Proper `needs:` relationships and artifact management

            ### Configuration Management (2 points)
            1. **Environment variables & parameterization** - Reusable env vars, workflow_dispatch inputs

            ### Requirements Compliance (2 points)
            1. **lib/ci-cd.yml patterns (1.5 points)** - Follows specification and best practices
            2. **PROMPT requirements (0.5 points)** - All requirements from lib/PROMPT.md implemented

            ## Step 3: Calculate Score (Total: 10 points)

            Sum all category scores directly (no conversion needed).
            - Minimum passing score: 8/10

            ## Step 4: Update metadata.json with Training Quality

            **MANDATORY ACTION**: You MUST update metadata.json with the training quality score using the Bash tool.

            After calculating your score, execute this command using the Bash tool (replace <score> with actual numeric value 0-10):

            ```bash
            jq --argjson tq <score> '.training_quality = $tq' metadata.json > metadata.json.tmp && mv metadata.json.tmp metadata.json && echo "Updated training_quality to <score>/10" && cat metadata.json
            ```

            Example if score is 10:
            ```bash
            jq --argjson tq 10 '.training_quality = $tq' metadata.json > metadata.json.tmp && mv metadata.json.tmp metadata.json && echo "Updated training_quality to 10/10" && cat metadata.json
            ```

            **Verification**: After running the command, verify the output shows training_quality field was added to metadata.json.

            **CRITICAL**: Do NOT skip this step. The metadata.json MUST be updated before posting the review comment.

            **Note**: CI/CD Pipeline tasks do NOT require aws_services field. Do NOT flag aws_services as missing for CI/CD tasks.

            ## Step 5: Post Review Comment

            Post a GitHub comment with the format specified in `.claude/prompts/cicd-pipeline-review.md`.

            **IMPORTANT**: Include metadata.json validation and root directory file validation sections in your comment.

            ## Step 6: Output Score

            At the very end of your output, on its own line:
            ```
            SCORE:<numeric_value>
            ```

            Example: `SCORE:8`

            ---

            # Standard IaC Review (If REVIEW_TYPE=iac-standard)

            If this is NOT a CI/CD Pipeline task, follow the standard review process:

            CRITICAL: you MUST validate the latest PROMPT file format, and add a github comment if it appears to be AI-generated rather than human-written:
            1. Identify all the PROMPT file (PROMPT.md, PROMPT2.md, PROMPT3.md, etc.) - always use the highest numbered one
            2. Read the PROMPT files
            3. Assess whether the content looks human-generated or AI-generated
            4. Look for signs of AI generation such as:
                - Emojis or special symbols (icons) used for formatting
                - Table structures
                - Overly formal or template-like language
                - Logs are allowed when we have multiple prompt files. 
                - Perfect formatting that suggests AI assistance
                - LLM Flavoured text, such as: "Here is a **comprehensive and high-level prompt** for your use..."
            5. If  any PROMPT file appears to be AI-generated rather than human-written, fail the job by exiting with code 1.
            6. Also validate that for each PROMPT file, there is a corresponding MODEL_RESPONSE file (e.g., PROMPT.md -> MODEL_RESPONSE.md, PROMPT2.md -> MODEL_RESPONSE2.md, etc.) and that it is well-formatted.
            7. PROMPT2.md and PROMPT3.md are optional, but if they exist, they must also be validated and it can contain Deployment/Test/Error logs as well.

            CRITICAL: you MUST validate lib/IDEAL_RESPONSE.md, read .claude/agents/iac-infra-qa-trainer.md to understand how that file should have been created.
            1. Read lib/IDEAL_RESPONSE.md if it exists
            2. lib/IDEAL_RESPONSE.md should be well-formatted, markdown.
            3. Important: There should not be code outside proper code blocks (```python, ```bash, etc.)
            4. Every code file inside lib/ folder should be represented in lib/IDEAL_RESPONSE.md in code_blocks.
            5. There should not be references in the lib/IDEAL_RESPONSE.md to the QA process, unit tests or integration tests.
            6. If the lib/IDEAL_RESPONSE.md does not meet the above criteria, fail the job by exiting with code 1.

            CRITICAL WARNING - FILE CONTEXT:
            - lib/MODEL_RESPONSE.md = Initial model output (MAY contain errors like Maven, multi-stack, etc.)
            - lib/IDEAL_RESPONSE.md = Final corrected code (THIS is what you validate for platform/language compliance)
            - lib/MODEL_FAILURES.md = Documentation of what WAS FIXED (past tense) - NOT current errors!

            When validating platform/language compliance, you MUST:
            1. Run: bash ./.claude/scripts/validate-code-platform.sh
            2. This script checks lib/IDEAL_RESPONSE.md (NOT MODEL_RESPONSE.md) against metadata.json
            3. If MODEL_FAILURES.md mentions "Maven â†’ Gradle" or "multi-stack â†’ single stack", those are PAST fixes
            4. DO NOT report those as current problems in IDEAL_RESPONSE.md
            5. Only validate what is CURRENTLY in lib/IDEAL_RESPONSE.md

            Follow instructions in .claude/agents/iac-code-reviewer.md. Do not commit any changes, but allow the metadata.json file to be updated.
            Important: The Metadata Enhancement phase in .claude/agents/iac-code-reviewer.md is very important for this project. Make sure that the
            metadata.json file is updated with the required fields:

            For standard IaC tasks:
            - training_quality (required)
            - aws_services (required)

            For CI/CD Pipeline tasks (subject_label: "CI/CD Pipeline" OR platform: "cicd"):
            - training_quality (required)
            - aws_services (NOT required - CI/CD pipelines don't deploy AWS services)

            If metadata.json is missing required fields for the task type, add a github comment indicating the issue.

            This validation is mandatory and must be completed before any other review activities.

            **IMPORTANT**: Include metadata.json validation and root directory file validation sections in your comment.

            Now follow the instructions in .claude/agents/iac-code-reviewer.md

            ---

            # âš ï¸ CRITICAL: Final Output Format (Both Review Types)

            **MANDATORY**: Your GitHub comment MUST end with this exact line:

            SCORE:X

            Where X is a number from 0 to 10 (your training quality score).

            **Format Requirements:**
            - Must be on its own line (no other text on that line)
            - Must be the LAST line of your GitHub comment
            - Must use exact format: SCORE:8 (no spaces between SCORE: and number)
            - Score must be 0-10 (scores > 10 will be rejected as false matches)
            - Do NOT use compliance scores (like 12/17) or other metrics

            **Examples of VALID formats:**
            ```
            SCORE:10
            SCORE:8
            SCORE:6
            ```

            **Examples of INVALID formats:**
            ```
            SCORE: 8 (extra space)
            Training Quality Score: 8/10 (wrong format, but acceptable as fallback)
            Compliance Score: 12/17 (will be rejected - exceeds maximum)
            ```

            **MANDATORY FINAL STEPS**:
            1. Validate metadata.json thoroughly (Step 0)
            2. Validate root directory files (Step 0.6)
            3. Update metadata.json with training_quality field (primary source)
            4. Post comprehensive GitHub comment with review findings including:
               - metadata.json validation results
               - Root directory file validation results
               - Code review findings
               - Training quality score
            5. End comment with SCORE:X line where X is 0-10

            **What happens if you don't follow this:**
            - Without SCORE line: Score defaults to 0, build FAILS
            - Score > 10: Rejected as false match, score set to 0, build FAILS
            - Without metadata.json update: Fallback to comment parsing, less reliable
            - Without metadata.json validation: Issues may go unnoticed

            After posting the comment with the SCORE line, finish execution successfully.
            Do not exit with a non-zero code unless explicitly instructed by the workflow."

      - name: Extract Claude Quality Score from PR Comment
        id: extract
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "ðŸ” Fetching latest Claude review comment..."
          gh pr view ${{ github.event.pull_request.number }} --comments --json comments \
            | jq -r '.comments[].body' > all_comments.txt

          echo "ðŸ€ Searching for Claude quality score patterns..."

          # 1ï¸âƒ£ Try to find most flexible Claude score pattern
          # Matches things like:
          # SCORE:9, Score: 9, Training Quality: **9.2/10**, Final Score â­ï¸: 8.5/10, etc.
          SCORE=$(grep -Poi \
            '(?i)(score|training\s*quality|training\s*score|final\s*score|estimated\s*score)[^0-9]{0,15}([0-9]+(\.[0-9]+)?)' \
            all_comments.txt | grep -Po '[0-9]+(\.[0-9]+)?' | tail -1)

          # 2ï¸âƒ£ If not found, try clean "SCORE:" line (in case of strict format)
          if [ -z "$SCORE" ]; then
            SCORE=$(grep -Poi '^\s*\*?\**\s*SCORE\s*[:=-]?\s*\**\*?\s*([0-9]+(\.[0-9]+)?)' all_comments.txt | grep -Po '[0-9]+(\.[0-9]+)?' | tail -1)
          fi

          # 3ï¸âƒ£ If still not found, check for any â€œX/10â€ patterns (sometimes Claude just ends with 9/10)
          if [ -z "$SCORE" ]; then
            SCORE=$(grep -Poi '\b([0-9]+(\.[0-9]+)?)/10\b' all_comments.txt | grep -Po '^[0-9]+(\.[0-9]+)?' | tail -1)
          fi

          # 4ï¸âƒ£ If comment parsing failed, fallback to metadata.json
          if [ -z "$SCORE" ]; then
            echo "âš ï¸ Could not find score in Claude comment. Falling back to metadata.json..."
            if [ -f "metadata.json" ]; then
              SCORE=$(jq -r '.training_quality // empty' metadata.json 2>/dev/null || echo "")
              if [ -n "$SCORE" ] && [[ "$SCORE" =~ ^[0-9]+([.][0-9]+)?$ ]]; then
                echo "âœ… Found fallback metadata training_quality: $SCORE"
              else
                echo "âš ï¸ metadata.json missing valid numeric score"
                SCORE=""
              fi
            else
              echo "âš ï¸ metadata.json not found"
            fi
          fi

          # 5ï¸âƒ£ Validate range and clean
          if [ -z "$SCORE" ]; then
            echo "âš ï¸ Could not find any quality score. Defaulting to 0."
            SCORE=0
          fi

          SCORE_INT=$(echo "$SCORE" | awk '{print int($1)}')

          if (( $(echo "$SCORE_INT > 10" | bc -l) )); then
            echo "âš ï¸ Invalid high score ($SCORE) â†’ set to 0"
            SCORE=0
          elif (( $(echo "$SCORE_INT < 0" | bc -l) )); then
            echo "âš ï¸ Invalid negative score ($SCORE) â†’ set to 0"
            SCORE=0
          fi

          echo "âœ… Final Claude quality score: $SCORE"
          echo "quality_score=$SCORE" >> $GITHUB_OUTPUT


      - name: Enforce Quality Threshold
        id: quality_gate
        run: |
          SCORE=${{ steps.extract.outputs.quality_score }}
          echo "Evaluating Claude quality gate..."
          if (( SCORE < 8 )); then
            echo "âŒ Quality score ($SCORE) below threshold (8)."
            exit 1
          else
            echo "âœ… Quality score ($SCORE) meets or exceeds threshold (8)."
          fi

      - name: Upload updated metadata.json
        uses: actions/upload-artifact@v4
        with:
          name: updated-metadata
          path: metadata.json
          if-no-files-found: warn
    outputs:
      quality_score: ${{ steps.extract.outputs.quality_score }}

  cleanup:
    name: Cleanup (Destroy Resources)
    runs-on: ubuntu-24.04
    needs:
      [
        detect-metadata,
        integration-tests-live,
        claude-code-action,
        analysis,
        cicd-pipeline-optimization,
      ]
    # Cleanup runs when:
    # 1. Claude review succeeded
    # 2. For Analysis/CI/CD tasks: int-tests can be success OR skipped
    # 3. For normal infra tasks: int-tests MUST be success (not skipped)
    if: |
      always() && !cancelled() &&
      needs.claude-code-action.result == 'success' &&
      (
        (needs.integration-tests-live.result == 'success') ||
        (needs.integration-tests-live.result == 'skipped' && 
         (contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.analysis_subject_label) ||
          contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.cicd_pipeline_subject_label)))
      ) &&
      ((github.event_name == 'pull_request' && github.event.action != 'closed') || 
       (github.event_name == 'workflow_dispatch' && !inputs.skip_cleanup)) &&
      !contains(github.event.head_commit.message, '[skip-jobs]')
    environment: qa
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check if cleanup is needed
        id: check-cleanup
        run: |
          SUBJECT_LABELS='${{ needs.detect-metadata.outputs.subject_labels }}'
          ANALYSIS_LABEL='${{ needs.detect-metadata.outputs.analysis_subject_label }}'
          CICD_LABEL='${{ needs.detect-metadata.outputs.cicd_pipeline_subject_label }}'

          if echo "$SUBJECT_LABELS" | grep -q "$ANALYSIS_LABEL" || echo "$SUBJECT_LABELS" | grep -q "$CICD_LABEL"; then
            echo "needs_cleanup=false" >> $GITHUB_OUTPUT
            echo "â„¹ï¸ Analysis or CI/CD Pipeline task - no infrastructure to cleanup"
          else
            echo "needs_cleanup=true" >> $GITHUB_OUTPUT
            echo "âœ… Infrastructure task - cleanup needed"
          fi

      - name: Setup Environment
        if: steps.check-cleanup.outputs.needs_cleanup == 'true'
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: "false"
          platform: ${{ needs.detect-metadata.outputs.platform }}
          language: ${{ needs.detect-metadata.outputs.language }}

      - name: Configure AWS
        if: steps.check-cleanup.outputs.needs_cleanup == 'true'
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Destroy Resources (cleanup any leftover resources)
        if: steps.check-cleanup.outputs.needs_cleanup == 'true'
        run: ./scripts/destroy.sh
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
          TF_VAR_environmentSuffix: ${{ env.ENVIRONMENT_SUFFIX }}
          TERRAFORM_STATE_BUCKET: ${{ env.TERRAFORM_STATE_BUCKET }}
          TERRAFORM_STATE_BUCKET_REGION: ${{ env.TERRAFORM_STATE_BUCKET_REGION }}
          TERRAFORM_STATE_BUCKET_KEY: ${{ env.ENVIRONMENT_SUFFIX }}
          PULUMI_BACKEND_URL: s3://${{ env.PULUMI_STATE_BUCKET }}?region=${{ env.PULUMI_BUCKET_REGION }}
          PULUMI_ORG: ${{ env.PULUMI_ORG }}

      - name: Cleanup Complete
        run: |
          if [ "${{ steps.check-cleanup.outputs.needs_cleanup }}" == "true" ]; then
            echo "âœ… Infrastructure cleanup completed"
          else
            echo "âœ… No cleanup needed - proceeding to archive"
          fi
  debug-claude-outputs:
    name: Debug Claude outputs
    runs-on: ubuntu-24.04
    needs: [claude-code-action]
    if: always()
    steps:
      - name: Dump needs info
        run: |
          echo "needs.claude-code-action.result=${{ needs.claude-code-action.result }}"
          echo "needs.claude-code-action.outputs.quality_score='${{ needs.claude-code-action.outputs.quality_score }}'"
          echo "---- full needs json ----"
          echo '${{ toJson(needs) }}'

  archive-folders:
    name: Archive Folders and Reset Repository
    runs-on: ubuntu-24.04
    needs: [detect-metadata, cleanup, claude-code-action]
    environment: stage
    if: ${{ always() && !cancelled() && needs.cleanup.result == 'success' && needs.claude-code-action.result == 'success' && needs.detect-metadata.result == 'success' && ((github.event_name == 'pull_request' && github.event.action != 'closed') || (github.event_name == 'workflow_dispatch')) && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    permissions:
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          repository: ${{ github.repository }}
          ref: ${{ github.head_ref }}

      - name: Download coverage reports
        if: ${{ !contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.analysis_subject_label) && !contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.cicd_pipeline_subject_label) }}
        uses: actions/download-artifact@v4
        with:
          name: coverage-reports
          path: coverage/

      - name: Download updated metadata
        uses: actions/download-artifact@v4
        with:
          name: updated-metadata
          path: .

      - name: Download analysis results
        if: ${{ contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.analysis_subject_label) }}
        uses: actions/download-artifact@v4
        with:
          name: analysis-results
          path: lib/

      - name: Download CI/CD Pipeline configuration
        if: ${{ contains(needs.detect-metadata.outputs.subject_labels, needs.detect-metadata.outputs.cicd_pipeline_subject_label) }}
        uses: actions/download-artifact@v4
        with:
          name: cicd-pipeline-config
          path: lib/

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Archive folders and reset repository
        run: |
          # Configure Git
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"

          # Use metadata from detect-metadata job
          PLATFORM="${{ needs.detect-metadata.outputs.platform }}"
          LANGUAGE="${{ needs.detect-metadata.outputs.language }}"
          echo "Using platform: $PLATFORM, language: $LANGUAGE"

          # Update metadata.json with coverage information and commit author
          # Get commit author from GitHub context
          COMMIT_AUTHOR="${{ github.event.pull_request.user.login || github.actor }}"
          echo "Commit author: $COMMIT_AUTHOR"

          # Extract coverage percentages based on language
          if [ "$LANGUAGE" = "yml" ]; then
            echo "CI/CD Pipeline task - no unit test coverage required"
            LINES_COVERAGE=100
            BRANCHES_COVERAGE=100
          elif [ "$LANGUAGE" = "py" ]; then
            echo "Extracting Python coverage from cov.json..."
            if [ -f "cov.json" ]; then
              LINES_COVERAGE=$(jq -r '.totals.percent_covered' cov.json)
              # Calculate branch coverage percentage
              BRANCHES_NUM=$(jq -r '.totals.num_branches' cov.json)
              if [ "$BRANCHES_NUM" = "0" ] || [ "$BRANCHES_NUM" = "null" ]; then
                # No branches to cover, set to 100%
                BRANCHES_COVERAGE=100
              else
                COVERED_BRANCHES=$(jq -r '.totals.covered_branches' cov.json)
                # Calculate percentage: (covered_branches / num_branches) * 100
                BRANCHES_COVERAGE=$(awk "BEGIN {print ($COVERED_BRANCHES / $BRANCHES_NUM) * 100}")
              fi
            else
              echo "Warning: cov.json not found, setting default coverage values"
              LINES_COVERAGE=100
              BRANCHES_COVERAGE=100
            fi
          elif [ "$LANGUAGE" = "go" ]; then
            echo "Extracting Go coverage from coverage-summary.json..."
            if [ -f "coverage/coverage-summary.json" ]; then
              LINES_COVERAGE=$(jq -r '.total.lines.pct' coverage/coverage-summary.json)
              BRANCHES_COVERAGE=$(jq -r '.total.branches.pct' coverage/coverage-summary.json)
            else
              echo "Warning: Go coverage-summary.json not found, setting default coverage values"
              LINES_COVERAGE=100
              BRANCHES_COVERAGE=100
            fi
          else
            echo "Extracting TypeScript/JavaScript coverage from coverage-summary.json..."
            if [ -f "coverage/coverage-summary.json" ]; then
              LINES_COVERAGE=$(jq -r '.total.lines.pct' coverage/coverage-summary.json)
              BRANCHES_COVERAGE=$(jq -r '.total.branches.pct' coverage/coverage-summary.json)
            else
              echo "Warning: coverage-summary.json not found, setting default coverage values"
              LINES_COVERAGE=100
              BRANCHES_COVERAGE=100
            fi
          fi

          # If coverage is "Unknown", set to 100
          if [ "$LINES_COVERAGE" = "Unknown" ]; then
            LINES_COVERAGE=100
          fi
          if [ "$BRANCHES_COVERAGE" = "Unknown" ]; then
            BRANCHES_COVERAGE=100
          fi

          echo "Lines coverage: $LINES_COVERAGE%"
          echo "Branches coverage: $BRANCHES_COVERAGE%"

          # Update metadata.json with coverage information and commit author (assuming it always exists)
          jq --arg lines "$LINES_COVERAGE" --arg branches "$BRANCHES_COVERAGE" --arg author "$COMMIT_AUTHOR" '. + {coverage: {lines: ($lines|tonumber), branches: ($branches|tonumber)}, author: $author}' metadata.json > metadata.json.tmp
          mv metadata.json.tmp metadata.json
          echo "Updated metadata.json with coverage information and commit author"

          cat metadata.json

          echo "Current directory contents before archiving:"
          ls -la

          # Get Docker S3 location from the previous step and add it to metadata.json
          DOCKER_S3_LOCATION="${DOCKER_S3_LOCATION}"
          echo "Docker S3 location from previous step: $DOCKER_S3_LOCATION"

          # Add the dockerS3Location to metadata.json
          jq --arg location "$DOCKER_S3_LOCATION" '. + {dockerS3Location: $location}' metadata.json > metadata.json.tmp
          mv metadata.json.tmp metadata.json
          echo "Updated metadata.json with dockerS3Location: $DOCKER_S3_LOCATION"

          # Create archive directory with platform, language, and PR number
          ARCHIVE_DIR="archive/${PLATFORM}-${LANGUAGE}/Pr${{ github.event.number }}"
          mkdir -p "$ARCHIVE_DIR"
          echo "Created archive directory: $ARCHIVE_DIR"

            # Define list of paths to move to archive
            PATHS_TO_ARCHIVE=(
            "lib"
            "bin" 
            "test"
            "tests"
            "cdk.json"
            "metadata.json"
            "tap.py"
            "tap.go"
            "setup.js"
            "cdktf.json"
            "Pulumi.yaml"
            )

          # Move paths to archive
          for path in "${PATHS_TO_ARCHIVE[@]}"; do
          if [[ -d "$path" || -f "$path" ]]; then
            mv "$path" "$ARCHIVE_DIR"/
            echo "Moved $path to archive"
          else
            echo "Path $path not found, skipping"
          fi
          done

          # Check if there are changes to commit
          if git diff --quiet && git diff --cached --quiet; then
            echo "No changes to commit - no folders found to archive"
          else
            # Extract metadata for commit message
            METADATA_FILE="$ARCHIVE_DIR/metadata.json"
            METADATA_PLATFORM=$(jq -r '.platform' "$METADATA_FILE")
            METADATA_PO_ID=$(jq -r '.po_id' "$METADATA_FILE")
            METADATA_SUBTASK=$(jq -r '.subtask | ascii_downcase' "$METADATA_FILE")
            METADATA_SUBJECT_LABELS=$(jq -r '.subject_labels | join(", ")' "$METADATA_FILE")

            METADATA_AUTHOR=$(jq -r '.author' "$METADATA_FILE")

            # Truncate subject_labels to 100 characters if needed
            if [ ${#METADATA_SUBJECT_LABELS} -gt 100 ]; then
              METADATA_SUBJECT_LABELS="${METADATA_SUBJECT_LABELS:0:97}..." # 97 chars + "..." = 100
            fi

            COMMIT_SUBJECT="feat(${METADATA_PLATFORM}): ${METADATA_PO_ID} ${METADATA_SUBTASK}"
            COMMIT_BODY="${METADATA_SUBJECT_LABELS}\nAuthor: ${METADATA_AUTHOR}"

            git add -A
            git commit -m "${COMMIT_SUBJECT}" -m "${COMMIT_BODY}" -m "[skip-jobs]"
            git push origin HEAD
            echo "Archive committed to current branch"
          fi

  upload-task-to-s3:
    name: Upload Task to S3
    runs-on: ubuntu-24.04
    environment: release
    if: ${{ github.event_name == 'pull_request' && github.event.pull_request.merged == true }}
    permissions:
      contents: read

    steps:
      - name: Checkout merged code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          repository: ${{ github.repository }}
          ref: ${{ github.event.pull_request.merge_commit_sha }}

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Extract platform and language for S3 upload
        id: extract-metadata
        run: |
          # We need to find the specific archive folder for this PR
          # Since we don't know platform/language yet, we'll search for the PR-specific folder
          PR_FOLDER_PATTERN="archive/*/Pr${{ github.event.number }}"

          # Find the archive folder for this specific PR
          ARCHIVE_FOLDER=""
          for folder in $PR_FOLDER_PATTERN; do
            if [ -d "$folder" ]; then
              ARCHIVE_FOLDER="$folder"
              break
            fi
          done

          if [ -n "$ARCHIVE_FOLDER" ] && [ -f "$ARCHIVE_FOLDER/metadata.json" ]; then
            echo "Found archive folder: $ARCHIVE_FOLDER"
            echo "Found metadata.json at: $ARCHIVE_FOLDER/metadata.json"
            PLATFORM=$(jq -r '.platform // "unknown"' "$ARCHIVE_FOLDER/metadata.json")
            LANGUAGE=$(jq -r '.language // "unknown"' "$ARCHIVE_FOLDER/metadata.json")
          else
            echo "Warning: Could not find archive folder or metadata.json for PR ${{ github.event.number }}"
            echo "Available archive folders:"
            ls -la archive/ || echo "No archive directory found"
            PLATFORM="unknown"
            LANGUAGE="unknown"
          fi
          echo "platform=$PLATFORM" >> $GITHUB_OUTPUT
          echo "language=$LANGUAGE" >> $GITHUB_OUTPUT
          echo "Extracted platform: $PLATFORM, language: $LANGUAGE"

      - name: Upload archive folder to S3
        run: |
          PLATFORM="${{ steps.extract-metadata.outputs.platform }}"
          LANGUAGE="${{ steps.extract-metadata.outputs.language }}"
          S3_PREFIX="${PLATFORM}-${LANGUAGE}/Pr${{ github.event.number }}"

          echo "Uploading archive folder to S3 bucket: ${{ env.S3_RELEASE_BUCKET_NAME }}"
          echo "S3 prefix: $S3_PREFIX"

          # Check if archive folder exists
          if [ -d "archive/${S3_PREFIX}" ]; then
            # Upload the archive folder contents to S3, preserving folder structure
            aws s3 sync "archive/${S3_PREFIX}/" "s3://${{ env.S3_RELEASE_BUCKET_NAME }}/${S3_PREFIX}/" \
              --delete \
              --exclude "*.git*" \
              --exclude "node_modules/*"
            echo "Successfully uploaded archive to S3"
          else
            echo "Archive folder archive/${S3_PREFIX} not found, nothing to upload"
            exit 1
          fi
      - name: Verify S3 upload
        run: |
          PLATFORM="${{ steps.extract-metadata.outputs.platform }}"
          LANGUAGE="${{ steps.extract-metadata.outputs.language }}"
          S3_PREFIX="${PLATFORM}-${LANGUAGE}/Pr${{ github.event.number }}"

          echo "Verifying S3 upload for prefix: $S3_PREFIX"
          aws s3 ls "s3://${{ env.S3_RELEASE_BUCKET_NAME }}/${S3_PREFIX}/" --recursive || echo "No files found or access denied"

  cleanup-pr:
    name: Cleanup (PR Closed)
    runs-on: ubuntu-24.04
    # This job should run regardless of the [skip-jobs] tag since it's for cleanup
    if: github.event_name == 'pull_request' && github.event.action == 'closed' && !github.event.pull_request.merged
    environment: release

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: "false"

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Destroy Resources (if resources exist)
        run: ./scripts/destroy.sh
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
          TERRAFORM_STATE_BUCKET: ${{ env.TERRAFORM_STATE_BUCKET }}
          TERRAFORM_STATE_BUCKET_REGION: ${{ env.TERRAFORM_STATE_BUCKET_REGION }}
          PULUMI_BACKEND_URL: s3://${{ env.PULUMI_STATE_BUCKET }}?region=${{ env.PULUMI_BUCKET_REGION }}
          PULUMI_ORG: ${{ env.PULUMI_ORG }}

  semantic-release:
    name: Semantic Release
    runs-on: ubuntu-24.04
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    # permissions:
    #   contents: write
    #   issues: write
    #   pull-requests: write
    concurrency:
      group: semantic_release_pipeline
      cancel-in-progress: true

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: "false"

      - name: Run semantic-release
        run: npm run release
        env:
          GITHUB_TOKEN: ${{ secrets.SEMANTIC_RELEASE_PAT }}
