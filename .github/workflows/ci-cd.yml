name: CI/CD Pipeline

# Special keyword [skip-jobs] can be added to commit messages to skip most jobs
# The upload-task-to-s3 job will still run when a PR is merged, regardless of this tag
# The cleanup-pr job will still run when a PR is closed, regardless of this tag

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
    types: [opened, synchronize, reopened, closed]
  workflow_dispatch:

permissions:
  contents: read

env:
  NODE_VERSION: '22.17.0'
  GO_VERSION: '1.23.12'
  # Use PR number for resource isolation, fallback to 'dev' for main branch
  # Prefix with 'pr' to ensure AWS resource names start with a letter
  ENVIRONMENT_SUFFIX: ${{ github.event.number && format('pr{0}', github.event.number) || 'dev' }}
  S3_RELEASE_BUCKET_NAME: 'iac-rlhf-aws-release'
  TERRAFORM_STATE_BUCKET: 'iac-rlhf-tf-states'
  TERRAFORM_STATE_BUCKET_REGION: 'us-east-1'
  TERRAFORM_STATE_BUCKET_KEY: ${{ github.event.pull_request.number }}
  S3_PRODUCTION_BUCKET_NAME: 'iac-rlhf-production'
  PULUMI_STATE_BUCKET: 'iac-rlhf-pulumi-states'
  PULUMI_BUCKET_REGION: 'us-east-1'
  PULUMI_CONFIG_PASSPHRASE: ${{ secrets.PULUMI_CONFIG_PASSPHRASE }}
  PULUMI_ORG: 'organization'
  AWS_REGION: ${{ vars.AWS_REGION }}
  # Go caching optimization
  GOCACHE: ${{ github.workspace }}/.cache/go-build
  GOMODCACHE: ${{ github.workspace }}/.cache/go-mod

jobs:
  detect-metadata:
    name: Detect Project Files
    runs-on: ubuntu-24.04
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    outputs:
      platform: ${{ steps.metadata.outputs.platform }}
      language: ${{ steps.metadata.outputs.language }}
      po_id: ${{ steps.metadata.outputs.po_id }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Detect metadata and validate project
        id: metadata
        run: ./scripts/detect-metadata.sh

  create-jump-box:
    name: Create Jump Box
    runs-on: ubuntu-24.04
    needs: detect-metadata
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    environment: dev

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Create Jump Box with Full AWS Permissions
        run: |
          echo "ðŸ” Debugging network configuration..."
          
          # Get default VPC and debug network configuration
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=is-default,Values=true" --query 'Vpcs[0].VpcId' --output text)
          echo "ðŸ” Default VPC ID: $VPC_ID"
          
          # List all Internet Gateways for this VPC
          echo "ðŸŒ Internet Gateways attached to VPC $VPC_ID:"
          aws ec2 describe-internet-gateways \
            --filters "Name=attachment.vpc-id,Values=$VPC_ID" \
            --query 'InternetGateways[*].[InternetGatewayId,State,Attachments[0].State]' \
            --output table
          
          # List all available (unattached) Internet Gateways
          echo "ðŸŒ Available (unattached) Internet Gateways:"
          aws ec2 describe-internet-gateways \
            --filters "Name=attachment.state,Values=detached" \
            --query 'InternetGateways[*].[InternetGatewayId,State]' \
            --output table
          
          # Check if the referenced IGW exists at all
          echo "ðŸ” Checking if IGW igw-01f3dc688f3cbea14 exists..."
          IGW_EXISTS=$(aws ec2 describe-internet-gateways \
            --internet-gateway-ids igw-01f3dc688f3cbea14 \
            --query 'InternetGateways | length(@)' \
            --output text 2>/dev/null || echo "0")
          
          if [ "$IGW_EXISTS" -gt 0 ]; then
            echo "âœ… IGW igw-01f3dc688f3cbea14 exists"
            IGW_STATE=$(aws ec2 describe-internet-gateways \
              --internet-gateway-ids igw-01f3dc688f3cbea14 \
              --query 'InternetGateways[0].Attachments[0].State' \
              --output text 2>/dev/null || echo "detached")
            echo "ðŸ” IGW state: $IGW_STATE"
          else
            echo "âŒ IGW igw-01f3dc688f3cbea14 does not exist - was deleted!"
          fi
          
          # List all subnets with their routing info
          echo "ðŸ—ºï¸  All subnets in VPC $VPC_ID:"
          aws ec2 describe-subnets \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'Subnets[*].[SubnetId,AvailabilityZone,MapPublicIpOnLaunch,CidrBlock]' \
            --output table
          
          # Find route tables and their routes
          echo "ðŸ›£ï¸  Route tables for VPC $VPC_ID:"
          ROUTE_TABLES=$(aws ec2 describe-route-tables \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'RouteTables[*].RouteTableId' \
            --output text)
          
          for rt_id in $ROUTE_TABLES; do
            echo "Route table $rt_id:"
            aws ec2 describe-route-tables \
              --route-table-ids $rt_id \
              --query 'RouteTables[0].Routes[*].[DestinationCidrBlock,GatewayId,State]' \
              --output table
            echo "Associated subnets:"
            aws ec2 describe-route-tables \
              --route-table-ids $rt_id \
              --query 'RouteTables[0].Associations[*].[SubnetId,Main]' \
              --output table
            echo "---"
          done
          
          # Check what subnet the existing instance is in
          echo "ðŸ§¹ Checking for existing jump box resources..."
          EXISTING_INSTANCE=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=JumpBox-${{ env.ENVIRONMENT_SUFFIX }}" \
                     "Name=instance-state-name,Values=running,pending" \
            --query 'Reservations[0].Instances[0].InstanceId' \
            --output text)
          
          if [ "$EXISTING_INSTANCE" != "None" ] && [ "$EXISTING_INSTANCE" != "null" ] && [ -n "$EXISTING_INSTANCE" ]; then
            echo "â™»ï¸  Found existing running jump box: $EXISTING_INSTANCE"
            
            # Get existing instance details
            EXISTING_IP=$(aws ec2 describe-instances \
              --instance-ids $EXISTING_INSTANCE \
              --query 'Reservations[0].Instances[0].PublicIpAddress' \
              --output text)
            
            EXISTING_SUBNET=$(aws ec2 describe-instances \
              --instance-ids $EXISTING_INSTANCE \
              --query 'Reservations[0].Instances[0].SubnetId' \
              --output text)
            
            echo "ðŸ” Existing instance is in subnet: $EXISTING_SUBNET"
            
            # Check if existing subnet has IGW route
            EXISTING_RT=$(aws ec2 describe-route-tables \
              --filters "Name=association.subnet-id,Values=$EXISTING_SUBNET" \
              --query 'RouteTables[0].RouteTableId' \
              --output text)
            
            if [ "$EXISTING_RT" = "None" ] || [ -z "$EXISTING_RT" ]; then
              # Check main route table
              EXISTING_RT=$(aws ec2 describe-route-tables \
                --filters "Name=vpc-id,Values=$VPC_ID" "Name=association.main,Values=true" \
                --query 'RouteTables[0].RouteTableId' \
                --output text)
              echo "ðŸ” Using main route table: $EXISTING_RT"
            else
              echo "ðŸ” Subnet-specific route table: $EXISTING_RT"
            fi
            
            # Check for IGW route in the route table
            IGW_ROUTE_EXISTS=$(aws ec2 describe-route-tables \
              --route-table-ids $EXISTING_RT \
              --query "RouteTables[0].Routes[?DestinationCidrBlock=='0.0.0.0/0' && starts_with(GatewayId, 'igw-')] | length(@)" \
              --output text)
            
            if [ "$IGW_ROUTE_EXISTS" -gt 0 ]; then
              # Check if the IGW route is in blackhole state
              BLACKHOLE_ROUTES=$(aws ec2 describe-route-tables \
                --route-table-ids $EXISTING_RT \
                --query "RouteTables[0].Routes[?DestinationCidrBlock=='0.0.0.0/0' && starts_with(GatewayId, 'igw-') && State=='blackhole'] | length(@)" \
                --output text)
              
              if [ "$BLACKHOLE_ROUTES" -gt 0 ]; then
                echo "âŒ IGW route exists but is in BLACKHOLE state - IGW is detached!"
                IGW_ID=$(aws ec2 describe-route-tables \
                  --route-table-ids $EXISTING_RT \
                  --query "RouteTables[0].Routes[?DestinationCidrBlock=='0.0.0.0/0' && starts_with(GatewayId, 'igw-')].GatewayId" \
                  --output text)
                echo "ðŸ”§ Attempting to reattach Internet Gateway $IGW_ID to VPC $VPC_ID"
                
                # Try to attach the IGW to the VPC
                aws ec2 attach-internet-gateway \
                  --internet-gateway-id $IGW_ID \
                  --vpc-id $VPC_ID \
                  && echo "âœ… Successfully reattached Internet Gateway!" \
                  || echo "âŒ Failed to reattach Internet Gateway"
                
                # Wait a moment for the change to propagate
                sleep 10
                
                # Check if route is now active
                ROUTE_STATE=$(aws ec2 describe-route-tables \
                  --route-table-ids $EXISTING_RT \
                  --query "RouteTables[0].Routes[?DestinationCidrBlock=='0.0.0.0/0' && starts_with(GatewayId, 'igw-')].State" \
                  --output text)
                echo "ðŸ” IGW route state after reattachment: $ROUTE_STATE"
              else
                echo "âœ… Existing instance subnet has active IGW route - connectivity should work"
              fi
            else
              echo "âŒ Existing instance subnet has NO IGW route - this explains the timeout!"
              echo "ðŸ” Routes in route table $EXISTING_RT:"
              aws ec2 describe-route-tables \
                --route-table-ids $EXISTING_RT \
                --query 'RouteTables[0].Routes[*].[DestinationCidrBlock,GatewayId,State]' \
                --output table
            fi
            
            # Get existing security group ID
            EXISTING_SG=$(aws ec2 describe-instances \
              --instance-ids $EXISTING_INSTANCE \
              --query 'Reservations[0].Instances[0].SecurityGroups[0].GroupId' \
              --output text)
            
            echo "ðŸ”§ Updating existing jump box security group: $EXISTING_SG"
            
            # Add ICMP rule if not already present
            aws ec2 authorize-security-group-ingress \
              --group-id $EXISTING_SG \
              --protocol icmp \
              --port -1 \
              --cidr 0.0.0.0/0 \
              || echo "ICMP rule may already exist"
            
            echo "ðŸ“‹ Reusing existing jump box with updated security group:"
            echo "   Instance ID: $EXISTING_INSTANCE"
            echo "   Public IP: $EXISTING_IP"
            echo "   Security Group: $EXISTING_SG (updated with ICMP)"
            echo "   âš ï¸  Note: Using existing instance - password may be different"
            
            # Store existing jump box info in outputs
            mkdir -p jump-box-outputs
            echo "INSTANCE_ID=$EXISTING_INSTANCE" > jump-box-outputs/jump-box-info.env
            echo "PUBLIC_IP=$EXISTING_IP" >> jump-box-outputs/jump-box-info.env
            echo "PASSWORD=<existing-password>" >> jump-box-outputs/jump-box-info.env
            echo "SECURITY_GROUP_ID=$EXISTING_SG" >> jump-box-outputs/jump-box-info.env
            echo "REGION=us-east-1" >> jump-box-outputs/jump-box-info.env
            
            echo "âœ… Reusing existing jump box with connectivity fixes applied"
            exit 0
          fi
          
          # Clean up any stopped/terminated instances for this environment
          echo "ðŸ—‘ï¸  Cleaning up old stopped instances..."
          OLD_INSTANCES=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=JumpBox-${{ env.ENVIRONMENT_SUFFIX }}" \
                     "Name=instance-state-name,Values=stopped,terminated" \
            --query 'Reservations[].Instances[].InstanceId' \
            --output text)
          
          if [ -n "$OLD_INSTANCES" ] && [ "$OLD_INSTANCES" != "None" ]; then
            echo "Found old instances to clean up: $OLD_INSTANCES"
            for instance_id in $OLD_INSTANCES; do
              echo "Terminating old instance: $instance_id"
              aws ec2 terminate-instances --instance-ids $instance_id || echo "Failed to terminate $instance_id"
            done
          fi
          
          echo "ðŸ†• Creating new jump box..."
          # Generate a random password for the jump box
          JUMP_BOX_PASSWORD=$(openssl rand -base64 16)
          echo "Generated password for jump box user"

          # Create IAM role for jump box with full AWS permissions
          aws iam create-role \
            --role-name "JumpBoxRole-${{ env.ENVIRONMENT_SUFFIX }}" \
            --assume-role-policy-document '{
              "Version": "2012-10-17",
              "Statement": [
                {
                  "Effect": "Allow",
                  "Principal": {
                    "Service": "ec2.amazonaws.com"
                  },
                  "Action": "sts:AssumeRole"
                }
              ]
            }' \
            --description "Full AWS permissions role for jump box" \
            || echo "Role may already exist"

          # Attach full AWS access policy
          aws iam attach-role-policy \
            --role-name "JumpBoxRole-${{ env.ENVIRONMENT_SUFFIX }}" \
            --policy-arn "arn:aws:iam::aws:policy/AdministratorAccess" \
            || echo "Policy may already be attached"

          # Create instance profile
          aws iam create-instance-profile \
            --instance-profile-name "JumpBoxProfile-${{ env.ENVIRONMENT_SUFFIX }}" \
            || echo "Instance profile may already exist"

          # Add role to instance profile
          aws iam add-role-to-instance-profile \
            --instance-profile-name "JumpBoxProfile-${{ env.ENVIRONMENT_SUFFIX }}" \
            --role-name "JumpBoxRole-${{ env.ENVIRONMENT_SUFFIX }}" \
            || echo "Role may already be added to instance profile"

          # Wait for IAM propagation
          sleep 30

          # Get default VPC and debug network configuration
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=is-default,Values=true" --query 'Vpcs[0].VpcId' --output text)
          echo "ðŸ” Default VPC ID: $VPC_ID"
          
          # List all Internet Gateways for this VPC
          echo "ðŸŒ Internet Gateways attached to VPC $VPC_ID:"
          aws ec2 describe-internet-gateways \
            --filters "Name=attachment.vpc-id,Values=$VPC_ID" \
            --query 'InternetGateways[*].[InternetGatewayId,State,Attachments[0].State]' \
            --output table
          
          # List all available (unattached) Internet Gateways
          echo "ðŸŒ Available (unattached) Internet Gateways:"
          aws ec2 describe-internet-gateways \
            --filters "Name=attachment.state,Values=detached" \
            --query 'InternetGateways[*].[InternetGatewayId,State]' \
            --output table
          
          # Check if the referenced IGW exists at all
          echo "ðŸ” Checking if IGW igw-01f3dc688f3cbea14 exists..."
          IGW_EXISTS=$(aws ec2 describe-internet-gateways \
            --internet-gateway-ids igw-01f3dc688f3cbea14 \
            --query 'InternetGateways | length(@)' \
            --output text 2>/dev/null || echo "0")
          
          if [ "$IGW_EXISTS" -gt 0 ]; then
            echo "âœ… IGW igw-01f3dc688f3cbea14 exists"
            IGW_STATE=$(aws ec2 describe-internet-gateways \
              --internet-gateway-ids igw-01f3dc688f3cbea14 \
              --query 'InternetGateways[0].Attachments[0].State' \
              --output text 2>/dev/null || echo "detached")
            echo "ðŸ” IGW state: $IGW_STATE"
          else
            echo "âŒ IGW igw-01f3dc688f3cbea14 does not exist - was deleted!"
          fi
          
          # List all subnets with their routing info
          echo "ðŸ—ºï¸  All subnets in VPC $VPC_ID:"
          aws ec2 describe-subnets \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'Subnets[*].[SubnetId,AvailabilityZone,MapPublicIpOnLaunch,CidrBlock]' \
            --output table
          
          # Find route tables and their routes
          echo "ðŸ›£ï¸  Route tables for VPC $VPC_ID:"
          ROUTE_TABLES=$(aws ec2 describe-route-tables \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'RouteTables[*].RouteTableId' \
            --output text)
          
          for rt_id in $ROUTE_TABLES; do
            echo "Route table $rt_id:"
            aws ec2 describe-route-tables \
              --route-table-ids $rt_id \
              --query 'RouteTables[0].Routes[*].[DestinationCidrBlock,GatewayId,State]' \
              --output table
            echo "Associated subnets:"
            aws ec2 describe-route-tables \
              --route-table-ids $rt_id \
              --query 'RouteTables[0].Associations[*].[SubnetId,Main]' \
              --output table
            echo "---"
          done
          
          # Find a truly public subnet (has route to IGW)
          echo "ðŸ” Finding public subnet with IGW route..."
          IGW_ID=$(aws ec2 describe-internet-gateways \
            --filters "Name=attachment.vpc-id,Values=$VPC_ID" \
            --query 'InternetGateways[0].InternetGatewayId' \
            --output text)
          echo "Internet Gateway ID: $IGW_ID"
          
          # Find subnet with route to this IGW
          PUBLIC_SUBNET=""
          for rt_id in $ROUTE_TABLES; do
            # Check if this route table has IGW route
            HAS_IGW_ROUTE=$(aws ec2 describe-route-tables \
              --route-table-ids $rt_id \
              --query "RouteTables[0].Routes[?GatewayId=='$IGW_ID' && DestinationCidrBlock=='0.0.0.0/0'] | length(@)" \
              --output text)
            
            if [ "$HAS_IGW_ROUTE" -gt 0 ]; then
              echo "âœ… Route table $rt_id has IGW route"
              # Get subnets associated with this route table
              ASSOCIATED_SUBNETS=$(aws ec2 describe-route-tables \
                --route-table-ids $rt_id \
                --query 'RouteTables[0].Associations[?SubnetId!=null].SubnetId' \
                --output text)
              
              if [ -n "$ASSOCIATED_SUBNETS" ]; then
                PUBLIC_SUBNET=$(echo $ASSOCIATED_SUBNETS | awk '{print $1}')
                echo "âœ… Found public subnet: $PUBLIC_SUBNET"
                break
              fi
            fi
          done
          
          # Use the public subnet we found, or fall back
          if [ -n "$PUBLIC_SUBNET" ] && [ "$PUBLIC_SUBNET" != "None" ]; then
            SUBNET_ID=$PUBLIC_SUBNET
            echo "ðŸŽ¯ Using verified public subnet: $SUBNET_ID"
          else
            echo "âš ï¸  No public subnet with IGW route found, using first available subnet"
            SUBNET_ID=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[0].SubnetId' --output text)
            echo "ðŸ“ Using subnet: $SUBNET_ID (may not have internet access)"
          fi

          # Create security group for jump box
          echo "Creating security group JumpBoxSG-${{ env.ENVIRONMENT_SUFFIX }}"
          if aws ec2 create-security-group \
            --group-name "JumpBoxSG-${{ env.ENVIRONMENT_SUFFIX }}" \
            --description "Security group for jump box with SSH and ICMP access" \
            --vpc-id $VPC_ID >/dev/null 2>&1; then
            echo "Created new security group"
          else
            echo "Security group already exists"
          fi
          
          # Get security group ID
          SECURITY_GROUP_ID=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=JumpBoxSG-${{ env.ENVIRONMENT_SUFFIX }}" \
            --query 'SecurityGroups[0].GroupId' \
            --output text)

          # Allow SSH access (port 22)
          aws ec2 authorize-security-group-ingress \
            --group-id $SECURITY_GROUP_ID \
            --protocol tcp \
            --port 22 \
            --cidr 0.0.0.0/0 \
            || echo "SSH rule may already exist"

          # Allow ICMP for ping (troubleshooting)
          aws ec2 authorize-security-group-ingress \
            --group-id $SECURITY_GROUP_ID \
            --protocol icmp \
            --port -1 \
            --cidr 0.0.0.0/0 \
            || echo "ICMP rule may already exist"

          # Get latest Amazon Linux 2 AMI ID
          AMI_ID=$(aws ec2 describe-images \
            --owners amazon \
            --filters "Name=name,Values=amzn2-ami-hvm-*-x86_64-gp2" "Name=state,Values=available" \
            --query 'Images | sort_by(@, &CreationDate) | [-1].ImageId' \
            --output text)

          echo "Using AMI: $AMI_ID"
          echo "Using VPC: $VPC_ID"
          echo "Using Subnet: $SUBNET_ID"
          echo "Using Security Group: $SECURITY_GROUP_ID"

          # Create user data script with password authentication
          USER_DATA=$(cat <<EOF
          #!/bin/bash
          yum update -y
          yum install -y aws-cli

          # Enable password authentication
          sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config
          systemctl restart sshd

          # Set password for ec2-user
          echo "ec2-user:${JUMP_BOX_PASSWORD}" | chpasswd

          # Install additional tools
          yum install -y git curl wget unzip vim htop

          # Install Docker
          amazon-linux-extras install docker -y
          service docker start
          usermod -a -G docker ec2-user

          # Install kubectl
          curl -LO "https://dl.k8s.io/release/\$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

          # Install Terraform
          yum install -y yum-utils
          yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
          yum -y install terraform

          # Install Python 3 and pip
          yum install -y python3 python3-pip

          # Create a welcome message
          cat > /etc/motd << 'WELCOME'
          =====================================
          ðŸš€ AWS Jump Box - Full Access Mode ðŸš€
          =====================================
          
          This jump box has full AWS administrative permissions.
          
          Available tools:
          - AWS CLI (configured with instance role)
          - Docker
          - kubectl
          - Terraform  
          - Python 3 & pip
          - Git, curl, wget, vim, htop
          
          AWS Region: us-east-1
          Instance Role: JumpBoxRole-${{ env.ENVIRONMENT_SUFFIX }}
          
          Happy cloud computing! â˜ï¸
          =====================================
          WELCOME

          echo "Jump box setup complete with full AWS permissions and password authentication"
          EOF
          )

          # Launch EC2 instance with public IP assignment
          INSTANCE_ID=$(aws ec2 run-instances \
            --image-id $AMI_ID \
            --count 1 \
            --instance-type t3.micro \
            --iam-instance-profile Name="JumpBoxProfile-${{ env.ENVIRONMENT_SUFFIX }}" \
            --security-group-ids $SECURITY_GROUP_ID \
            --subnet-id $SUBNET_ID \
            --associate-public-ip-address \
            --tag-specifications "ResourceType=instance,Tags=[{Key=Name,Value=JumpBox-${{ env.ENVIRONMENT_SUFFIX }}},{Key=Environment,Value=${{ env.ENVIRONMENT_SUFFIX }}},{Key=Purpose,Value=Jump Box with Full AWS Access}]" \
            --user-data "$USER_DATA" \
            --query 'Instances[0].InstanceId' \
            --output text)

          echo "Created jump box instance: $INSTANCE_ID"

          # Wait for instance to be running
          echo "Waiting for instance to be in running state..."
          aws ec2 wait instance-running --instance-ids $INSTANCE_ID

          # Get public IP address
          PUBLIC_IP=$(aws ec2 describe-instances \
            --instance-ids $INSTANCE_ID \
            --query 'Reservations[0].Instances[0].PublicIpAddress' \
            --output text)

          echo ""
          echo "======================================"
          echo "ðŸŽ‰ JUMP BOX CREATED SUCCESSFULLY! ðŸŽ‰"
          echo "======================================"
          echo ""
          echo "ðŸ“‹ Connection Details:"
          echo "   Instance ID: $INSTANCE_ID"
          echo "   Public IP: $PUBLIC_IP"
          echo "   Region: us-east-1"
          echo "   Security Group: $SECURITY_GROUP_ID"
          echo "   IAM Role: JumpBoxRole-${{ env.ENVIRONMENT_SUFFIX }}"
          echo ""
          echo "ðŸ” SSH Connection (Password):"
          echo "   ssh ec2-user@$PUBLIC_IP"
          echo "   Password: $JUMP_BOX_PASSWORD"
          echo ""
          echo "âš ï¸  IMPORTANT NOTES:"
          echo "   - This instance has FULL AWS administrative permissions"
          echo "   - Password authentication is enabled for convenience"
          echo "   - The instance is accessible from anywhere (0.0.0.0/0)"
          echo "   - Please secure this instance appropriately"
          echo ""
          echo "ðŸ› ï¸  Pre-installed Tools:"
          echo "   - AWS CLI (auto-configured with IAM role)"
          echo "   - Docker, kubectl, Terraform"
          echo "   - Python 3, Git, and other utilities"
          echo ""
          echo "Wait 3-5 minutes for the instance to fully initialize before connecting."
          echo "If connection still fails, check that the security group allows traffic"
          echo "and that the subnet has proper internet gateway routing."
          echo "======================================"

          # Store jump box info in outputs file
          mkdir -p jump-box-outputs
          echo "INSTANCE_ID=$INSTANCE_ID" > jump-box-outputs/jump-box-info.env
          echo "PUBLIC_IP=$PUBLIC_IP" >> jump-box-outputs/jump-box-info.env
          echo "PASSWORD=$JUMP_BOX_PASSWORD" >> jump-box-outputs/jump-box-info.env
          echo "SECURITY_GROUP_ID=$SECURITY_GROUP_ID" >> jump-box-outputs/jump-box-info.env
          echo "REGION=us-east-1" >> jump-box-outputs/jump-box-info.env
          
          # Create a connection info file
          cat > jump-box-outputs/connection-info.txt << EOF
          ====================================
          AWS Jump Box Connection Information
          ====================================
          
          Instance ID: $INSTANCE_ID
          Public IP: $PUBLIC_IP
          Region: us-east-1
          Username: ec2-user
          Password: $JUMP_BOX_PASSWORD
          
          SSH Command:
          ssh ec2-user@$PUBLIC_IP
          
          Note: Wait 2-3 minutes after instance creation before connecting
          to allow for full initialization.
          EOF
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}

      - name: Upload Jump Box Information
        uses: actions/upload-artifact@v4
        with:
          name: jump-box-info
          path: jump-box-outputs/

  build:
    name: Build
    runs-on: ubuntu-24.04
    needs: [detect-metadata, create-jump-box]
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    environment: dev

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'false'
          upload-artifacts: 'true'
          platform: ${{ needs.detect-metadata.outputs.platform }}
          language: ${{ needs.detect-metadata.outputs.language }}

      - name: Build
        run: ./scripts/build.sh

  synth:
    name: Synth
    runs-on: ubuntu-24.04
    needs: [detect-metadata, build]
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' }}
    environment: dev
    permissions:
      contents: read
    outputs:
      platform: ${{ needs.detect-metadata.outputs.platform }}
      language: ${{ needs.detect-metadata.outputs.language }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        if: ${{ needs.detect-metadata.outputs.platform == 'cdk' || needs.detect-metadata.outputs.platform == 'cdktf' }}
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'true'
          platform: ${{ needs.detect-metadata.outputs.platform }}
          language: ${{ needs.detect-metadata.outputs.language }}

      - name: Configure AWS
        if: ${{ needs.detect-metadata.outputs.platform == 'cdk' || needs.detect-metadata.outputs.platform == 'cdktf' }}
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Synth
        if: ${{ needs.detect-metadata.outputs.platform == 'cdk' || needs.detect-metadata.outputs.platform == 'cdktf' }}
        run: ./scripts/synth.sh
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
          TERRAFORM_STATE_BUCKET: ${{ env.TERRAFORM_STATE_BUCKET }}
          TERRAFORM_STATE_BUCKET_REGION: ${{ env.TERRAFORM_STATE_BUCKET_REGION }}
          PULUMI_BACKEND_URL: s3://${{ env.PULUMI_STATE_BUCKET }}?region=${{ env.PULUMI_BUCKET_REGION }}
          PULUMI_ORG: ${{ env.PULUMI_ORG }}

  deploy:
    name: Deploy
    runs-on: ubuntu-24.04
    needs: [detect-metadata, synth]
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    environment: dev

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'true'
          platform: ${{ needs.detect-metadata.outputs.platform }}
          language: ${{ needs.detect-metadata.outputs.language }}

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      # Terraform State Management Setup
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3

      # (Removed DynamoDB lock table step per PR review)

      - name: Bootstrap
        run: ./scripts/bootstrap.sh
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
          REPOSITORY: ${{ github.repository }}
          COMMIT_AUTHOR: ${{ github.event.head_commit.author.name || github.actor }}
          PULUMI_BACKEND_URL: s3://${{ env.PULUMI_STATE_BUCKET }}?region=${{ env.PULUMI_BUCKET_REGION }}
          PULUMI_ORG: ${{ env.PULUMI_ORG }}

      - name: Deploy
        run: ./scripts/deploy.sh
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
          REPOSITORY: ${{ github.repository }}
          COMMIT_AUTHOR: ${{ github.event.head_commit.author.name || github.actor }}
          TERRAFORM_STATE_BUCKET: ${{ env.TERRAFORM_STATE_BUCKET }}
          TERRAFORM_STATE_BUCKET_REGION: ${{ env.TERRAFORM_STATE_BUCKET_REGION }}
          TERRAFORM_STATE_BUCKET_KEY: ${{ env.ENVIRONMENT_SUFFIX }}
          PULUMI_BACKEND_URL: s3://${{ env.PULUMI_STATE_BUCKET }}?region=${{ env.PULUMI_BUCKET_REGION }}
          PULUMI_CONFIG_PASSPHRASE: ${{ secrets.PULUMI_CONFIG_PASSPHRASE }}
          PULUMI_ORG: ${{ env.PULUMI_ORG }}

      # Note: Deployment outputs are now collected by the deploy.sh script

      - name: Upload Deployment Outputs
        uses: actions/upload-artifact@v4
        with:
          name: cfn-outputs
          path: |
            cfn-outputs/
            cdk-stacks.json
  lint:
    name: Lint
    runs-on: ubuntu-24.04
    needs: [detect-metadata, build]
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    environment: qa
    outputs:
      platform: ${{ needs.detect-metadata.outputs.platform }}
      language: ${{ needs.detect-metadata.outputs.language }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'true'
          platform: ${{ needs.detect-metadata.outputs.platform }}
          language: ${{ needs.detect-metadata.outputs.language }}

      # Python setup is handled by the setup-environment action

      - name: Run Linting
        run: ./scripts/lint.sh

  unit-tests:
    name: Unit Testing
    runs-on: ubuntu-24.04
    needs: [detect-metadata, lint]
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    environment: qa

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'true'
          platform: ${{ needs.detect-metadata.outputs.platform }}
          language: ${{ needs.detect-metadata.outputs.language }}

      - name: Run unit tests
        run: ./scripts/unit-tests.sh

      - name: Prepare coverage reports for Java projects
        if: ${{ needs.detect-metadata.outputs.language == 'java' }}
        run: |
          echo "Preparing Java coverage reports for upload..."
          echo "Debug: Checking current directory structure:"
          ls -la

          echo "Debug: Checking build directory:"
          if [ -d "build" ]; then
            ls -la build/
            if [ -d "build/reports" ]; then
              echo "Reports directory contents:"
              ls -la build/reports/
              if [ -d "build/reports/jacoco" ]; then
                echo "JaCoCo directory contents:"
                find build/reports/jacoco -type f | head -10
              fi
            fi
          else
            echo "No build directory found"
          fi

          mkdir -p coverage

          # Try different possible locations for JaCoCo reports
          if [ -d "build/reports/jacoco/test/" ]; then
            cp -r build/reports/jacoco/test/* coverage/
            echo "âœ… Copied JaCoCo reports from build/reports/jacoco/test/ to coverage/ directory"
          elif [ -d "build/reports/jacoco/" ]; then
            cp -r build/reports/jacoco/* coverage/
            echo "âœ… Copied JaCoCo reports from build/reports/jacoco/ to coverage/ directory"
          else
            echo "âš ï¸ JaCoCo reports not found in expected locations"
            # Create a placeholder to avoid upload failure
            echo "No coverage reports generated" > coverage/no-reports.txt
          fi

          echo "Final coverage directory contents:"
          ls -la coverage/
      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: |
            coverage/
            cov.json

  # mocked-integration-tests:
  #   name: Mocked Integration Tests
  #   runs-on: ubuntu-24.04
  #   needs: unit-tests

  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@v4

  #     - name: Setup Environment
  #       uses: ./.github/actions/setup-environment
  #       with:
  #         node-version: ${{ env.NODE_VERSION }}
  #         download-artifacts: "true"

  #     - name: Run mocked integration tests
  #       run: npm run test:integration
  #       env:
  #         # Mock environment variables for testing
  #         API_GATEWAY_ENDPOINT: "https://mock-api.example.com/prod"
  #         READ_ONLY_API_KEY: "mock-readonly-key"
  #         ADMIN_API_KEY: "mock-admin-key"

  integration-tests-live:
    name: Integration Tests (Live)
    runs-on: ubuntu-24.04
    needs: [detect-metadata, lint, unit-tests, deploy]
    environment: qa
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'true'
          platform: ${{ needs.detect-metadata.outputs.platform }}

      - name: Download Deployment Outputs
        uses: actions/download-artifact@v4
        with:
          name: cfn-outputs
          path: .

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Run integration tests against live environment
        run: ./scripts/integration-tests.sh
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
          CI: '1' # Set CI for the entire step
          PULUMI_BACKEND_URL: s3://${{ env.PULUMI_STATE_BUCKET }}?region=${{ env.PULUMI_BUCKET_REGION }}
          PULUMI_CONFIG_PASSPHRASE: ${{ secrets.PULUMI_CONFIG_PASSPHRASE }}
          PULUMI_ORG: ${{ env.PULUMI_ORG }}

  claude-code-action:
    runs-on: ubuntu-24.04
    name: 'Claude Review'
    needs: [integration-tests-live]
    permissions:
      contents: write
      pull-requests: write
      issues: write
      id-token: write
      actions: read
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: refs/pull/${{ github.event.pull_request.number }}/head

      - name: Check for required documentation files
        id: docs-check
        run: |
          if [ ! -f "lib/PROMPT.md" ]; then
            echo "âŒ lib/PROMPT.md not found, exiting with failure"
            exit 1
          fi

          if [ ! -f "lib/MODEL_RESPONSE.md" ]; then
            echo "âŒ lib/MODEL_RESPONSE.md not found, exiting with failure"
            exit 1
          fi

          if [ ! -f "lib/IDEAL_RESPONSE.md" ]; then
            echo "âŒ lib/IDEAL_RESPONSE.md not found, exiting with failure"
            exit 1
          fi

          if [ ! -f "lib/MODEL_FAILURES.md" ]; then
            echo "âŒ lib/MODEL_FAILURES.md not found, exiting with failure"
            exit 1
          fi

      - name: Run Claude PR Review Action
        uses: anthropics/claude-code-action@v1
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          trigger_phrase: ''
          prompt: |
            CRITICAL: you MUST validate the latest PROMPT file format, and fail the check if it appears to be AI-generated rather than human-written:
            1. Identify all the PROMPT file (PROMPT.md, PROMPT2.md, PROMPT3.md, etc.) - always use the highest numbered one
            2. Read the PROMPT files
            3. Assess whether the content looks human-generated or AI-generated
            4. Look for signs of AI generation such as:
               - Emojis or special symbols (icons) used for formatting
               - Table structures
               - Overly formal or template-like language
               - Perfect formatting that suggests AI assistance
               - LLM Flavoured text, such as: "Here is a **comprehensive and high-level prompt** for your use..."
            5. If  any PROMPT file appears to be AI-generated rather than human-written, immediately FAIL the check by exiting with an error and fail the pipeline.
            6. Also validate that for each PROMPT file, there is a corresponding MODEL_RESPONSE file (e.g., PROMPT.md -> MODEL_RESPONSE.md, PROMPT2.md -> MODEL_RESPONSE2.md, etc.) and that it is well-formatted.

            CRITICAL: you MUST validate lib/IDEAL_RESPONSE.md, read .claude/agents/iac-infra-qa-trainer.md to understand how that file should have been created.
            1. Read lib/IDEAL_RESPONSE.md if it exists
            2. lib/IDEAL_RESPONSE.md should be well-formatted, markdown.
            3. Important: There should not be code outside proper code blocks (```python, ```bash, etc.)
            4. Every code file inside lib/ folder should be represented in lib/IDEAL_RESPONSE.md in code_blocks.
            5. There should not be references in the lib/IDEAL_RESPONSE.md to the QA process, unit tests or integration tests.
            6. If the lib/IDEAL_RESPONSE.md does not meet the above criteria, immediately FAIL the check by exiting with an error and fail the pipeline.

            Follow instructions in .claude/agents/iac-code-reviewer.md. Do not commit any changes, but allow the metadata.json file to be updated.
            Important: The Metadata Enhancement phase in .claude/agents/iac-code-reviewer.md is very important for this project. Make sure that the 
            metadata.json file is updated with the fields stated there. If the metadata.json file after the review does not contain the fields:
            - subtask
            - subject_labels
            - training_quality
            - aws_services
            then immediately FAIL the check by exiting with an error and fail the pipeline.

            This validation is mandatory and must be completed before any other review activities.
          claude_args: |
            --allowedTools Bash,Read,Write,Edit,MultiEdit,Glob,Grep,LS,Task,TodoWrite

      - name: Upload updated metadata.json
        uses: actions/upload-artifact@v4
        with:
          name: updated-metadata
          path: metadata.json
          if-no-files-found: warn

  cleanup:
    name: Cleanup (Destroy Resources)
    runs-on: ubuntu-24.04
    needs: [detect-metadata, integration-tests-live, claude-code-action]
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    environment: stage

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'false'
          platform: ${{ needs.detect-metadata.outputs.platform }}
          language: ${{ needs.detect-metadata.outputs.language }}

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Cleanup Jump Box Resources
        run: |
          echo "ðŸ§¹ Cleaning up jump box resources for environment: ${{ env.ENVIRONMENT_SUFFIX }}"
          
          # Terminate jump box instances
          JUMP_BOX_INSTANCES=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=JumpBox-${{ env.ENVIRONMENT_SUFFIX }}" \
                     "Name=instance-state-name,Values=running,pending,stopped" \
            --query 'Reservations[].Instances[].InstanceId' \
            --output text)
          
          if [ -n "$JUMP_BOX_INSTANCES" ] && [ "$JUMP_BOX_INSTANCES" != "None" ]; then
            echo "Terminating jump box instances: $JUMP_BOX_INSTANCES"
            for instance_id in $JUMP_BOX_INSTANCES; do
              aws ec2 terminate-instances --instance-ids $instance_id || echo "Failed to terminate $instance_id"
            done
            
            # Wait for instances to terminate before cleaning up security group
            echo "Waiting for instances to terminate..."
            for instance_id in $JUMP_BOX_INSTANCES; do
              aws ec2 wait instance-terminated --instance-ids $instance_id || echo "Timeout waiting for $instance_id"
            done
          fi
          
          # Delete security group
          SECURITY_GROUP_ID=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=JumpBoxSG-${{ env.ENVIRONMENT_SUFFIX }}" \
            --query 'SecurityGroups[0].GroupId' \
            --output text)
          
          if [ "$SECURITY_GROUP_ID" != "None" ] && [ -n "$SECURITY_GROUP_ID" ]; then
            echo "Deleting security group: $SECURITY_GROUP_ID"
            aws ec2 delete-security-group --group-id $SECURITY_GROUP_ID || echo "Failed to delete security group"
          fi
          
          # Clean up IAM resources
          echo "Cleaning up IAM resources..."
          
          # Remove role from instance profile
          aws iam remove-role-from-instance-profile \
            --instance-profile-name "JumpBoxProfile-${{ env.ENVIRONMENT_SUFFIX }}" \
            --role-name "JumpBoxRole-${{ env.ENVIRONMENT_SUFFIX }}" \
            || echo "Role may not be attached to instance profile"
          
          # Delete instance profile
          aws iam delete-instance-profile \
            --instance-profile-name "JumpBoxProfile-${{ env.ENVIRONMENT_SUFFIX }}" \
            || echo "Instance profile may not exist"
          
          # Detach policy from role
          aws iam detach-role-policy \
            --role-name "JumpBoxRole-${{ env.ENVIRONMENT_SUFFIX }}" \
            --policy-arn "arn:aws:iam::aws:policy/AdministratorAccess" \
            || echo "Policy may not be attached"
          
          # Delete role
          aws iam delete-role \
            --role-name "JumpBoxRole-${{ env.ENVIRONMENT_SUFFIX }}" \
            || echo "Role may not exist"
          
          echo "âœ… Jump box cleanup completed"

      - name: Destroy Resources (cleanup any leftover resources)
        run: ./scripts/destroy.sh
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
          TERRAFORM_STATE_BUCKET: ${{ env.TERRAFORM_STATE_BUCKET }}
          TERRAFORM_STATE_BUCKET_REGION: ${{ env.TERRAFORM_STATE_BUCKET_REGION }}
          TERRAFORM_STATE_BUCKET_KEY: ${{ env.ENVIRONMENT_SUFFIX }}
          PULUMI_BACKEND_URL: s3://${{ env.PULUMI_STATE_BUCKET }}?region=${{ env.PULUMI_BUCKET_REGION }}
          PULUMI_ORG: ${{ env.PULUMI_ORG }}

  archive-folders:
    name: Archive Folders and Reset Repository
    runs-on: ubuntu-24.04
    needs: [detect-metadata, cleanup, claude-code-action]
    environment: release
    if: ${{ github.event_name == 'pull_request' && github.event.action != 'closed' && !contains(github.event.head_commit.message, '[skip-jobs]') }}
    permissions:
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          repository: ${{ github.repository }}
          ref: ${{ github.head_ref }}

      - name: Download coverage reports
        uses: actions/download-artifact@v4
        with:
          name: coverage-reports
          path: coverage/

      - name: Download updated metadata
        uses: actions/download-artifact@v4
        with:
          name: updated-metadata
          path: .

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Archive folders and reset repository
        run: |
          # Configure Git
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"

          # Use metadata from detect-metadata job
          PLATFORM="${{ needs.detect-metadata.outputs.platform }}"
          LANGUAGE="${{ needs.detect-metadata.outputs.language }}"
          echo "Using platform: $PLATFORM, language: $LANGUAGE"

          # Update metadata.json with coverage information and commit author
          # Get commit author from GitHub context
          COMMIT_AUTHOR="${{ github.event.pull_request.user.login || github.actor }}"
          echo "Commit author: $COMMIT_AUTHOR"

          # Extract coverage percentages based on language
          if [ "$LANGUAGE" = "py" ]; then
            echo "Extracting Python coverage from cov.json..."
            if [ -f "cov.json" ]; then
              LINES_COVERAGE=$(jq -r '.totals.percent_covered' cov.json)
              # Calculate branch coverage percentage
              BRANCHES_NUM=$(jq -r '.totals.num_branches' cov.json)
              if [ "$BRANCHES_NUM" = "0" ] || [ "$BRANCHES_NUM" = "null" ]; then
                # No branches to cover, set to 100%
                BRANCHES_COVERAGE=100
              else
                COVERED_BRANCHES=$(jq -r '.totals.covered_branches' cov.json)
                # Calculate percentage: (covered_branches / num_branches) * 100
                BRANCHES_COVERAGE=$(awk "BEGIN {print ($COVERED_BRANCHES / $BRANCHES_NUM) * 100}")
              fi
            else
              echo "Warning: cov.json not found, setting default coverage values"
              LINES_COVERAGE=100
              BRANCHES_COVERAGE=100
            fi
          elif [ "$LANGUAGE" = "go" ]; then
            echo "Extracting Go coverage from coverage-summary.json..."
            if [ -f "coverage/coverage-summary.json" ]; then
              LINES_COVERAGE=$(jq -r '.total.lines.pct' coverage/coverage-summary.json)
              BRANCHES_COVERAGE=$(jq -r '.total.branches.pct' coverage/coverage-summary.json)
            else
              echo "Warning: Go coverage-summary.json not found, setting default coverage values"
              LINES_COVERAGE=100
              BRANCHES_COVERAGE=100
            fi
          else
            echo "Extracting TypeScript/JavaScript coverage from coverage-summary.json..."
            if [ -f "coverage/coverage-summary.json" ]; then
              LINES_COVERAGE=$(jq -r '.total.lines.pct' coverage/coverage-summary.json)
              BRANCHES_COVERAGE=$(jq -r '.total.branches.pct' coverage/coverage-summary.json)
            else
              echo "Warning: coverage-summary.json not found, setting default coverage values"
              LINES_COVERAGE=100
              BRANCHES_COVERAGE=100
            fi
          fi

          # If coverage is "Unknown", set to 100
          if [ "$LINES_COVERAGE" = "Unknown" ]; then
            LINES_COVERAGE=100
          fi
          if [ "$BRANCHES_COVERAGE" = "Unknown" ]; then
            BRANCHES_COVERAGE=100
          fi

          echo "Lines coverage: $LINES_COVERAGE%"
          echo "Branches coverage: $BRANCHES_COVERAGE%"

          # Update metadata.json with coverage information and commit author (assuming it always exists)
          jq --arg lines "$LINES_COVERAGE" --arg branches "$BRANCHES_COVERAGE" --arg author "$COMMIT_AUTHOR" '. + {coverage: {lines: ($lines|tonumber), branches: ($branches|tonumber)}, author: $author}' metadata.json > metadata.json.tmp
          mv metadata.json.tmp metadata.json
          echo "Updated metadata.json with coverage information and commit author"

          cat metadata.json

          echo "Current directory contents before archiving:"
          ls -la

          # Get Docker S3 location from the previous step and add it to metadata.json
          DOCKER_S3_LOCATION="${DOCKER_S3_LOCATION}"
          echo "Docker S3 location from previous step: $DOCKER_S3_LOCATION"

          # Add the dockerS3Location to metadata.json
          jq --arg location "$DOCKER_S3_LOCATION" '. + {dockerS3Location: $location}' metadata.json > metadata.json.tmp
          mv metadata.json.tmp metadata.json
          echo "Updated metadata.json with dockerS3Location: $DOCKER_S3_LOCATION"

          # Create archive directory with platform, language, and PR number
          ARCHIVE_DIR="archive/${PLATFORM}-${LANGUAGE}/Pr${{ github.event.number }}"
          mkdir -p "$ARCHIVE_DIR"
          echo "Created archive directory: $ARCHIVE_DIR"

            # Define list of paths to move to archive
            PATHS_TO_ARCHIVE=(
            "lib"
            "bin" 
            "test"
            "tests"
            "cdk.json"
            "metadata.json"
            "tap.py"
            "tap.go"
            "setup.js"
            "cdktf.json"
            "Pulumi.yaml"
            )

          # Move paths to archive
          for path in "${PATHS_TO_ARCHIVE[@]}"; do
          if [[ -d "$path" || -f "$path" ]]; then
            mv "$path" "$ARCHIVE_DIR"/
            echo "Moved $path to archive"
          else
            echo "Path $path not found, skipping"
          fi
          done

          # Check if there are changes to commit
          if git diff --quiet && git diff --cached --quiet; then
            echo "No changes to commit - no folders found to archive"
          else
            git add -A
            git commit -m "Archive build artifacts for PR ${{ github.event.number }} [skip-jobs]"
            git push origin HEAD
            echo "Archive committed to current branch"
          fi

  upload-task-to-s3:
    name: Upload Task to S3
    runs-on: ubuntu-24.04
    environment: release
    if: ${{ github.event_name == 'pull_request' && github.event.pull_request.merged == true }}
    permissions:
      contents: read

    steps:
      - name: Checkout merged code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          repository: ${{ github.repository }}
          ref: ${{ github.event.pull_request.merge_commit_sha }}

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Extract platform and language for S3 upload
        id: extract-metadata
        run: |
          # We need to find the specific archive folder for this PR
          # Since we don't know platform/language yet, we'll search for the PR-specific folder
          PR_FOLDER_PATTERN="archive/*/Pr${{ github.event.number }}"

          # Find the archive folder for this specific PR
          ARCHIVE_FOLDER=""
          for folder in $PR_FOLDER_PATTERN; do
            if [ -d "$folder" ]; then
              ARCHIVE_FOLDER="$folder"
              break
            fi
          done

          if [ -n "$ARCHIVE_FOLDER" ] && [ -f "$ARCHIVE_FOLDER/metadata.json" ]; then
            echo "Found archive folder: $ARCHIVE_FOLDER"
            echo "Found metadata.json at: $ARCHIVE_FOLDER/metadata.json"
            PLATFORM=$(jq -r '.platform // "unknown"' "$ARCHIVE_FOLDER/metadata.json")
            LANGUAGE=$(jq -r '.language // "unknown"' "$ARCHIVE_FOLDER/metadata.json")
          else
            echo "Warning: Could not find archive folder or metadata.json for PR ${{ github.event.number }}"
            echo "Available archive folders:"
            ls -la archive/ || echo "No archive directory found"
            PLATFORM="unknown"
            LANGUAGE="unknown"
          fi
          echo "platform=$PLATFORM" >> $GITHUB_OUTPUT
          echo "language=$LANGUAGE" >> $GITHUB_OUTPUT
          echo "Extracted platform: $PLATFORM, language: $LANGUAGE"

      - name: Upload archive folder to S3
        run: |
          PLATFORM="${{ steps.extract-metadata.outputs.platform }}"
          LANGUAGE="${{ steps.extract-metadata.outputs.language }}"
          S3_PREFIX="${PLATFORM}-${LANGUAGE}/Pr${{ github.event.number }}"

          echo "Uploading archive folder to S3 bucket: ${{ env.S3_RELEASE_BUCKET_NAME }}"
          echo "S3 prefix: $S3_PREFIX"

          # Check if archive folder exists
          if [ -d "archive/${S3_PREFIX}" ]; then
            # Upload the archive folder contents to S3, preserving folder structure
            aws s3 sync "archive/${S3_PREFIX}/" "s3://${{ env.S3_RELEASE_BUCKET_NAME }}/${S3_PREFIX}/" \
              --delete \
              --exclude "*.git*" \
              --exclude "node_modules/*"
            echo "Successfully uploaded archive to S3"
          else
            echo "Archive folder archive/${S3_PREFIX} not found, nothing to upload"
            exit 1
          fi
      - name: Verify S3 upload
        run: |
          PLATFORM="${{ steps.extract-metadata.outputs.platform }}"
          LANGUAGE="${{ steps.extract-metadata.outputs.language }}"
          S3_PREFIX="${PLATFORM}-${LANGUAGE}/Pr${{ github.event.number }}"

          echo "Verifying S3 upload for prefix: $S3_PREFIX"
          aws s3 ls "s3://${{ env.S3_RELEASE_BUCKET_NAME }}/${S3_PREFIX}/" --recursive || echo "No files found or access denied"

  cleanup-pr:
    name: Cleanup (PR Closed)
    runs-on: ubuntu-24.04
    # This job should run regardless of the [skip-jobs] tag since it's for cleanup
    if: github.event_name == 'pull_request' && github.event.action == 'closed' && !github.event.pull_request.merged
    environment: qa

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Environment
        uses: ./.github/actions/setup-environment
        with:
          node-version: ${{ env.NODE_VERSION }}
          download-artifacts: 'false'

      - name: Configure AWS
        uses: ./.github/actions/configure-aws
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Destroy Resources (if resources exist)
        run: ./scripts/destroy.sh
        env:
          ENVIRONMENT_SUFFIX: ${{ env.ENVIRONMENT_SUFFIX }}
          TERRAFORM_STATE_BUCKET: ${{ env.TERRAFORM_STATE_BUCKET }}
          TERRAFORM_STATE_BUCKET_REGION: ${{ env.TERRAFORM_STATE_BUCKET_REGION }}
          PULUMI_BACKEND_URL: s3://${{ env.PULUMI_STATE_BUCKET }}?region=${{ env.PULUMI_BUCKET_REGION }}
          PULUMI_ORG: ${{ env.PULUMI_ORG }}

  # TODO: For future use
  # ? Need fine-grained Personal Access Token from repo owner
  # semantic-release:
  #   name: Semantic Release
  #   runs-on: ubuntu-24.04
  #   if: github.event_name == 'push' && github.ref == 'refs/heads/main'
  #   permissions:
  #     contents: write
  #     issues: write
  #     pull-requests: write

  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@v4
  #       with:
  #         token: ${{ secrets.GITHUB_TOKEN }}
  #         fetch-depth: 0

  #     - name: Setup Environment
  #       uses: ./.github/actions/setup-environment
  #       with:
  #         node-version: ${{ env.NODE_VERSION }}
  #         download-artifacts: 'false'

  #     - name: Run semantic-release
  #       run: npm run release
  #       env:
  #         GITHUB_TOKEN: ${{ secrets.SEMANTIC_RELEASE_TOKEN }}
