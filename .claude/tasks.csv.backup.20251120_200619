task_id,status,platform,language,difficulty,subtask,subject_labels,problem,background,environment,constraints
y2r2f5,in_progress,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to migrate a payment processing system from on-premises infrastructure to AWS using a blue-green deployment approach. The configuration must: 1. Set up separate VPCs for blue (current) and green (new) environments connected via Transit Gateway 2. Deploy Aurora PostgreSQL clusters in both environments with automated failover capabilities 3. Configure ECS Fargate services with task definitions for payment API, transaction processor, and reporting service 4. Implement Application Load Balancers with path-based routing and health checks for each service 5. Create S3 buckets with versioning and lifecycle policies for transaction logs and compliance documents 6. Set up DynamoDB tables with global secondary indexes for session management and API rate limiting 7. Configure AWS WAF rules to protect against SQL injection and cross-site scripting attacks 8. Implement CloudWatch dashboards showing transaction metrics, error rates, and system performance 9. Create Lambda functions for data migration tasks that sync from blue to green environment 10. Set up SNS topics and CloudWatch alarms for monitoring migration progress and system health 11. Configure Route 53 weighted routing policies for gradual traffic shifting between environments 12. Implement IAM roles with least privilege access for all services and cross-account assumptions. Expected output: A complete Pulumi program that creates both blue and green environments with all specified services, monitoring, and security configurations. The program should include stack outputs for ALB endpoints, database connection strings, and a migration status dashboard URL. The solution must support incremental data sync and allow rollback to the blue environment if issues arise during migration.","A fintech startup is migrating their payment processing infrastructure from a legacy on-premises datacenter to AWS. The existing system handles 50,000 transactions daily with strict compliance requirements for PCI-DSS. The migration must be performed with zero downtime using a blue-green deployment strategy.","Blue-green deployment infrastructure in us-east-1 region for migrating payment processing system from on-premises to AWS. Uses Aurora PostgreSQL 14.6 for transaction data, ECS Fargate for containerized microservices, Application Load Balancer with WAF protection, S3 for encrypted document storage, and DynamoDB for session management. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI v2 configured with appropriate IAM permissions. Multi-AZ VPC with private subnets across 3 availability zones, Transit Gateway for network isolation, VPC endpoints for AWS services. Staging environment mirrors production with reduced capacity for validation testing before cutover.","[""Use AWS Systems Manager Parameter Store for non-sensitive configuration"", ""Implement AWS Secrets Manager rotation for database credentials every 30 days"", ""Configure S3 bucket policies to enforce SSL/TLS for all requests"", ""Configure VPC endpoints for S3 and DynamoDB to avoid internet routing"", ""Use AWS Transit Gateway for network isolation between environments"", ""Implement AWS Config rules to monitor compliance with security policies"", ""Implement CloudWatch Logs retention of 90 days for audit compliance"", ""Use AWS WAF with rate limiting rules on the ALB"", ""Deploy Aurora PostgreSQL with encryption at rest using customer-managed KMS keys"", ""Deploy in at least 3 availability zones for high availability""]"
m1x1r1,in_progress,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy a payment processing API infrastructure. The configuration must: 1. Set up a VPC with 3 availability zones, each containing public, private, and database subnets with appropriate route tables and NAT instances. 2. Deploy an ALB in public subnets with AWS WAF integration and custom rule groups for OWASP Top 10 protection. 3. Configure ECS Fargate service running containerized API with auto-scaling based on target tracking (CPU 70%, Memory 80%). 4. Create RDS Aurora PostgreSQL cluster with one writer and two reader instances, encrypted with customer-managed KMS keys. 5. Implement API Gateway with mutual TLS authentication, request throttling (1000 req/sec), and usage plans. 6. Deploy Lambda functions for async payment processing with SQS dead letter queues and error handling. 7. Configure S3 buckets for document storage with lifecycle policies, versioning, and cross-region replication. 8. Set up Secrets Manager for database credentials and API keys with automatic rotation Lambda functions. 9. Create CloudWatch dashboards with custom metrics for transaction processing time and success rates. 10. Implement VPC endpoints for S3, DynamoDB, ECR, and Secrets Manager to avoid internet traffic. Expected output: A fully functional CDK application that deploys the entire infrastructure stack with proper security configurations, monitoring, and compliance controls. The stack should output the ALB DNS name, API Gateway endpoint, and CloudWatch dashboard URL.","A fintech startup needs to deploy their payment processing API with strict compliance requirements. The application must handle sensitive financial data with end-to-end encryption and maintain PCI DSS compliance while supporting 10,000+ concurrent transactions.","Production-grade infrastructure deployed in us-east-1 with multi-AZ configuration for high availability. Core services include ECS Fargate for containerized API services, RDS Aurora PostgreSQL with read replicas, API Gateway for external access, Lambda for async processing, S3 for encrypted document storage. VPC spans 3 availability zones with public subnets for ALB, private subnets for compute resources, and database subnets for RDS. Requires Python 3.9+, AWS CDK 2.100+, Docker for container builds. NAT instances in each AZ for outbound connectivity. VPC endpoints configured for S3, DynamoDB, and ECR to minimize data transfer costs.","[""VPC endpoints must be used for all AWS service communications to avoid internet routing"", ""RDS instances must use IAM database authentication instead of passwords"", ""All data at rest must use customer-managed KMS keys with automatic rotation"", ""API Gateway must enforce mutual TLS authentication for all endpoints"", ""ALB must use AWS WAF with custom rule groups for SQL injection and XSS protection"", ""Lambda functions must have execution roles with least-privilege permissions"", ""Secrets Manager must store all database credentials with automatic rotation every 30 days"", ""CloudWatch Logs must retain audit logs for exactly 7 years"", ""S3 buckets must block all public access and use SSE-KMS encryption"", ""ECS tasks must run in private subnets with no direct internet access""]"
i7p9q3,error,CDK,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CDK TypeScript program to implement a multi-region disaster recovery architecture. The configuration must: 1. Set up DynamoDB global tables spanning us-east-1 and us-east-2 with on-demand billing and point-in-time recovery. 2. Configure an Aurora Global Database with a writer cluster in us-east-1 and reader cluster in us-east-2, using db.r5.large instances. 3. Deploy identical Lambda functions in both regions that read from DynamoDB and Aurora, with 512MB memory and 30-second timeout. 4. Create S3 buckets in both regions with cross-region replication, versioning enabled, and RTC for objects under 128MB. 5. Implement Route 53 health checks monitoring ALB endpoints in both regions with failover routing (primary: us-east-1, secondary: us-east-2). 6. Set up EventBridge with global endpoints that route custom events to the active region's event bus. 7. Configure AWS Backup plans in both regions to create daily EBS snapshots with cross-region copy to the opposite region. 8. Deploy CloudWatch dashboards in both regions showing cross-region metrics for Lambda invocations, DynamoDB capacity, and Aurora connections. 9. Use Systems Manager Parameter Store to store configuration values with automated cross-region replication. 10. Create Step Functions state machines in both regions with identical definitions for order processing workflows. Expected output: CDK stacks that deploy primary infrastructure in us-east-1 and secondary infrastructure in us-east-2, with automated failover capabilities and RPO under 5 minutes.","A financial services company requires a disaster recovery solution for their critical transaction processing system. After experiencing a 4-hour outage due to a regional failure, they need to implement a multi-region active-passive architecture that can failover within 15 minutes while maintaining data consistency.","Multi-region disaster recovery infrastructure deployed across us-east-1 (primary) and us-east-2 (secondary). Uses DynamoDB global tables for session data, RDS Aurora Global Database for transactional data, Lambda functions for business logic, S3 with cross-region replication for static assets. Requires CDK 2.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate IAM permissions. VPCs in both regions with private subnets for databases, public subnets for ALBs. Transit Gateway for cross-region VPC peering. Route 53 hosted zone for DNS failover management.","[""Use Route 53 health checks with failover routing policies for automatic DNS failover"", ""Configure RDS Aurora Global Database with one primary cluster and one secondary cluster"", ""Use EventBridge global endpoints to route events to the active region automatically"", ""Set up CloudWatch cross-region dashboards to monitor both regions from a single pane"", ""Use Systems Manager Parameter Store with cross-region replication for configuration data"", ""Configure S3 bucket replication with RTC (Replication Time Control) for objects under 128 MB"", ""Implement DynamoDB global tables for multi-region data replication with point-in-time recovery enabled"", ""Implement Step Functions with cross-region state machine replication for workflow consistency"", ""Implement AWS Backup for cross-region backups of EBS volumes attached to EC2 instances"", ""Deploy Lambda functions in both regions with identical configurations and environment variables""]"
g3c0d6,error,Pulumi,Python,expert,Application Deployment,Web Application Deployment,"Create a Pulumi Python program to deploy a fraud detection API infrastructure. The configuration must: 1. Set up a VPC with 3 availability zones, each containing public and private subnets 2. Deploy an ECS cluster with Fargate launch type and a task definition for the fraud-api container (image: fraud-api:latest) with 1 vCPU and 2GB memory 3. Create an Application Load Balancer in public subnets with path-based routing to /api/* endpoints 4. Configure API Gateway REST API with usage plans and API key authentication forwarding requests to the ALB 5. Provision Aurora Serverless v2 PostgreSQL cluster with automated backups and encryption at rest 6. Set up Secrets Manager secrets for database credentials and API configuration with 30-day rotation 7. Implement CloudWatch dashboards showing API response times, error rates, and database connections 8. Configure WAF web ACL attached to API Gateway with rate-based rules and geo-blocking for high-risk countries 9. Create IAM roles with least privilege access for ECS tasks to access Secrets Manager and Aurora 10. Set up CloudWatch alarms for API latency, ECS task health, and Aurora connection count. Expected output: Complete Pulumi program generating all infrastructure components with proper security configurations, monitoring setup, and auto-scaling policies. The program should use Pulumi's Component Resources to organize related resources and export the API Gateway endpoint URL, ALB DNS name, and CloudWatch dashboard URL.",A fintech startup needs to deploy their fraud detection API that processes transaction data in real-time. The system requires low-latency response times and must handle variable traffic patterns throughout the day while maintaining PCI DSS compliance standards.,"Deploy infrastructure in us-east-1 region using ECS Fargate for containerized API services, Aurora Serverless v2 PostgreSQL for transaction data storage, API Gateway for request management, and Application Load Balancer for traffic distribution. Requires Pulumi 3.x with Python 3.9+, AWS CLI configured with appropriate IAM permissions. VPC setup includes 3 availability zones with public subnets for ALB and private subnets for ECS tasks and Aurora. NAT Gateway enables outbound internet access for containers. CloudWatch Logs for centralized logging and X-Ray for distributed tracing.","[""Store API keys and database credentials in AWS Secrets Manager with automatic rotation"", ""Use Aurora Serverless v2 PostgreSQL with minimum 0.5 ACUs and maximum 4 ACUs"", ""Set up CloudWatch alarms for response times exceeding 200ms"", ""Configure ECS service auto-scaling based on average CPU utilization above 70%"", ""Use AWS Certificate Manager for SSL/TLS certificates with automatic renewal"", ""Enable VPC flow logs and store them in S3 with lifecycle policies"", ""Use AWS Fargate for ECS tasks to avoid EC2 instance management"", ""Configure WAF rules to block SQL injection and XSS attempts"", ""Implement API Gateway request throttling at 1000 requests per second per API key"", ""Implement blue-green deployment capability using Application Load Balancer target groups""]"
j1j4i1,in_progress,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi Go program to deploy a highly available payment processing web application. The configuration must: 1. Set up a VPC with 3 public subnets, 3 private subnets, and 3 database subnets across different AZs. 2. Deploy an Aurora PostgreSQL Serverless v2 cluster with automatic scaling between 0.5 and 2 ACUs, encrypted with a customer-managed KMS key. 3. Create an ECS cluster with Fargate tasks running a containerized Go application from ECR, with auto-scaling based on CPU > 70% OR memory > 80%. 4. Configure an Application Load Balancer with AWS WAF enabled, including custom rules to block requests with SQL keywords in query parameters. 5. Set up CloudWatch Log Groups with 7-year retention and export to S3 for long-term storage. 6. Implement blue-green deployment using target groups and weighted routing. 7. Create Lambda functions for database backup verification that run daily and alert on failures. 8. Configure API Gateway with usage plans and API keys for rate limiting (1000 requests per minute per key). 9. Set up X-Ray tracing for all services with sampling rate of 10% for production traffic. 10. Create CloudWatch dashboards showing real-time metrics for response times, error rates, and transaction volumes. Expected output: A complete Pulumi program that provisions all infrastructure components with proper error handling, resource dependencies, and outputs including the ALB DNS name, API Gateway URL, and CloudWatch dashboard URL. The program should use Pulumi's automation API features for programmatic deployment control.","A fintech startup needs to deploy their payment processing web application with strict compliance requirements for data isolation and audit logging. The application requires real-time transaction processing with sub-second response times and must handle 10,000 concurrent users during peak hours.","Production environment deployed in us-east-1 across 3 availability zones using ECS Fargate for containerized Go microservices, Aurora PostgreSQL Serverless v2 for the database tier, and Application Load Balancer with AWS WAF integration. Infrastructure requires Pulumi 3.x with Go SDK, Docker for container builds, and AWS CLI configured with appropriate IAM permissions. VPC configured with public subnets for ALB, private subnets for ECS tasks, and database subnets for Aurora. NAT Gateways provide outbound internet access for private resources. CloudWatch Logs with 7-year retention for compliance logging.","[""Implement request tracing across all services with correlation IDs"", ""Enable AWS WAF with custom rules for SQL injection and XSS prevention"", ""All compute resources must run in private subnets with no direct internet access"", ""Implement blue-green deployment capability for zero-downtime updates"", ""Application logs must be retained for exactly 7 years for compliance"", ""Use separate security groups for each tier (web, app, database) with minimal permissions"", ""Each API endpoint must have rate limiting configured to prevent abuse"", ""All database connections must use SSL/TLS encryption with certificate validation"", ""Configure auto-scaling based on both CPU and memory metrics with custom thresholds"", ""Database backups must be encrypted at rest using customer-managed KMS keys""]"
c8m2f3,done,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a secure payment processing web application. The configuration must: 1. Create a VPC with 3 public and 3 private subnets across 3 availability zones. 2. Set up an Application Load Balancer with HTTPS listeners using ACM certificates. 3. Deploy an ECS Fargate cluster running the payment application from ECR. 4. Configure RDS Aurora PostgreSQL with Multi-AZ deployment and encryption. 5. Create S3 buckets for static assets with CloudFront distribution. 6. Implement Secrets Manager for database credentials with 30-day rotation. 7. Configure CloudWatch Log Groups with 7-year retention for compliance. 8. Set up VPC flow logs capturing all traffic to a dedicated S3 bucket. 9. Create IAM roles with minimal permissions for ECS tasks and RDS. 10. Implement security groups allowing only HTTPS from internet and restricting database access to ECS tasks. 11. Configure auto-scaling for ECS services based on CPU utilization. 12. Add CloudWatch alarms for high CPU, memory usage, and failed health checks. Expected output: A complete Pulumi TypeScript program that creates all resources with proper error handling, exports key resource ARNs and endpoints, and follows TypeScript best practices with strong typing throughout.",A fintech startup needs to deploy their payment processing web application with strict compliance requirements. The application handles sensitive financial data and must meet PCI DSS standards while maintaining high availability across multiple availability zones.,"Production deployment in us-east-1 region across 3 availability zones for high availability. Infrastructure includes VPC with public and private subnets, Application Load Balancer for traffic distribution, ECS Fargate for containerized application hosting, RDS Aurora PostgreSQL Multi-AZ for database, S3 for static assets and logs storage, Secrets Manager for credentials, CloudWatch for monitoring and logging. Requires Pulumi CLI 3.x, Node.js 18+, TypeScript 5.x, AWS CLI configured with appropriate IAM permissions. Network architecture includes NAT gateways in each AZ for outbound connectivity from private subnets.","[""All S3 buckets must have versioning enabled and lifecycle policies for compliance"", ""Security groups must follow least-privilege with explicit port allowlists"", ""VPC flow logs must be enabled and sent to a dedicated S3 bucket"", ""All RDS instances must use encrypted storage with customer-managed KMS keys"", ""Database credentials must be stored in AWS Secrets Manager with automatic rotation"", ""ECS tasks must run in private subnets with no direct internet access"", ""Application load balancer must terminate SSL/TLS with ACM certificates"", ""CloudWatch Logs retention must be set to 7 years for audit trails"", ""All resources must be tagged with Environment, Project, and CostCenter tags"", ""ECS task definitions must use specific image tags, not 'latest'""]"
w6j2x6,error,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi Go program to orchestrate a phased migration of payment processing infrastructure from on-premises to AWS. The configuration must: 1. Set up a new VPC with public and private subnets across 3 AZs for the target environment. 2. Create an RDS Aurora PostgreSQL cluster with encryption at rest and automated backups. 3. Deploy ECS Fargate services for payment processing containers with auto-scaling policies. 4. Configure an Application Load Balancer with health checks and SSL termination. 5. Implement AWS Database Migration Service for continuous data replication from on-premises PostgreSQL. 6. Create a DynamoDB table to track migration checkpoints and rollback states. 7. Set up VPN connectivity between the new VPC and on-premises network for hybrid operation. 8. Configure Route 53 weighted routing policies to gradually shift traffic (0%, 25%, 50%, 75%, 100%). 9. Deploy CloudWatch dashboards and alarms for monitoring migration metrics and application health. 10. Implement Lambda functions to orchestrate migration phases and handle rollback scenarios. Expected output: A complete Pulumi program that creates all infrastructure components with proper dependencies, outputs the ALB DNS name, RDS endpoint, VPN connection ID, and DynamoDB table name. The program should support destroying and recreating resources cleanly for rollback scenarios.",A financial services company is migrating their payment processing infrastructure from a legacy on-premises setup to AWS. The existing system runs in Docker containers with PostgreSQL databases and requires zero-downtime migration with the ability to roll back if issues arise during the transition.,"AWS multi-account setup in us-east-1 region for production migration from on-premises to cloud. Infrastructure includes ECS Fargate for container workloads, RDS Aurora PostgreSQL with read replicas, Application Load Balancer with WAF, VPN connection to on-premises data center. Requires Pulumi CLI 3.x with Go 1.21+, AWS CLI configured with appropriate IAM roles. VPC setup with 3 availability zones, private subnets for compute and database tiers, public subnets for load balancers. Transit Gateway for cross-account connectivity. AWS Organizations with separate accounts for dev, staging, and production environments.","[""Resource tagging must follow FinancialServices-ENV-Component pattern"", ""Migration state must be tracked in DynamoDB for rollback capability"", ""Legacy system connections must be maintained through VPN during migration"", ""Must use blue-green deployment strategy for zero-downtime migration"", ""Database migration must support incremental data sync during transition"", ""Database backups must be taken before and after migration steps"", ""All traffic must remain encrypted in transit using TLS 1.2 or higher"", ""Network isolation between environments using separate VPCs is required"", ""Container images must be scanned for vulnerabilities before deployment"", ""CloudWatch alarms must monitor migration progress and system health""]"
d4w8c1,done,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi Go program to deploy a payment processing web application infrastructure. The configuration must: 1. Create a VPC with 3 public and 3 private subnets across different availability zones. 2. Deploy an ECS cluster with Fargate launch type and a task definition for the payment service container. 3. Set up an RDS Aurora MySQL cluster with multi-AZ deployment and encryption at rest. 4. Configure an Application Load Balancer with HTTPS listener and target group for ECS tasks. 5. Implement CloudWatch Log Groups with 7-year retention for ECS tasks and RDS slow query logs. 6. Create S3 buckets for VPC flow logs with lifecycle rules to transition to Glacier after 90 days. 7. Set up IAM roles for ECS task execution and task role with specific permissions for S3 and Secrets Manager. 8. Configure security groups allowing only HTTPS traffic to ALB and database connections from ECS tasks. 9. Enable VPC flow logs and route them to the S3 bucket with proper access policies. 10. Apply consistent tagging scheme across all resources with Environment, Application, and CostCenter tags. 11. Output the ALB DNS name, RDS cluster endpoint, and S3 bucket names for flow logs. 12. Ensure all passwords and sensitive data use Pulumi secrets or AWS Secrets Manager. Expected output: A complete Pulumi Go program that creates all infrastructure components with proper security configurations, exports critical endpoints and resource identifiers, and maintains compliance with financial industry requirements for audit trails and data protection.",A fintech startup needs to deploy their payment processing web application with strict compliance requirements for data isolation and audit logging. The application processes sensitive financial transactions and must maintain PCI DSS compliance with complete infrastructure traceability.,"Production-grade infrastructure in us-east-2 region for payment processing application. Uses ECS Fargate for containerized services, RDS Aurora MySQL for transaction data, Application Load Balancer for traffic distribution, and CloudWatch for monitoring. Requires Go 1.19+, Pulumi 3.x CLI, AWS CLI configured with appropriate credentials. Multi-AZ VPC setup with 3 public subnets for ALB and 3 private subnets for ECS tasks and RDS. NAT Gateways in each AZ for outbound connectivity. S3 buckets for VPC flow logs and application artifacts.","[""VPC flow logs must be enabled and stored in S3 with lifecycle policies"", ""All security groups must explicitly deny all traffic except required ports"", ""All RDS instances must use encrypted storage with customer-managed KMS keys"", ""Application Load Balancer must terminate SSL with ACM certificates"", ""ECS task definitions must use specific CPU and memory limits"", ""IAM roles must follow principle of least privilege with no wildcard permissions"", ""ECS tasks must run in private subnets with no direct internet access"", ""All resources must be tagged with Environment, Application, and CostCenter"", ""RDS automated backups must be retained for 35 days minimum"", ""CloudWatch Logs retention must be set to 7 years for compliance""]"
t1i7w4,error,Pulumi,Python,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi Python program to orchestrate a zero-downtime migration of a payment processing system from on-premises to AWS. The configuration must: 1. Set up dual VPCs (production and migration) with Transit Gateway connectivity 2. Deploy RDS Aurora PostgreSQL clusters with read replicas in both regions 3. Configure DMS replication instances with full-load and CDC for database migration 4. Create API Gateway endpoints that can route traffic between old and new systems 5. Implement Lambda functions to validate data consistency during migration 6. Set up Step Functions state machines to control the migration phases 7. Configure S3 buckets to store migration checkpoints and rollback states 8. Deploy CloudWatch dashboards to monitor replication lag and error rates 9. Create SNS topics and subscriptions for alerting the operations team 10. Implement automated rollback mechanisms using Lambda and Step Functions 11. Configure Secrets Manager with automatic rotation for all credentials 12. Set up Parameter Store hierarchies for environment-specific configurations. Expected output: A complete Pulumi program that creates all infrastructure components with proper dependencies, outputs the migration state machine ARN, API Gateway endpoints, and monitoring dashboard URL. The program should support both forward migration and rollback operations through configuration flags.",A financial services company needs to migrate their payment processing infrastructure from their legacy on-premises data center to AWS. The existing system handles millions of transactions daily and requires strict compliance with PCI-DSS standards. The migration must be performed with zero downtime and full rollback capabilities.,"Production-ready infrastructure deployed across us-east-1 and us-east-2 for the migration process. The setup includes VPCs in both regions with Transit Gateway connecting to on-premises networks via VPN. Core services include RDS Aurora PostgreSQL clusters, API Gateway, Lambda functions for validation, DMS replication instances, and Step Functions for orchestration. Requires Pulumi 3.x with Python 3.9+, AWS CLI configured with appropriate IAM permissions. The infrastructure spans multiple availability zones with private subnets for databases and compute resources, public subnets for NAT gateways, and dedicated subnets for DMS replication instances.","[""Use Step Functions to orchestrate the migration workflow"", ""Use AWS Transit Gateway for network connectivity between migrated and legacy components"", ""Implement AWS Database Migration Service (DMS) for real-time database replication"", ""Configure AWS Secrets Manager rotation for all database credentials"", ""Implement CloudWatch Logs with metric filters for migration monitoring"", ""Implement SNS topics for migration status notifications"", ""Deploy Lambda functions for data validation between old and new systems"", ""Deploy API Gateway with custom authorizers for authentication during transition"", ""Use Parameter Store for environment-specific configuration values"", ""Configure S3 buckets with versioning for migration state backups""]"
q4w4o3,error,Pulumi,Python,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi Python program to deploy a multi-region disaster recovery infrastructure for a trading platform. The configuration must: 1. Set up Route 53 hosted zone with health checks monitoring primary region endpoints every 30 seconds. 2. Create Aurora PostgreSQL global database cluster with a primary cluster in us-east-1 and secondary in us-east-2. 3. Deploy identical Lambda functions in both regions that process trading orders from SQS queues. 4. Configure DynamoDB global tables for user session storage with on-demand billing and point-in-time recovery. 5. Establish S3 buckets in both regions with cross-region replication rules for static content and trading reports. 6. Set up API Gateway REST APIs in both regions with Lambda proxy integrations and custom domain names. 7. Create CloudWatch composite alarms that monitor Aurora writer availability, Lambda error rates, and API Gateway 5xx errors. 8. Deploy Lambda-based failover orchestrator that promotes Aurora read replica and updates Route 53 records upon alarm trigger. 9. Configure SNS topics in both regions for alerting DevOps team during failover events. 10. Implement CloudWatch Synthetics canaries that continuously test critical API endpoints in both regions. Expected output: A Pulumi stack that provisions complete multi-region infrastructure with automated failover capabilities. The solution should achieve RTO of under 5 minutes and RPO of under 1 minute for database transactions.","A fintech company operates a critical trading platform that processes millions of transactions daily. After experiencing a regional AWS outage that resulted in 4 hours of downtime and significant revenue loss, they need to implement a robust multi-region disaster recovery solution that can automatically failover within minutes.","Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-east-2 (DR). Utilizes Route 53 for DNS failover, Aurora Global Database for data persistence, Lambda and API Gateway for compute, DynamoDB global tables for session data, S3 with cross-region replication for static assets. Requires Pulumi 3.x with Python 3.9+, boto3, and AWS CLI configured with cross-region permissions. Each region has its own VPC with 3 availability zones, private subnets for databases, and public subnets for load balancers. Automated failover orchestration through CloudWatch Events and Lambda.","[""Configure SNS cross-region subscriptions for failover notifications"", ""Set up S3 cross-region replication with RTC (Replication Time Control) for objects under 128 MB"", ""Implement API Gateway custom domain names with regional endpoints in both regions"", ""Configure DynamoDB global tables with point-in-time recovery enabled"", ""Use Route 53 health checks with failover routing policies for automatic DNS failover"", ""Use Systems Manager Parameter Store with cross-region replication for configuration data"", ""Deploy Lambda functions in both regions using identical deployment packages"", ""Implement CloudWatch Synthetics canaries in both regions for continuous availability monitoring"", ""Use CloudWatch cross-region composite alarms to trigger failover automation"", ""Implement cross-region RDS Aurora Global Database with automated promotion capabilities""]"
s7t0y2,done,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement a multi-region disaster recovery infrastructure for a financial trading platform. The configuration must: 1. Set up primary infrastructure in us-east-1 with Aurora PostgreSQL cluster, DynamoDB table, and ECS Fargate service behind an ALB. 2. Create matching DR infrastructure in us-east-2 with Aurora read replica promoted to cluster on failover. 3. Configure Route 53 hosted zone with primary and secondary record sets using health checks. 4. Implement S3 buckets in both regions with cross-region replication for application artifacts. 5. Create DynamoDB global table spanning both regions for session management. 6. Deploy Lambda functions in both regions to handle failover orchestration and health monitoring. 7. Set up CloudWatch alarms in both regions monitoring RDS lag, ECS health, and ALB target health. 8. Configure SNS topics in both regions with cross-region subscriptions for alert forwarding. 9. Create IAM roles with cross-region assume role policies for failover automation. 10. Implement automated failover logic that promotes Aurora read replica, updates Route 53 weights, and scales ECS services. 11. Add CloudWatch dashboards showing real-time replication lag and system health across regions. 12. Export primary and DR endpoint URLs, health check IDs, and failover Lambda ARNs. Expected output: A fully automated disaster recovery infrastructure where Route 53 automatically redirects traffic to us-east-2 when primary region health checks fail. The system should handle database promotion, service scaling, and notification delivery without manual intervention, achieving sub-5-minute RTO.","A financial services company operates a critical trading platform that processes millions of transactions daily. Following a recent outage that cost them $2.3M in lost revenue, they need to implement a multi-region active-passive disaster recovery solution with automated failover capabilities.","Multi-region deployment across us-east-1 (primary) and us-east-2 (DR) for a high-availability trading platform. Infrastructure includes Aurora PostgreSQL Global Database, DynamoDB global tables, ALB with auto-scaling ECS Fargate services, S3 buckets with cross-region replication, Route 53 health checks, and Lambda functions for failover automation. Requires Pulumi CLI 3.x with TypeScript, AWS CLI configured with appropriate credentials. VPCs in both regions with private subnets across 3 AZs, VPC peering for inter-region communication, and NAT gateways for outbound traffic.","[""Configure CloudWatch cross-region alarms and SNS notifications"", ""Implement cross-region RDS Aurora Global Database with read replicas"", ""RTO (Recovery Time Objective) must be under 5 minutes"", ""Ensure all IAM roles support cross-region assume role permissions"", ""Tag all resources with Environment, Region, and DR-Role tags"", ""RPO (Recovery Point Objective) must be under 1 minute"", ""Use Route 53 health checks with automatic DNS failover"", ""Use S3 cross-region replication for static assets and backups"", ""Configure DynamoDB global tables for session state replication"", ""Implement Lambda functions for automated failover orchestration""]"
o6j0d1,done,CDK,Python,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a CDK Python program to establish secure VPC peering between a trading platform VPC and a data analytics VPC across different AWS accounts. The configuration must: 1. Create VPC peering connection between account A (trading) and account B (analytics) 2. Configure route tables in both VPCs to enable communication through the peering connection 3. Set up security groups that allow HTTPS (443) and PostgreSQL (5432) traffic only from specific CIDR blocks 4. Enable VPC Flow Logs with S3 storage and 5-minute capture intervals 5. Configure DNS resolution options for the peering connection 6. Implement Network ACLs that restrict traffic to only required subnets 7. Create IAM roles for cross-account peering acceptance 8. Set up CloudWatch alarms for unusual network traffic patterns 9. Configure VPC endpoints for S3 and DynamoDB to avoid internet routing 10. Enable AWS Config rules to monitor VPC peering compliance 11. Tag all resources with mandatory compliance tags 12. Output the peering connection ID and status for verification. Expected output: A fully deployed cross-account VPC peering setup with bidirectional connectivity between private subnets, comprehensive security controls, and monitoring. The stack should output the peering connection ID, route table IDs, and CloudWatch dashboard URL for network monitoring.",A financial services company operates multiple AWS accounts for different business units. They need to establish secure network connectivity between their trading platform VPC and data analytics VPC while maintaining strict network isolation and compliance requirements.,"Multi-account AWS infrastructure deployed across us-east-1 and us-east-2 regions. Primary VPC hosts trading applications on ECS Fargate with RDS Aurora PostgreSQL backend. Secondary VPC contains data analytics workloads using EMR and S3. Both VPCs use private subnets across 3 availability zones with NAT gateways for outbound connectivity. Requires CDK 2.x with Python 3.9+, AWS CLI configured with appropriate cross-account assume role permissions. VPC CIDR ranges: 10.0.0.0/16 (trading) and 10.1.0.0/16 (analytics).","[""All resources must be tagged with CostCenter and Environment tags"", ""All traffic between VPCs must be encrypted using AWS PrivateLink where applicable"", ""Security groups must follow a whitelist approach with no 0.0.0.0/0 rules"", ""Route tables must be configured with least-privilege routing rules"", ""VPC Flow Logs must be enabled for all network interfaces"", ""Network traffic must not traverse the public internet"", ""Network ACLs must explicitly allow only required ports between specific subnets"", ""VPC peering connections must use non-overlapping CIDR blocks"", ""DNS resolution must work bidirectionally between peered VPCs"", ""The solution must support cross-account VPC peering""]"
q5g8k2,error,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi Go program to implement a multi-region disaster recovery infrastructure with automated failover capabilities. The configuration must: 1. Set up VPCs in both us-east-1 and us-east-2 with 3 AZs each and establish VPC peering. 2. Deploy an RDS Aurora Global Database cluster with a primary cluster in us-east-1 and secondary in us-east-2. 3. Create DynamoDB global tables for session data replication across regions. 4. Configure Auto Scaling groups in both regions with identical launch configurations. 5. Deploy Application Load Balancers in each region with target groups pointing to the Auto Scaling groups. 6. Implement Route53 hosted zone with health check-based routing policies for automatic failover. 7. Set up CloudWatch metric streams to replicate metrics from primary to DR region. 8. Configure AWS Backup plans for automated RDS snapshots with cross-region copying. 9. Create CloudWatch alarms for monitoring database lag, replication status, and health check failures. 10. Implement Lambda functions for automated failover orchestration and notification. 11. Configure SNS topics in both regions for alerting on failover events. 12. Output the primary and DR endpoint URLs, health check status, and replication lag metrics. Expected output: The program should create a fully automated disaster recovery setup where traffic automatically fails over to us-east-2 when us-east-1 experiences issues, with database replication lag under 1 second and DNS failover completing within 5 minutes.","A financial services company requires a highly available disaster recovery solution for their critical payment processing system. The system must automatically failover to a secondary region within minutes of detecting a regional outage, with minimal data loss and automated health checks.","Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-east-2 (DR). Utilizes RDS Aurora Global Database for PostgreSQL 14.x, DynamoDB global tables for session management, Auto Scaling groups with EC2 t3.medium instances, Application Load Balancers in each region, and Route53 for DNS failover. Requires Pulumi CLI 3.x with Go 1.20+, AWS CLI configured with appropriate IAM permissions for multi-region deployments. VPCs in both regions with 3 availability zones each, private subnets for databases, public subnets for ALBs, and VPC peering for cross-region communication.","[""RPO (Recovery Point Objective) must be under 1 minute"", ""Implement cross-region RDS Aurora Global Database"", ""All resources must be tagged with Environment and DR-Role tags"", ""Use Route53 health checks for automatic DNS failover"", ""RTO (Recovery Time Objective) must be under 5 minutes"", ""Deploy identical Auto Scaling groups in both regions"", ""Use AWS Backup for automated snapshot management"", ""Implement CloudWatch cross-region metric streams"", ""Primary region must be us-east-1 and DR region must be us-east-2"", ""Use DynamoDB global tables for session data""]"
s4i7r3,done,CDK,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CDK TypeScript program to implement a multi-region disaster recovery architecture. The configuration must: 1. Set up Aurora Global Database with a primary cluster in us-east-1 and secondary in us-east-2, including automated backtrack. 2. Deploy identical ECS Fargate services in both regions running a containerized application. 3. Configure DynamoDB global tables for session state with on-demand billing. 4. Implement Route 53 hosted zone with health checks and automatic DNS failover between regions. 5. Set up S3 buckets with cross-region replication and Replication Time Control enabled. 6. Deploy EventBridge global endpoints for cross-region event routing. 7. Configure AWS Backup plans for Aurora, DynamoDB, and ECS services in both regions. 8. Implement CloudWatch Synthetics canaries to monitor application endpoints in each region. 9. Create Step Functions state machines to orchestrate failover procedures. 10. Set up Systems Manager Parameter Store with cross-region parameter replication. Expected output: A CDK application that deploys a complete multi-region DR solution with automated failover capabilities, maintaining data consistency across regions and achieving an RTO of under 15 minutes.","A financial services company requires a disaster recovery solution for their critical trading application. The primary region hosts the main application stack, while a secondary region must maintain a warm standby with automated failover capabilities. The solution must ensure data consistency and minimize recovery time objectives (RTO) to under 15 minutes.","Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-east-2 (secondary). Uses Aurora Global Database for PostgreSQL 14.x with automated backtrack, ECS Fargate for containerized applications, DynamoDB global tables for session management, Route 53 for DNS failover with health checks. Requires CDK 2.100+, TypeScript 4.9+, Node.js 18+. VPCs in both regions with private subnets across 3 AZs each, VPC peering for cross-region communication. S3 buckets with cross-region replication and RTC enabled. EventBridge global endpoints for event routing between regions.","[""Deploy CloudWatch Synthetics canaries in both regions for monitoring"", ""Implement Step Functions for orchestrating failover procedures"", ""Configure S3 cross-region replication with RTC (Replication Time Control)"", ""Deploy identical ECS task definitions in both regions using Fargate"", ""Configure Aurora Global Database with automated backtrack enabled"", ""Use EventBridge global endpoints for cross-region event routing"", ""Use Route 53 health checks with automatic DNS failover between regions"", ""Implement AWS Backup for point-in-time recovery across regions"", ""Implement DynamoDB global tables for session state replication"", ""Use Systems Manager Parameter Store for configuration synchronization""]"
s9l4d6,done,CDK,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a CDK TypeScript program to deploy a production-ready AWS environment for a financial trading analytics platform. The configuration must: 1. Set up a VPC with CIDR 10.0.0.0/16 spanning exactly 3 availability zones, with public and private subnets in each AZ. 2. Deploy an Aurora Serverless v2 PostgreSQL cluster in private subnets with automated backups and encryption using KMS. 3. Create DynamoDB tables for user sessions and API keys with on-demand billing and point-in-time recovery. 4. Configure an S3 bucket hierarchy for raw data ingestion, processed analytics, and long-term archival with appropriate lifecycle rules. 5. Implement Lambda functions using Graviton2 processors for data processing pipelines with environment-specific configuration. 6. Set up API Gateway REST APIs with usage plans, API keys, and request throttling at 1000 RPS per key. 7. Configure CloudWatch Log Groups for all services with 30-day retention and subscription filters for alerting. 8. Create KMS keys for database encryption, S3 encryption, and Lambda environment variables. 9. Implement IAM roles and policies following least-privilege principles with explicit regional restrictions. 10. Deploy AWS Config with rules for PCI-DSS compliance checking including encryption validation and access logging. 11. Tag all resources with Environment, CostCenter, Compliance, and DataClassification tags. 12. Output the VPC ID, database endpoint, API Gateway URL, and S3 bucket names for application configuration. Expected output: A fully functional CDK TypeScript application with proper stack organization, type safety, and modular construction. The synthesized CloudFormation template should create all resources with appropriate dependencies and be deployable in a single command.","A financial services startup needs to establish their first production AWS environment for a new trading analytics platform. The platform processes real-time market data and provides insights to institutional clients through a web dashboard. They require a secure, compliant infrastructure that meets PCI-DSS requirements while maintaining cost efficiency.","Production financial services environment deployed in us-east-1 across 3 availability zones. Core services include Aurora Serverless v2 PostgreSQL for transactional data, DynamoDB for session management, Lambda functions on Graviton2 for data processing, API Gateway for client access, and S3 for data archival. VPC configured with 10.0.0.0/16 CIDR, public subnets for NAT gateways, private subnets for compute and database resources. Requires AWS CDK 2.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate credentials. All resources tagged for cost allocation and compliance tracking.","[""All S3 buckets must have versioning enabled and lifecycle policies for 90-day archival"", ""All data must be encrypted at rest using AWS KMS customer-managed keys"", ""All IAM roles must follow least-privilege principle with explicit deny for unused regions"", ""CloudWatch Logs retention must be set to 30 days for all log groups"", ""Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""RDS instances must use Aurora Serverless v2 to optimize costs during low-traffic periods"", ""Infrastructure must include AWS Config rules for PCI-DSS compliance monitoring"", ""DynamoDB tables must use on-demand billing mode with point-in-time recovery enabled"", ""API Gateway must implement request throttling at 1000 requests per second per API key"", ""VPC must span exactly 3 availability zones with CIDR 10.0.0.0/16""]"
e5j8w4,in_progress,CDK,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a CDK TypeScript program to deploy identical infrastructure across development, staging, and production environments while maintaining environment-specific configurations. The configuration must: 1. Define a base CDK app structure with separate stacks for each environment that inherit from a common abstract stack class. 2. Create reusable L3 constructs for RDS Aurora clusters that accept environment-specific parameters for instance count and size. 3. Implement VPC configuration with consistent subnet layouts but environment-specific CIDR ranges following the pattern 10.{env}.0.0/16. 4. Configure ECS Fargate services with environment-specific task definitions that pull container images from a shared ECR repository. 5. Set up Application Load Balancers with environment-specific SSL certificates from AWS Certificate Manager. 6. Create S3 buckets with environment-prefixed names and consistent lifecycle policies across all environments. 7. Implement IAM roles with least-privilege policies that restrict cross-environment access. 8. Configure RDS Aurora read replicas where production data is replicated to staging for testing purposes. 9. Set up CloudWatch dashboards that aggregate metrics across all environments with environment-specific alarm thresholds. 10. Implement CDK pipelines that validate infrastructure consistency before deployment using custom validation constructs. 11. Create a configuration management system using CDK context and SSM parameters for environment-specific values. 12. Generate a deployment manifest that documents all resources created per environment with their configurations. Expected output: A complete CDK TypeScript application with modular stack definitions, reusable constructs, and deployment scripts that ensure consistent infrastructure across all three environments while allowing controlled variations for environment-specific requirements.","A financial services company operates trading applications across development, staging, and production environments. They need to ensure identical infrastructure configurations across environments while maintaining environment-specific parameters like instance sizes and database credentials. The team struggles with configuration drift and manual environment synchronization.","Multi-environment AWS infrastructure spanning three accounts (dev: 123456789012, staging: 234567890123, prod: 345678901234) in us-east-1 region. Each environment requires isolated VPCs with 3 availability zones, RDS Aurora PostgreSQL clusters, ECS Fargate services, Application Load Balancers, and S3 buckets for static assets. Infrastructure uses AWS CDK 2.x with TypeScript, requiring Node.js 18+, TypeScript 5.x, and AWS CLI configured with appropriate cross-account assume role permissions. VPCs follow CIDR pattern 10.{env}.0.0/16 where env is 1 for dev, 2 for staging, 3 for production.","[""Use CDK stack dependencies to ensure proper deployment order across environments"", ""Use CDK custom resources to verify environment parity post-deployment"", ""Use CDK context values for environment-specific configuration without hardcoding"", ""Implement automated drift detection using CDK diff in deployment pipeline"", ""Implement a custom CDK construct for reusable RDS Aurora cluster configuration"", ""Configure cross-environment read replicas for production database in staging"", ""Each environment must have isolated VPCs with consistent CIDR block patterns"", ""Environment configurations must be validated at synthesis time using CDK aspects"", ""Implement tagging strategy with environment, cost center, and deployment timestamp"", ""Use AWS Systems Manager Parameter Store for sensitive values with hierarchical paths""]"
x2y8j9,error,Pulumi,Python,expert,Application Deployment,Web Application Deployment,"Create a Pulumi Python program to deploy a serverless fraud detection pipeline. The configuration must: 1. Create an API Gateway REST API with /transactions POST endpoint that triggers a Lambda function. 2. Set up a DynamoDB table named 'transactions' with partition key 'transaction_id' (string) and sort key 'timestamp' (number). 3. Configure the API Lambda to validate incoming transactions and write them to DynamoDB. 4. Create an EventBridge rule that captures DynamoDB stream records when items are inserted. 5. Deploy a fraud detection Lambda triggered by the EventBridge rule that analyzes transactions. 6. Set up an SQS queue for suspicious transactions with a dead letter queue for failed processing. 7. Configure the fraud detection Lambda to send suspicious transactions to the SQS queue. 8. Create a notification Lambda that polls the SQS queue and sends alerts via SNS topic. 9. Deploy an SNS topic with email subscription endpoint for fraud alerts. 10. Implement proper IAM roles and policies for each Lambda function with minimal required permissions. 11. Add CloudWatch log groups for all Lambda functions with 7-day retention. 12. Configure X-Ray tracing for all Lambda functions and API Gateway. Expected output: A fully functional serverless pipeline that accepts transaction data via API, stores it in DynamoDB, analyzes it for fraud patterns, queues suspicious transactions, and sends email notifications for potential fraud cases.","A fintech startup needs to process millions of daily transaction records for fraud detection. The system must handle variable loads, process events in near real-time, and maintain strict data privacy requirements while keeping operational costs minimal.","Serverless infrastructure deployed in us-east-1 for GDPR compliance. Core services include Lambda for compute, API Gateway for REST endpoints, DynamoDB for transaction storage, SQS for message queuing, EventBridge for event routing, and SNS for notifications. Requires Pulumi 3.x with Python 3.8+, AWS CLI configured with appropriate credentials. No VPC required for most components except data processing Lambdas which need private subnet access. KMS customer-managed keys for encryption at rest.","[""Dead letter queues must retain messages for 14 days"", ""Lambda functions must have environment variables encrypted with a customer-managed KMS key"", ""All Lambda functions must use Python 3.11 runtime"", ""All IAM roles must follow principle of least privilege with no wildcard permissions"", ""Lambda functions processing sensitive data must run inside a VPC"", ""API Gateway must implement request throttling at 1000 requests per second"", ""DynamoDB tables must use on-demand billing mode"", ""Lambda functions must have reserved concurrent executions of at least 100"", ""EventBridge rules must use event pattern matching, not scheduled expressions"", ""SQS queues must have visibility timeout of exactly 6 minutes""]"
m8d0w1,error,CDK,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CDK TypeScript program to implement an automated multi-region disaster recovery solution for a financial trading platform. The configuration must: 1. Deploy primary infrastructure in us-east-1 with ECS Fargate services behind an ALB. 2. Set up Aurora PostgreSQL with automated backups and cross-region read replica in us-east-2. 3. Configure DynamoDB global tables for session state replication between regions. 4. Implement Route 53 health checks with automatic DNS failover using weighted routing policies. 5. Create Lambda functions to orchestrate the failover process, including database promotion and ECS service activation. 6. Deploy standby infrastructure in us-east-2 with minimal resources until activated. 7. Set up CloudWatch alarms that trigger failover when primary region availability drops below 99%. 8. Implement SNS topics for failover notifications to operations team. 9. Configure automated scaling policies that activate only after successful failover. 10. Create CloudWatch dashboards showing real-time replication lag and system health metrics. 11. Ensure all IAM roles follow least privilege principle with cross-region assume role capabilities. 12. Tag all resources with DR tier classifications for cost tracking. Expected output: A complete CDK TypeScript application with separate stacks for primary and secondary regions, shared constructs for common infrastructure patterns, and a failover orchestration stack that coordinates the disaster recovery process. The solution should demonstrate proper use of CDK aspects for cross-cutting concerns and custom resources for failover automation.","A financial services company operates a critical trading platform that processes millions of transactions daily. After experiencing a regional outage that cost them $2M in lost revenue, they need to implement an automated disaster recovery solution that can failover their entire application stack to a secondary region within minutes.","Multi-region AWS deployment spanning us-east-1 (primary) and us-east-2 (secondary) regions. Infrastructure includes Application Load Balancers, ECS Fargate services running containerized applications, Aurora PostgreSQL with cross-region read replicas, DynamoDB global tables for session management, and Route 53 for DNS failover. Requires AWS CDK 2.x with TypeScript, Node.js 18+, and Docker. VPCs in both regions with private subnets across 3 availability zones, connected via VPC peering. CloudWatch cross-region monitoring with custom metrics for replication lag and health status.","[""Database replication lag between regions must not exceed 1 second"", ""The entire failover process must complete within 5 minutes of detection"", ""The secondary region must remain in standby mode with minimal resource allocation until failover"", ""The solution must include automated rollback capabilities if failover fails"", ""CloudWatch dashboards in both regions must track replication metrics"", ""DynamoDB global tables must be used for session state management"", ""Lambda functions must handle the orchestration of failover steps sequentially"", ""All API endpoints must maintain the same domain name during and after failover"", ""Route 53 health checks must monitor both primary and secondary regions continuously"", ""The failover mechanism must be triggered automatically based on CloudWatch alarms""]"
z0f1r0,done,Pulumi,Python,expert,Application Deployment,Web Application Deployment,"Create a Pulumi Python program to deploy a serverless transaction processing system. The configuration must: 1. Create an API Gateway REST API with /transaction POST endpoint protected by API key authentication. 2. Deploy a Lambda function that validates incoming transactions against a DynamoDB table of merchant configurations. 3. Set up an SQS queue for valid transactions with visibility timeout of 300 seconds. 4. Create a second Lambda function triggered by SQS to perform fraud detection using pattern matching. 5. Store processed transactions in a DynamoDB table with partition key 'transaction_id' and sort key 'timestamp'. 6. Implement a third Lambda for failed transaction handling triggered by a separate DLQ. 7. Configure CloudWatch Log Groups with 30-day retention for all Lambda functions. 8. Set up SNS topic for alerting on fraud detection with email subscription. 9. Create CloudWatch dashboard displaying Lambda invocations, errors, and duration metrics. 10. Export API Gateway endpoint URL and CloudWatch dashboard URL as stack outputs. Expected output: A fully deployed serverless architecture with three Lambda functions connected through SQS queues, API Gateway endpoint for transaction submission, DynamoDB tables for storage, and comprehensive monitoring through CloudWatch and X-Ray. The system should handle 1000+ transactions per second during peak loads.","A fintech startup needs to process millions of daily transactions from various payment providers. They require a serverless architecture that can handle burst traffic during peak hours while maintaining PCI compliance. The system must validate transactions, detect fraud patterns, and store results for audit purposes.","Serverless infrastructure deployed in us-east-2 using AWS Lambda for transaction processing, API Gateway for REST endpoints, DynamoDB for transaction storage, and SQS for message queuing. Requires Pulumi 3.x with Python 3.9+, AWS CLI configured with appropriate credentials. VPC setup with private subnets across 3 AZs, VPC endpoints for AWS services to avoid internet routing. CloudWatch Logs for centralized logging, X-Ray for distributed tracing, and AWS WAF for API protection. KMS encryption required for all data storage.","[""Deploy all Lambda functions within a VPC using private subnets only"", ""Implement least-privilege IAM roles with condition keys for resource access"", ""Use KMS customer-managed keys for encrypting all data at rest"", ""Use DynamoDB with point-in-time recovery enabled and on-demand billing"", ""Implement X-Ray tracing for all Lambda functions and API Gateway"", ""Set up CloudWatch alarms for Lambda errors exceeding 1% error rate"", ""Configure Lambda functions with 512MB memory and 60-second timeout"", ""Use AWS Lambda with reserved concurrent executions set to 100 for the main processing function"", ""Implement dead letter queues for all SQS queues with a retention period of 14 days"", ""Configure API Gateway with AWS WAF integration using managed rule sets""]"
q3b8f0,done,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to deploy a payment processing environment in AWS. The configuration must: 1. Set up a VPC with CIDR 10.0.0.0/16 across 3 availability zones with both public and private subnets. 2. Deploy an API Gateway REST API with Lambda proxy integration for a payments endpoint. 3. Create three Lambda functions: payment-validator, payment-processor, and payment-notifier with 512MB memory and 30-second timeout. 4. Configure a DynamoDB table named 'transactions' with partition key 'transactionId' (String) and sort key 'timestamp' (Number). 5. Set up an S3 bucket for audit logs with server-side encryption using AWS-managed keys. 6. Create CloudWatch Log Groups for each Lambda function with 7-day retention. 7. Implement SNS topic for payment notifications with email subscription endpoint. 8. Configure NAT Gateways in each public subnet for Lambda outbound connectivity. 9. Set up VPC endpoints for S3 and DynamoDB to keep traffic within AWS network. 10. Create CloudWatch dashboard displaying Lambda invocations, errors, and DynamoDB read/write capacity metrics. Expected output: A complete Pulumi program that exports the API Gateway URL, S3 bucket name, DynamoDB table name, and CloudWatch dashboard URL. The program should use Pulumi's Component Resource pattern to organize related resources and implement proper tagging with Environment, Project, and ManagedBy tags.",A financial services startup is expanding to the Asia-Pacific region and needs to establish a new cloud environment for their payment processing system. The infrastructure must comply with PCI DSS requirements and support high-throughput transaction processing with sub-second latency.,"New AWS environment in us-east-2 (Singapore) region for payment processing infrastructure. Requires Pulumi 3.x with TypeScript, Node.js 18+, and AWS CLI v2 configured with appropriate credentials. The setup includes VPC with 3 availability zones, private subnets for compute resources, public subnets for NAT gateways, API Gateway for external API access, Lambda functions for transaction processing, DynamoDB for transaction storage, S3 for audit logs, and CloudWatch for monitoring. Network architecture uses Transit Gateway for future multi-region connectivity.","[""All compute resources must run in private subnets with no direct internet access"", ""CloudWatch alarms must trigger SNS notifications for any Lambda errors exceeding 1%"", ""Lambda functions must have reserved concurrent executions set to prevent cold starts"", ""DynamoDB tables must use on-demand billing mode with point-in-time recovery enabled"", ""API Gateway must implement request throttling at 10,000 requests per minute"", ""S3 buckets must have versioning enabled and lifecycle policies for 90-day archival"", ""VPC flow logs must be enabled and sent to CloudWatch Logs"", ""All IAM roles must use session policies with maximum session duration of 1 hour"", ""Database backups must be encrypted with customer-managed KMS keys"", ""All security groups must follow least-privilege principle with explicit port definitions""]"
c9q4o3,done,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a serverless data processing pipeline for financial analytics. The configuration must: 1. Create an S3 bucket for raw market data ingestion with server-side encryption using AWS-managed keys. 2. Set up a DynamoDB table named 'MarketDataState' with partition key 'symbol' (String) and sort key 'timestamp' (Number). 3. Deploy three Lambda functions: 'DataIngestion' (triggered by S3 events), 'DataProcessor' (triggered by SQS), and 'DataAggregator' (triggered by EventBridge scheduled rule every 5 minutes). 4. Create an SQS queue named 'ProcessingQueue' with a corresponding dead letter queue for failed messages. 5. Configure EventBridge with a rule that captures custom events from the DataProcessor function and routes them to DataAggregator. 6. Set up API Gateway REST API with POST endpoint '/ingest' that triggers the DataIngestion Lambda synchronously. 7. Implement proper IAM roles for each Lambda function with permissions only for required services. 8. Configure CloudWatch Log Groups for each Lambda function with metric filters for error tracking. 9. Add resource tags including 'Environment:Production' and 'Project:MarketAnalytics' to all resources. 10. Export the API Gateway URL, S3 bucket name, and DynamoDB table ARN as stack outputs. Expected output: A fully functional serverless pipeline where market data uploaded to S3 triggers processing through Lambda functions, with state stored in DynamoDB and aggregated results available via API Gateway. The system should handle thousands of concurrent data streams with automatic scaling and comprehensive error handling through dead letter queues.",A financial analytics startup needs to process millions of stock market data points daily through a serverless pipeline. The system must handle variable loads during market hours and scale to zero during off-hours to minimize costs.,"Serverless infrastructure deployed in us-east-1 for processing financial data streams. Core services include Lambda functions for data processing, DynamoDB for state storage, S3 for raw data ingestion, SQS for message queuing, EventBridge for event orchestration, and API Gateway for external integrations. Requires Pulumi CLI 3.x with TypeScript, Node.js 18.x, and AWS CLI configured with appropriate credentials. Architecture uses event-driven patterns with multiple Lambda functions connected through SQS and EventBridge. No VPC required as all services are serverless and publicly accessible with IAM authentication.","[""S3 buckets must have versioning enabled and lifecycle policies for 30-day retention"", ""DynamoDB tables must use on-demand billing mode with point-in-time recovery enabled"", ""Lambda functions must use Node.js 18.x runtime with 3GB memory allocation"", ""All Lambda functions must have X-Ray tracing enabled for performance monitoring"", ""CloudWatch Logs retention must be set to 7 days for all Lambda function logs"", ""API Gateway must implement request throttling at 10,000 requests per second"", ""SQS queues must have message retention period of 4 days and visibility timeout of 5 minutes"", ""IAM roles must follow least privilege principle with explicit deny statements"", ""EventBridge rules must trigger Lambda functions with at least once delivery guarantee"", ""Dead letter queues must be configured for all asynchronous Lambda invocations""]"
u6p9c0,error,Pulumi,Python,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi Python program to implement a multi-region disaster recovery architecture. The configuration must: 1. Set up VPCs in us-east-1 and us-east-2 with 3 availability zones each, including public and private subnets. 2. Create an Aurora Global Database cluster with a primary cluster in us-east-1 and secondary read replica cluster in us-east-2. 3. Configure S3 buckets in both regions with cross-region replication and replication time control (RTC) enabled for objects under /critical path. 4. Deploy identical Lambda functions in both regions that connect to the regional Aurora endpoints. 5. Set up API Gateway REST APIs in both regions with custom domain names using regional certificates. 6. Create DynamoDB global tables with replicas in both regions for session management. 7. Configure Route 53 hosted zone with weighted routing policy (100% to primary, 0% to secondary) and health checks on the primary region endpoints. 8. Implement CloudWatch alarms in the primary region that trigger SNS notifications when database connections exceed 80% or API Gateway 5xx errors exceed 10 per minute. 9. Create an EventBridge rule in us-east-1 that monitors Aurora cluster events and can trigger failover automation. 10. Output the Route 53 hosted zone ID, primary and secondary API endpoints, and Aurora global cluster identifier. Expected output: A Pulumi stack that creates all infrastructure components with proper cross-region references, exports key resource identifiers, and enables single-command failover by updating Route 53 weights.",A financial services company requires a disaster recovery solution for their critical payment processing application. The system must maintain near real-time data replication across regions and support automated failover with minimal data loss. The current single-region deployment has experienced outages affecting customer transactions.,"Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-east-2 (secondary). Uses Aurora PostgreSQL 15.x Global Database for transactional data, DynamoDB Global Tables for session management, S3 with cross-region replication for static assets. API Gateway with Lambda backends in both regions behind Route 53 weighted routing. VPCs in each region with 3 AZs, private subnets for databases, public subnets for ALBs. Requires Pulumi 3.x with Python 3.9+, AWS CLI configured with appropriate IAM permissions for multi-region resource creation.","[""RPO (Recovery Point Objective) must be under 1 minute"", ""DynamoDB global tables required for session state management"", ""Implement Route 53 health checks with automatic DNS failover"", ""API Gateway must use custom domain with regional endpoints"", ""Primary region must be us-east-1 with failover to us-east-2"", ""RTO (Recovery Time Objective) must be under 5 minutes"", ""S3 bucket replication must use cross-region replication with RTC enabled"", ""Use Aurora Global Database for cross-region replication"", ""Lambda functions must be deployed identically in both regions"", ""All resources must be tagged with Environment, Region, and DR-Role tags""]"
k4o7k4,done,CDK,Python,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CDK Python program to implement a multi-region disaster recovery architecture for a payment processing API. The configuration must: 1. Deploy API Gateway REST APIs in both us-east-1 and us-east-2 with custom domain names. 2. Create DynamoDB global tables with automatic replication between regions. 3. Implement Lambda functions for payment validation and processing in both regions. 4. Configure Route 53 hosted zone with health checks and automatic failover routing. 5. Set up SQS queues with dead letter queues for failed transaction handling. 6. Create CloudWatch alarms monitoring API latency, Lambda errors, and DynamoDB throttles. 7. Configure SNS topics for operational alerts in both regions. 8. Implement Lambda function for automated failover orchestration. 9. Set up CloudWatch dashboards showing cross-region metrics. 10. Create IAM roles with least privilege access for all components. Expected output: A complete CDK application that deploys identical stacks in both regions, with Route 53 automatically directing traffic to the healthy region. The system should handle regional failures transparently, maintaining transaction integrity during failover events.",A financial services company requires a disaster recovery solution for their critical payment processing API. The system must maintain 99.99% availability and automatically failover between regions with minimal data loss. Recent outages in their primary region have highlighted the need for a robust multi-region architecture.,"Multi-region active-passive disaster recovery infrastructure spanning us-east-1 (primary) and us-east-2 (secondary). Architecture includes API Gateway with custom domains, Lambda functions for payment processing, DynamoDB global tables for transaction storage, and SQS for asynchronous processing. Route 53 manages DNS failover with health checks monitoring API Gateway endpoints. CloudWatch monitors system health and triggers automated failover through SNS and Lambda. Requires AWS CDK 2.x with Python 3.9+, boto3, and AWS CLI configured with appropriate credentials. VPCs in both regions with private subnets for Lambda functions.","[""DynamoDB tables must use on-demand billing mode"", ""Dead letter queues must be configured for all SQS queues"", ""All resources must be tagged with Environment and Region tags"", ""All Lambda functions must have reserved concurrent executions set"", ""DynamoDB global tables must have point-in-time recovery enabled"", ""Use Route 53 health checks with failover routing policy"", ""Cross-region replication must complete within 1 second"", ""API Gateway must use custom domain names with ACM certificates"", ""CloudWatch alarms must trigger SNS notifications for failover events"", ""Lambda functions must be deployed in both us-east-1 and us-east-2""]"
w0w8v9,done,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to orchestrate a multi-service containerized application on ECS Fargate. The configuration must: 1. Define a VPC with public and private subnets across 3 AZs with proper routing 2. Create ECR repositories for frontend, api-gateway, and processing-service containers with tag immutability 3. Configure an ECS cluster with capacity providers for Fargate and Fargate Spot 4. Define task definitions for each service with appropriate resource limits and environment variables 5. Set up ALB with target groups and listeners for public-facing services 6. Deploy ECS services with desired count, auto-scaling policies, and health check grace periods 7. Configure service discovery using AWS Cloud Map for internal service communication 8. Implement IAM roles and policies for task execution and task roles per service 9. Create CloudWatch log groups and configure container logging drivers 10. Set up Secrets Manager secrets and reference them in task definitions 11. Configure security groups with strict ingress/egress rules between services 12. Export key outputs including ALB DNS names and service ARNs. Expected output: A fully functional Pulumi program that creates all infrastructure components with proper dependencies, exports the ALB endpoint for frontend access, and enables zero-downtime deployments through ECS service update mechanisms.","A financial services company needs to migrate their monolithic trading application to a containerized architecture. The application consists of a web frontend, multiple backend services, and requires strict isolation between customer data processing workloads.","Production environment in us-east-1 region hosting containerized microservices on ECS Fargate. Infrastructure includes VPC with 3 availability zones, private subnets for containers, public subnets for ALBs, NAT gateways for outbound traffic. Services include ECR for container registry, ECS Fargate for compute, Application Load Balancers for traffic distribution, Secrets Manager for sensitive data, CloudWatch for monitoring and logging. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate credentials. Architecture supports high availability with multi-AZ deployments and auto-scaling based on metrics.","[""Each service must run in its own ECS task definition with specific CPU and memory allocations"", ""Task execution roles must follow least-privilege principle with service-specific permissions"", ""Services must communicate through internal load balancers only"", ""Secrets must be stored in AWS Secrets Manager and injected as environment variables"", ""Network isolation must prevent direct internet access for backend services"", ""Deployment must support blue-green deployments with traffic shifting capabilities"", ""Container logs must stream to CloudWatch Log Groups with 30-day retention"", ""Container images must be stored in private ECR repositories with lifecycle policies"", ""Health checks must be implemented with specific thresholds for each service type"", ""Auto-scaling must be configured based on CPU utilization with min 2 and max 10 tasks""]"
r8b3v6,done,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy a serverless payment webhook processing system. The configuration must: 1. Create an API Gateway REST API with /webhook/{provider} endpoint accepting POST requests. 2. Implement three Lambda functions: webhook receiver, payment processor, and audit logger, all using Python 3.11 runtime on ARM64. 3. Configure DynamoDB table 'PaymentWebhooks' with partition key 'webhookId' and sort key 'timestamp'. 4. Set up DynamoDB streams on the table to trigger the audit logger Lambda. 5. Create an SQS dead letter queue for failed webhook processing with 14-day retention. 6. Configure SNS topic for critical alerts when dead letter queue receives messages. 7. Implement Lambda Layers for shared boto3 and cryptography libraries. 8. Set reserved concurrent executions to 100 for webhook receiver and 50 for processor. 9. Configure API Gateway with 10 requests per second per IP rate limiting using AWS WAF. 10. Enable X-Ray tracing on all Lambda functions and API Gateway. 11. Use customer-managed KMS key for all encryption needs. 12. Set Lambda timeout to 30 seconds for receiver and 5 minutes for processor. Expected output: A CDK stack that creates a production-ready serverless webhook processing pipeline with proper error handling, monitoring, and security controls. The system should handle payment webhooks from providers like Stripe, PayPal, and Square with automatic retries and audit logging.",A fintech startup needs to process payment webhooks from multiple providers in real-time. The system must handle variable traffic spikes during peak shopping seasons and maintain strict compliance with PCI DSS requirements for payment data handling.,"Serverless infrastructure deployed in us-east-1 region using API Gateway REST API, Lambda functions with Python 3.11 runtime on ARM64 architecture, DynamoDB for payment webhook storage, SQS for dead letter queues, and SNS for alerting. Requires CDK 2.x with Python 3.8+, AWS CLI configured with appropriate permissions. All resources deployed within a single AWS account with X-Ray tracing enabled for distributed tracing. Customer-managed KMS keys for encryption at rest and in transit.","[""DynamoDB tables must use on-demand billing mode"", ""API Gateway must use AWS WAF with rate-based rules"", ""Lambda functions must have reserved concurrent executions set to prevent cold starts"", ""Lambda functions must use Lambda Layers for shared dependencies"", ""API Gateway must implement request throttling at 1000 requests per second"", ""X-Ray tracing must be enabled across all services"", ""Lambda functions must use ARM64 architecture for cost optimization"", ""All Lambda environment variables must be encrypted with a customer-managed KMS key"", ""DynamoDB streams must trigger a separate audit Lambda function"", ""Dead letter queues must be configured for all asynchronous invocations""]"
y2o9v8,error,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy an EKS cluster for a payment processing platform. The configuration must: 1. Create an EKS cluster with Kubernetes 1.28+ and IPv6 support enabled across 3 availability zones. 2. Configure the control plane with private endpoint access only and enable audit logging to CloudWatch. 3. Set up 4 managed node groups: 2 general-purpose (min: 2, max: 10, desired: 4 nodes each), 1 memory-optimized (min: 1, max: 5, desired: 2), and 1 GPU-enabled (min: 1, max: 3, desired: 1). 4. Use Bottlerocket AMI for all node groups with SSM agent pre-installed. 5. Enable IRSA and create 3 service accounts: cluster-autoscaler, aws-load-balancer-controller, and external-secrets-operator. 6. Configure AWS KMS encryption for EKS secrets with automatic key rotation. 7. Install AWS Load Balancer Controller as an EKS add-on with proper IAM permissions. 8. Implement cluster autoscaler configuration with proper node group tags. 9. Set up OIDC identity provider for GitHub Actions integration. 10. Enable Container Insights and configure log retention for 30 days. 11. Create CloudWatch dashboards for cluster metrics and node group health. 12. Output cluster endpoint, OIDC issuer URL, and kubectl configuration commands. Expected output: A fully functional EKS cluster accessible via kubectl with automated scaling, GPU support for ML workloads, enhanced security through Bottlerocket and KMS encryption, and complete observability through Container Insights. The cluster should handle both stateless API services and stateful payment processing workloads with appropriate node selection.","A fintech company needs to deploy a Kubernetes-based microservices platform for their payment processing system. They require a highly available EKS cluster with strict security controls, automated node scaling, and integration with their existing AWS infrastructure. The cluster must support both CPU and GPU workloads for fraud detection ML models.","Production EKS infrastructure deployed in us-east-1 region using AWS CDK with Python. Requires CDK 2.100+ with aws-cdk-lib, constructs, and boto3 packages. VPC spans 3 AZs with private subnets for worker nodes and public subnets for load balancers. Cluster endpoint accessible only through private network. Integration with AWS Systems Manager for node access. CloudWatch Container Insights enabled for monitoring. AWS Secrets Manager for storing sensitive configuration. Total of 4 node groups: 2 for general workloads (t3.large), 1 for memory-intensive tasks (r5.xlarge), and 1 for GPU ML workloads (g4dn.xlarge).","[""Implement IRSA (IAM Roles for Service Accounts) for pod-level permissions"", ""Configure OIDC provider for external authentication integration"", ""Create separate node groups for GPU workloads using g4dn instances"", ""Deploy cluster across exactly 3 availability zones"", ""Implement pod security standards with restricted baseline"", ""Use EKS version 1.28 or higher with IPv6 enabled"", ""Set up AWS Load Balancer Controller as a managed add-on"", ""Use AWS KMS for envelope encryption of Kubernetes secrets"", ""Enable cluster autoscaler with tagging strategy for node groups"", ""Node groups must use Bottlerocket AMI for enhanced security""]"
w8d7q1,done,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy a containerized microservices architecture for a trading analytics platform. The configuration must: 1. Define an ECS cluster with capacity providers for both Fargate and Fargate Spot. 2. Create three microservices (data-ingestion, analytics-engine, api-gateway) each with its own ECS service definition. 3. Configure AWS App Mesh with virtual nodes for each service and virtual routers for traffic distribution. 4. Set up ECR repositories with image scanning on push and lifecycle policies to retain only the last 10 images. 5. Implement auto-scaling for each service with target tracking policies on CPU and memory metrics. 6. Create an Application Load Balancer with path-based routing to the api-gateway service. 7. Configure AWS Cloud Map namespace for service discovery within the VPC. 8. Define IAM task execution and task roles with least-privilege permissions. 9. Set up CloudWatch Log Groups with metric filters for error tracking. 10. Create Secrets Manager secrets for database endpoints and rotate them every 90 days. 11. Configure App Mesh retry policies with exponential backoff for resilience. 12. Implement health checks for both ALB target groups and ECS service definitions. Expected output: A fully functional CDK stack that deploys the complete container orchestration infrastructure with all services running in Fargate, accessible through the ALB, with service mesh traffic management and comprehensive monitoring.",A financial services company needs to deploy a microservices-based trading analytics platform. The system processes real-time market data through multiple containerized services that must communicate securely and scale independently based on load.,"Multi-AZ container orchestration environment in us-east-1 region using Amazon ECS with AWS Fargate for serverless container execution. Infrastructure includes App Mesh service mesh for traffic management, ECR for container registry, CloudWatch for monitoring, and Secrets Manager for credentials. VPC spans 3 AZs with private subnets for ECS tasks and public subnets for ALB. NAT Gateways provide outbound internet access. Requires AWS CDK 2.x with Python 3.9+, Docker for local testing, and AWS CLI configured with appropriate permissions.","[""Container images must be stored in private ECR repositories with vulnerability scanning enabled"", ""Implement blue-green deployment strategy using ECS deployment configuration"", ""Use AWS Service Discovery for internal service name resolution"", ""Service-to-service communication must use AWS App Mesh with mTLS encryption"", ""ECS services must use Fargate Spot instances for cost optimization"", ""Container logs must be sent to CloudWatch Logs with 30-day retention"", ""Implement circuit breaker pattern using App Mesh retry policies"", ""Deploy across 3 availability zones for high availability"", ""Use AWS Secrets Manager for database credentials and API keys"", ""Each microservice must have its own auto-scaling policy based on custom CloudWatch metrics""]"
e0l6u1,done,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to deploy a production VPC infrastructure. The configuration must: 1. Create a VPC with CIDR 10.0.0.0/16 and enable DNS hostnames and DNS resolution. 2. Deploy 3 public subnets (10.0.1.0/24, 10.0.2.0/24, 10.0.3.0/24) across us-east-1a, us-east-1b, and us-east-1c. 3. Deploy 3 private subnets (10.0.10.0/23, 10.0.12.0/23, 10.0.14.0/23) in the same AZs. 4. Deploy 3 database subnets (10.0.20.0/24, 10.0.21.0/24, 10.0.22.0/24) with no internet connectivity. 5. Create an Internet Gateway and attach it to the VPC. 6. Launch 3 NAT instances (t3.micro) using the latest Ubuntu 20.04 AMI, one per public subnet, with source/destination checks disabled. 7. Configure route tables with proper associations and routes for each subnet tier. 8. Create security groups for web tier (ports 80, 443 from 0.0.0.0/0), app tier (port 8080 from web tier only), and database tier (port 5432 from app tier only). 9. Implement Network ACLs with inbound rules allowing only necessary traffic and restricting ephemeral ports to 32768-65535. 10. Enable VPC Flow Logs publishing to an S3 bucket with server-side encryption and 7-day object expiration. 11. Create an S3 VPC endpoint for private subnet access to S3 without internet routing. 12. Apply consistent tagging across all resources with Environment='production', Project='payment-platform', and CostCenter='engineering'. Expected output: A fully functional VPC with proper network segmentation, custom NAT instances for cost efficiency, comprehensive logging, and S3 endpoint for private access. The stack should export VPC ID, subnet IDs grouped by tier, NAT instance IDs, security group IDs, and S3 bucket name for flow logs.","A fintech startup needs to establish a secure cloud foundation for their payment processing platform. They require strict network isolation, regulatory compliance, and the ability to integrate with on-premises systems through VPN connections.","Production-grade VPC infrastructure in us-east-1 region spanning 3 availability zones. Deployment uses Pulumi TypeScript SDK v3.x with AWS Classic provider. Architecture includes public subnets for load balancers, private subnets for application servers, and database subnets with no internet access. Custom NAT instances on Ubuntu 20.04 AMIs handle outbound traffic. S3 bucket stores VPC Flow Logs with encryption. Site-to-site VPN connection ready for on-premises integration. Requires Node.js 16+, Pulumi CLI 3.x, and AWS credentials with VPC, EC2, S3, and CloudWatch permissions.","[""NAT instances must be used instead of NAT Gateways for cost optimization"", ""All route tables must have explicit names following pattern: {env}-{tier}-{az}-rt"", ""All resources must be tagged with Environment, Project, and CostCenter"", ""Security groups must deny all traffic by default with only explicit allow rules"", ""VPC Flow Logs must be enabled and stored in S3 with 7-day lifecycle policy"", ""Network ACLs must restrict ephemeral port range to 32768-65535"", ""Private subnets must have no direct internet gateway routes"", ""VPC must span exactly 3 availability zones in us-east-1"", ""S3 gateway endpoint must be created for private subnet access to S3"", ""Public subnets must use /24 CIDR blocks, private subnets must use /23""]"
o4f5s0,in_progress,CDK,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a CDK TypeScript program to deploy a payment processing web application infrastructure. The configuration must: 1. Set up a VPC with public, private, and isolated subnets across 3 AZs. 2. Deploy an RDS Aurora PostgreSQL cluster in isolated subnets with automated backups and encryption. 3. Configure ECS Fargate service running Node.js API containers with ALB in public subnets. 4. Implement autoscaling for ECS tasks based on 70% CPU threshold. 5. Create S3 bucket for React frontend with CloudFront distribution and OAI. 6. Store database credentials in Secrets Manager with automatic rotation every 30 days. 7. Configure WAF rules on CloudFront to block common attack patterns. 8. Set up CloudWatch dashboards monitoring API latency, error rates, and database connections. 9. Implement blue-green deployment capability for the ECS service. 10. Create SNS topic for alerting on application errors exceeding 1% threshold. Expected output: A complete CDK TypeScript application with stack definitions for networking, compute, storage, and monitoring components. The code should include proper error handling, environment-specific configurations, and deployment scripts for staging and production environments.","A fintech startup needs to deploy their real-time payment processing web application on AWS. The application consists of a React frontend, Node.js API backend, and PostgreSQL database for transaction records. They require zero-downtime deployments and strict compliance with PCI-DSS standards.","Production infrastructure deployed in us-east-1 across 3 availability zones using ECS Fargate for containerized Node.js API, RDS Aurora PostgreSQL for transaction storage, S3 and CloudFront for React frontend hosting. Requires AWS CDK 2.x with TypeScript, Node.js 18+, Docker installed for container builds. VPC configured with public subnets for ALB, private subnets for ECS tasks and RDS, isolated subnets for database. NAT Gateways in each AZ for outbound connectivity. AWS Systems Manager Session Manager for secure instance access without SSH keys.","[""All IAM roles must follow least privilege principle with explicit deny for unnecessary actions"", ""API endpoints must be accessible only through CloudFront distribution"", ""Frontend assets must be served from S3 with CloudFront caching"", ""All resources must be deployed within a single VPC with proper subnet isolation"", ""Load balancer must perform health checks every 30 seconds with 2 consecutive failures triggering replacement"", ""Backend API must run on ECS Fargate with autoscaling based on CPU utilization"", ""Application logs must be centralized in CloudWatch Logs with 90-day retention"", ""Database credentials must be stored in AWS Secrets Manager and rotated automatically"", ""All data at rest must be encrypted using AWS KMS customer-managed keys""]"
u4t3s8,error,CDK,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CDK TypeScript program to implement an automated disaster recovery solution with active-passive failover capabilities. The configuration must: 1. Set up Route 53 hosted zone with primary and secondary record sets using failover routing policy. 2. Deploy RDS Aurora PostgreSQL clusters in both regions with automated backups and cross-region read replicas. 3. Configure DynamoDB global tables for session management with on-demand billing and point-in-time recovery. 4. Implement S3 buckets with cross-region replication rules for static content and application artifacts. 5. Create Lambda functions to perform health checks on primary region services every 60 seconds. 6. Deploy Application Load Balancers with target groups pointing to ECS Fargate services in both regions. 7. Set up EventBridge rules to orchestrate failover procedures when health checks fail consecutively. 8. Configure CloudWatch dashboards displaying RTO/RPO metrics and service health across regions. 9. Implement automated DNS failover using Route 53 health checks with 3 consecutive failures threshold. 10. Create SNS topics for alerting with email subscriptions for operations team notifications. 11. Deploy AWS Backup plans for RDS and DynamoDB with 7-day retention in both regions. 12. Establish CloudWatch Logs groups with 30-day retention for application and infrastructure logs. Expected output: A fully deployable CDK application that provisions disaster recovery infrastructure across two regions, with automated health monitoring and failover mechanisms that achieve 15-minute RPO and 30-minute RTO targets.","A financial services company requires a disaster recovery solution for their critical payment processing application. The primary region (us-east-1) hosts the production workload, and they need an automated failover mechanism to us-east-2 with RPO of 15 minutes and RTO of 30 minutes.","Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-east-2 (secondary). Utilizes Route 53 for DNS failover, RDS Aurora PostgreSQL with cross-region read replicas, DynamoDB global tables for session data, S3 with cross-region replication for static assets, Lambda functions for health monitoring, Application Load Balancers with auto-scaling groups, EventBridge for orchestration, and CloudWatch for monitoring. Requires CDK 2.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate permissions. VPCs in both regions with 3 availability zones each, private subnets for compute and database tiers, public subnets for ALBs, VPC peering for cross-region communication.","[""RDS cross-region read replicas must use encrypted snapshots with KMS CMK"", ""All IAM roles must follow least privilege with explicit deny statements for unused regions"", ""CloudWatch alarms must trigger SNS notifications to multiple email endpoints"", ""S3 bucket replication must include object metadata and tags"", ""Application Load Balancers must use AWS Certificate Manager certificates in both regions"", ""Route 53 health checks must monitor both primary and secondary endpoints with 30-second intervals"", ""EventBridge rules must coordinate failover orchestration with 5-minute retry intervals"", ""DynamoDB global tables must have contributor insights enabled for both regions"", ""Lambda functions for health monitoring must have dead letter queues configured""]"
w3k3x9,done,Pulumi,Python,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi Python program to deploy a multi-region disaster recovery infrastructure. The configuration must: 1. Set up VPCs in us-east-1 and us-east-2 with 3 availability zones each, including private and public subnets. 2. Deploy DynamoDB Global Tables with on-demand billing and point-in-time recovery enabled. 3. Create Lambda functions in both regions that process payment transactions, with identical code and environment variables. 4. Configure API Gateway REST APIs in each region with custom domain names and AWS Certificate Manager certificates. 5. Implement Route 53 hosted zone with health checks and failover routing between primary and secondary API endpoints. 6. Set up AWS Global Accelerator with endpoints in both regions and automatic failover based on health checks. 7. Create S3 buckets with cross-region replication including delete marker replication and RTC for sub-15-minute replication. 8. Deploy Aurora Global Database with PostgreSQL 14 engine, one write cluster in us-east-1 and read replica cluster in us-east-2. 9. Configure EventBridge Global Endpoints to route events between regions with dead letter queues. 10. Implement AWS Backup plans for Aurora clusters with cross-region copy enabled. 11. Set up CloudWatch dashboards in both regions to monitor replication lag and system health. 12. Create SNS topics for alerting on failover events and replication issues. Expected output: A Pulumi program that creates all resources with proper tagging (Environment: production, DR-Region: primary/secondary), outputs the Global Accelerator DNS name, primary and secondary API endpoints, and health check URLs. The solution should handle automatic failover within 60 seconds of primary region failure.",A financial services company needs to implement a multi-region disaster recovery solution for their critical payment processing system. The system currently runs in us-east-1 and requires near-zero RPO with automated failover capabilities to us-east-2.,"Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-east-2 (secondary). Utilizes DynamoDB Global Tables for data replication, Lambda for compute, API Gateway for endpoints, Route 53 for DNS failover, Global Accelerator for traffic routing, S3 with cross-region replication, Aurora Global Database for relational data, EventBridge for event processing, and Systems Manager for configuration management. Requires Pulumi 3.x with Python 3.9+, AWS CLI configured with appropriate permissions. VPCs in both regions with private subnets for compute resources and public subnets for load balancers. Direct Connect or VPN connectivity between regions for secure replication.","[""Configure S3 cross-region replication with RTC (Replication Time Control)"", ""Use AWS Global Accelerator for automatic traffic routing between regions"", ""Use EventBridge Global Endpoints for event routing across regions"", ""Configure AWS Systems Manager Parameter Store replication for configuration data"", ""Implement AWS Backup for automated cross-region backup of RDS Aurora"", ""Deploy Lambda functions in both regions with identical configurations"", ""Use Route 53 health checks with failover routing policies"", ""Deploy API Gateway with custom domain names in each region"", ""Implement DynamoDB Global Tables for multi-region data replication""]"
c0c7a2,done,CDK,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CDK TypeScript program to implement a multi-region disaster recovery solution for a trading platform. The configuration must: 1. Set up Route 53 hosted zone with health checks monitoring endpoints in us-east-1 and us-east-2. 2. Deploy Aurora PostgreSQL global database with writer in us-east-1 and read replica in us-east-2. 3. Create Lambda functions in both regions processing trade orders from SQS queues. 4. Configure DynamoDB global tables for user session data with point-in-time recovery enabled. 5. Implement S3 buckets with cross-region replication for application configurations and audit logs. 6. Set up CloudWatch alarms monitoring RDS lag, Lambda errors, and API Gateway latency. 7. Create Step Functions state machine orchestrating failover process including RDS promotion and Route 53 updates. 8. Deploy API Gateway REST APIs in both regions with custom domain names. 9. Configure EventBridge rules forwarding critical events between regions. 10. Implement automated testing Lambda that validates failover readiness every hour. Expected output: CDK application with separate stacks for primary and secondary regions, shared constructs for cross-region resources, and deployment scripts that validate both regions are synchronized before completing.","A financial services company operates a critical trading platform that must maintain 99.99% uptime. After experiencing a regional outage that cost millions in lost trades, they need to implement a multi-region disaster recovery solution that can automatically failover within 60 seconds.","Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-east-2 (secondary). Utilizes Route 53 for DNS failover, Aurora Global Database for data persistence, Lambda for compute, DynamoDB global tables for session management, and S3 with cross-region replication. Requires CDK 2.x with TypeScript, AWS CLI configured with multi-region access. VPCs in both regions with private subnets, VPC peering for cross-region communication. CloudWatch cross-region monitoring with automated failover triggers via Step Functions.","[""Configure DynamoDB global tables for session state replication"", ""Use S3 cross-region replication for static assets and configuration files"", ""Implement CloudWatch cross-region alarms and SNS notifications"", ""Ensure all IAM roles support cross-region assume role capabilities"", ""Deploy Lambda functions in both regions using identical deployment packages"", ""Use Systems Manager Parameter Store with region-specific configurations"", ""Implement cross-region RDS Aurora Global Database with automated promotion"", ""Use Route 53 health checks with failover routing policy for automatic DNS failover"", ""Implement automated failback procedures using Step Functions""]"
u5e5g1,done,CDK,Python,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a CDK Python program to migrate a payment processing system from on-premises to AWS while maintaining continuous operation. The configuration must: 1. Define a VPC with 3 availability zones, each containing public, private, and database subnets. 2. Create an RDS Aurora PostgreSQL cluster with automated backups and read replicas for the customer database. 3. Set up DynamoDB tables for transaction records with global secondary indexes for query optimization. 4. Deploy Lambda functions for payment validation, fraud detection, and transaction processing with VPC connectivity. 5. Configure API Gateway with request validation and VPC Link to private ALB. 6. Implement blue-green deployment using two target groups on the ALB with weighted routing. 7. Create S3 buckets for audit logs with 90-day retention and compliance archival. 8. Set up CloudWatch dashboards displaying API response times, error rates, and database performance metrics. 9. Configure SNS topics for alerting on failed transactions and system errors. 10. Implement AWS Secrets Manager rotation for database credentials with Lambda rotation function. Expected output: A complete CDK Python application with stack definitions for networking, compute, storage, and monitoring resources. The code should include proper tagging for cost allocation, IAM roles with least privilege access, and CloudFormation outputs for key resource identifiers needed by the operations team.",A fintech company needs to migrate their payment processing infrastructure from a legacy on-premises setup to AWS. The existing system handles credit card transactions with strict PCI compliance requirements and must maintain zero downtime during the migration phase.,"Production-grade payment processing infrastructure deployed in us-east-1 with multi-AZ failover capabilities. Core services include API Gateway with VPC Link, ALB with target groups for blue-green deployments, Lambda functions for payment processing logic, DynamoDB for transaction records, S3 for audit logs, and RDS Aurora PostgreSQL for customer data. Requires AWS CDK 2.x with Python 3.9+, boto3, and AWS CLI configured with appropriate IAM permissions. VPC spans 3 availability zones with private subnets for compute resources and database subnets for RDS. NAT Gateways provide outbound internet access for Lambda functions in private subnets.","[""All databases must use encrypted storage with AWS KMS customer-managed keys"", ""All S3 buckets must use versioning and lifecycle policies"", ""CloudWatch alarms must monitor API latency with 99th percentile metrics"", ""DynamoDB tables must have point-in-time recovery enabled"", ""Use AWS Systems Manager Parameter Store for all configuration values"", ""Implement blue-green deployment strategy for zero-downtime migration"", ""API Gateway must use VPC Link to connect to private ALB endpoints"", ""Lambda functions must use reserved concurrency to prevent cold starts"", ""Use AWS CDK v2 with Python 3.9 or higher""]"
m3g4o8,6888,CDK,Python,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CDK Python program to deploy a multi-region disaster recovery infrastructure for a payment processing system. The configuration must: 1. Set up Aurora PostgreSQL Global Database with a primary cluster in us-east-1 and secondary in us-east-2. 2. Configure DynamoDB Global Tables for session data with on-demand billing and point-in-time recovery. 3. Deploy identical Lambda functions in both regions for payment validation, transaction processing, and notification services. 4. Create S3 buckets in both regions with cross-region replication and lifecycle policies to archive data after 90 days. 5. Implement API Gateway REST APIs in both regions with request validation and throttling limits of 10,000 requests per second. 6. Configure Route 53 hosted zone with weighted routing policy (100% to primary, 0% to secondary) and health checks that monitor API Gateway endpoints. 7. Set up CloudWatch alarms for RDS replication lag, Lambda errors, and API Gateway 5XX errors with SNS notifications. 8. Create Systems Manager parameters for database endpoints, API URLs, and feature flags that synchronize between regions. 9. Implement automated failover mechanism using Lambda and Step Functions that can promote secondary region to primary. 10. Deploy CloudWatch dashboards showing cross-region metrics for database connections, API latency, and replication status. Expected output: A complete CDK stack that deploys all infrastructure components across both regions with proper cross-region references, IAM roles for cross-account access, and automated failover capabilities that can switch traffic from primary to secondary region within 5 minutes of detecting an outage.","A financial services company requires a multi-region disaster recovery solution for their critical payment processing system. The primary region hosts the production workload, while a secondary region must maintain a warm standby configuration that can be promoted to primary within minutes during an outage.","Multi-region AWS deployment spanning us-east-1 (primary) and us-east-2 (secondary) for disaster recovery. Architecture includes Aurora Global Database for transactional data, DynamoDB Global Tables for session management, S3 with cross-region replication for static assets. Lambda functions process payment transactions behind API Gateway. Route 53 manages DNS failover with health checks. Requires CDK 2.x with Python 3.9+, AWS CLI configured with appropriate IAM permissions. VPC in each region with 3 AZs, private subnets for compute and database resources, public subnets for NAT gateways and ALBs.","[""All resources must be tagged with 'DR-Role' indicating 'primary' or 'secondary'"", ""CloudWatch dashboards must aggregate metrics from both regions"", ""Use Systems Manager Parameter Store for cross-region configuration synchronization"", ""S3 buckets must use cross-region replication with RTC (Replication Time Control)"", ""API Gateway must use custom domain names with regional endpoints"", ""Use Route 53 health checks with automatic failover between regions"", ""DynamoDB global tables must be configured for bi-directional replication"", ""Lambda functions must be replicated across both regions with identical configurations"", ""RDS Aurora Global Database must have automated backups enabled""]"
e4k2x3,done,Pulumi,Python,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi Python program to deploy a multi-region disaster recovery infrastructure for a payment processing system. The configuration must: 1. Set up Aurora Global Database cluster in us-east-1 as primary with us-east-2 as secondary region. 2. Configure DynamoDB global tables for transaction data with point-in-time recovery enabled. 3. Deploy identical Lambda functions in both regions for payment validation logic. 4. Create S3 buckets with cross-region replication for audit logs and transaction receipts. 5. Implement Route 53 failover routing with health checks monitoring RDS endpoints. 6. Configure CloudWatch dashboards in both regions showing replication lag metrics. 7. Set up SNS topics for automated failover notifications to operations team. 8. Create IAM roles allowing cross-region resource access for disaster recovery operations. 9. Implement CloudWatch alarms monitoring Aurora replication lag exceeding 1 second. 10. Deploy API Gateway endpoints in both regions with custom domain names. Expected output: A Pulumi stack that provisions all resources in both regions with automated failover capabilities, monitoring dashboards showing replication health, and documented procedures for triggering manual failover if needed.",A financial services company requires a multi-region disaster recovery solution for their critical payment processing application. The system must maintain sub-second RPO and ensure automatic failover capabilities while minimizing data loss.,"Multi-region AWS deployment spanning us-east-1 (primary) and us-east-2 (disaster recovery). Infrastructure includes Aurora Global Database for PostgreSQL 13.7, DynamoDB global tables, Lambda functions for payment processing, S3 buckets with cross-region replication, and Route 53 failover routing policies. Requires Pulumi 3.x with Python 3.9+, AWS CLI v2 configured with appropriate credentials. VPCs in both regions with private subnets across 3 availability zones, VPC peering for secure cross-region communication, and NAT gateways for outbound internet access.","[""S3 buckets must use cross-region replication with versioning enabled"", ""Lambda functions must be replicated across regions with identical configurations"", ""Route 53 health checks must monitor both regions continuously"", ""Primary region must be us-east-1 with failover to us-east-2"", ""IAM roles must follow least-privilege principle with cross-region assume permissions"", ""RDS instances must use Aurora Global Database with automated backups"", ""DynamoDB global tables must be configured for bi-directional replication"", ""All resources must be tagged with Environment, Region, and DR-Role tags"", ""CloudWatch alarms must trigger SNS notifications for failover events""]"
x4g2e7,done,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to deploy and maintain consistent infrastructure across three environments (dev, staging, prod) with controlled variations. The configuration must: 1. Define a reusable component class that accepts environment configuration as parameters. 2. Implement VPC creation with environment-specific CIDR blocks (dev: 10.0.0.0/16, staging: 10.1.0.0/16, prod: 10.2.0.0/16). 3. Create RDS PostgreSQL instances with environment-appropriate instance classes (dev: db.t3.micro, staging: db.t3.small, prod: db.m5.large). 4. Deploy Lambda functions for payment processing with consistent code but environment-specific environment variables. 5. Configure API Gateway with rate limiting that scales by environment (dev: 100 req/min, staging: 500 req/min, prod: 2000 req/min). 6. Set up DynamoDB tables for transaction history with consistent schema but varying capacity units. 7. Implement S3 buckets for audit logs with environment-specific lifecycle rules and retention policies. 8. Create CloudWatch dashboards that aggregate metrics across all resources within each environment. 9. Generate a comparison report showing resource configurations across all three environments. 10. Implement drift detection by comparing actual AWS resources against Pulumi state. 11. Ensure all resources are tagged with Environment, ManagedBy, and CostCenter tags. Expected output: A Pulumi program that deploys three identical yet appropriately scaled environments, with a custom resource provider that validates cross-environment consistency and outputs a JSON report detailing any configuration differences between environments.","A fintech company needs to maintain identical infrastructure across development, staging, and production environments to ensure consistent testing and deployment. They currently face configuration drift issues where environments diverge over time, causing deployment failures and security inconsistencies.","Multi-environment AWS infrastructure spanning us-east-1 (production), us-east-2 (staging), and us-east-1 (development). Each environment requires isolated VPCs with 3 availability zones, private and public subnets, NAT gateways, and VPC endpoints for S3 and DynamoDB. Infrastructure includes RDS PostgreSQL 15.x instances, Lambda functions with Node.js 18.x runtime, API Gateway REST APIs, DynamoDB tables, S3 buckets, and CloudWatch monitoring. Requires Pulumi CLI 3.x with TypeScript, AWS CLI configured with appropriate credentials, Node.js 18+, and npm. Each environment uses separate AWS accounts for isolation with cross-account IAM roles for deployment.","[""DynamoDB tables must have consistent GSI configurations but environment-specific read/write capacity"", ""CloudWatch alarms must have environment-adjusted thresholds (dev: 80%, staging: 70%, prod: 60%)"", ""Each environment must have its own isolated VPC with matching CIDR blocks offset by environment"", ""S3 buckets must have lifecycle policies that vary retention by environment (dev: 7 days, staging: 30 days, prod: 90 days)"", ""Lambda functions must have identical memory and timeout configurations across environments"", ""Secrets must be stored in AWS Secrets Manager and rotated every 30 days"", ""RDS instances must use encrypted storage with environment-specific KMS keys"", ""All environments must use identical resource naming conventions with environment prefixes"", ""API Gateway stages must have request throttling limits that scale with environment tier""]"
a1w9m4,done,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi Go program to deploy a highly available payment processing API with automatic regional failover capabilities. The configuration must: 1. Set up API Gateway REST APIs in both us-east-1 and us-east-2 with identical Lambda backend functions. 2. Configure DynamoDB global tables with point-in-time recovery enabled for transaction data storage. 3. Implement Route53 health checks that monitor API Gateway endpoints every 30 seconds. 4. Create failover routing policies that automatically switch traffic to the secondary region within 2 minutes of primary failure. 5. Deploy CloudWatch Synthetics canaries that test API endpoints every 5 minutes in both regions. 6. Configure S3 buckets with cross-region replication for storing transaction audit logs with lifecycle policies. 7. Set up SNS topics in both regions with email subscriptions for failover event notifications. 8. Implement Lambda functions with 100ms timeout for health check responses and 10 seconds for payment processing. 9. Configure Secrets Manager with automatic cross-region replication for API keys and database credentials. 10. Create CloudWatch alarms that trigger when API latency exceeds 500ms or error rate exceeds 1%. Expected output: A Pulumi program that creates identical infrastructure stacks in two regions with Route53 managing automatic failover based on health check results, ensuring zero downtime during regional outages.",A financial services company requires a disaster recovery solution for their payment processing API that can automatically failover between AWS regions. The system must maintain transaction integrity during regional outages and provide automated health checks with rapid failover capabilities.,"Multi-region deployment spanning us-east-1 (primary) and us-east-2 (secondary) for high availability disaster recovery. Infrastructure includes API Gateway with custom domains, Lambda functions for payment processing, DynamoDB global tables for transaction data, S3 with cross-region replication for audit logs. Route53 manages DNS failover between regions. Requires Go 1.19+, Pulumi 3.x CLI, AWS CLI configured with appropriate IAM permissions for multi-region deployments. VPCs in both regions with private subnets for Lambda execution.","[""Implement AWS Systems Manager Parameter Store for configuration synchronization"", ""Use Route53 health checks with failover routing policies for automatic regional failover"", ""Configure S3 buckets with cross-region replication for static assets"", ""Deploy API Gateway with custom domain names in both primary and secondary regions"", ""Implement DynamoDB global tables for cross-region data replication"", ""Configure Lambda functions with reserved concurrency to ensure availability"", ""Implement CloudWatch Synthetics for endpoint monitoring in both regions"", ""Use AWS Secrets Manager with cross-region replication for API credentials"", ""Set up SNS topics for failover notifications with cross-region subscriptions""]"
q7v8c4,error,Pulumi,Python,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi Python program to implement an active-passive disaster recovery solution for a payment processing system. The configuration must: 1. Set up Aurora PostgreSQL Global Database with primary cluster in us-east-1 and secondary in us-east-2. 2. Deploy identical Lambda functions in both regions for payment processing logic. 3. Configure API Gateway REST APIs in both regions with custom domain names. 4. Implement Route 53 hosted zone with primary and secondary record sets using failover routing policy. 5. Create health checks monitoring the primary region's API Gateway endpoint. 6. Set up S3 buckets in both regions with cross-region replication including delete markers and encryption. 7. Configure CloudWatch dashboard aggregating metrics from both regions. 8. Implement SNS topics in both regions for alerting on failover events. 9. Create IAM roles with cross-region assume permissions for Lambda execution. 10. Set up DynamoDB global tables for session state management. 11. Configure automated backups with 7-day retention for all databases. 12. Tag all resources with Environment=DR, CostCenter=Operations, and Criticality=High. Expected output: A complete Pulumi Python program that creates all infrastructure components in both regions, with automated failover capabilities tested to meet RTO/RPO requirements. The program should use Pulumi's ComponentResource pattern to organize regional deployments and include proper error handling for multi-region operations.","A financial services company needs to implement a disaster recovery solution for their payment processing system. The primary region experienced a 4-hour outage last quarter, resulting in significant revenue loss. They require an active-passive DR setup with automated failover capabilities and real-time data replication.","Active-passive disaster recovery infrastructure spanning us-east-1 (primary) and us-east-2 (DR). Uses Aurora PostgreSQL Global Database for data tier with read replicas, Lambda functions for business logic, API Gateway for endpoints, S3 for static assets with cross-region replication. Route 53 manages DNS failover with health checks. CloudWatch aggregates metrics from both regions. Requires Pulumi 3.x with Python 3.9+, AWS CLI configured with appropriate IAM permissions for multi-region deployments. VPCs in both regions with private subnets for databases and Lambda functions.","[""Use Aurora Global Database for cross-region replication"", ""RPO (Recovery Point Objective) must be under 1 minute"", ""RTO (Recovery Time Objective) must be under 5 minutes"", ""Implement Route 53 health checks with automatic DNS failover"", ""Primary region must be us-east-1 and DR region must be us-east-2"", ""Implement CloudWatch cross-region metrics aggregation"", ""All Lambda functions must be replicated to both regions"", ""S3 buckets must use cross-region replication with delete markers"", ""Total monthly cost must not exceed $5000 for the DR infrastructure""]"
m8q0t8,done,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a containerized payment processing system on AWS ECS Fargate. The configuration must: 1. Set up an ECS cluster with Fargate compute capacity across 3 availability zones. 2. Create ECR repositories for three microservices: api-gateway, payment-processor, and fraud-detector. 3. Define ECS task definitions for each service with 1 vCPU and 2GB memory, including health checks. 4. Configure auto-scaling policies for each service (min: 2, max: 10 tasks) based on CPU/memory metrics. 5. Implement AWS Cloud Map service discovery namespace for inter-service communication. 6. Create IAM task roles with specific permissions for each service to access required AWS resources. 7. Set up CloudWatch log groups with encryption enabled and 30-day retention for container logs. 8. Configure Secrets Manager secrets for database connection strings and third-party API keys. 9. Deploy an Application Load Balancer with target groups for the api-gateway service. 10. Ensure all ECS tasks run in private subnets with security groups allowing only required ports. 11. Enable container insights for the ECS cluster for monitoring and observability. 12. Tag all resources with Environment, Service, and CostCenter tags for compliance. Expected output: A fully functional ECS Fargate cluster running three microservices with auto-scaling, service discovery, encrypted logging, and secure credential management. The api-gateway should be accessible through the ALB, while payment-processor and fraud-detector communicate internally via Cloud Map DNS names.","A fintech startup needs to deploy their microservices architecture on AWS ECS with strict compliance requirements. Their payment processing system consists of multiple containerized services that must be highly available, secure, and capable of handling variable transaction loads throughout the day.","Multi-AZ ECS Fargate cluster deployed in us-east-1 across three availability zones for high availability. Infrastructure includes VPC with private subnets, NAT gateways for outbound connectivity, Application Load Balancer for ingress, ECR repositories for container images, CloudWatch Logs for centralized logging, and Secrets Manager for credential storage. Requires Pulumi CLI 3.x with TypeScript, Node.js 16+, AWS CLI configured with appropriate credentials. Services communicate via AWS Cloud Map service discovery within the VPC.","[""Network traffic between services must use service discovery with AWS Cloud Map"", ""Task definitions must specify resource limits and health check configurations"", ""All ECS tasks must run in private subnets with no direct internet access"", ""All container images must be scanned for vulnerabilities before deployment"", ""Each service must have dedicated IAM task roles with least-privilege permissions"", ""Services must use AWS Secrets Manager for database credentials and API keys"", ""Container logs must be encrypted and sent to CloudWatch Logs with 30-day retention"", ""Services must auto-scale based on CPU and memory metrics with 70% thresholds"", ""Deployment must use blue/green strategy with automated rollback on failures""]"
t7j2q6,done,CDK,Python,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a CDK Python program to deploy a payment processing system that maintains consistency across development, staging, and production environments. The configuration must: 1. Define a base stack class that accepts environment-specific parameters through CDK context 2. Create reusable constructs for API Gateway with Lambda integration, DynamoDB tables, and S3 buckets 3. Implement environment-specific configurations where DynamoDB uses on-demand billing for dev/staging but provisioned capacity (5 RCU, 5 WCU) for production 4. Configure S3 buckets with versioning and lifecycle rules that transition objects to Glacier after 90 days in production, 30 days in staging/dev 5. Set up Lambda functions with 512MB memory and 30-second timeout consistently across all environments 6. Configure API Gateway with throttling of 100 requests/second for dev, 1000 for staging, and 10000 for production 7. Create environment-specific KMS keys for encrypting DynamoDB tables and S3 buckets 8. Deploy CloudWatch alarms for Lambda errors and API Gateway 4xx/5xx rates only in staging and production 9. Implement CDK aspects to enforce mandatory tags: Environment, CostCenter, Owner, and DataClassification 10. Use a single app.py file that can deploy to any environment based on CDK context values 11. Ensure all IAM roles follow least-privilege principles with environment-specific resource ARNs 12. Configure SQS dead-letter queues with different retention periods: 3 days for dev, 7 days for staging, 14 days for production. Expected output: A complete CDK application with app.py, base stack class, custom constructs, and CDK aspects that can be deployed to any environment using context flags like 'cdk deploy -c env=production'. The solution should prevent configuration drift by centralizing all environment-specific values in a context configuration structure.","A financial services company needs to ensure their payment processing infrastructure is identical across development, staging, and production environments. They've experienced issues where configurations drift between environments, leading to failed deployments and security audit findings.","Multi-environment AWS deployment across us-east-1 (production), us-east-2 (staging), and us-east-1 (development). Each environment requires isolated VPCs with 3 availability zones, private subnets for compute resources, and public subnets for load balancers. Infrastructure includes API Gateway, Lambda functions for payment processing, DynamoDB for transaction storage, S3 for audit logs, and SQS for asynchronous processing. Requires Python 3.9+, AWS CDK 2.x, and AWS CLI configured with appropriate credentials for all three accounts.","[""Each environment must have its own KMS key for encryption"", ""Use CDK aspects to validate that all resources follow the company's tagging standards"", ""DynamoDB tables must use on-demand billing in dev/staging but provisioned capacity in production"", ""Implement a custom CDK construct for reusable payment processing components"", ""Lambda functions must have identical memory and timeout settings across all environments"", ""All S3 buckets must have versioning enabled and lifecycle policies"", ""Use CDK context values to parameterize environment-specific configurations"", ""API Gateway stages must use identical throttling rules except for rate limits"", ""CloudWatch alarms must be created only for staging and production environments""]"
v1r5w5,done,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi Go program to deploy a serverless transaction processing system. The configuration must: 1. Create an API Gateway REST API with /transaction POST endpoint that validates request body against OpenAPI schema. 2. Deploy a transaction validator Lambda function that processes incoming requests and writes to DynamoDB. 3. Set up a DynamoDB table with partition key 'transactionId' and sort key 'timestamp', with streams enabled. 4. Create a fraud detection Lambda triggered by DynamoDB streams to analyze transaction patterns. 5. Implement SQS FIFO queue for maintaining transaction order with visibility timeout of 30 seconds. 6. Configure a notification Lambda that reads from SQS and sends results to SNS topic. 7. Set up dead letter queues for both fraud detection and notification Lambdas. 8. Create custom KMS key for encrypting Lambda environment variables. 9. Configure API Gateway usage plan with API key requirement and throttling. 10. Set up CloudWatch Log groups with 30-day retention for all Lambda functions. 11. Export API Gateway invoke URL and API key for client access. Expected output: A fully deployed serverless architecture that processes transactions through API Gateway, validates and stores them in DynamoDB, performs asynchronous fraud detection, and sends notifications while maintaining transaction ordering through SQS FIFO queues.","A fintech startup needs to process credit card transactions in real-time, validate them against fraud patterns, and store results for compliance auditing. The system must handle variable loads during peak shopping seasons while maintaining sub-second response times.","Serverless infrastructure deployed in us-east-1 region using API Gateway REST API, Lambda functions with Go runtime, DynamoDB for transaction storage, SQS FIFO queues for message ordering, and KMS for encryption. Requires Pulumi CLI 3.x with Go SDK, AWS CLI configured with appropriate credentials. No VPC required as all services are serverless. CloudWatch Logs for monitoring with 30-day retention. IAM roles follow least-privilege principle with separate roles for each Lambda function.","[""Use SQS FIFO queues for transaction sequencing"", ""DynamoDB streams must trigger fraud detection Lambda asynchronously"", ""Lambda environment variables must be encrypted with a custom KMS key"", ""All Lambda functions must use Go 1.x runtime"", ""Lambda functions must have reserved concurrent executions of exactly 100"", ""API Gateway must use request validation with OpenAPI 3.0 schema"", ""API Gateway must have usage plans with 10000 requests/day limit"", ""Use DynamoDB with on-demand billing for transaction storage"", ""Dead letter queues must retain failed messages for 14 days""]"
a7d2g1,error,CDK,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a CDK TypeScript program to implement a multi-environment infrastructure replication system for a trading platform. The configuration must: 1. Define a base stack class that accepts environment configuration objects containing region, account ID, and environment name. 2. Implement a configuration management system using TypeScript interfaces that enforces type safety for environment-specific values. 3. Create Lambda functions for order processing with environment-specific memory allocations (dev: 512MB, staging: 1024MB, prod: 2048MB). 4. Deploy DynamoDB tables with environment-appropriate read/write capacity units and point-in-time recovery enabled only for production. 5. Set up API Gateway REST APIs with environment-specific throttling (dev: 100 req/sec, staging: 500 req/sec, prod: 2000 req/sec). 6. Configure S3 buckets for trade data storage with lifecycle policies that retain data for 30 days in dev, 90 days in staging, and indefinitely in production. 7. Implement SQS queues for order processing with environment-specific message retention periods and dead letter queue configurations. 8. Create a deployment pipeline that validates infrastructure consistency across environments before promotion. 9. Generate CloudFormation drift detection alarms that trigger when any environment deviates from the defined configuration. 10. Implement automated rollback mechanisms that revert changes if post-deployment validation fails. Expected output: A complete CDK application with separate stack files for each service component, a central configuration management system, environment-specific configuration files, and a CDK pipeline that ensures infrastructure consistency across all three environments while allowing controlled variations in capacity and retention settings.","A financial services company operates identical trading platforms across development, staging, and production environments. Recent configuration drift incidents have caused production issues when features tested in staging behaved differently in production due to mismatched infrastructure settings.","Multi-environment AWS infrastructure spanning us-east-1 (production), us-east-2 (staging), and us-east-1 (development). Each environment requires isolated VPCs with 3 availability zones, private and public subnets, and NAT gateways. Core services include API Gateway, Lambda functions, DynamoDB tables, S3 buckets, and SQS queues. Infrastructure managed through CDK 2.x with TypeScript, requiring Node.js 18+, AWS CLI configured with appropriate IAM permissions. Each environment maintains separate AWS accounts for isolation with cross-account deployment capabilities through CDK pipelines.","[""API Gateway stages must be automatically created with matching throttling limits per environment"", ""All S3 buckets must have environment-appropriate lifecycle policies and encryption settings"", ""Environment-specific tags must be automatically applied for cost tracking and compliance"", ""Each environment must have identical resource naming patterns with environment prefixes"", ""Stack outputs must be automatically exported to SSM Parameter Store for cross-stack references"", ""All IAM roles must follow least-privilege principles with environment-specific boundaries"", ""Lambda functions must use environment-specific reserved concurrent executions"", ""All environment configurations must be derived from a single source of truth with environment-specific overrides"", ""Parameter Store must be used for runtime configuration values that differ between environments""]"
r8h4p1,error,Pulumi,Python,expert,Application Deployment,Web Application Deployment,"Create a Pulumi Python program to deploy a production-ready EKS cluster with advanced security and multi-tenancy features. The configuration must: 1. Create an EKS cluster version 1.28 with private endpoint access and enabled control plane logging for all log types. 2. Deploy a VPC with 3 private subnets across different AZs and appropriate tagging for EKS. 3. Configure managed node groups using Bottlerocket AMI with min=3, max=10, desired=5 instances of t3.large. 4. Implement IRSA by creating an OIDC provider and linking it to the cluster. 5. Deploy the Cluster Autoscaler with proper IAM role and service account configuration. 6. Create three tenant namespaces (tenant-a, tenant-b, tenant-c) with Pod Security Standards set to 'restricted'. 7. Configure NetworkPolicies to deny all inter-namespace traffic by default. 8. Create IAM roles for each tenant namespace with policies limiting access only to their specific S3 bucket prefix. 9. Deploy AWS Load Balancer Controller with required IAM permissions. 10. Configure CloudWatch Container Insights for cluster monitoring. 11. Enable envelope encryption for Kubernetes secrets using AWS KMS. 12. Output the cluster endpoint, OIDC issuer URL, and kubeconfig command. Expected output: A fully functional EKS cluster with multi-tenant isolation, where each tenant can only access their designated AWS resources through IRSA, network traffic is isolated between namespaces, and the cluster can automatically scale nodes based on pod requirements while maintaining security best practices.","A financial services company needs to deploy a production-grade Kubernetes cluster on AWS EKS to host their microservices platform. The cluster must support multi-tenancy with strict security boundaries, automated node scaling based on workload demands, and integration with existing AWS services for logging and monitoring.","Production EKS cluster deployment in us-east-1 region using Pulumi Python. Requires AWS CLI configured with appropriate permissions, Python 3.9+, and Pulumi 3.x installed. The infrastructure includes EKS control plane with managed node groups using Bottlerocket AMI, VPC with private subnets across 3 AZs, NAT gateways for outbound traffic, and integration with CloudWatch for logging. The cluster will host multi-tenant workloads with namespace isolation and RBAC controls.","[""Network policies must isolate tenant namespaces from each other"", ""All control plane logs must be enabled and sent to CloudWatch"", ""Node groups must span at least 3 availability zones for high availability"", ""The cluster must use IRSA (IAM Roles for Service Accounts) for AWS service access"", ""Pod Security Standards must be enforced at the namespace level"", ""Each tenant namespace must have its own IAM service account with minimal permissions"", ""The EKS cluster must use Kubernetes version 1.28 or higher"", ""Node groups must use Bottlerocket AMI for enhanced security"", ""Cluster autoscaler must be configured with proper RBAC permissions""]"
z8l2n5,error,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi Go program to deploy a production EKS cluster with advanced security and networking configurations. The configuration must: 1. Create a dedicated VPC with 3 private subnets (10.0.1.0/24, 10.0.2.0/24, 10.0.3.0/24) and 3 public subnets (10.0.101.0/24, 10.0.102.0/24, 10.0.103.0/24) across different AZs. 2. Deploy an EKS cluster with Kubernetes 1.28 and enable control plane logging for audit, authenticator, and API server logs. 3. Configure OIDC provider and create IAM roles for the cluster autoscaler and AWS Load Balancer Controller service accounts. 4. Create two managed node groups: one for general workloads (t3.large, min 2, max 10 nodes) and one for compute-intensive workloads (c5.2xlarge, min 1, max 5 nodes) using Bottlerocket AMI. 5. Install Calico CNI plugin version 3.26.x using Helm and configure at least two NetworkPolicy resources for pod isolation. 6. Deploy cluster autoscaler v1.28.x with priority expander configuration to prefer general node group. 7. Configure pod security standards admission controller with 'restricted' as the baseline enforcement level. 8. Create custom launch templates for node groups with encrypted EBS volumes (gp3, 100GB) and IMDSv2 enforcement. 9. Deploy AWS Load Balancer Controller v2.6.x with proper IRSA configuration. 10. Tag all resources with Environment=Production, ManagedBy=Pulumi, and CostCenter=Engineering tags. Expected output: A fully functional EKS cluster accessible via kubectl with two node groups running Bottlerocket OS, Calico network policies enforcing pod isolation, cluster autoscaler managing node scaling based on workload demands, and AWS Load Balancer Controller ready to provision ALB/NLB resources. The cluster should pass CIS Kubernetes Benchmark security checks and support zero-downtime deployments.","A financial services company needs to deploy a production-grade Kubernetes cluster on AWS EKS to host their microservices architecture. The cluster must support automatic scaling, secure pod-to-pod communication, and integration with existing monitoring infrastructure while meeting strict compliance requirements for data isolation.","Production EKS cluster deployed in us-east-2 across 3 availability zones using managed node groups with Bottlerocket AMI. VPC with RFC1918 private subnets (10.0.0.0/16) and public subnets for load balancers. Requires Pulumi CLI 3.x, Go 1.20+, AWS CLI configured with appropriate IAM permissions. Integration with CloudWatch Container Insights for monitoring and AWS Systems Manager for node management. OIDC provider configured for pod-level IAM roles.","[""Configure cluster autoscaler with priority-based node selection"", ""Implement pod security standards with restricted baseline"", ""Enable EKS control plane logging to CloudWatch for audit, authenticator, and API logs"", ""Configure OIDC provider for IRSA (IAM Roles for Service Accounts)"", ""EKS cluster must use Kubernetes version 1.28 or higher"", ""Use Pulumi Go SDK v3.x with strongly typed resource definitions"", ""Use managed node groups with custom launch templates"", ""Node groups must use Bottlerocket AMI for enhanced security"", ""Implement Calico network policies for inter-pod traffic control""]"
h3c4o2,error,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a secure payment processing infrastructure with defense-in-depth security controls. The configuration must: 1. Create a VPC with separate subnet tiers for firewall, application, and database layers across 3 AZs. 2. Deploy AWS Network Firewall with stateful rules blocking all traffic except HTTPS and PostgreSQL from application tier. 3. Configure RDS Aurora PostgreSQL with encryption using a customer-managed KMS key and restricted security group. 4. Set up ECS Fargate service in private subnets with IAM task roles following least privilege principles. 5. Deploy Application Load Balancer with AWS WAF configured with managed rule sets for OWASP protection. 6. Create VPC endpoints for S3, ECR, CloudWatch, and Secrets Manager to avoid internet traffic. 7. Configure Secrets Manager to store database credentials with Lambda-based rotation every 30 days. 8. Enable VPC Flow Logs and CloudWatch Logs for all services with 90-day retention period. 9. Implement CloudWatch alarms for security events like failed authentication attempts. 10. Create KMS keys with strict key policies allowing only specific IAM roles to decrypt. 11. Configure security groups with explicit deny-all rules and minimal allow rules for required ports. 12. Set up AWS Config rules to monitor compliance with security best practices. Expected output: A complete Pulumi program that creates all security infrastructure components with proper configurations, outputs the ALB DNS name, RDS endpoint (without exposing credentials), and a security compliance dashboard URL for monitoring.","A financial services company requires strict security controls for their payment processing infrastructure. They need to implement defense-in-depth security measures including network isolation, encryption at rest and in transit, and comprehensive access controls to meet PCI DSS compliance requirements.","Highly secured multi-AZ deployment in us-east-1 region for payment processing workloads. Uses RDS Aurora PostgreSQL with encryption, ECS Fargate in private subnets, Application Load Balancer with WAF, Network Firewall for inspection. Requires Pulumi CLI 3.x with TypeScript, AWS CLI configured with appropriate permissions. VPC spans 3 availability zones with dedicated subnets for database, application, and firewall layers. No direct internet access for compute resources, all traffic routed through Network Firewall and NAT instances.","[""IAM roles must follow principle of least privilege with session-based temporary credentials"", ""Secrets Manager must store all database credentials with automatic rotation every 30 days"", ""WAF rules must protect against OWASP Top 10 vulnerabilities with rate limiting"", ""CloudWatch Logs must capture all API calls with 90-day retention for audit trails"", ""Network traffic must flow through AWS Network Firewall with stateful inspection rules"", ""VPC endpoints must be used for all AWS service communications to avoid internet exposure"", ""Security groups must explicitly deny all traffic except required ports (443, 5432)"", ""All data must be encrypted using AWS KMS with customer-managed keys (CMKs)""]"
h4a7w5,error,Pulumi,Python,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi Python program to deploy a secure AWS foundation for a financial services startup. The configuration must: 1. Create a VPC with CIDR 10.0.0.0/16 across 3 availability zones with only private subnets (10.0.1.0/24, 10.0.2.0/24, 10.0.3.0/24). 2. Deploy NAT instances (not NAT Gateways) in each AZ using the latest Amazon Linux 2 AMI. 3. Set up AWS Systems Manager Parameter Store with SecureString parameters for API keys with KMS encryption. 4. Create Lambda functions to rotate Parameter Store values every 30 days using EventBridge scheduled rules. 5. Configure VPC Flow Logs to capture ALL traffic and store in an S3 bucket with 90-day retention lifecycle policy. 6. Enable AWS Config with recording to S3 and create custom Config Rules to check for unencrypted EBS volumes and public S3 buckets. 7. Implement an EventBridge event bus for application events with rules forwarding to CloudWatch Logs. 8. Apply consistent tagging schema across all resources with Environment, Owner, and CostCenter tags. 9. Use Pulumi stack references to create explicit dependencies between the networking stack and application stacks. 10. Output the VPC ID, private subnet IDs, NAT instance IPs, and Parameter Store ARNs for use by other stacks. Expected output: A main.py file defining all infrastructure components using Pulumi's Python SDK with proper error handling, type hints, and modular organization. The program should create separate logical groupings for networking, security, and monitoring components. Stack exports should include all critical resource identifiers needed by dependent application stacks.","A financial services startup needs to establish their first production-grade cloud infrastructure on AWS. They require a secure, compliant environment with proper network isolation, centralized logging, and automated secret rotation for their trading platform API keys.","Production-grade AWS infrastructure in us-east-2 region spanning 3 availability zones. Core services include VPC with custom CIDR ranges, private subnets with NAT instances for outbound traffic, AWS Systems Manager Parameter Store for secrets management, Lambda functions for automation, EventBridge for event orchestration, AWS Config for compliance monitoring, and S3 for centralized log storage. Requires Pulumi 3.x with Python 3.9+, boto3 SDK, and AWS CLI configured with appropriate IAM permissions for VPC, Lambda, SSM, Config, and EventBridge service access.","[""Use only private subnets for compute resources with NAT instances instead of NAT Gateways"", ""Implement Lambda functions for automated secret rotation every 30 days"", ""Use EventBridge for all event-driven workflows instead of CloudWatch Events"", ""Configure VPC flow logs to ship to a dedicated S3 bucket with lifecycle policies"", ""Implement custom resource tags with 'Environment', 'Owner', and 'CostCenter' on all resources"", ""Use AWS Systems Manager Parameter Store for secrets, not Secrets Manager"", ""Enable AWS Config with custom rules for compliance checking"", ""Deploy all infrastructure stacks with explicit stack dependencies using Pulumi's ResourceOptions""]"
w8y5x6,error,Pulumi,Python,expert,Application Deployment,Web Application Deployment,"Create a Pulumi Python program to deploy a payment processing web application with blue-green deployment support. The configuration must: 1. Set up a VPC with 3 availability zones, each containing public and private subnets 2. Deploy an RDS PostgreSQL instance (db.t3.medium) in Multi-AZ configuration with automated backups every 6 hours 3. Create an ECS Fargate cluster with a service running the FastAPI backend container 4. Configure an Application Load Balancer with path-based routing and health checks 5. Set up CloudFront distribution pointing to both the ALB (for /api/*) and S3 bucket (for static assets) 6. Implement blue-green deployment using ECS service with target group switching 7. Store database credentials and API keys in Secrets Manager with 30-day automatic rotation 8. Configure CloudWatch log groups with 90-day retention for ECS tasks and ALB access logs 9. Create S3 buckets for frontend hosting with CloudFront OAI and versioning enabled 10. Set up security groups that only allow traffic from CloudFront to ALB, and from ECS to RDS 11. Enable VPC flow logs and store them in a dedicated S3 bucket with lifecycle policies 12. Tag all resources with Environment, Application, and CostCenter tags. Expected output: A fully functional payment processing infrastructure with automated blue-green deployments, where updates can be rolled out with zero downtime by switching ALB target groups between blue and green ECS services. The system should handle 1000 concurrent users with sub-second API response times and maintain 99.9% uptime.","A fintech startup needs to deploy their payment processing web application with strict compliance requirements for PCI DSS. The application consists of a React frontend, Python FastAPI backend, and PostgreSQL database. They require complete infrastructure auditability and blue-green deployment capabilities to minimize downtime during updates.","Production payment processing infrastructure deployed in us-east-1 region using ECS Fargate for containerized FastAPI backend, RDS PostgreSQL Multi-AZ for database tier, CloudFront for global content delivery, and S3 for static React frontend hosting. Requires Python 3.9+, Pulumi CLI 3.x, AWS CLI configured with appropriate permissions. VPC spans 3 availability zones with public subnets for ALB and NAT gateways, private subnets for ECS tasks and RDS instances. Application Load Balancer handles SSL termination with ACM certificates.","[""Backend API must only accept traffic from the CloudFront distribution"", ""All S3 buckets must have versioning enabled and lifecycle policies configured"", ""Database backups must occur every 6 hours with point-in-time recovery enabled"", ""Deployment must support zero-downtime updates using ECS blue-green deployment"", ""All secrets must be stored in AWS Secrets Manager with automatic rotation"", ""ECS tasks must run in private subnets with no direct internet access"", ""All database connections must use SSL/TLS encryption with certificate validation"", ""Application logs must be retained for exactly 90 days for compliance auditing""]"
j6h7n8,done,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to implement a migration infrastructure for moving an on-premises payment processing system to AWS. The configuration must: 1. Create a VPC with 3 public and 3 private subnets across different availability zones. 2. Deploy an RDS Aurora PostgreSQL cluster with one writer and two reader instances, encryption at rest, and point-in-time recovery enabled. 3. Set up an ECS cluster with a Fargate service running at least 3 tasks of the payment processing application. 4. Configure an Application Load Balancer with target group health checks pointing to the ECS service. 5. Implement AWS Database Migration Service with a replication instance and migration task for PostgreSQL to Aurora migration with CDC enabled. 6. Create a Lambda function that queries both source and target databases to validate record counts and data integrity. 7. Set up CloudWatch alarms for DMS replication lag, ECS task health, and RDS CPU utilization. 8. Configure security groups that allow traffic only between necessary components (ALB  ECS  RDS, DMS  RDS). 9. Implement proper IAM roles for ECS task execution, Lambda function, and DMS replication. 10. Apply consistent tagging across all resources with Environment='prod-migration', CostCenter='finance', and MigrationPhase='active'. Expected output: A complete Pulumi TypeScript program that provisions the entire migration infrastructure, outputs the ALB DNS name, RDS cluster endpoint, and DMS replication task ARN. The program should use Pulumi's component resources to organize related infrastructure and include exported stack outputs for integration with monitoring dashboards.","A financial services company is migrating their payment processing infrastructure from their legacy on-premises setup to AWS. The current system processes credit card transactions through a Java-based application that connects to a PostgreSQL database. They need to replicate their production environment in AWS first, then create a migration strategy that allows for zero-downtime cutover.","Production-grade infrastructure deployed in us-east-1 across 3 availability zones. Uses ECS Fargate for containerized Java application hosting, RDS Aurora PostgreSQL 13.7 for database, AWS DMS for database migration with ongoing replication. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 16+, and AWS CLI configured with appropriate IAM permissions. VPC spans 10.0.0.0/16 with public subnets (10.0.1.0/24, 10.0.2.0/24, 10.0.3.0/24) for ALB and private subnets (10.0.11.0/24, 10.0.12.0/24, 10.0.13.0/24) for ECS tasks and RDS. NAT Gateways provide outbound internet access from private subnets.","[""All infrastructure must be tagged with Environment, CostCenter, and MigrationPhase tags"", ""Database migration must use AWS DMS with CDC enabled for real-time replication"", ""RDS Aurora PostgreSQL must be configured with encrypted storage and automated backups"", ""CloudWatch alarms must monitor DMS replication lag and alert if it exceeds 60 seconds"", ""The solution must include a Lambda function to validate data consistency post-migration"", ""The migration must support blue-green deployment pattern for zero-downtime cutover"", ""Network traffic between ECS tasks and RDS must remain within private subnets"", ""The application must run in ECS Fargate with at least 3 tasks across multiple AZs""]"
o3j4p4,in_progress,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy a production-grade Amazon EKS cluster with enhanced security configurations. The configuration must: 1. Deploy an EKS cluster version 1.28 in private subnets across three availability zones. 2. Create two managed node groups: one for system workloads (2 t3.medium instances) and one for application workloads (3-5 t3.large instances with auto-scaling). 3. Configure OIDC provider with proper thumbprint verification for IRSA functionality. 4. Enable all five cluster logging types and configure log retention to 30 days in CloudWatch. 5. Create and attach IAM roles for the AWS Load Balancer Controller add-on with least-privilege permissions. 6. Configure node groups to use Bottlerocket AMI with custom launch templates. 7. Implement KMS encryption for all EBS volumes using a customer-managed key with proper key policies. 8. Set up VPC endpoints for ECR (api and dkr), S3, and EC2 to avoid internet traffic for pulling container images. 9. Configure AWS Systems Manager Session Manager access for nodes by adding required IAM policies. 10. Install AWS Load Balancer Controller as an EKS add-on with proper service account configuration. 11.","A financial services company needs to deploy a managed Kubernetes cluster for their microservices architecture. They require a production-ready EKS cluster with strict security requirements, automated node management, and integration with existing monitoring systems.","Production EKS cluster deployment in us-east-1 across 3 availability zones. Uses VPC with private subnets only, NAT gateways for outbound traffic, VPC endpoints for ECR, S3, and EC2. Requires CDK 2.x with Python 3.9+, kubectl 1.28+, and AWS CLI v2. Architecture includes managed node groups with Bottlerocket OS, IRSA configuration, CloudWatch Container Insights, and AWS Load Balancer Controller. KMS encryption for all EBS volumes and secrets.","[""The EKS cluster must use only private subnets with no direct internet access for worker nodes"", ""All cluster logging (API, audit, authenticator, controller manager, scheduler) must be enabled and sent to CloudWatch"", ""The cluster must enforce IRSA (IAM Roles for Service Accounts) for all pod-level AWS API access"", ""All node groups must use AWS Systems Manager Session Manager for access instead of SSH"", ""Node groups must use Bottlerocket AMI for enhanced security and reduced attack surface"", ""Node groups must use custom launch templates with encrypted EBS volumes using customer-managed KMS keys"", ""The cluster must have AWS Load Balancer Controller pre-installed as an add-on with proper IAM roles"", ""The cluster must have OIDC provider configured with thumbprint verification""]"
b8t3r6,done,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi Go program to deploy consistent payment processing infrastructure across three environments (dev, staging, prod). The configuration must: 1. Define a reusable component that accepts environment-specific parameters while enforcing consistency. 2. Create VPCs with 10.0.0.0/16 CIDR in each environment with 3 private subnets. 3. Deploy RDS PostgreSQL instances with encryption using environment-specific KMS keys. 4. Set up Lambda functions with 512MB memory and environment-based reserved concurrency (dev: 10, staging: 50, prod: 200). 5. Configure API Gateway with custom domains and AWS WAF integration for prod only. 6. Create DynamoDB tables for transaction logs with on-demand billing and PITR enabled. 7. Establish S3 buckets for audit trails with versioning and lifecycle policies based on environment. 8. Implement IAM roles and policies ensuring least-privilege access with environment prefixes. 9. Configure CloudWatch log groups with environment-specific retention periods. 10. Set up CloudWatch alarms for RDS CPU usage with different thresholds per environment. 11. Export all resource ARNs and endpoints as stack outputs for cross-stack references. Expected output: A Pulumi program with a main.go file containing reusable components and three separate stack configurations (dev.yaml, staging.yaml, prod.yaml) that demonstrate infrastructure consistency while allowing environment-specific variations in scaling, retention, and monitoring parameters.","A financial services company needs to maintain identical infrastructure across development, staging, and production environments for their payment processing system. They require strict consistency in security configurations, resource naming, and networking setup across all environments while allowing for environment-specific scaling parameters.","Multi-environment AWS deployment spanning us-east-1 (production), us-east-2 (staging), and us-east-2 (development). Each environment requires a VPC with 3 availability zones, private subnets for RDS PostgreSQL instances, Lambda functions for payment processing, API Gateway for REST endpoints, DynamoDB for transaction logs, and S3 for audit trails. Infrastructure managed through Pulumi with Go SDK, requiring Go 1.19+ and AWS CLI configured with appropriate credentials for each environment. Each environment has dedicated AWS accounts with cross-account IAM roles for deployment.","[""All IAM roles must be prefixed with the environment name for clear identification"", ""All environments must use identical VPC CIDR blocks with non-overlapping subnet ranges"", ""CloudWatch log retention must be 7 days for dev, 30 days for staging, and 90 days for production"", ""RDS instances must use encrypted storage with environment-specific KMS keys"", ""API Gateway must use custom domain names following the pattern api-{env}.payments.internal"", ""Lambda functions must have identical memory allocations but environment-based concurrency limits"", ""S3 buckets must follow the naming convention payments-{env}-{purpose}-{random-suffix}"", ""DynamoDB tables must have point-in-time recovery enabled in all environments""]"
c6o3q8,done,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to manage infrastructure consistency across three environments (dev, staging, prod) in different AWS regions. The configuration must: 1. Define a base infrastructure template that includes VPC, subnets, security groups, ECS cluster, and RDS Aurora database. 2. Create environment-specific stack configurations that inherit from the base template while allowing for size variations (t3.medium for dev, m5.large for staging, m5.xlarge for prod). 3. Implement cross-stack references to ensure network configurations are synchronized across environments. 4. Set up AWS Systems Manager Parameter Store hierarchies for each environment with shared and environment-specific parameters. 5. Create ECS task definitions that automatically pull the correct container images based on environment (dev uses :latest, staging uses :staging-*, prod uses :v*.*.*). 6. Configure RDS Aurora clusters with appropriate instance counts (1 for dev, 2 for staging, 3 for prod) and backup retention policies. 7. Implement CloudWatch dashboards that aggregate metrics across all environments for comparison. 8. Set up SNS topics for configuration drift alerts when environments diverge from expected state. Expected output: Three Pulumi stacks deployed across different regions with consistent network topology, security configurations, and service definitions, while maintaining environment-appropriate sizing and redundancy levels.","A financial services company operates identical trading platforms across development, staging, and production environments in different AWS regions. Recent incidents where staging configurations drifted from production led to failed deployments and trading disruptions.","Multi-region AWS deployment spanning us-east-1 (production), us-east-1 (staging), and us-east-2 (development). Infrastructure includes VPCs with 3 availability zones each, Application Load Balancers, ECS Fargate services running containerized trading applications, RDS Aurora PostgreSQL clusters with read replicas, S3 buckets for data storage, and CloudWatch for monitoring. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, AWS CLI configured with cross-region permissions. Each environment maintains isolated VPCs with peering connections for secure inter-environment communication.","[""Implement automated drift detection between environments"", ""Implement environment-specific parameter validation using TypeScript interfaces"", ""Deploy to at least 3 different AWS regions with region-specific configurations"", ""Create reusable components using Pulumi ComponentResource pattern"", ""Use Pulumi's stack references to share outputs between environments"", ""Use Pulumi's configuration system for environment-specific values"", ""Use AWS Systems Manager Parameter Store for shared configuration"", ""Implement rollback capabilities using Pulumi's state management""]"
y4d2j2,done,CDK,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a CDK TypeScript program to deploy a serverless stock pattern detection system. The configuration must: 1. Deploy an API Gateway REST API with /patterns and /alerts endpoints with request validation 2. Create a Lambda function 'PatternDetector' that processes incoming market data with 512MB memory 3. Set up a DynamoDB table 'TradingPatterns' with partition key 'patternId' and sort key 'timestamp' 4. Configure an SQS queue 'AlertQueue' with visibility timeout of 300 seconds for alert processing 5. Implement a Lambda function 'AlertProcessor' that reads from the SQS queue with batch size of 10 6. Create an EventBridge rule that triggers every 5 minutes to check pattern thresholds 7. Add a Lambda function 'ThresholdChecker' triggered by EventBridge with environment variables for thresholds 8. Implement CloudWatch Logs retention of 7 days for all Lambda functions 9. Create SNS topic 'TradingAlerts' with email subscription for critical alerts 10. Set up Lambda DLQ for AlertProcessor with maximum receive count of 3 11. Configure CloudWatch alarms for Lambda errors exceeding 1% error rate 12. Output the API Gateway URL and SQS queue URL for integration testing. Expected output: A fully deployed serverless architecture with API endpoints for pattern submission, automated pattern detection and alerting, proper error handling with DLQs, and monitoring through CloudWatch. The system should automatically scale based on load and provide cost-effective processing of trading patterns.",A financial services company needs to process real-time stock market data feeds and generate alerts when specific trading patterns are detected. The system must handle variable load patterns during market hours and scale down to zero during off-hours to minimize costs.,"Serverless infrastructure deployed in us-east-1 region for proximity to financial markets. Architecture uses API Gateway REST API, Lambda functions with Graviton2 processors, DynamoDB for pattern storage, SQS for message queuing, and EventBridge for event routing. Requires CDK 2.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate permissions. No VPC required as all services are fully managed. IAM roles follow least-privilege principle with boundary policies.","[""Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""SQS queues must have message retention period of exactly 4 days"", ""EventBridge rules must use custom event patterns with at least 3 matching conditions"", ""All Lambda functions must have X-Ray tracing enabled with custom segments"", ""Lambda functions must use Lambda Layers for shared dependencies with versioning"", ""Use AWS Lambda with reserved concurrency of exactly 50 for the pattern detection function"", ""API Gateway must implement request throttling at 1000 requests per second with burst of 2000"", ""DynamoDB tables must use on-demand billing mode with point-in-time recovery enabled""]"
y8s8e4,in_progress,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi Go program to deploy a multi-region disaster recovery infrastructure for a payment processing system. The configuration must: 1. Create DynamoDB global tables with on-demand billing and point-in-time recovery enabled. 2. Deploy identical Lambda functions in both us-east-1 and us-east-2 for payment processing. 3. Configure API Gateway REST APIs in both regions with custom domain names. 4. Set up Route 53 hosted zone with health checks and failover routing policies. 5. Create S3 buckets in both regions with cross-region replication for transaction logs. 6. Implement CloudWatch alarms monitoring DynamoDB replication lag exceeding 30 seconds. 7. Configure SSM parameters storing region-specific endpoints and configurations. 8. Set up SQS dead letter queues in both regions for failed transaction retry. 9. Create IAM roles with least-privilege permissions for all services. 10. Output primary and secondary API endpoints, health check URLs, and alarm ARNs. Expected output: The program should create a fully automated disaster recovery setup where traffic automatically fails over to us-east-2 if us-east-1 becomes unavailable, with all transaction data synchronized and accessible in both regions.",A financial services company needs to implement a disaster recovery solution for their payment processing system. The primary region hosts critical transaction data and must be replicated to a secondary region with automated failover capabilities. The system must maintain RTO of 15 minutes and RPO of 5 minutes while ensuring data consistency across regions.,"Multi-region AWS infrastructure spanning us-east-1 (primary) and us-east-2 (disaster recovery). Deployment includes DynamoDB global tables for transaction data, Lambda functions for payment processing, API Gateway for REST endpoints, S3 buckets with cross-region replication for backups. Route 53 manages DNS failover between regions. CloudWatch monitors replication metrics and triggers alarms. SSM Parameter Store holds region-specific configurations. Requires Pulumi 3.x with Go SDK, AWS CLI configured with appropriate credentials. VPCs in both regions with private subnets for Lambda functions.","[""Deploy Lambda functions in both regions with identical configurations"", ""Use SSM Parameter Store for storing region-specific configurations"", ""Use S3 cross-region replication for static assets and backups"", ""Set up CloudWatch alarms for monitoring replication lag"", ""Implement Route 53 health checks with automatic DNS failover"", ""Implement dead letter queues for failed transactions during failover"", ""Configure API Gateway with custom domain names in each region"", ""Use DynamoDB global tables for multi-region data replication""]"
b3e7k7,error,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement a multi-region disaster recovery infrastructure for a payment processing system. The configuration must: 1. Set up DynamoDB global tables with automated backups and point-in-time recovery enabled in both us-east-1 and us-east-2. 2. Deploy identical Lambda functions in both regions for payment processing with environment variables pointing to regional resources. 3. Create S3 buckets in both regions with cross-region replication and RTC enabled for sub-minute replication. 4. Configure API Gateway REST APIs in both regions with custom domain names. 5. Implement Route53 health checks monitoring the primary region's API Gateway endpoint. 6. Set up Route53 failover routing policies with automatic DNS failover to secondary region. 7. Create CloudWatch alarms for DynamoDB table health, Lambda errors, and S3 replication lag. 8. Configure SNS topics in both regions for alerting on failover events. 9. Implement IAM roles with cross-region assume role policies for disaster recovery operations. 10. Set up CloudWatch Logs with cross-region log group subscriptions for centralized monitoring. 11. Deploy all resources with consistent naming convention: {service}-{region}-{environment}. 12. Output the primary and failover API endpoints, health check IDs, and alarm ARNs. Expected output: The program should create a fully functional multi-region infrastructure where the primary region handles all traffic under normal conditions. When the primary region experiences an outage detected by Route53 health checks, DNS automatically fails over to the secondary region within 5 minutes, with data synchronized within the last minute via DynamoDB global tables and S3 cross-region replication.",A financial services company needs to implement a disaster recovery solution for their critical payment processing system. They require automatic failover capabilities between regions with minimal data loss and downtime. The system must maintain transaction integrity during regional outages.,"Multi-region AWS infrastructure spanning us-east-1 (primary) and us-east-2 (secondary) for disaster recovery. Core services include DynamoDB global tables for transaction data, Lambda functions for payment processing, S3 for document storage with cross-region replication, API Gateway for REST endpoints, and Route53 for DNS failover. Requires Pulumi 3.x with TypeScript, Node.js 18+, and AWS SDK v3. Both regions will have identical VPC configurations with 3 availability zones, private subnets for compute resources, and VPC endpoints for AWS services. CloudWatch alarms and SNS topics configured for monitoring failover events.","[""RPO (Recovery Point Objective) must be under 1 minute"", ""DynamoDB global tables must be configured with point-in-time recovery"", ""RTO (Recovery Time Objective) must be under 5 minutes"", ""Primary region must be us-east-1 with failover to us-east-2"", ""Use Route53 health checks for automatic DNS failover"", ""All resources must be tagged with Environment, Region, and DR-Role tags"", ""S3 buckets must use cross-region replication with RTC (Replication Time Control)"", ""Lambda functions must be deployed in both regions with identical configurations""]"
m3a4c6,done,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi Go program to deploy a serverless file processing pipeline. The configuration must: 1. Create an S3 bucket with versioning and lifecycle rules to transition objects to Glacier after 90 days 2. Deploy three Lambda functions: file validator, data processor, and result aggregator, each with 512MB memory 3. Configure S3 event notifications to trigger the validator Lambda on object creation 4. Set up SQS FIFO queues between Lambda functions for ordered processing 5. Implement DynamoDB table with TTL for tracking processing status 6. Create dead letter queues for each Lambda with maximum receive count of 3 7. Deploy API Gateway REST API with GET endpoint to query processing status 8. Configure request throttling on API Gateway at 1000 requests per second 9. Set up CloudWatch Logs retention to 7 days for all Lambda functions 10. Add resource tags for cost tracking: Environment=Production, Team=Analytics Expected output: The program should create a complete serverless pipeline where files uploaded to S3 trigger validation, processing flows through FIFO queues maintaining order, and results are queryable via API Gateway while failed messages are captured in DLQs.",A financial analytics startup needs to process large volumes of market data files uploaded by partners. The system must handle bursty workloads during market hours and scale down during off-hours to minimize costs.,"Serverless infrastructure deployed in us-east-1 using Lambda functions for compute, S3 for file storage, DynamoDB for metadata tracking, SQS FIFO queues for message ordering, and API Gateway for REST endpoints. Requires Pulumi CLI 3.x with Go SDK, AWS CLI configured with appropriate credentials. No VPC required as all services are managed. Lambda functions process market data files asynchronously with automatic scaling based on S3 events. DynamoDB configured with on-demand billing and point-in-time recovery.","[""All Lambda functions must use Go 1.x runtime"", ""Use DynamoDB for storing processing metadata with TTL enabled"", ""Lambda functions must have 512MB memory allocation"", ""Use SQS FIFO queues for ordered message processing"", ""S3 buckets must have versioning enabled and lifecycle policies"", ""Deploy API Gateway with request throttling at 1000 requests per second"", ""Implement DLQ for failed processing attempts with retry logic"", ""Use S3 event notifications to trigger Lambda functions without polling""]"
z5z1x6,error,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi Go program to deploy a production-ready EKS cluster with advanced networking and autoscaling configurations. The configuration must: 1. Create an EKS cluster with private API endpoint access only and enable all control plane logging types. 2. Configure a custom VPC with 3 private subnets for nodes and 3 private subnets for pods using secondary CIDR block. 3. Deploy two node groups: one for system workloads (2-4 nodes) and one for application workloads (3-10 nodes), both using spot instances. 4. Install and configure VPC CNI addon with custom environment variables for pod subnet usage. 5. Set up OIDC provider and create IAM roles for cluster autoscaler and AWS Load Balancer Controller. 6. Deploy cluster autoscaler using Helm chart with node selector for system node group. 7. Configure aws-auth ConfigMap to allow specific IAM roles for developers and CI/CD pipelines. 8. Create CloudWatch log groups with 7-day retention for all EKS control plane logs. 9. Output the cluster endpoint, OIDC issuer URL, and kubeconfig for cluster access. Expected output: A fully functional EKS cluster accessible via kubectl, with autoscaling enabled, proper IAM integration for service accounts, and all nodes running across multiple availability zones using cost-optimized spot instances.","A financial services company needs to deploy a Kubernetes-based microservices platform for their trading applications. The platform must support automatic scaling, secure network isolation between services, and integration with existing monitoring tools. The infrastructure must be deployed across multiple availability zones for high availability.","Production-grade EKS infrastructure deployed in us-east-1 across 3 availability zones (us-east-1a, us-east-1b, us-east-1c). Requires Pulumi CLI 3.x with Go SDK, kubectl 1.28+, Helm 3.x, and AWS CLI configured with appropriate permissions. VPC with dedicated subnets for EKS nodes (10.0.0.0/16), separate pod subnet (100.64.0.0/16), and private endpoint access only. Uses AWS Systems Manager for node access, CloudWatch Container Insights for monitoring, and AWS Load Balancer Controller for ingress.","[""EKS cluster must use Kubernetes version 1.28 or higher"", ""All node groups must use spot instances with at least 3 different instance types"", ""Cluster autoscaler must be deployed using Helm with specific resource limits"", ""Node groups must use launch templates with custom user data for monitoring agents"", ""OIDC provider must be configured for IRSA (IAM Roles for Service Accounts)"", ""Worker nodes must use Graviton3-based instances (t4g or c7g family)"", ""Each node group must span exactly 3 availability zones"", ""VPC CNI addon must be installed with custom configuration for pod subnet""]"
i1y3p4,done,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi Go program to deploy consistent infrastructure across three environments (dev, staging, prod) with proper configuration management. The configuration must: 1. Define a reusable component resource that creates a VPC with public/private subnets across 2 AZs. 2. Implement environment-specific RDS PostgreSQL instances with automated backups and encryption. 3. Create S3 buckets with environment-specific naming and retention policies (7 days for dev, 30 days for staging, 90 days for prod). 4. Deploy Lambda functions with environment-specific configurations for memory (128MB dev, 256MB staging, 512MB prod) and timeout values. 5. Set up API Gateway REST APIs that invoke the Lambda functions with proper IAM roles. 6. Configure CloudWatch log groups with environment-specific retention periods. 7. Export critical resource IDs and endpoints as stack outputs for cross-stack references. 8. Implement a configuration validation function that ensures all required environment-specific values are present. 9. Create IAM roles and policies following least-privilege principles for each service. 10. Set up CloudWatch alarms with environment-specific thresholds for RDS CPU utilization. 11. Ensure all resources use consistent naming conventions with environment prefixes. 12. Implement proper error handling and rollback capabilities for failed deployments. Expected output: Three separate Pulumi stacks (dev, staging, prod) with identical infrastructure topology but environment-specific configurations, demonstrating how to maintain consistency while allowing controlled variations between environments.","A fintech startup needs to maintain identical infrastructure across development, staging, and production environments while ensuring configuration differences are properly managed. The company uses GitOps practices and requires automated environment promotion with drift detection capabilities.","Multi-environment AWS deployment across us-east-1 region with separate VPCs per environment (dev: 10.0.0.0/16, staging: 10.1.0.0/16, prod: 10.2.0.0/16). Each environment includes RDS PostgreSQL instances, S3 buckets for application data, Lambda functions for data processing, and API Gateway endpoints. Requires Pulumi 3.x with Go SDK, AWS CLI configured with appropriate IAM permissions. VPCs include public and private subnets across 2 AZs with NAT gateways for Lambda outbound traffic. Infrastructure state stored in Pulumi Cloud with stack-based organization.","[""RDS instances must use different instance types per environment (t3.micro for dev, t3.small for staging, t3.medium for prod)"", ""All resources must be tagged with Environment, ManagedBy, and CostCenter tags"", ""Lambda functions must have environment-specific memory allocations and timeout values"", ""All S3 buckets must have versioning enabled and lifecycle policies"", ""Each environment must have its own VPC with identical CIDR block structure but different ranges"", ""Must use Pulumi stack references to share outputs between environments"", ""Environment-specific configurations must be stored in Pulumi config files"", ""Must implement a custom Pulumi component resource for reusable infrastructure patterns""]"
n4g8n2,done,CDK,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,Create a CDK TypeScript program to deploy a multi-region disaster recovery architecture for a PostgreSQL database. The configuration must: 1. Deploy RDS PostgreSQL instances with cross-region read replicas between us-east-1 and us-east-2. 2. Configure automated backups with point-in-time recovery enabled in both regions. 3. Implement Route53 health checks and failover routing policies for database endpoints. 4. Deploy Lambda functions to monitor replication lag and trigger alerts when thresholds exceed 5 minutes. 6. Configure EventBridge rules to orchestrate automated failover procedures. 7. Implement IAM roles with least-privilege access for all disaster recovery operations. 8.,"A financial services company requires a disaster recovery solution for their critical PostgreSQL database that processes payment transactions. The system must maintain RPO of under 1 hour and RTO of under 4 hours, with automated failover capabilities between us-east-1 (primary) and us-east-2 (DR).","Multi-region AWS deployment spanning us-east-1 (primary) and us-east-2 (disaster recovery). Infrastructure includes RDS PostgreSQL 14 with Multi-AZ deployments, cross-region read replicas, Route53 failover routing, S3 buckets with versioning and replication, Lambda functions for monitoring, and EventBridge for orchestration. Requires CDK 2.x with TypeScript, Node.js 18+, AWS CLI configured with credentials for both regions. VPCs in each region with private subnets for database instances, VPC peering for cross-region communication, and NAT gateways for Lambda outbound traffic. KMS keys in each region for encryption at rest.","['RDS instances must use db.r6g.xlarge instance class with encrypted storage using customer-managed KMS keys', 'Read replica lag monitoring must trigger SNS notifications when lag exceeds 300 seconds', 'Route53 health checks must verify both database connectivity and replication status before marking endpoints as healthy', 'S3 replication must complete within 15 minutes for objects under 5GB with replication metrics enabled', 'Lambda functions must be deployed in private subnets with VPC endpoints for AWS service access', 'All inter-region traffic must use AWS PrivateLink or VPC peering with encryption in transit', 'CloudWatch alarms must have composite alarms that consider multiple failure scenarios before triggering failover', 'CDK stacks must use cross-stack references to share resources between regions without hardcoding values']"
a5p3b5,done,CDK,Python,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CDK Python program to build an automated infrastructure compliance auditing system. The configuration must: 1. Deploy AWS Config with custom rules to evaluate S3 bucket encryption, VPC flow log configuration, and Lambda function settings. 2. Create a Lambda function (Python 3.9, 1GB memory) that performs cross-account infrastructure scanning using AssumeRole. 3. Configure EventBridge rules to trigger compliance scans every 6 hours and on-demand via custom events. 4. Set up an S3 bucket with versioning and lifecycle rules to store audit reports for 90 days. 5. Implement Lambda functions to generate compliance reports in both JSON and CSV formats. 6. Create SNS topics for critical non-compliance alerts with email subscriptions. 7. Configure AWS Config aggregator to collect compliance data from multiple accounts. 8. Deploy CloudWatch dashboards showing compliance metrics and trend analysis. 9. Ensure all Lambda functions have X-Ray tracing enabled for debugging. 10. Implement automatic remediation Lambda for specific violations (like enabling S3 encryption). Expected output: A CDK stack that deploys a complete compliance auditing system capable of scanning infrastructure across multiple accounts, generating detailed reports, and alerting on violations with some automatic remediation capabilities.","A financial services company needs automated infrastructure compliance auditing to meet regulatory requirements. Their existing manual reviews are time-consuming and error-prone, requiring a programmatic solution to continuously validate infrastructure configurations against security policies and generate audit reports.","""Infrastructure compliance validation system deployed in us-east-1 using AWS Config for configuration tracking, Lambda for custom rule evaluation, S3 for audit report storage, and EventBridge for scheduling periodic compliance checks. Requires CDK 2.x with Python 3.9+, boto3 SDK installed. Resources span multiple AWS accounts accessed via AssumeRole. VPC with private subnets for Lambda execution, VPC endpoints for AWS service access. Audit reports generated in JSON and CSV formats stored in versioned S3 buckets with lifecycle policies.""","[""All Lambda functions must have reserved concurrent executions set to prevent resource exhaustion"", ""S3 buckets must use separate KMS keys for encryption, not the default AWS managed key"", ""VPC flow logs must be enabled and stored with specific naming conventions matching 'audit-flowlogs-{region}-{date}'"", ""Lambda execution roles must not contain any inline policies, only managed policies"", ""All resources must include mandatory tags: 'Environment', 'Owner', 'CostCenter', and 'ComplianceLevel'""]"
y1i3p6,in_progress,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy a production-ready EKS cluster for microservices hosting. The configuration must: 1. Create a VPC with 3 public and 3 private subnets across 3 AZs. 2. Deploy an EKS cluster with Kubernetes 1.28 using private endpoint access only. 3. Create a managed node group with Bottlerocket AMI, t3.medium instances, min 3/max 10 nodes. 4. Enable IRSA by creating an OIDC provider for the cluster. 5. Install and configure the Cluster Autoscaler as a Kubernetes addon with proper IRSA permissions. 6. Configure CloudWatch Container Insights for monitoring with 30-day log retention. 7. Create an IAM role for the AWS Load Balancer Controller with trust policy for IRSA. 8. Output the cluster name, OIDC provider ARN, and kubectl configuration command. Expected output: A CDK Python application that deploys a fully functional EKS cluster with autoscaling capabilities, ready to host production microservices with pod-level IAM permissions.",A fintech startup needs to deploy their microservices architecture on Kubernetes to handle transaction processing workloads. They require a production-grade EKS cluster with strong security controls and automated node scaling to handle variable traffic patterns throughout the day.,"""Production EKS cluster deployed in us-east-1 across 3 availability zones. Uses Amazon EKS managed Kubernetes control plane with EC2 node groups running Bottlerocket OS. VPC with public subnets for load balancers and private subnets for worker nodes. NAT gateways provide outbound internet access. Requires CDK 2.x with Python 3.9+, kubectl 1.28+, and AWS CLI configured with appropriate permissions. Cluster uses OIDC provider for IRSA integration.""","[""EKS cluster must use Kubernetes version 1.28 or higher"", ""Node groups must use Bottlerocket AMI for enhanced security"", ""IRSA (IAM Roles for Service Accounts) must be enabled for pod-level permissions"", ""Cluster autoscaler must scale between 3-10 nodes based on CPU utilization"", ""All worker nodes must be deployed in private subnets only"", ""EKS cluster endpoint must be private with no public access""]"
o7h5r5,done,CDK,Python,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CDK Python program to implement a multi-region disaster recovery architecture for a trading platform. The configuration must: 1. Set up Route 53 hosted zone with weighted routing policies (100% primary, 0% secondary initially). 2. Deploy ECS Fargate services running a containerized trading app (2 tasks, 1 vCPU, 2GB memory) in both regions. 3. Create Application Load Balancers with target groups pointing to ECS services. 4. Configure Aurora Global Database with a primary cluster in us-east-1 and secondary in us-west-2. 5. Implement DynamoDB global tables for session data with on-demand billing. 6. Set up S3 buckets with cross-region replication and versioning enabled. 7. Configure Route 53 health checks for both ALBs with failover threshold of 2 failures. 8. Create EventBridge rules to replicate critical events between regions. 9. Implement CloudWatch alarms for database lag monitoring (threshold: 60 seconds). 10. Output the Route 53 hosted zone ID and both ALB endpoints. Expected output: A CDK application that deploys a complete multi-region infrastructure with automatic failover capabilities, allowing the trading platform to continue operations within 5 minutes if the primary region fails.","A financial services company requires a disaster recovery solution for their critical trading platform. The primary region hosts a containerized application with real-time data processing, and they need automatic failover capabilities to a secondary region within 5 minutes RTO.","""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary) regions. Deployment includes ECS Fargate for containerized applications, Aurora PostgreSQL Global Database for transactional data, DynamoDB global tables for session storage, S3 with cross-region replication for static assets. Each region has its own VPC with 3 availability zones, private subnets for compute resources, and public subnets for load balancers. Route 53 manages DNS failover between regions. Requires AWS CDK 2.x with Python 3.9+, Docker for container builds, and AWS CLI configured with appropriate multi-region permissions.""","[""Use Route 53 health checks with 30-second intervals for automatic DNS failover"", ""Deploy identical ECS Fargate services in both us-east-1 (primary) and us-west-2 (secondary)"", ""Configure Aurora Global Database with write forwarding enabled"", ""Implement DynamoDB global tables with point-in-time recovery enabled"", ""Set up cross-region replication for S3 buckets with RTC (Replication Time Control)"", ""Use EventBridge global endpoints for event routing between regions"", ""All resources must have deletion_protection=False for testing environments""]"
f5l0f3,done,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy a multi-tier containerized payment processing system using ECS Fargate. The configuration must: 1. Set up ECS cluster with capacity providers for both Fargate and Fargate Spot. 2. Deploy three microservices (payment-api, transaction-processor, notification-service) as separate ECS services. 3. Configure Application Load Balancer with path-based routing to different services. 4. Implement service discovery using for inter-service communication. 5. Create task definitions with 2 vCPU and 4GB memory for API services, 1 vCPU and 2GB for others. 6. Configure auto-scaling policies targeting 70% CPU utilization with scale-out/in thresholds. 7. Set up CloudWatch Container Insights and sidecar containers for observability. 8. Implement health checks with 30-second intervals and 3 consecutive failures before marking unhealthy. 9. Configure log groups with 30-day retention and encryption using KMS. 10. Create application for blue-green deployments with 10-minute traffic shifting. Expected output: A complete CDK Python application that deploys a production-ready containerized microservices architecture with full observability, auto-scaling, and blue-green deployment capabilities.",A fintech company needs to migrate their monolithic payment processing application to a containerized microservices architecture. The application handles sensitive financial transactions and must maintain PCI DSS compliance while supporting automatic scaling based on transaction volume.,"""Production-grade container orchestration environment in us-east-1 region using ECS Fargate for container hosting, Application Load Balancer for traffic distribution, and Aurora Serverless v2 for database tier. Infrastructure spans 3 availability zones with VPC containing public subnets for ALB and private subnets for ECS tasks. NAT Gateways provide outbound connectivity. Requires CDK 2.x with Python 3.9+, Docker installed locally, and AWS CLI configured with appropriate permissions. ECR repositories for container images with vulnerability scanning enabled.""","[""ECS tasks must run in private subnets with no direct internet access"", ""Container images must be scanned for vulnerabilities before deployment"", ""Each microservice must have its own dedicated IAM task role"", ""Secrets must be stored in AWS Secrets Manager and injected at runtime"", ""Enable container insights and tracing for all services"", ""Implement blue-green deployment strategy with automatic rollback"", ""Configure auto-scaling based on custom CloudWatch metrics"", ""Use Fargate Spot for non-critical background processing tasks"", ""Implement circuit breaker pattern between microservices"", ""Deploy containers across at least 3 availability zones""]"
e4g7u7,done,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to build an advanced observability platform for a payment processing system. The configuration must: 1. Deploy Lambda functions with X-Ray tracing enabled and custom subsegments for DynamoDB operations and external API calls. 2. Create CloudWatch Log groups with 30-day retention and metric filters that extract error rates, response times, and payment failure reasons from structured JSON logs. 3. Implement custom CloudWatch metrics using EMF for business KPIs including transaction volume, payment success rate, and processing latency percentiles (p50, p95, p99). 4. Build CloudWatch dashboards with widgets showing real-time metrics, logs insights queries, and X-Ray service maps across multiple time windows. 5. Configure composite CloudWatch alarms that trigger when multiple conditions are met (e.g., high error rate + high latency + low throughput). 6. Create SNS FIFO topics with email and Slack subscriptions for critical alerts, implementing content-based deduplication. 7. Deploy Lambda functions for custom metric aggregation that run every minute to calculate rolling averages and anomaly detection. 8. Set up CloudWatch Logs Insights scheduled queries that generate daily reports on system health and performance trends. 9. Implement cross-service tracing correlation using X-Ray trace headers propagated through SQS messages. 10. Configure metric math expressions for calculated metrics like error percentage and throughput efficiency. Expected output: A complete CDK Python application that deploys a production-grade observability platform with distributed tracing, custom metrics, intelligent alerting, and visual dashboards for monitoring a high-volume payment processing system.","A financial services company needs to implement end-to-end observability for their payment processing system. The system handles millions of transactions daily and requires detailed monitoring, distributed tracing, and alerting to maintain their 99.99% uptime SLA.","""Production monitoring infrastructure deployed in us-east-1 for a distributed payment processing system using Lambda functions, DynamoDB tables, and SQS queues. The observability stack includes X-Ray for distributed tracing, CloudWatch Logs with Insights queries, custom CloudWatch dashboards with composite alarms, and SNS for multi-channel alerting. Requires CDK 2.x with Python 3.9+, boto3 installed. VPC endpoints configured for CloudWatch and X-Ray to reduce data transfer costs. The environment processes 5M+ transactions daily across 3 availability zones with sub-second latency requirements.""","[""All Lambda functions must have X-Ray tracing enabled with custom segments for database operations"", ""CloudWatch dashboards must use composite alarms that combine at least 3 different metric dimensions"", ""Custom metrics must be published using EMF (Embedded Metric Format) with namespace isolation per environment"", ""Log groups must implement metric filters that extract structured JSON fields for error tracking"", ""SNS topics must use FIFO delivery with content-based deduplication for alert notifications""]"
x5b3g4,done,CDK,Python,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a CDK Python program to orchestrate a blue-green migration strategy for database and containerized workloads. MANDATORY REQUIREMENTS (Must complete): 1. Deploy Aurora PostgreSQL cluster with read replicas across 3 AZs (CORE: ) 2. Create service with rolling deployment configuration (CORE: ) 3. Implement rotation for database credentials (CORE: ) 4. Configure custom VPC with isolated subnets for database and application tiers 5. Enable automated backups with 7-day retention and point-in-time recovery 6. Set up CloudWatch alarms for CPU, memory, and database connection metrics 7. Implement least-privilege IAM roles with session-based credentials 8. Create Lambda function to validate database schema compatibility pre-migration 9. Configure Application Load Balancer with health checks and SSL termination 10. Enable deletion protection on production resources with environment-based toggles OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Database Migration Service for zero-downtime cutover (OPTIONAL: DMS) - enables live replication  Implement Route 53 weighted routing for gradual traffic shifting (OPTIONAL: Route 53) - reduces migration risk  Add AWS Backup for centralized backup management (OPTIONAL: AWS Backup) - improves recovery options Expected output: A complete CDK Python application that deploys both blue and green environments with automated switchover capabilities, including database migration validation and rollback procedures.",A financial services company needs to migrate their monolithic application from on-premises to AWS cloud. The application consists of a PostgreSQL database and containerized web services that must maintain strict data consistency during the migration window.,"""Multi-region AWS deployment primarily in us-east-1 with disaster recovery in us-west-2. Infrastructure includes Aurora PostgreSQL Multi-AZ cluster, services running containerized applications, and for credential rotation. Requires Python 3.9+, AWS CDK 2.x, Docker installed for local testing. VPC spans 3 availability zones with public subnets for ALB, private subnets for tasks, and isolated subnets for instances. NAT Gateways provide outbound connectivity for private resources.""","["" cluster must use encrypted storage with customer-managed keys"", "" tasks must pull images only from private repositories"", ""Database migration must support rollback within 15-minute RTO"", ""All inter-service communication must use to avoid internet exposure"", ""Lambda validation function must complete schema checks within 5-minute timeout"", ""Secrets rotation must not cause application downtime during credential updates"", ""CloudWatch logs must be encrypted and retained for compliance requirements"", ""Load balancer must enforce TLS 1.2 minimum with specific cipher suites"", ""Stack must use CDK Aspects to enforce tagging standards across all resources"", ""Migration process must maintain ACID compliance throughout cutover window""]"
j3s6p9,error,CDK,Python,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CDK Python program to deploy a multi-region disaster recovery infrastructure for a trading platform. The configuration must: 1. Set up VPCs in us-east-1 (10.0.0.0/16) and eu-west-1 (10.1.0.0/16) with 3 AZs each and establish VPC peering between them. 2. Deploy RDS Aurora Global Database with MySQL 8.0, enabling backtrack and point-in-time recovery. 3. Create S3 buckets in both regions with SSE-KMS encryption using customer managed keys and configure cross-region replication. 4. Deploy Lambda functions in both regions with reserved concurrency set to 100 and environment variables for database endpoints. 5. Implement Route 53 hosted zone with primary and secondary record sets using health checks for automatic failover. 6. Configure health checks to monitor RDS cluster endpoints with 30-second intervals and 2 failure threshold. 7. Create IAM roles with least-privilege policies for Lambda execution and S3 replication. 8. Set up CloudWatch alarms for RDS replication lag exceeding 5 minutes. 9. Enable deletion protection on production resources but allow deletion of development resources. 10. Tag all resources with Environment=Production and DisasterRecovery=Enabled. Expected output: A CDK application that deploys identical infrastructure stacks in two regions with automated failover capabilities, meeting the specified RPO/RTO requirements and maintaining data consistency through Aurora Global Database and S3 cross-region replication.",A financial services company requires a multi-region disaster recovery setup for their critical trading platform. The system must maintain sub-5 minute RPO and RTO targets while ensuring data consistency across regions. Compliance requires all data to be encrypted at rest and in transit.,"""Multi-region AWS infrastructure spanning us-east-1 (primary) and eu-west-1 (secondary) regions. Core services include RDS Aurora Global Database for transactional data, S3 with cross-region replication for object storage, Lambda functions for business logic processing, and Route 53 for DNS failover. Each region has its own VPC with 10.0.0.0/16 (us-east-1) and 10.1.0.0/16 (eu-west-1) CIDR blocks, connected via VPC peering. Requires AWS CDK 2.x with Python 3.9+, boto3, and AWS CLI configured with appropriate permissions. The infrastructure supports automatic failover with Route 53 health checks monitoring primary region availability.""","[""Primary region must be us-east-1 with failover to eu-west-1"", ""RDS Aurora Global Database must use MySQL 8.0 with backtrack enabled"", ""All S3 buckets must use SSE-KMS encryption with customer managed keys"", ""Lambda functions must use reserved concurrency of exactly 100"", ""Cross-region replication must complete within 5 minutes"", ""VPC peering connections must use non-overlapping CIDR blocks"", ""Route 53 health checks must trigger failover within 30 seconds""]"
l7i8o2,in_progress,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy a multi-environment financial services API platform with strict configuration consistency requirements. MANDATORY REQUIREMENTS (Must complete): 1. Define a base Stack class that accepts environment configuration and creates VPC with 2 public and 2 private subnets (CORE: /VPC) 2. Create RDS Aurora PostgreSQL cluster with environment-specific sizing but shared parameter groups (CORE: RDS) 3. Deploy ECS Fargate service running a containerized API with environment-specific task definitions (CORE: ECS) 4. Configure Application Load Balancer with target groups pointing to ECS tasks 5. Implement DynamoDB tables for session storage with environment-specific capacity modes 6. Create Lambda functions for async processing with shared code but environment-specific configurations 7. Set up S3 buckets following strict naming conventions for static asset storage 8. Implement centralized configuration management using 9. Configure for database credentials with conditional rotation 10. Apply consistent tagging strategy across all resources using Stack-level tags OPTIONAL ENHANCEMENTS (If time permits):  Add AWS to ALB in production only (OPTIONAL: ) - improves security posture  Implement distribution for static assets (OPTIONAL: ) - reduces latency  Add rules for automated backups (OPTIONAL: ) - improves disaster recovery Expected output: A CDK Python application with a base stack class and three environment-specific stack instances (dev, staging, prod) that can be deployed using cdk deploy with consistent infrastructure patterns but environment-appropriate sizing and configurations.","A fintech startup needs to maintain identical infrastructure across development, staging, and production environments to ensure consistent testing and deployment workflows. The company requires automated environment provisioning with proper isolation while maintaining configuration parity across all environments to prevent environment-specific bugs.","""Multi-environment AWS deployment across us-east-1 spanning three isolated environments (dev, staging, prod). Each environment contains VPC with public/private subnets across 2 AZs, Application Load Balancer, ECS Fargate service running containerized API, RDS Aurora PostgreSQL cluster, DynamoDB tables for session storage, S3 buckets for static assets, Lambda functions for async processing. Requires Python 3.9+, AWS CDK 2.x, Docker for local testing. Central manages environment-specific configurations. Implements blue-green deployment strategy in production only.""","[""Each environment must have its own isolated VPC with identical CIDR blocks relative to environment (10.1.0.0/16 for dev, 10.2.0.0/16 for staging, 10.3.0.0/16 for prod)"", ""RDS instances must use different instance types per environment (t3.micro for dev, t3.small for staging, m5.large for prod) but identical parameter groups"", ""All Lambda functions must share the same code but have environment-specific configuration through environment variables"", ""S3 buckets must follow naming convention: company-{service}-{environment}-{region}-{random}"", ""DynamoDB tables must have identical schemas but different capacity settings (on-demand for dev/staging, provisioned for prod)"", ""All resources must be tagged with Environment, Team, and CostCenter tags derived from a central configuration"", "" must store database credentials with automatic rotation enabled only in production"", ""CloudFormation stack names must include git commit hash for traceability""]"
y2t3p9,error,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy a high-performance web application for real-time financial data visualization. MANDATORY REQUIREMENTS (Must complete): 1. Deploy CloudFront distribution with S3 origin for React frontend assets and ALB origin for API endpoints (CORE: CloudFront) 2. Create ECS Fargate service running FastAPI containers behind Application Load Balancer with WebSocket support (CORE: ECS) 3. Provision Aurora Serverless v2 PostgreSQL cluster with Data API enabled for database operations (CORE: Aurora) 4. Configure ALB with path-based routing: /api/* to ECS service, / to CloudFront 5. Implement IAM roles with least-privilege access for ECS tasks to Aurora and S3 6. Set up CloudWatch Log Groups with 30-day retention for all services 7. Enable deletion protection on Aurora cluster but set removal_policy=DESTROY for other resources 8. Configure ECS auto-scaling based on ALB request count metric (target: 1000 requests per task) OPTIONAL ENHANCEMENTS (If time permits):  Add Lambda function for scheduled data aggregation tasks (OPTIONAL: Lambda) - reduces database load  Implement queue for asynchronous trade notifications (OPTIONAL: ) - improves responsiveness  Add tracing across all services (OPTIONAL: ) - enables performance debugging Expected output: A complete CDK Python application that deploys the entire infrastructure stack with proper networking, security groups, and service integrations. The stack should be deployable with 'cdk deploy' and include proper error handling and resource tagging.",A financial services startup needs to deploy their real-time trading dashboard application with strict performance requirements. The application consists of a React frontend and Python FastAPI backend that requires WebSocket support for live price updates.,"""Production deployment in us-east-1 with multi-AZ configuration across 3 availability zones. Infrastructure includes CloudFront distribution, Application Load Balancer, ECS Fargate cluster with Spot capacity providers, Aurora Serverless v2 PostgreSQL database, and Lambda functions for background processing. Requires Python 3.9+, AWS CDK 2.100+, Docker for container builds. VPC with public subnets for ALB and private subnets for ECS tasks and database. NAT gateways in each AZ for outbound connectivity from private subnets.""","[""ALB must use WebSocket-enabled target groups with stickiness enabled for session affinity"", ""ECS tasks must use Fargate Spot instances with capacity provider strategies for cost optimization"", ""Aurora Serverless v2 must scale between 0.5 and 2 ACUs with Data API enabled"", ""CloudFront behaviors must implement separate cache policies for static assets (1 hour) and API calls (no cache)"", ""All Lambda functions must use ARM-based Graviton2 processors for cost efficiency""]"
j8a2g2,done,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to build a serverless ETL pipeline for processing large CSV transaction files. MANDATORY REQUIREMENTS (Must complete): 1. Deploy Step Functions state machine to orchestrate file processing workflow (CORE: Step Functions). 2. Create Lambda function to split large CSV files into 50MB chunks for parallel processing (CORE: Lambda). 3. Implement DynamoDB table to track processing status with partition key 'file_id' and sort key 'chunk_id' (CORE: DynamoDB). 4. Configure S3 bucket with lifecycle policy to move processed files to Glacier after 30 days. 5. Set up EventBridge rule to trigger workflow when files land in S3 with prefix 'incoming/'. 6. Create Lambda function for data validation that checks CSV headers and data types. 7. Implement error handling with exponential backoff (max 3 retries) in Step Functions. 8. Configure all Lambda functions with 3GB memory and 15-minute timeout. 9. Create CloudWatch dashboard showing processing metrics and failure rates. 10. Ensure all resources use consistent tagging: Environment=Production, Project=ETL. OPTIONAL ENHANCEMENTS (If time permits):  Add SQS FIFO queue for processing results notification (OPTIONAL: SQS) - ensures ordered delivery.  Implement tracing across all Lambda functions (OPTIONAL: ) - enables distributed debugging.  Add SNS topic for alerting on processing failures (OPTIONAL: SNS) - improves incident response. Expected output: Complete CDK Python stack that deploys a production-ready serverless ETL pipeline capable of processing large CSV files in parallel with full observability and error handling.","A financial analytics company needs to process daily transaction files from multiple partner banks. The files arrive at unpredictable times and vary in size from 100MB to 2GB, requiring a robust ETL pipeline that can handle parallel processing and maintain audit trails.","""AWS multi-region deployment supporting us-east-1 as primary and eu-west-1 for disaster recovery. Core services include Step Functions for workflow orchestration, Lambda functions with 3GB memory for processing, DynamoDB for state management, S3 for file storage with Glacier lifecycle. Requires Python 3.9+, AWS CDK 2.x, boto3 installed. VPC not required as all services are serverless. EventBridge for event-driven triggers, CloudWatch for monitoring. IAM roles follow least-privilege with specific actions per service. Stack designed for high-throughput file processing with automatic scaling.""","[""Lambda functions must use Python 3.9 runtime with pandas and boto3 pre-installed as layers"", ""Step Functions state machine must implement Map state for parallel chunk processing with MaxConcurrency of 10"", ""DynamoDB table must use on-demand billing mode with point-in-time recovery enabled"", ""S3 bucket must enable versioning and server-side encryption with AWS managed keys (SSE-S3)"", ""All IAM roles must explicitly deny s3:DeleteBucket and dynamodb:DeleteTable actions for safety"", ""CloudWatch Logs retention must be set to 30 days for all Lambda function logs to control costs""]"
l7y7r4,error,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to implement a multi-stage CI/CD pipeline for containerized applications. MANDATORY REQUIREMENTS (Must complete): 1. Create with GitHub webhook trigger and OAuth token from (CORE: ) 2. Configure project with Docker runtime, 4GB memory, and artifact encryption (CORE: ) 3. Deploy to ECS Fargate service using blue-green deployment with 10-minute rollback window (CORE: ECS) 4. Store build artifacts in S3 with KMS encryption and versioning enabled 5. Implement manual approval stage before production deployment with notifications 6. Configure CloudWatch alarms for deployment failures with automatic rollback 7. Create separate build environments for unit tests, security scans, and container builds OPTIONAL ENHANCEMENTS (If time permits):  Add CodeGuru Reviewer for automated code quality checks (OPTIONAL: CodeGuru) - improves code quality  Implement Lambda function for custom deployment validation (OPTIONAL: Lambda) - adds deployment safety  Configure rules for pipeline state changes (OPTIONAL: ) - enables external integrations Expected output: CDK Python application that deploys a complete CI/CD pipeline with automated testing, security scanning, and blue-green deployments to ECS Fargate.",A fintech startup needs an automated deployment pipeline for their containerized payment processing application. The pipeline must support blue-green deployments to ECS Fargate with automatic rollback capabilities. Security compliance requires all build artifacts to be encrypted and deployment approvals for production.,"""AWS multi-account setup in us-east-1 with separate development (111111111111) and production (222222222222) accounts. Requires CDK 2.x with Python 3.9+, Docker installed, AWS CLI v2 configured with cross-account assume role permissions. VPC with 3 availability zones, private subnets for ECS tasks, NAT gateways for outbound traffic. Application Load Balancer in public subnets. GitHub repository integration requires OAuth token stored in . ECS Fargate cluster with auto-scaling configured.""","["" compute type must be BUILD_GENERAL1_MEDIUM or higher for container builds"", ""Pipeline artifacts must use customer-managed KMS keys with key rotation enabled"", ""ECS task definition must specify exact container image tags, no 'latest' tags allowed"", ""All IAM roles must follow least-privilege principle with no AdministratorAccess policies"", "" projects must use VPC mode with no internet access except through NAT gateway"", ""Pipeline must complete end-to-end in under 15 minutes for standard deployments"", ""Stack must use CDK Aspects to enforce tagging compliance across all resources""]"
q4n9i5,error,CDK,Python,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CDK Python program to deploy a highly available payment processing API with automatic regional failover capabilities.

MANDATORY REQUIREMENTS (Must complete):
1. Create DynamoDB global tables in us-east-1 and us-west-2 with on-demand billing mode (CORE: DynamoDB)
2. Deploy Lambda functions in both regions to process payment transactions with 3GB memory allocation (CORE: Lambda)
3. Configure Route 53 hosted zone with failover routing between regions based on health checks
4. Set up CloudWatch alarms in both regions to monitor Lambda errors and DynamoDB throttling
5. Implement least-privilege IAM roles with cross-region permissions for global table access
6. Enable point-in-time recovery and encryption at rest for all DynamoDB tables
7. Configure Lambda dead letter queues in each region for failed transaction handling
8. Set deletion protection to false and use unique stack names per region

OPTIONAL ENHANCEMENTS (If time permits):
 Add API Gateway REST endpoints in each region for HTTP access (OPTIONAL: API Gateway) - enables direct API testing
 Implement EventBridge rules for cross-region event replication (OPTIONAL: EventBridge) - improves state synchronization
 Add AWS Backup plans for additional data protection (OPTIONAL: AWS Backup) - enhances disaster recovery

Expected output: A CDK Python application that deploys identical infrastructure stacks in two regions with automatic failover capabilities. The solution should handle regional outages gracefully, maintaining data consistency through DynamoDB global tables and ensuring payment processing continues with minimal disruption.","A fintech startup needs to ensure their payment processing API remains available during regional AWS outages. They require an active-passive multi-region setup where traffic automatically fails over to a secondary region if the primary becomes unhealthy, with automated database replication and synchronized application state.","""Multi-region active-passive deployment across us-east-1 (primary) and us-west-2 (secondary) regions. Infrastructure includes API Gateway, Lambda functions, DynamoDB global tables, Route 53 for DNS failover, and CloudWatch for monitoring. Requires AWS CDK 2.x with Python 3.9+, boto3 installed. Each region has isolated VPCs with private subnets for Lambda functions. Cross-region replication enabled for all stateful components. IAM roles configured for cross-region access where needed.""","[""Use Route 53 health checks with failover routing policy for automatic region switching"", ""Implement DynamoDB global tables for cross-region data replication with point-in-time recovery enabled"", ""Deploy Lambda functions in both regions with identical environment variables and layers"", ""Configure CloudWatch alarms to trigger SNS notifications when failover occurs"", ""Set RTO (Recovery Time Objective) of 5 minutes and RPO (Recovery Point Objective) of 1 minute""]"
v6i3f4,done,CDK,Python,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CDK Python program to deploy a secure document processing pipeline with automated compliance scanning. The configuration must: 1. Create KMS keys with automatic rotation for bucket encryption and environment variables. 2. Deploy buckets with versioning, access logging to a separate bucket, and block all public access. 3. Implement functions for document validation, encryption, and compliance scanning with 15-second timeouts. 4. Configure API Gateway REST API with WAF rules blocking SQL injection and XSS attempts. 5. Set up VPC with private subnets only and for , DynamoDB, and . 6. Create DynamoDB table for audit logs with point-in-time recovery and encryption. 7. Implement CloudWatch Events rules to capture all API calls and store in CloudWatch Logs. 8. Deploy GuardDuty with rules triggering remediation Lambdas for high-severity findings. 9. Configure Secrets Manager to store API keys and database credentials with automatic rotation. 10. Create IAM roles with external ID requirements and session policies limiting access duration. 11. Implement custom checks validating encryption and access policies. 12. Set up topics with encrypted email notifications for security alerts. Expected output: A complete CDK Python stack that deploys a zero-trust document processing system meeting PCI-DSS requirements with automated security monitoring and remediation.","A financial services company needs to implement a secure document processing system that meets PCI-DSS compliance requirements. The system must handle sensitive financial documents with end-to-end encryption, audit logging, and automated security scanning. All data must be encrypted at rest and in transit with customer-managed keys.","""Highly secure multi-AZ deployment in us-east-1 for PCI-DSS compliant document processing. Uses for processing, with KMS encryption for storage, API Gateway with WAF for endpoints. Requires Python 3.9+, CDK 2.x, AWS CLI v2 configured with MFA. VPC spans 3 AZs with private subnets only, using for AWS service access. Security-focused architecture with GuardDuty, and automated compliance scanning. No internet gateway, all traffic routes through .""","[""All buckets must use SSE-KMS encryption with customer-managed CMKs"", "" functions must use separate execution roles with least-privilege policies"", ""All API Gateway endpoints must require API keys and implement request throttling"", ""CloudTrail alternatives must be implemented using CloudWatch Events and Logs"", "" must be used for all AWS service communications"", ""Security Groups must explicitly deny all traffic except required ports"", ""All IAM policies must use condition keys to restrict access by IP and MFA"", ""Secrets Manager must rotate database credentials every 30 days automatically"", ""GuardDuty findings must trigger automated remediation functions""]"
g5u6e1,done,CDK,Python,expert,Application Deployment,Web Application Deployment,Create a CDK Python program to optimize an existing EKS cluster deployment that currently suffers from slow deployment times and API throttling issues. MANDATORY REQUIREMENTS (Must complete): 1. Create optimized EKS cluster with version 1.28 and OIDC provider (CORE: EKS) 2. Deploy 3 managed node groups with parallel creation strategy (CORE: EC2) 3. Implement custom resource Lambda with exponential backoff for kubectl operations 4. Add IAM role caching mechanism using CDK context values 5. Create CloudWatch dashboard showing deployment metrics and API call patterns 6. Use removal_policy=DESTROY for all resources in non-production environments 7. Implement stack dependencies using parameters instead of CloudFormation exports 8. Add error handling with automatic retry for transient EKS API failures 9. Configure Lambda functions with 512MB memory and 5-minute timeout 10. Output total deployment time metric to CloudWatch OPTIONAL ENHANCEMENTS (If time permits):  Add workflow for complex deployments (OPTIONAL: ) - provides better orchestration control  Implement SQS queue for batching kubectl commands (OPTIONAL: SQS) - reduces API calls  Add EventBridge rules for deployment notifications (OPTIONAL: EventBridge) - improves visibility Expected output: Optimized CDK Python application that reduces EKS deployment time to under 15 minutes with robust error handling and API throttling prevention.,Your company's EKS cluster deployment takes over 45 minutes and frequently hits AWS API rate limits during peak hours. The existing CDK stack creates unnecessary resources and uses inefficient patterns that cause deployment failures.,"""Production EKS environment in us-east-1 requiring optimization of existing CDK Python code. Current setup includes EKS 1.28 cluster with 3 node groups across 3 AZs, ALB Ingress Controller, and Cluster Autoscaler. VPC uses private subnets with NAT Gateways. Deployment pipeline runs in CodeBuild with Python 3.9, CDK 2.x, kubectl 1.28. Focus on reducing deployment time from 45 to under 15 minutes while preventing API throttling errors.""","[""Use CDK v2 with Python 3.9+ and AWS CDK Toolkit 2.x"", ""Implement custom resource providers with exponential backoff for API calls"", ""Cache IAM role ARNs to avoid repeated DescribeRole API calls"", ""Use single CloudFormation custom resource for all kubectl operations"", ""Implement parallel node group creation with dependency ordering"", ""Add circuit breaker pattern for EKS API calls with 5-minute cooldown"", ""Use for sharing outputs between stacks"", ""Implement stack-level retry logic with 3 attempts maximum"", ""Add CloudWatch metrics for deployment duration tracking"", ""Ensure all Lambda functions use ARM64 architecture for cost optimization""]"
y7f2l2,done,CDK,Python,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CDK Python program to deploy a production-ready EKS cluster with enhanced security controls. The configuration must: 1. Create an EKS cluster v1.28 in private subnets only with API endpoint access restricted to CIDR ranges. 2. Deploy two managed node groups: 'app-nodes' (t3.large, min=2, max=10) and 'system-nodes' (t3.medium, min=1, max=3) with taints for workload isolation. 3. Enable IRSA and create three service accounts: 'app-sa' with S3 read access, 'ci-sa' with ECR push access, and 'monitoring-sa' with CloudWatch write access. 4. Install cluster autoscaler as a Helm chart with IRSA authentication and configure it to respect node group taints. 5. Create custom launch templates enforcing IMDSv2, disabling IMDSv1, and adding required security group rules. 6. Configure OIDC provider and map IAM roles to Kubernetes RBAC for developer access without kubectl exec permissions. 7. Enable control plane logging for api, audit, authenticator, controllerManager, and scheduler to CloudWatch. 8. Create for ECR, S3, EC2, and other required services to avoid internet routing. 9. Implement pod security standards with 'restricted' as default and 'baseline' for system namespace. 10. Set up AWS Load Balancer Controller with IRSA for ingress management. Expected output: A CDK Python application that deploys a fully functional EKS cluster meeting all security requirements, with node groups properly configured for workload isolation and all necessary IAM roles and service accounts created through IRSA.",A fintech startup needs to deploy their microservices architecture on AWS EKS with strict security requirements for PCI compliance. The infrastructure must support blue-green deployments and integrate with their existing CI/CD pipelines while maintaining network isolation between development and production workloads.,"""Production-grade EKS infrastructure deployed in us-east-1 across 3 availability zones. Uses EKS 1.28 with managed node groups running Amazon Linux 2 AMIs. VPC with private subnets only, no NAT gateways - all traffic routed through AWS PrivateLink endpoints. Requires CDK 2.x with Python 3.9+, kubectl 1.28+, and AWS CLI v2 configured. Integration with for node access, AWS Secrets Manager for sensitive configurations, and CloudWatch Container Insights for monitoring.""","[""EKS cluster must use only private subnets with no direct internet access"", ""All node groups must use custom launch templates with IMDSv2 enforcement"", ""Cluster autoscaler must scale based on pending pods, not CPU metrics"", ""IRSA roles must follow least-privilege with no wildcard permissions"", ""Kubernetes API endpoint must be private with -only access""]"
k0j9w2,in_progress,CDK,Python,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CDK Python program to implement an active-passive multi-region disaster recovery system for a PostgreSQL database. The configuration must: MANDATORY REQUIREMENTS (Must complete): 1. Deploy RDS Aurora PostgreSQL Global Database with primary cluster in us-east-1 and secondary in us-west-2 (CORE: RDS) 2. Configure Route53 health checks and weighted routing with automatic failover between regions (CORE: Route53) 3. Create Lambda functions in both regions to monitor database health and trigger failover procedures (CORE: Lambda) 4. Implement cross-region VPC peering with proper security group rules for database replication 5. Set up CloudWatch alarms for replication lag exceeding 60 seconds 6. Configure automated backups with 7-day retention and cross-region snapshot copying 7. Create IAM roles with least privilege for Lambda execution and RDS operations 8. Enable deletion protection on production resources, disable on non-production OPTIONAL ENHANCEMENTS (If time permits):  Add SNS topics for alerting operations team during failover events (OPTIONAL: SNS) - improves incident response  Implement Step Functions for orchestrating complex failover workflows (OPTIONAL: Step Functions) - reduces manual intervention  Deploy CloudFront distribution for static asset caching during regional outages (OPTIONAL: CloudFront) - maintains partial availability Expected output: CDK Python application that deploys a complete multi-region disaster recovery infrastructure with automated failover capabilities, health monitoring, and data replication between regions.","A financial services company requires a disaster recovery solution for their critical PostgreSQL database that processes payment transactions. The system must automatically failover to a secondary region within 5 minutes of primary region failure, maintaining transaction integrity and minimizing data loss.","""Multi-region AWS deployment spanning us-east-1 (primary) and us-west-2 (disaster recovery) for PostgreSQL database failover. Uses RDS Aurora Global Database with cross-region read replicas, Route53 weighted routing with health checks, Lambda functions for automated failover orchestration. Requires CDK 2.x with Python 3.9+, AWS CLI configured with appropriate permissions. VPCs in both regions with private subnets for database clusters, public subnets for health check endpoints. Cross-region VPC peering for secure replication traffic. CloudWatch monitoring with custom metrics for replication lag and failover status.""","[""RTO (Recovery Time Objective) must not exceed 5 minutes from failure detection to failover completion"", ""RPO (Recovery Point Objective) must be less than 1 minute with continuous replication monitoring"", ""All database connections must use SSL/TLS encryption with certificate validation"", ""Health check endpoints must respond within 2 seconds to avoid false-positive failovers"", ""Cross-region data transfer costs must be minimized by using Aurora Global Database native replication"", ""Lambda functions must implement exponential backoff for API calls to prevent throttling during failover""]"
j0d0e7,done,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy a container orchestration platform for microservices. MANDATORY REQUIREMENTS (Must complete): 1. Create an ECS cluster with capacity providers for Fargate and Fargate Spot (CORE: ECS) 2. Deploy App Mesh with virtual nodes and services for service discovery (CORE: App Mesh) 3. Configure Application Load Balancer with path-based routing to different services (CORE: ALB) 4. Set up ECR repositories with vulnerability scanning on push enabled 5. Implement blue-green deployment using ECS deployment circuit breaker 6. Create task definitions with 1 vCPU and 2GB memory for each service 7. Configure CloudWatch Container Insights for cluster monitoring 8. Set up IAM task roles with least-privilege policies for S3 and DynamoDB access 9. Implement auto-scaling policies based on CPU utilization (target 70%) 10. Create CloudWatch dashboards showing service health and performance metrics OPTIONAL ENHANCEMENTS (If time permits):  Add sidecar containers for distributed tracing (OPTIONAL: ) - improves debugging of service dependencies  Implement for non-sensitive configs (OPTIONAL: ) - centralizes configuration management  Set up rules for ECS task state changes (OPTIONAL: ) - enables automated response to failures Expected output: Complete CDK Python application that deploys a production-ready ECS cluster with App Mesh service mesh, automated deployments, and comprehensive monitoring. The stack should support running at least 3 microservices with independent scaling and deployment lifecycles.","A fintech startup needs to deploy their microservices architecture for processing real-time financial transactions. The system requires strict isolation between services, automatic scaling based on transaction volume, and blue-green deployment capabilities for zero-downtime updates.","""Production-grade container orchestration platform deployed in us-east-1 across 3 availability zones. Core infrastructure includes ECS Fargate cluster with App Mesh service mesh, Application Load Balancers, ECR repositories, and Aurora Serverless v2 PostgreSQL. Requires AWS CDK 2.x with Python 3.9+, Docker Desktop installed for local testing. VPC with public subnets for ALBs and private subnets for ECS tasks. NAT Gateways in each AZ for outbound internet access. AWS CLI configured with appropriate permissions for ECS, ECR, IAM, and networking services.""","[""Each ECS service must use Fargate Spot instances with a minimum of 2 tasks for cost optimization"", ""Container images must be stored in private ECR repositories with lifecycle policies to retain only the last 10 images"", ""Service-to-service communication must use AWS App Mesh with mTLS encryption enabled"", ""Each service must have dedicated Application Load Balancer target groups with health checks every 10 seconds"", ""Container logs must be streamed to CloudWatch Logs with encryption using customer-managed keys"", ""Task definitions must use for database credentials with automatic rotation every 30 days"", ""All resources must be tagged with Environment, Team, and CostCenter tags for billing allocation""]"
i9k3e7,in_progress,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to orchestrate infrastructure migration from us-east-1 to eu-west-1. The configuration must: 1. Define source and target stacks with parameterized region configurations. 2. Create DynamoDB Global Tables for zero-downtime data migration. 3. Set up S3 bucket replication rules with encryption in transit. 4. Deploy Lambda functions in both regions with environment-specific configurations. 5. Configure API Gateway custom domains with weighted routing. 6. Implement Cognito user pool migration using Lambda triggers. 7. Create CloudWatch dashboards to monitor migration progress. 8. Deploy workflow to coordinate migration phases. 9. Set up topics for migration status notifications. 10. Configure IAM roles with cross-region trust relationships. Expected output: CDK application with separate stacks for source and target regions, migration orchestration stack, and automated validation stack that ensures data integrity post-migration.","A fintech startup currently runs their payment processing infrastructure in us-east-1 but needs to migrate to eu-west-1 for regulatory compliance. The existing infrastructure uses hardcoded values and manual deployments, making the migration error-prone.","""Multi-region AWS environment spanning us-east-1 (current) and eu-west-1 (target). Infrastructure includes API Gateway, Lambda functions, DynamoDB tables, S3 buckets, and Cognito user pools. VPC with 3 private subnets and NAT Gateway in each region. Requires Python 3.9+, AWS CDK 2.x, boto3, and AWS CLI configured with appropriate IAM permissions for cross-region operations. Migration must maintain zero downtime for critical payment processing services.""","[""Use CDK context variables for all region-specific configurations"", ""Implement automated state migration for DynamoDB tables using "", ""Configure cross-region replication for S3 buckets before cutover"", ""Use for storing migration status"", ""Implement canary deployments with weighted routing during migration"", ""Create rollback mechanism using CloudFormation stack policies"", ""Generate migration validation reports using Lambda and ""]"
n3f5l8,done,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy a production-grade VPC with advanced networking features. The configuration must: 1. Create a VPC with CIDR 172.31.0.0/16 and enable DNS hostnames and DNS resolution. 2. Deploy 6 subnets total - one public and one private per AZ across 3 AZs with /24 CIDR blocks. 3. Launch NAT instances in each public subnet using t3.micro instances with the latest Amazon Linux 2023 AMI. 4. Configure NAT instance security groups to allow traffic only from their respective private subnet. 5. Create custom route tables ensuring private subnets route 0.0.0.0/0 through their AZ's NAT instance. 6. Implement Network ACLs that deny all inbound traffic except HTTP/HTTPS from 0.0.0.0/0 and SSH from 192.168.1.0/24. 7. Enable VPC Flow Logs with 'ALL' traffic type - store in S3 with 90-day lifecycle and CloudWatch Logs with 30-day retention. 8. Create a Lambda function that publishes NAT instance network metrics to CloudWatch every 5 minutes. 9. Generate Transit Gateway attachment configuration as CloudFormation outputs without creating the attachment. 10. Implement AMI mapping for at least 3 regions to support cross-region deployment. 11. Tag all resources with Environment='Production' and CostCenter='NetworkOps'. 12. Output VPC ID, subnet IDs grouped by type, NAT instance IDs, and Transit Gateway attachment config. Expected output: A CDK Python application that creates a fully functional multi-AZ VPC with NAT instance-based routing, comprehensive logging, monitoring, and Transit Gateway readiness. The stack should be deployable across regions with proper AMI resolution and include all specified security controls.","A financial services company needs to establish a secure, compliant network foundation for their new AWS environment. The infrastructure must support both public-facing services and internal microservices while maintaining strict network isolation and audit capabilities. The company requires a repeatable deployment process across multiple regions.","""Multi-AZ network foundation deployed across any AWS region using CDK 2.x with Python 3.9+. Core components include VPC with custom CIDR allocation, NAT instances for outbound traffic, Transit Gateway preparation, and comprehensive network logging. Requires AWS CLI configured with appropriate permissions. The infrastructure spans 3 Availability Zones with public subnets (for NAT instances) and private subnets (for application workloads). VPC Flow Logs configured for compliance with dual destinations. Custom CloudWatch metrics track NAT instance performance. Region-agnostic design supports deployment in us-east-1, eu-west-1, or ap-southeast-1.""","[""VPC must use non-overlapping CIDR ranges that allow for future peering with 10.0.0.0/8 networks"", ""All private subnets must route through NAT instances (not NAT Gateways) for cost control"", ""Deploy exactly 3 Availability Zones with both public and private subnets in each"", ""Network ACLs must explicitly deny all traffic except ports 80, 443, and 22 from specific CIDR blocks"", ""VPC Flow Logs must be enabled and sent to both S3 and CloudWatch Logs with different retention policies"", ""Transit Gateway attachment must be prepared but not created (output the attachment configuration)"", ""All NAT instances must use t3.micro with Amazon Linux 2023 AMI"", ""Implement custom CloudWatch metrics for NAT instance bandwidth utilization"", ""Stack must support cross-region deployment with region-specific AMI mappings""]"
m8l9x7,done,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy a fraud detection pipeline consistently across three environments. The configuration must: 1. Define a reusable CDK stack class that accepts environment configuration as parameters. 2. Use CDK context to store environment-specific values for dev, staging, and prod. 3. Deploy Kinesis Data Streams with appropriate shard counts per environment. 4. Create Lambda functions for stream processing with environment-based memory settings. 5. Set up DynamoDB tables with environment-specific capacity units. 6. Configure CloudWatch alarms for Lambda error rates with different thresholds. 7. Implement SSM Parameter Store integration for secure configuration management. 8. Create S3 buckets following the pattern: company-fraud-data-{env}-{region}. 9. Define consistent IAM roles across all environments using CDK's Role construct. 10. Conditionally enable tracing based on environment type. Expected output: A complete CDK Python application with app.py, stack definition, and cdk.json context configuration that deploys identical infrastructure with environment-specific parameters.","A financial services company needs to maintain identical infrastructure across development, staging, and production environments for their real-time fraud detection system. The infrastructure must be deployed consistently across all environments with environment-specific configurations for compute resources, data retention policies, and alerting thresholds.","""Multi-environment AWS deployment across us-east-1 (production), us-west-2 (staging), and eu-west-1 (development) regions. Infrastructure includes Kinesis Data Streams for real-time data ingestion, Lambda functions for stream processing, DynamoDB for storing processed results, and S3 for archival. Requires Python 3.8+, AWS CDK 2.x, and AWS CLI configured with appropriate credentials. Each environment operates in isolated VPCs with private subnets and VPC endpoints for AWS services. CloudWatch Logs retention varies by environment: 7 days (dev), 14 days (staging), 30 days (production).""","[""Use CDK context variables to manage environment-specific configurations without code duplication"", ""Implement a single CDK stack class that accepts environment parameters"", ""Deploy Kinesis Data Streams with environment-specific shard counts (dev: 1, staging: 2, prod: 4)"", ""Configure Lambda functions with environment-based memory allocation (dev: 512MB, staging: 1GB, prod: 2GB)"", ""Set DynamoDB read/write capacity based on environment (dev: 5/5, staging: 10/10, prod: 25/25)"", ""Implement CloudWatch alarms with environment-specific thresholds for Lambda errors"", ""Use SSM Parameter Store to manage environment-specific API keys and connection strings"", ""Create separate S3 buckets per environment with consistent naming patterns"", ""Implement IAM roles with identical permissions across environments using CDK patterns"", ""Enable tracing only for staging and production environments""]"
g8k9o0,done,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy a high-availability web application for financial transaction processing. MANDATORY REQUIREMENTS (Must complete): 1. Deploy ECS Fargate service with auto-scaling (2-10 tasks) running containerized web app (CORE: ECS) 2. Create Aurora PostgreSQL cluster with one writer and one reader instance (CORE: RDS Aurora) 3. Configure Application Load Balancer with health checks and target group 4. Set up table for session storage with on-demand billing 5. Deploy Lambda function for async transaction validation 6. Create S3 bucket for static assets with distribution 7. Implement VPC with 3 AZs, public/private subnets, and NAT gateways 8. Configure CloudWatch dashboards for key application metrics 9. Set up topic for critical alerts with email subscription OPTIONAL ENHANCEMENTS (If time permits):  Add rules to ALB for DDoS protection (OPTIONAL: ) - enhances security posture  Implement queue between web app and Lambda (OPTIONAL: ) - improves decoupling and reliability  Add Redis for session caching (OPTIONAL: ) - reduces database load Expected output: Complete CDK Python application that synthesizes templates for a production-ready web application infrastructure with all mandatory components properly configured and integrated.","A growing fintech startup needs to deploy their transaction processing web application with strict compliance requirements. The application handles sensitive financial data and must maintain audit trails while serving 10,000+ concurrent users during peak hours.","""Production multi-AZ deployment in us-east-1 region hosting financial transaction processing application. Infrastructure includes ECS Fargate cluster with ALB, RDS Aurora PostgreSQL Multi-AZ, for session management, S3 for static assets with CDN, Lambda functions for async processing. VPC spans 3 availability zones with public subnets for ALB/NAT Gateways and private subnets for compute/database resources. Requires AWS CDK 2.x with Python 3.9+, Docker installed for container builds. AWS account must have service limits increased for ECS tasks and RDS instances.""","[""RDS instances must use encrypted storage with customer-managed keys"", ""ALB must enforce TLS 1.2 minimum and use AWS-managed SSL certificates"", ""All Lambda functions must have reserved concurrent executions set"", "" distribution must use origin access identity for S3 bucket access"", ""ECS tasks must use Fargate Spot instances for at least 50% of capacity"", ""All IAM roles must follow least-privilege principle with no inline policies"", "" tables must have point-in-time recovery enabled"", ""S3 buckets must have versioning enabled and lifecycle policies configured"", ""All resources must be tagged with Environment, Team, and CostCenter tags""]"
a0h1n8,done,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to implement a serverless transaction validation pipeline. MANDATORY REQUIREMENTS (Must complete): 1. Deploy three Lambda functions (512MB memory each) for data ingestion, validation, and enrichment stages (CORE: Lambda). 2. Configure DynamoDB table with on-demand billing for storing transaction states with GSI for status queries (CORE: DynamoDB). 3. Implement state machine to orchestrate the three-stage pipeline with error handling (CORE: ). 4. Set up queues between each processing stage with visibility timeout of 300 seconds. 5. Configure Lambda Dead Letter Queues with maxReceiveCount of 3 for all functions. 6. Enable tracing across all Lambda functions and . 7. Implement CloudWatch Logs with 14-day retention for all services. 8. Create custom CloudWatch metrics for transaction processing rates and error counts. 9. Deploy all resources with deletion protection disabled for testing environments. OPTIONAL ENHANCEMENTS (If time permits):  Add rule to trigger pipeline on S3 uploads (OPTIONAL: ) - enables event-driven processing  Implement API Gateway REST endpoint for manual transaction submission (OPTIONAL: API Gateway) - provides external integration  Add SNS topic for failure notifications (OPTIONAL: SNS) - improves operational alerting. Expected output: A complete CDK Python application that deploys a production-ready serverless transaction processing pipeline with proper error handling, monitoring, and orchestration capabilities.",A financial analytics company needs to process millions of transaction records daily through a multi-stage validation pipeline. The system must handle variable load patterns with strict data consistency requirements while maintaining cost efficiency through serverless architecture.,"""AWS multi-AZ deployment in us-east-1 region using Lambda functions for compute, DynamoDB for state management, for orchestration, and for decoupling. Infrastructure requires CDK 2.x with Python 3.9+, AWS CLI configured with appropriate permissions. for DynamoDB and S3 to reduce data transfer costs. Lambda functions deployed across multiple AZs for high availability. CloudWatch Logs and for comprehensive observability. No NAT Gateway required as all services are AWS-managed.""","[""Lambda functions must use Python 3.9 runtime with boto3 pre-installed"", ""DynamoDB table must have point-in-time recovery enabled"", "" state machine must implement exponential backoff for retries"", ""All IAM roles must follow least-privilege principle with no wildcard actions"", "" queues must enable server-side encryption using AWS managed keys"", ""Lambda environment variables must be encrypted at rest"", "" sampling rate must be set to 10% for cost optimization"", ""CloudWatch Log Groups must use /aws/lambda/ prefix naming convention"", ""Stack must include CloudFormation outputs for all queue URLs and function ARNs""]"
g6s0n1,done,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy a multi-stage CI/CD pipeline for containerized applications. The configuration must: 1. Set up with source, build, test, and deploy stages across three AWS accounts (dev, staging, prod). 2. Configure projects for each stage using ECR-hosted build images with compute type BUILD_GENERAL1_SMALL. 3. Implement manual approval actions before staging and production deployments with SNS notifications. 4. Create S3 buckets for pipeline artifacts with versioning, encryption, and 90-day lifecycle rules. 5. Deploy ECS Fargate services using with blue/green deployment configuration. 6. Set up cross-account IAM roles for pipeline execution with explicit deny for ec2:TerminateInstances. 7. Configure CloudWatch event rules to trigger SNS alerts only on pipeline failures. 8. Implement caching using S3 with 7-day expiration for dependency management. 9. Create entries for Docker registry credentials with SecureString type. 10. Add CloudWatch dashboards showing pipeline execution metrics and success rates. Expected output: A complete CDK Python application that deploys a production-ready CI/CD pipeline with proper security controls, cross-account deployments, and monitoring. The stack should output the pipeline ARN, artifact bucket name, and SNS topic ARN for notifications.",A fintech startup needs to implement automated deployment pipelines for their microservices architecture. Each service requires isolated testing environments and strict security controls before production deployment. The team wants Infrastructure as Code to manage their CI/CD pipelines alongside their application infrastructure.,"""AWS multi-account setup in us-east-1 region for CI/CD infrastructure. Uses for orchestration, for build/test stages, ECR for Docker image storage, and for ECS Fargate deployments. Requires CDK 2.x with Python 3.9+, AWS CLI v2 configured with appropriate permissions. for S3, ECR, and to keep traffic private. Separate AWS accounts for dev, staging, and production environments with cross-account IAM roles for deployment. CloudWatch Logs for centralized logging with metric filters for error detection.""","[""All projects must use custom Docker images stored in ECR"", ""Pipeline artifacts must be encrypted with customer-managed KMS keys"", ""Each pipeline stage must have separate IAM roles with least-privilege permissions"", ""All build logs must be retained in CloudWatch for exactly 30 days"", ""Pipeline notifications must use SNS with email subscriptions for failures only"", "" must use S3 versioning for artifact storage with lifecycle policies""]"
y0b2v6,in_progress,CDK,Python,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CDK Python program to implement a multi-region disaster recovery solution. The configuration must: 1. Set up Aurora Global Database cluster with writer in us-east-1 and reader in us-west-2. 2. Configure DynamoDB global tables for transaction metadata with PITR enabled. 3. Deploy identical Lambda functions in both regions for processing logic. 4. Create S3 buckets with cross-region replication for document storage. 5. Implement AWS Backup plans with 1-hour RPO and cross-region copy. 6. Configure Route 53 health checks and weighted routing policies. 7. Set up EventBridge rules to monitor backup jobs and send alerts. 8. Create customer-managed keys in both regions with key policies. 9. Implement least-privilege IAM roles for all services. 10. Add CloudWatch dashboards for monitoring replication lag. 11. Configure deletion protection on all production resources. 12. Tag all resources with Environment=Production and DR-Role tags. Expected output: CDK Python stacks that deploy a complete disaster recovery infrastructure with automated failover capabilities, meeting 1-hour RPO and 4-hour RTO requirements while maintaining data encryption and compliance.",A financial services company needs automated backup and disaster recovery for their critical transaction processing system. The system must maintain RPO of 1 hour and RTO of 4 hours across regions. All backups must be encrypted and comply with financial data retention policies.,"""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary). Core services include Aurora PostgreSQL 14.x Global Database, DynamoDB global tables, Lambda functions for transaction processing, and S3 with cross-region replication. VPC peering connects regions with private connectivity. AWS Backup manages automated snapshots. Route 53 provides DNS failover. Requires CDK 2.100+ with Python 3.9+, AWS CLI v2 configured with appropriate IAM permissions for multi-region deployments.""","[""Use AWS Backup for centralized backup management with cross-region replication"", ""Implement Aurora Global Database with automated failover capabilities"", ""Deploy Lambda functions in both primary and secondary regions with identical configurations"", ""Use DynamoDB global tables with point-in-time recovery enabled"", ""Configure Route 53 health checks with automated DNS failover"", ""All keys must use customer-managed keys with automatic rotation"", ""Implement EventBridge rules to monitor backup completion and alert on failures""]"
y2j4u0,error,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to optimize an existing payment processing infrastructure by refactoring inefficient resource allocations. The configuration must: 1. Analyze and resize Lambda functions from 3008MB to appropriate memory based on CloudWatch metrics (target: 512MB-1024MB). 2. Convert DynamoDB tables from provisioned to on-demand billing mode. 3. Consolidate duplicate API Gateway REST APIs into a single deployment. 4. Implement Lambda reserved concurrency limits to prevent throttling spikes. 5. Add lifecycle policies to S3 buckets to transition logs to Glacier after 30 days. 6. Configure CloudWatch Log Groups with 7-day retention instead of never-expire. 7. Replace NAT Gateways with NAT Instances for development environment. 8. Implement automatic scaling policies for ECS services based on CPU/memory metrics. 9. Add CloudWatch dashboards to monitor cost optimization metrics. 10. Generate cost comparison report showing before/after monthly estimates. Expected output: Optimized CDK Python code that reduces infrastructure costs by 40% while maintaining performance SLAs, with automated cost tracking and monitoring dashboards.","A fintech startup's payment processing infrastructure was hastily built during rapid growth, resulting in excessive costs and performance bottlenecks. The existing CDK code deploys redundant resources, uses oversized compute instances, and lacks proper resource tagging for cost allocation.","""Production payment processing infrastructure in us-east-1 region across 2 availability zones. Current setup includes oversized Lambda functions (3008MB memory), underutilized DynamoDB tables with provisioned capacity, and redundant API Gateway deployments. Environment requires Python 3.9+, AWS CDK 2.100.0+, and boto3. VPC spans 10.0.0.0/16 with public/private subnets. AWS Organizations with consolidated billing enabled. Cost Explorer API access required for validation.""","[""Must maintain zero-downtime during the optimization deployment"", ""Total monthly AWS costs must be reduced by at least 40%"", ""All Lambda functions must use ARM-based Graviton2 processors"", ""Resource naming must follow pattern: {env}-{service}-{resource-type}-{identifier}"", ""Must implement cost allocation tags: Environment, Team, CostCenter, Project""]"
n8z4j1,done,CDK,Python,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CDK Python program to build an automated infrastructure compliance auditing system. MANDATORY REQUIREMENTS (Must complete): 1. Deploy AWS Config with custom rules to monitor S3 bucket encryption, RDS instance encryption, and EC2 instance metadata service v2 enforcement (CORE: AWS Config). 2. Create Lambda function to aggregate non-compliant resources and generate JSON compliance reports (CORE: Lambda). 3. Store compliance reports in S3 bucket with versioning enabled and lifecycle policies. 4. Implement IAM roles with least privilege access for Config and Lambda. 5. Configure Config delivery channel with topic for real-time alerts. 6. Set up CloudWatch Logs for Lambda with 30-day retention. 7. Tag all resources with Environment=audit and CostCenter=compliance. OPTIONAL ENHANCEMENTS (If time permits):  Add EventBridge rule to trigger weekly compliance summaries (OPTIONAL: EventBridge) - enables scheduled reporting.  Implement workflow for multi-account scanning (OPTIONAL: ) - scales to organization level.  Add Parameter Store for dynamic rule configurations (OPTIONAL: ) - simplifies rule updates. Expected output: CDK application that deploys a fully functional compliance auditing system capable of continuous monitoring and automated report generation.",A financial services company needs automated infrastructure compliance scanning to meet regulatory requirements. The system must continuously monitor resource configurations and generate audit reports for quarterly compliance reviews.,"""Production compliance infrastructure deployed in us-east-1 using AWS Config for continuous configuration monitoring, Lambda for report processing, and S3 for audit trail storage. Requires Python 3.9+, AWS CDK 2.x, boto3 installed. Single-region deployment with cross-account assume role capabilities. Config rules evaluate resources every 24 hours with change-triggered evaluations. Lambda functions process compliance data into structured JSON reports stored in versioned S3 buckets with 7-year retention for regulatory compliance.""","[""All Config rules must use AWS managed rules where available"", ""Lambda function timeout must not exceed 5 minutes"", ""S3 buckets must have server-side encryption with AWS managed keys"", ""Config recording must exclude CloudWatch Logs to avoid circular dependencies"", "" topic must have a dead letter queue configured"", ""All resources must have deletion protection disabled for testing"", ""Lambda function must use Python 3.9 runtime with arm64 architecture""]"
m4v2h2,in_progress,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy an EKS cluster optimized for ML workloads. The configuration must: 1. Create an EKS cluster with private API endpoint access only and enable control plane logging for all log types. 2. Deploy three distinct node groups: one with g4dn.xlarge GPU instances (min 1, max 3), one with mixed spot instances (t3a.large, t3.large, t2.large) for batch processing (min 2, max 10), and one with t3.medium on-demand instances for system workloads (min 2, max 5). 3. Configure OIDC provider and create at least two IRSA roles: one for cluster autoscaler and one for AWS Load Balancer Controller. 4. Install and configure EKS add-ons programmatically: VPC CNI v1.15+, CoreDNS v1.10+, kube-proxy v1.28+, and EBS CSI driver v1.20+. 5. Create custom launch templates for each node group with specific user data to install monitoring agents and configure container runtime settings. 6. Implement node group security rules that only allow ports 443, 10250, and 53 between nodes, with stricter rules for GPU nodes. 7. Set up CloudWatch Container Insights for the cluster with enhanced monitoring metrics enabled. 8. Configure Kubernetes RBAC with aws-auth ConfigMap to map specific IAM roles to Kubernetes groups. 9. Enable cluster autoscaling with proper tags on node groups and configure scaling policies based on CPU and GPU utilization. 10. Create dedicated IAM roles for each node group with minimal required permissions following principle of least privilege. Expected output: A complete CDK Python application that deploys a production-ready EKS cluster with GPU support, spot instance integration, and enhanced security controls. The deployment should create all necessary IAM roles, security groups, and configurations for immediate workload deployment.","A financial services company needs to deploy a containerized fraud detection system on AWS EKS. The system requires GPU nodes for ML model inference, spot instances for batch processing, and strict network isolation between different processing tiers.","""Production EKS infrastructure in us-east-2 region with multi-AZ deployment across 3 availability zones. Requires AWS CDK 2.100+ with Python 3.9+, kubectl 1.28+, and AWS CLI v2 configured with appropriate permissions. VPC with private subnets only, no internet gateway, using VPC endpoints for ECR, S3, and EC2. Cluster will host fraud detection workloads requiring GPU acceleration and high-throughput batch processing. Total of 3 node groups: on-demand GPU nodes, spot instances for batch, and on-demand general compute nodes.""","[""EKS cluster must use version 1.28 or higher with both managed and self-managed node groups"", ""GPU node group must use g4dn.xlarge instances with NVIDIA device plugin pre-installed"", ""Spot instance node group must have at least 3 instance types for better availability"", ""All node groups must use custom launch templates with specific user data scripts"", ""Cluster must have OIDC provider configured for IRSA (IAM Roles for Service Accounts)"", ""VPC CNI, CoreDNS, and kube-proxy add-ons must be explicitly managed with specific versions"", ""Private endpoint access only - no public API endpoint exposure"", ""Node security groups must restrict inter-node communication to specific ports only""]"
z5v0e3,ERROR,CDK,Python,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CDK Python program to implement multi-region disaster recovery for a PostgreSQL database. The configuration must: 1. Deploy RDS PostgreSQL primary instance in us-east-1 with Multi-AZ enabled. 2. Create RDS read replica in eu-west-1 configured for promotion to primary. 3. Implement Route53 health checks monitoring primary database endpoint availability. 4. Configure weighted routing policy (100% primary, 0% secondary initially). 5. Deploy Lambda function to automate failover by promoting replica and updating Route53 weights. 6. Set backup retention to 7 days on both instances. 7. Enable encryption at rest using AWS managed keys. 8. Configure parameter groups with log_statement='all' for audit compliance. 9. Create CloudWatch alarms for replication lag exceeding 60 seconds. Expected output: CDK stack that provisions a PostgreSQL database with automated cross-region failover capability, ensuring minimal downtime during regional outages.",A financial services company requires a disaster recovery solution for their critical transaction database. The system must maintain a hot standby in a secondary region with automated failover capabilities to meet strict RTO requirements of under 5 minutes.,"""Multi-region AWS deployment spanning us-east-1 (primary) and eu-west-1 (DR). Requires CDK 2.x with Python 3.8+, AWS CLI configured with appropriate permissions. Infrastructure includes RDS PostgreSQL 15.x instances in private subnets, Route53 private hosted zone, Lambda functions for failover automation. VPCs in both regions with peering connection established. Deployment requires cross-region replication permissions and Route53 health check configuration.""","[""RDS instances must use db.r6g.large instance class minimum"", ""Database passwords must be stored in Secrets Manager"", ""Lambda failover function must complete execution within 300 seconds"", ""Route53 health checks must use HTTPS protocol on port 5432"", ""All resources must have deletion_protection=False for testing"", ""Read replica must have automated backups enabled independently"", ""Parameter groups must disable force_ssl for legacy application compatibility"", ""CloudWatch logs must use /aws/rds/ prefix for all log groups"", ""Stack must output both regional endpoints and the Route53 CNAME""]"
a7h2b8,in_progress,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy a container orchestration platform for microservices. The configuration must: 1. Create an ECS cluster with capacity providers for both Fargate and Fargate Spot 2. Deploy three microservices (API, Worker, Analytics) each with 2GB memory and 1 vCPU 3. Configure Application Load Balancer with path-based routing (/api/*, /analytics/*) 4. Implement AWS App Mesh with virtual nodes and virtual services for each microservice 5. Set up auto-scaling policies: scale out at 70% CPU, scale in at 30% CPU, min 2 max 10 tasks 6. Create SQS queues for asynchronous processing with visibility timeout of 300 seconds 7. Configure CloudWatch Container Insights for cluster-level monitoring 8. Implement blue-green deployment capability using ECS deployment circuit breaker 9. Set up ECR repositories with lifecycle policies to retain only last 10 images 10. Configure IAM roles with least privilege access for each service Expected output: A complete CDK application that deploys a production-ready container platform with three microservices, supporting both synchronous and asynchronous workloads, with built-in resilience and observability features.","A financial services company needs to deploy their microservices architecture on AWS using containers. They require a solution that automatically scales based on load, handles blue-green deployments, and maintains strict network isolation between services. The infrastructure must support both synchronous REST APIs and asynchronous message processing workloads.","""Production-grade container orchestration platform deployed in us-east-1 across 3 availability zones. Architecture uses ECS Fargate for container hosting, Application Load Balancer for traffic distribution, and App Mesh for service-to-service communication. VPC configured with public subnets for ALB and private subnets for ECS tasks. NAT Gateways provide outbound internet access. Requires AWS CDK 2.x with Python 3.9+, Docker installed for local testing. ECR repositories for container images with vulnerability scanning enabled.""","[""Each microservice must run in its own ECS service with dedicated task definition"", ""Use Fargate Spot for non-critical background workers to reduce costs by 70%"", ""Implement service mesh using AWS App Mesh for inter-service communication"", ""Deploy services across exactly 3 availability zones for high availability"", ""Configure auto-scaling based on both CPU and custom CloudWatch metrics"", ""Use separate ALB target groups for each service with health check intervals of 10 seconds"", ""Implement circuit breaker pattern with 5 consecutive failures triggering service isolation"", ""Tag all resources with Environment, Service, and CostCenter tags for billing allocation""]"
t8d7m0,done,CDK,Python,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a CDK Python program to orchestrate a phased migration from on-premises infrastructure to AWS cloud. MANDATORY REQUIREMENTS (Must complete): 1. Create DMS replication instance with multi-AZ deployment for database migration (CORE: DMS) 2. Set up CloudEndure replication servers with appropriate IAM roles (CORE: CloudEndure) 3. Configure Site-to-Site VPN with customer gateway for hybrid connectivity 4. Implement Route 53 private hosted zone for gradual DNS cutover 5. Create migration tracking table in for status monitoring 6. Set up topic for migration status notifications 7. Configure Systems Manager documents for post-migration validation 8. Implement automated rollback mechanism using Lambda functions 9. Create CloudWatch dashboard for migration metrics visualization OPTIONAL ENHANCEMENTS (If time permits):  Add Application Migration Service for additional server migration options (OPTIONAL: MGN) - provides block-level replication  Implement DataSync tasks for incremental file transfers (OPTIONAL: DataSync) - reduces migration window  Configure Migration Hub for centralized progress tracking (OPTIONAL: Migration Hub) - improves visibility Expected output: A CDK Python application that deploys all migration infrastructure components with proper IAM roles, networking configuration, and monitoring. The stack should support phased migration with rollback capabilities and provide real-time migration status through CloudWatch and notifications.","A financial services company needs to migrate their legacy monolithic application from on-premises to AWS. The application consists of a web tier, API services, and a PostgreSQL database. They require a phased migration approach with minimal downtime and the ability to roll back if issues arise.","""Hybrid cloud environment spanning on-premises datacenter in Virginia and AWS us-east-1 region. Requires CDK 2.x with Python 3.8+, AWS CLI configured with appropriate credentials. Target architecture includes VPC with 10.0.0.0/16 CIDR, spanning 3 availability zones with public and private subnets. On-premises network uses 192.168.0.0/16 addressing. Migration involves PostgreSQL 13 database (500GB), 12 instances for application servers, and 2TB of file storage. Requires Site-to-Site VPN with BGP routing and Direct Connect virtual interface for redundant connectivity. Migration Hub dashboard for tracking progress across all migration tools.""","[""Use AWS Database Migration Service (DMS) for continuous data replication"", ""Implement blue-green deployment strategy with Route 53 weighted routing"", ""Configure VPN connection between on-premises network and AWS VPC"", ""Set up CloudEndure for server replication during migration phase"", ""Use AWS Application Migration Service for instance migration"", ""Implement AWS DataSync for file system migration to "", ""Configure AWS Direct Connect as backup to VPN for hybrid connectivity"", ""Set up AWS Migration Hub to track migration progress"", ""Use Systems Manager for post-migration configuration management""]"
y4c5t4,in_progress,CDK,Python,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a CDK Python program to deploy a multi-account transit gateway network architecture.

MANDATORY REQUIREMENTS (Must complete):
1. Deploy a Transit Gateway in the network hub account with DNS support enabled (CORE: Transit Gateway)
2. Create three VPCs across different accounts: production (10.0.0.0/16), development (10.1.0.0/16), and shared services (10.2.0.0/16) (CORE: VPC)
3. Configure Route53 Resolver endpoints in the shared services VPC for centralized DNS (CORE: Route53 Resolver)
4. Implement transit gateway route tables with isolation between production and development
5. Set up VPC attachments with association and propagation rules
6. Configure security group rules allowing only necessary inter-VPC communication
7. Enable VPC Flow Logs to S3 buckets with 30-day lifecycle policies
8. Tag all resources with Environment, CostCenter, and ManagedBy tags

OPTIONAL ENHANCEMENTS (If time permits):
 Add AWS Network Firewall in shared services VPC (OPTIONAL: Network Firewall) - provides centralized security inspection
 Implement Direct Connect virtual interfaces (OPTIONAL: Direct Connect) - enables hybrid connectivity
 Add RAM sharing for cross-account resource access (OPTIONAL: Resource Access Manager) - simplifies multi-account management

Expected output: A CDK Python application that deploys a production-ready hub-and-spoke network with centralized DNS resolution and proper network segmentation across multiple AWS accounts.","A financial services company needs to establish a hub-and-spoke network architecture across multiple AWS accounts. They require centralized DNS resolution and network connectivity between isolated production, development, and shared services environments while maintaining strict network segmentation.","""Multi-account AWS environment deployed across us-east-1 region using Transit Gateway for hub-and-spoke networking, VPCs with private and public subnets, and Route53 Resolver for centralized DNS. Requires CDK 2.x with Python 3.9+, AWS CLI configured with cross-account assume role permissions. Architecture includes production VPC (10.0.0.0/16), development VPC (10.1.0.0/16), and shared services VPC (10.2.0.0/16) with transit gateway attachments. Network segmentation enforced through route tables and security groups.""","[""Transit Gateway must use custom route tables, not the default route table"", ""Production and development VPCs must not have direct routing between them"", ""All VPC subnets must be private with no internet gateway attachments"", ""Route53 Resolver endpoints must use at least 2 availability zones"", ""Security groups must follow least-privilege with explicit CIDR blocks"", ""Cross-account stack deployments must use CDK's cross-account capabilities"", ""VPC Flow Logs must capture ALL traffic, not just accepted or rejected"", ""All IAM roles must include external ID for cross-account trust relationships""]"
e4x2t2,done,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy a multi-environment trading analytics platform with consistent infrastructure across development, staging, and production AWS accounts. MANDATORY REQUIREMENTS (Must complete): 1. Create an ECS Fargate service running a containerized Flask API with auto-scaling (2-10 tasks) based on CPU utilization (CORE: ECS) 2. Deploy Aurora PostgreSQL Serverless v2 cluster with automated backups and point-in-time recovery (CORE: RDS Aurora) 3. Implement environment-specific configuration using CDK context with separate files for dev.json, staging.json, and prod.json 4. Create a custom CDK construct called TradingEnvironmentStack that accepts environment configuration as props 5. Configure cross-account deployment using IAM roles with trust relationships between accounts 6. Set up Lambda functions for data processing with environment-specific memory allocations (dev: 512MB, staging: 1GB, prod: 2GB) 7. Implement CloudWatch dashboards that aggregate metrics across all three environments in a single view 8. Use CDK pipelines to automate deployment with manual approval gates for staging and production 9. Configure for database credentials with automatic rotation every 30 days 10. Implement tagging strategy with Environment, Project, and CostCenter tags on all resources OPTIONAL ENHANCEMENTS (If time permits):  Add with usage plans for external API access (OPTIONAL: ) - enables third-party integrations  Implement queues for decoupling services (OPTIONAL: ) - improves system resilience  Add for complex workflows (OPTIONAL: ) - orchestrates multi-step processes Expected output: A complete CDK Python application with stack definitions, custom constructs, and deployment scripts that can provision identical infrastructure across three AWS accounts with environment-specific configurations maintained through context files.","A financial services company needs to maintain identical infrastructure across development, staging, and production environments for their trading analytics platform. The platform processes real-time market data and must ensure complete parity across environments to prevent production issues. All environments must be deployable from a single CDK codebase with environment-specific configurations.","""Multi-account AWS setup with separate accounts for dev (123456789012), staging (234567890123), and production (345678901234) in us-east-1 region. Each environment requires VPC with 3 private subnets across availability zones, NAT gateways for outbound traffic, and for S3 and . Infrastructure includes ECS Fargate clusters running containerized Python services, Aurora PostgreSQL Serverless v2 for data storage, and Lambda functions for async processing. CDK 2.x with Python 3.9+ required, AWS CLI profiles configured for each account with assume role permissions. Total infrastructure spans approximately 50 AWS resources per environment.""","[""Use CDK context variables to manage environment-specific configurations without hardcoding values"", ""Implement a custom construct that encapsulates common patterns across all environments"", ""Deploy to separate AWS accounts for each environment using cross-account assume role patterns"", ""All Lambda functions must use container images stored in ECR with environment-specific tags"", ""Implement automated drift detection using CDK diff in a project"", ""Database schemas must be versioned and applied consistently using AWS Lambda custom resources"", ""Use for runtime configuration with hierarchical naming""]"
j4p4f9,in_progress,CDK,Python,expert,Application Deployment,Web Application Deployment,Create a CDK Python program to deploy a containerized web application with blue-green deployment capabilities. MANDATORY REQUIREMENTS (Must complete): 1. Deploy ECS Fargate service with task definition using 2 vCPU and 4GB memory (CORE: ECS) 2. Create RDS Aurora PostgreSQL cluster with 2 instances across different AZs (CORE: RDS) 3. Configure Application Load Balancer with target group health checks every 30 seconds (CORE: ALB) 4. Implement weighted target groups for blue-green deployments (80/20 traffic split) 5. Set up VPC with 3 private subnets and 3 public subnets across 3 AZs 6. Create custom CloudWatch dashboard showing ECS task count and RDS connections 7. Configure automatic RDS snapshots with 7-day retention period 8. Output ALB DNS name and database endpoint for application configuration OPTIONAL ENHANCEMENTS (If time permits):  Add custom domain with health checks (OPTIONAL: ) - provides branded URL and DNS failover  Implement for database credentials (OPTIONAL: ) - improves credential security  Add CloudFront distribution for static assets (OPTIONAL: CloudFront) - reduces latency globally Expected output: Complete CDK Python stack that provisions production-ready web application infrastructure with blue-green deployment support and automated database backups.,"A financial services startup needs to deploy their transaction processing web application with strict compliance requirements. The application requires high availability, automated database failover, and granular access controls for their development team.","""Production-grade web application infrastructure in us-east-1 using ECS Fargate for containers, RDS Aurora PostgreSQL for persistent storage, and Application Load Balancer for traffic distribution. Requires AWS CDK 2.x with Python 3.9+, Docker installed for container builds. Multi-AZ VPC setup with 6 subnets (3 public, 3 private) spanning availability zones us-east-1a, us-east-1b, and us-east-1c. NAT Gateways in each AZ for high availability. Production AWS account with appropriate IAM permissions for ECS, RDS, EC2, and CloudWatch services.""","[""All resources must use removal_policy=RemovalPolicy.DESTROY for easy cleanup"", ""ECS tasks must use awslogs driver with log group retention of 3 days"", ""RDS cluster must have deletion_protection=False for testing environments"", ""Security groups must follow least privilege with explicit port definitions"", ""All IAM roles must avoid wildcard permissions except for CloudWatch Logs"", ""Stack must include CfnOutput for all connection endpoints"", ""Database subnet group must span exactly 3 availability zones"", ""ALB must use internet-facing scheme with IPv4 addressing only""]"
l9r9o5,done,CDK,Python,expert,Application Deployment,Web Application Deployment,Create a CDK Python program to build a serverless ETL pipeline for processing financial transaction files. MANDATORY REQUIREMENTS (Must complete): 1. Create S3 buckets for raw uploads and processed data with event notifications (CORE: S3) 2. Implement state machine with parallel processing branches (CORE: ) 3. Deploy Lambda function for file validation checking CSV/JSON schema compliance 4. Deploy Lambda function for data transformation applying business rules 5. Create DynamoDB table to track processing status with partition key file_id 6. Configure EventBridge to route S3 events to execution 7. Implement CloudWatch Logs with 30-day retention for all Lambda functions 8. Add CloudWatch alarms for Lambda errors exceeding 5% threshold OPTIONAL ENHANCEMENTS (If time permits):  Add SNS topic for failure notifications (OPTIONAL: SNS) - enables real-time alerting  Implement SQS queue for batch processing (OPTIONAL: SQS) - improves throughput for large files  Add AWS Glue crawler for schema discovery (OPTIONAL: Glue) - automates metadata management Expected output: Complete CDK Python application that deploys a production-ready serverless ETL pipeline capable of processing financial files within 15 minutes SLA.,"A financial analytics company needs to process daily transaction files uploaded by partner banks. The files arrive at unpredictable times throughout the day and must be validated, transformed, and stored for downstream analytics within 15 minutes of upload.","""Serverless ETL pipeline deployed in us-east-1 using Lambda for compute, for orchestration, DynamoDB for metadata tracking, S3 for file storage, and EventBridge for event routing. Requires CDK 2.x with Python 3.8+, AWS CLI configured with appropriate permissions. No VPC required as all services are managed. Architecture uses event-driven patterns with S3 event notifications triggering workflows. All resources tagged with Environment=Production and Project=ETL-Pipeline.""","[""Lambda functions must use Python 3.11 runtime with 3GB memory allocation"", ""DynamoDB tables must use on-demand billing mode with point-in-time recovery enabled"", ""All S3 buckets must have versioning enabled and lifecycle policies for 90-day archival"", "" state machine must implement exponential backoff retry logic with maximum 3 attempts"", ""EventBridge rules must filter for .csv and .json file extensions only"", ""Lambda functions must complete execution within 5 minutes timeout limit"", ""All IAM roles must follow least privilege principle with no wildcard resource permissions""]"
g6c9y0,error,CDK,Python,expert,Application Deployment,Web Application Deployment,"Create a CDK Python program to deploy a complete blue-green deployment pipeline for containerized applications. The configuration must: 1. Create a with source, build, test, approval, and deploy stages triggered by CodeCommit pushes to main branch. 2. Configure projects for Docker image building with buildspec.yml inline, using Amazon Linux 2 standard runtime. 3. Implement ECS Fargate services with blue and green target groups behind an Application Load Balancer. 4. Set up automated integration tests in that validate the green environment before traffic switch. 5. Create manual approval action with SNS topic for production deployment notifications. 6. Implement traffic shifting from blue to green using ALB target group weights (100% switch). 7. Configure CloudWatch alarms on ECS service metrics with automatic rollback on high error rates. 8. Store Docker images in ECR with vulnerability scanning enabled and retention policy of 10 images. 9. Ensure all IAM roles follow least privilege with no Admin or wildcard permissions. 10. Output the pipeline URL, ALB DNS name, and ECR repository URI for application teams. Expected output: A CDK Python application that deploys a production-ready blue-green pipeline with automated builds, testing, manual approvals, and zero-downtime deployments, complete with monitoring and rollback capabilities.","Your DevOps team needs to establish a blue-green deployment pipeline for a containerized microservices application. The pipeline must support automated testing, approval gates, and zero-downtime deployments while maintaining strict security boundaries between build and deployment stages.","""Blue-green deployment infrastructure in us-east-1 using for orchestration, for container builds, ECS Fargate for compute, and Application Load Balancer for traffic management. Requires CDK 2.x with Python 3.8+, Docker installed locally. VPC with 2 public and 4 private subnets across 2 AZs. CodeCommit repository pre-configured with main and develop branches. contains database connection strings and API keys.""","[""Use CodeCommit as the source repository with branch-based triggers"", "" projects must use separate IAM roles for build and integration test stages"", ""Store all sensitive parameters in with KMS encryption"", ""ECS services must use Fargate launch type with task definitions versioned in the pipeline"", ""Application Load Balancer must perform health checks on /health endpoint before traffic switching"", ""Manual approval stage required before production deployment with SNS notifications"", ""Pipeline artifacts must be stored in S3 with server-side encryption and lifecycle policies"", ""CloudWatch Events must trigger rollback if error rate exceeds 5% within 5 minutes post-deployment"", ""All resources must be tagged with Environment, Pipeline, and CostCenter tags""]"
v6w1q4,done,CDK,Python,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CDK Python program to deploy a highly available transaction processing system with automated cross-region failover capabilities. The configuration must: 1. Deploy Aurora PostgreSQL Global Database with one primary cluster in us-east-1 and secondary in us-west-2, with automated promotion capability. 2. Create ECS services running transaction processors across 3 AZs in each region with target tracking auto-scaling. 3. Configure Route 53 hosted zone with health checks and weighted routing policies for automatic regional failover. 4. Set up DynamoDB global tables to replicate user session data between regions with point-in-time recovery enabled. 5. Implement S3 buckets with cross-region replication and RTC for transaction logs and audit trails. 6. Deploy EventBridge rules in both regions with cross-region event replication for order processing workflows. 7. Create Lambda functions with circuit breaker extensions to handle downstream service failures gracefully. 8. Configure with automatic cross-region replication for database credentials and API keys. 9. Set up distribution with origin groups for static assets with automatic failover between regional S3 buckets. 10. Implement CloudWatch dashboards and alarms for multi-region monitoring with cross-region forwarding. 11. Configure AWS Backup for automated cross-region backups of Aurora and DynamoDB. 12. Deploy all resources with consistent tagging for cost allocation and enable deletion protection on critical resources. Expected output: A CDK Python application that deploys a fully automated multi-region infrastructure capable of surviving complete regional failures with sub-5-minute RTO, including health monitoring, automatic failover, and data consistency guarantees.",A financial services company needs to ensure their critical transaction processing system can withstand AZ failures without data loss. The system currently experiences 15-minute outages during AZ failures due to manual failover procedures. They require automated recovery with RPO < 1 minute and RTO < 5 minutes.,"""Multi-region deployment spanning us-east-1 (primary) and us-west-2 (DR) with Aurora Global Database cluster, ECS services distributed across 3 AZs per region, DynamoDB global tables for session management, EventBridge for event-driven workflows. Requires CDK 2.x with Python 3.9+, boto3, AWS CLI configured with cross-region permissions. VPC peering between regions with for secure communication. Each region has public/private subnets across 3 AZs with NAT instances for cost optimization.""","[""Use Aurora PostgreSQL with Global Database for cross-region replication"", ""Implement Route 53 health checks with automatic DNS failover"", ""Deploy ECS services across minimum 3 AZs with auto-scaling"", ""Configure DynamoDB global tables for session state replication"", ""Set up cross-region S3 replication with RTC (Replication Time Control)"", ""Use EventBridge with cross-region event replication for async workflows"", ""Implement Circuit Breaker pattern using Lambda extensions"", ""Configure multi-region replication for credentials"", ""Deploy with multiple origin failover groups""]"
z0n4e9,done,CDK,Python,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CDK Python program to implement a zero-trust data processing pipeline with end-to-end encryption. The configuration must: 1. Deploy Lambda functions in isolated private subnets with no internet access. 2. Create customer-managed KMS keys with automatic rotation every 90 days. 3. Configure S3 buckets with bucket encryption using separate KMS keys per environment. 4. Implement Secrets Manager for storing API credentials with automatic rotation. 5. Set up VPC endpoints for S3, Lambda, KMS, and Secrets Manager services. 6. Create security groups allowing only HTTPS traffic between components. 7. Enable CloudWatch Logs encryption with dedicated KMS key. 8. Configure IAM roles with explicit deny statements for non-encrypted operations. 9. Implement resource tagging for compliance tracking (Environment, DataClassification, Owner). 10. Enable S3 bucket versioning and MFA delete protection. Expected output: A CDK Python application that deploys a fully encrypted data pipeline where Lambda functions process sensitive data from S3, using Secrets Manager for credentials, with all operations logged and encrypted.","Your organization requires a zero-trust security architecture for its data processing pipeline. The security team has mandated encryption at every layer, strict network isolation, and comprehensive audit logging to meet SOC 2 compliance requirements.","""Zero-trust security deployment in us-east-1 with Lambda functions for data processing, S3 for encrypted storage, and Secrets Manager for credential management. Requires CDK 2.x with Python 3.9+, AWS CLI configured with appropriate permissions. VPC with private subnets only, no internet gateway. All traffic flows through VPC endpoints for S3, Lambda, KMS, and Secrets Manager. Multi-account setup with separate security audit account for CloudTrail logs.""","[""All data must be encrypted at rest using customer-managed KMS keys with automatic rotation"", ""Lambda functions must use VPC endpoints to access AWS services without internet exposure"", ""Each environment (dev/staging/prod) must have isolated KMS keys and cannot share encryption contexts"", ""All API calls must be logged to CloudWatch with 90-day retention and encrypted log groups"", ""Security groups must follow least-privilege with no 0.0.0.0/0 inbound rules""]"
m8i4x9,done,CloudFormation,JSON,expert,Failure Recovery and High Availability,Failure Recovery Automation,Create a CloudFormation template to implement a multi-region disaster recovery solution for a payment processing system. MANDATORY REQUIREMENTS (Must complete): 1. Create Lambda functions in both regions with identical code but region-specific environment variables (CORE: Lambda) 2. Set up DynamoDB global tables with on-demand billing and point-in-time recovery (CORE: DynamoDB) 3. Configure S3 buckets in both regions with versioning and cross-region replication 4. Implement Route 53 hosted zone with weighted routing policy and health checks 5. Deploy secrets with cross-region replication for API keys 6. Create CloudWatch alarms monitoring Lambda errors and DynamoDB throttling 7. Configure SNS topics for failover notifications with email subscriptions 8. Set Lambda reserved concurrent executions to 100 per function 9. Export critical resource ARNs as stack outputs for cross-stack references OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Backup for automated DynamoDB table backups (OPTIONAL: AWS Backup) - provides additional data protection  Implement EventBridge rules for automated failover triggers (OPTIONAL: EventBridge) - enables event-driven DR workflows  Add CloudWatch Synthetics canaries for endpoint monitoring (OPTIONAL: CloudWatch Synthetics) - improves health check accuracy Expected output: A CloudFormation template in JSON format that deploys the complete disaster recovery infrastructure. The template should use parameters for region-specific configurations and include conditions for primary vs secondary region deployments. All resources must be tagged with Environment and Region tags.,"A financial services company requires a disaster recovery solution for their critical payment processing application. The primary region hosts the production workload, while a secondary region must maintain a warm standby configuration that can be activated within minutes during an outage. The solution must replicate data continuously and allow for automated failover testing.","""Multi-region disaster recovery deployment spanning us-east-1 (primary) and us-west-2 (secondary). Architecture includes Lambda functions for payment processing, DynamoDB global tables for transaction data, S3 buckets with cross-region replication for audit logs. Route 53 manages DNS failover between regions with health checks monitoring application endpoints. CloudFormation stacks deployed independently in each region with parameter-based configuration. VPCs not required as all services are managed. Requires AWS CLI configured with appropriate IAM permissions for multi-region deployments.""","[""Use AWS Lambda functions with environment-specific configurations in both regions"", ""Implement cross-region replication for S3 buckets storing transaction logs"", ""Configure Route 53 health checks with automatic DNS failover"", ""Set up DynamoDB global tables with point-in-time recovery enabled"", ""Use with automatic cross-region replication"", ""Implement CloudWatch alarms with SNS notifications for failover events"", ""Ensure all Lambda functions use Python 3.11 runtime"", ""Configure Lambda reserved concurrent executions to prevent throttling"", ""Use stack exports for cross-stack references within each region""]"
d0l3x7,done,CloudFormation,JSON,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CloudFormation template to deploy a highly available payment processing infrastructure with automated failover capabilities. MANDATORY REQUIREMENTS (Must complete): 1. Deploy Aurora MySQL cluster with one writer and two reader instances across 3 AZs (CORE: Aurora) 2. Configure Auto Scaling Group with minimum 6 instances (2 per AZ) behind Application Load Balancer (CORE: Auto Scaling) 3. Implement Route 53 health checks with automatic DNS failover to secondary region endpoint 4. Set up S3 bucket with versioning enabled and cross-region replication to us-west-2 5. Configure CloudWatch alarms for failover events with email notifications 6. Create backup retention policy of 7 days for Aurora cluster with point-in-time recovery 7. Implement least-privilege IAM roles for instances to access S3 and Aurora 8. Enable deletion protection on production resources with DeletionPolicy: Snapshot OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Backup for centralized backup management (OPTIONAL: AWS Backup) - simplifies compliance reporting  Implement Lambda function for automated recovery testing (OPTIONAL: Lambda) - validates DR procedures  Add distribution for static assets (OPTIONAL: ) - improves global performance Expected output: A single CloudFormation JSON template that creates all resources with proper dependencies, implements multi-AZ high availability, automated failover mechanisms, and disaster recovery capabilities. The infrastructure should automatically recover from AZ failures without manual intervention.",A financial services company needs to ensure their payment processing API remains available during zone failures. They require automated failover capabilities with minimal data loss and rapid recovery time objectives (RTO < 5 minutes).,"""Multi-AZ highly available infrastructure deployed in us-east-1 region spanning 3 availability zones (us-east-1a, us-east-1b, us-east-1c). Core services include Aurora MySQL cluster with automated failover, Auto Scaling Groups behind Application Load Balancer, and Route 53 for DNS failover. VPC configured with public and private subnets in each AZ, NAT Gateways for outbound traffic from private subnets. S3 buckets configured for cross-region replication to us-west-2 for disaster recovery. CloudWatch monitoring with alerting for all critical events. Requires AWS CLI configured with appropriate permissions.""","["" Aurora must use MySQL 8.0 with automated backups every 12 hours"", ""Application Load Balancer health checks must use HTTP GET on /health endpoint"", ""Auto Scaling Group must maintain exactly 2 instances per AZ during normal operations"", ""Route 53 health checks must failover within 30 seconds of detection"", ""All S3 buckets must use cross-region replication to us-west-2"", ""CloudWatch alarms must trigger notifications for database failover events"", ""DeletionPolicy must be set to Snapshot for all stateful resources""]"
p5r2e5,done,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy AWS WAF security controls for API protection.

MANDATORY REQUIREMENTS (Must complete):
1. Create a WAFv2 Web ACL with CloudWatch metrics enabled (CORE: WAF)
2. Configure rate-based rule limiting 2000 requests per 5-minute window per IP
3. Add AWS Managed Rule Group for SQL injection protection (AWSManagedRulesSQLiRuleSet)
4. Create geo-blocking rule to deny traffic from North Korea (KP) and Iran (IR)
5. Set up S3 bucket for WAF logs with AES256 encryption (CORE: S3)
6. Configure WAF logging to the S3 bucket with proper resource policies
7. Create IP set for allowlisting office IPs (10.0.0.0/24 and 192.168.1.0/24)
8. Associate Web ACL with ALB using ARN parameter
9. Output Web ACL ARN and S3 bucket name for verification

OPTIONAL ENHANCEMENTS (If time permits):
 Add AWS Managed Rules for Known Bad Inputs (OPTIONAL: WAF RuleGroup) - blocks common attack patterns
 Implement custom rule for User-Agent filtering (OPTIONAL: WAF Custom Rule) - prevents bot traffic
 Configure Kinesis Firehose for real-time log streaming (OPTIONAL: Kinesis Firehose) - enables real-time analysis

Expected output: A CloudFormation JSON template that creates a production-ready WAF configuration with rate limiting, geo-blocking, SQL injection protection, and centralized logging to S3.",A fintech startup needs to protect their API endpoints from common web attacks and implement rate limiting to prevent abuse. The security team requires AWS WAF rules that can be easily audited and version-controlled through Infrastructure as Code.,"""Production security infrastructure deployed in us-east-1 region. Uses AWS WAFv2 for web application firewall protection, S3 for centralized logging with AES256 encryption, and integrates with existing Application Load Balancer. Requires AWS CLI configured with appropriate IAM permissions for WAF, S3, and ELB services. VPC already exists with ALB deployed across multiple availability zones serving HTTPS traffic on port 443.""","[""Use AWS WAFv2 (not WAF Classic) for all rules and configurations"", ""Rate limiting must allow 2000 requests per 5-minute window per IP address"", ""SQL injection protection must use AWS managed rule groups"", ""All WAF logs must be sent to a specific S3 bucket with encryption enabled"", ""Web ACL must be associated with an existing Application Load Balancer"", ""Custom rules must block requests from specific countries (North Korea, Iran)"", ""IP sets must be defined for allowlisting trusted office IPs"", ""All resources must have Cost Allocation tags with 'Environment' and 'Project' keys""]"
m1d2p4,done,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to optimize an existing financial transaction processing system that's experiencing deployment failures and timeouts.

MANDATORY REQUIREMENTS (Must complete):
1. Define RDS Aurora MySQL cluster with ServerlessV2 scaling (0.5-1.0 ACU) and proper UpdateReplacePolicy (CORE: RDS)
2. Create Lambda function for transaction processing with 3GB memory and ReservedConcurrentExecutions set to 100 (CORE: Lambda)
3. Add Parameters for EnvironmentName (dev/staging/prod), DBUsername, and VPCId with appropriate constraints
4. Use Conditions to enable/disable enhanced monitoring based on EnvironmentName (prod=true, others=false)
5. Configure explicit DependsOn between Lambda and RDS to prevent race conditions
6. Set DeletionPolicy to 'Retain' for RDS cluster and 'Delete' for Lambda
7. Create Outputs with Export names for RDS endpoint, Lambda ARN, and security group IDs
8. Use Fn::Sub for all IAM role names to include ${AWS::StackName} prefix

OPTIONAL ENHANCEMENTS (If time permits):
 Add CloudWatch dashboard with custom metrics (OPTIONAL: CloudWatch) - improves monitoring visibility
 Implement AWS Secrets Manager for database credentials (OPTIONAL: Secrets Manager) - enhances security
 Add SNS topic for deployment notifications (OPTIONAL: SNS) - enables team alerts

Expected output: Optimized CloudFormation JSON template that deploys in under 15 minutes with zero circular dependencies and proper update handling.","A financial services company discovered their CloudFormation templates are causing stack update failures and prolonged deployment times. The existing templates have circular dependencies, hardcoded values, and inefficient resource configurations that need immediate remediation to meet their 15-minute deployment window requirement.","""Production environment in us-east-1 requiring optimization of existing CloudFormation stacks. Current infrastructure includes RDS Aurora MySQL cluster, Lambda functions processing financial transactions, and DynamoDB tables for session management. Stack updates currently take 45+ minutes due to inefficient resource dependencies and missing update policies. VPC already exists with CIDR 10.0.0.0/16 across 3 AZs. Requires CloudFormation JSON format compatible with AWS CLI 2.x. Goal is to reduce deployment time to under 15 minutes while maintaining zero-downtime updates.""","[""All resource dependencies must be explicit using DependsOn or Ref/GetAtt without creating circular references"", ""Template must use Parameters for all environment-specific values with proper AllowedValues constraints"", ""UpdateReplacePolicy must be set to 'Retain' for stateful resources like RDS and DynamoDB"", ""All Lambda functions must use ReservedConcurrentExecutions to prevent runaway scaling"", ""Template must include Conditions to handle different deployment scenarios without duplication"", ""Output values must use Export names following pattern: ${AWS::StackName}-ResourceName""]"
j2j2m9,in_progress,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy an automated infrastructure compliance analyzer. The configuration must: 1. Define AWS Config Rules for bucket encryption (AES256 or KMS), instance encryption, and allowed EC2 instance types (t3.micro, t3.small only). 2. Create Lambda functions with Python 3.9 runtime that parse CloudFormation templates from and validate resources against Config Rules. 3. Set up DynamoDB table with on-demand billing, partition key 'accountId#timestamp', and sort key 'resourceId'. 4. Configure topic with email subscriptions for critical violations like unencrypted instances or publicly accessible buckets. 5. Implement state machine that orchestrates template fetching, parsing, validation, and report generation. 6. Create EventBridge rules that trigger scans on CloudFormation CREATE_COMPLETE and UPDATE_COMPLETE events. 7. Deploy CloudWatch dashboard displaying compliance metrics with custom widgets for pass/fail rates by service. 8. Configure bucket for compliance reports with versioning enabled and lifecycle rules for Glacier transition. 9. Set up cross-account IAM roles with sts:AssumeRole permissions and external ID for secure access. 10. Enable tracing on all Lambda functions and for performance monitoring. Expected output: A CloudFormation JSON template that deploys a complete compliance analysis system capable of scanning CloudFormation templates across multiple accounts, validating against Config Rules, and generating actionable compliance reports with automated notifications.","Your organization's security team has discovered multiple CloudFormation stacks deployed across different AWS accounts that don't meet corporate compliance standards. They need an automated solution to analyze existing CloudFormation templates, identify non-compliant resources, and generate detailed compliance reports. The solution must handle templates with complex resource dependencies and cross-stack references.","""Multi-account AWS environment deployed in us-east-1 region focusing on compliance analysis infrastructure. Core services include AWS Config for compliance rules, Lambda for template parsing logic, DynamoDB for storing scan results, for workflow orchestration, and EventBridge for event-driven triggers. Requires CloudFormation JSON templates, cross-account IAM roles with assume permissions, VPC with private subnets for Lambda execution, and for outbound API calls. The solution analyzes CloudFormation templates stored in buckets across multiple AWS accounts, validates them against predefined Config Rules, and generates compliance reports stored in a centralized DynamoDB table.""","[""Use AWS Config Rules to define compliance criteria for bucket encryption, encryption, and EC2 instance types"", ""Deploy Lambda functions that parse CloudFormation templates and validate against Config Rules"", ""Store compliance scan results in DynamoDB with partition keys based on account ID and scan timestamp"", ""Implement notifications for critical compliance violations (unencrypted databases, public buckets)"", ""Create IAM roles with cross-account assume permissions for scanning templates in multiple accounts"", ""Use to orchestrate the compliance scanning workflow across multiple templates"", ""Configure EventBridge rules to trigger scans when CloudFormation stack events occur"", ""Implement CloudWatch custom metrics for compliance score tracking (compliant resources / total resources)"", ""Set up lifecycle policies to archive compliance reports older than 90 days to Glacier"", ""Enable tracing on Lambda functions for performance analysis of template parsing""]"
u6j4j1,in_progress,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a production-ready Amazon EKS cluster for containerized microservices. The configuration must: 1. Create an EKS cluster with Kubernetes version 1.28. 2. Configure a managed node group with auto-scaling capabilities (min: 2, max: 10, desired: 3). 3. Enable all cluster logging types and send logs to CloudWatch. 4. Create an OIDC provider for the cluster to support IRSA. 5. Use a launch template that enforces IMDSv2 and blocks IMDSv1. 6. Deploy nodes across 3 availability zones using m5.large instances. 7. Tag all resources with Environment=Production and ManagedBy=CloudFormation. 8. Configure the cluster endpoint for public access with CIDR restrictions. 9. Create IAM roles with least-privilege policies for cluster and node groups. 10. Output the cluster endpoint, OIDC issuer URL, and node group ARN. Expected output: A complete CloudFormation JSON template that creates a production EKS cluster with managed node groups, proper security configurations, and outputs for integration with other services.",A financial services company needs to deploy a containerized microservices platform for their trading applications. They require a managed Kubernetes environment with strict security controls and automated node scaling to handle variable trading volumes throughout the day.,"""Production EKS cluster deployed in us-east-1 region using Amazon EKS 1.28 with managed node groups. Infrastructure includes VPC with 3 public and 3 private subnets across availability zones us-east-1a, us-east-1b, and us-east-1c. Requires AWS CLI 2.x configured with appropriate permissions, kubectl 1.28.x installed. The cluster will use AWS Load Balancer Controller for ingress and CoreDNS for service discovery. Node groups will use m5.large instances with gp3 EBS volumes.""","[""The EKS cluster must use managed node groups with automatic scaling between 2-10 nodes"", ""All nodes must use Amazon Linux 2 EKS-optimized AMIs with m5.large instances"", ""The cluster must span across 3 availability zones for high availability"", ""Enable OIDC provider for the cluster to support IAM roles for service accounts"", ""Configure cluster logging for all log types (api, audit, authenticator, controllerManager, scheduler)"", ""Use launch templates for node groups to enable IMDSv2 and disable IMDSv1""]"
y6f2n6,done,CloudFormation,JSON,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CloudFormation template to deploy an Aurora Global Database for cross-region disaster recovery.

MANDATORY REQUIREMENTS (Must complete):
1. Create Aurora Global Database cluster with MySQL 8.0 compatibility (CORE: RDS Aurora)
2. Deploy primary cluster in us-east-1 with one writer and one reader instance
3. Deploy secondary cluster in eu-west-1 configured as read replica region
4. Configure Route 53 health checks and failover routing between regions (CORE: Route 53)
5. Enable encryption at rest using AWS KMS in both regions
6. Set up CloudWatch alarms for replication lag monitoring in secondary region
7. Configure automated backups with 7-day retention in both clusters
8. Implement proper IAM roles for Aurora to access KMS keys

OPTIONAL ENHANCEMENTS (If time permits):
 Add Lambda function for automated failover orchestration (OPTIONAL: Lambda) - enables automated DR testing
 Configure EventBridge rules for database event notifications (OPTIONAL: EventBridge) - improves incident response
 Set up AWS Backup for additional cross-region backup copies (OPTIONAL: AWS Backup) - adds extra data protection layer

Expected output: A CloudFormation JSON template that creates a fully functional Aurora Global Database with automated failover capabilities, monitoring, and encryption across two regions.",A financial services company requires a disaster recovery solution for their critical trading database to meet regulatory requirements for 99.99% uptime. The primary region handles real-time transactions while the secondary region must be ready for immediate failover with minimal data loss.,"""Multi-region Aurora Global Database deployment spanning us-east-1 (primary) and eu-west-1 (secondary) for disaster recovery. Requires existing VPCs with private database subnets in both regions, each spanning 3 AZs. Uses Aurora MySQL 8.0 compatible engine with db.r6g.2xlarge instances for high-performance trading workloads. KMS encryption enabled in both regions with separate keys. CloudWatch monitoring configured for replication lag tracking. Assumes AWS CLI configured with appropriate permissions for cross-region resource creation.""","[""Aurora Global Database cluster name must follow pattern: 'trading-db-{region}-cluster'"", ""Primary region must be us-east-1 and secondary must be eu-west-1"", ""Database instances must use db.r6g.2xlarge instance class for performance requirements"", ""Enable encryption at rest using AWS-managed KMS keys in each region"", ""Configure automated backups with 7-day retention period in both regions"", ""Set up CloudWatch alarms for replica lag exceeding 1000ms threshold"", ""Database subnet groups must span at least 3 availability zones"", ""Enable deletion protection on production clusters but allow CloudFormation rollback""]"
l6p7m0,in_progress,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a containerized fraud detection service on ECS Fargate. MANDATORY REQUIREMENTS (Must complete): 1. Define an ECS cluster with containerInsights enabled (CORE: ECS) 2. Create an ECS task definition with 2 vCPU and 4GB memory for the fraud-detector container 3. Configure an Application Load Balancer with target group health checks on /health endpoint (CORE: ALB) 4. Deploy an ECS service with desired count of 3 tasks distributed across availability zones 5. Implement auto-scaling policy based on CPU utilization with min 2 and max 10 tasks 6. Configure CloudWatch log group with 30-day retention for container logs 7. Create security groups allowing ALB to communicate with ECS tasks on port 8080 8. Define all required IAM roles with least-privilege policies 9. Output the ALB DNS name and ECS cluster ARN OPTIONAL ENHANCEMENTS (If time permits):  Add CloudWatch alarms for high CPU and memory usage (OPTIONAL: CloudWatch) - improves operational visibility  Implement sidecar container for distributed tracing (OPTIONAL: ) - enhances debugging capabilities  Configure AWS App Mesh for service mesh capabilities (OPTIONAL: App Mesh) - adds advanced traffic management Expected output: A single CloudFormation JSON template that creates a production-ready ECS Fargate service with automatic scaling, proper health checks, and secure networking configuration.",A financial services company needs to deploy a containerized fraud detection system that processes real-time transaction data. The system requires high availability across multiple availability zones with automatic scaling based on queue depth. Container images are stored in ECR and the application communicates with an existing RDS Aurora cluster.,"""Production environment in us-east-1 region spanning 3 availability zones (us-east-1a, us-east-1b, us-east-1c). Deploy ECS Fargate cluster for containerized fraud detection service with Application Load Balancer for traffic distribution. Existing VPC (vpc-0123456789abcdef0) with private subnets (subnet-1a, subnet-1b, subnet-1c) and public subnets for ALB. ECR repository already contains fraud-detector:latest image. Requires integration with existing RDS Aurora cluster in same VPC. CloudWatch Container Insights enabled for monitoring. AWS CLI and CloudFormation CLI tools required for deployment.""","[""ECS tasks must use Fargate launch type with platform version 1.4.0"", ""Container health checks must fail after 3 consecutive failures with 30-second intervals"", ""ALB target group must use least_outstanding_requests routing algorithm"", ""ECS service must maintain exactly 2 tasks during deployments (minimumHealthyPercent: 100, maximumPercent: 200)"", ""All container logs must be encrypted using AWS-managed keys"", ""ECS task execution role must not have wildcard actions in IAM policies"", ""Auto-scaling must trigger at 70% average CPU utilization with 2-minute cooldown""]"
n6g8r0,error,CloudFormation,JSON,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a CloudFormation template to implement a blue-green deployment infrastructure for migrating a payment processing system. The configuration must: 1. Define a parent stack that orchestrates nested stacks for networking, database, and compute resources. 2. Create separate Aurora MySQL clusters for blue and green environments with encryption enabled. 3. Configure AWS DMS replication instance and tasks to sync data between blue and green databases. 4. Deploy services in both environments running the payment processing application. 5. Implement an Application Load Balancer with weighted target groups for traffic distribution. 6. Create Route 53 weighted routing policies for gradual traffic migration. 7. Configure CloudWatch alarms monitoring database replication lag and application health. 8. Implement Lambda functions to automate traffic shifting based on health metrics. 9. Set up AWS Backup plans for both database clusters with 7-day retention. 10. Create Systems Manager parameters to store environment-specific configuration. 11. Define stack outputs exposing ALB DNS names and database endpoints. Expected output: A modular CloudFormation template structure with a master stack and nested stacks that creates complete blue-green infrastructure, enabling zero-downtime migration through controlled traffic shifting and automated rollback capabilities.","A financial services company needs to migrate their legacy payment processing system from on-premises to AWS. The system currently handles 50,000 transactions per hour and requires strict compliance with PCI DSS standards. The migration must be performed with zero downtime using a blue-green deployment strategy.","""Blue-green deployment infrastructure in us-east-1 for migrating payment processing system. Uses Application Load Balancer for traffic switching between environments, Aurora MySQL clusters in Multi-AZ configuration, and for containerized services. VPC spans 3 availability zones with private subnets for compute and database tiers, public subnets for ALB. Requires AWS CLI 2.x configured with appropriate permissions, CloudFormation JSON templates. Each environment (blue/green) maintains separate Aurora clusters with AWS DMS for data synchronization. NAT Gateways provide outbound internet access for private resources.""","[""All data must be encrypted at rest using customer-managed keys"", ""Database credentials must be stored in and rotated every 30 days"", ""The solution must support automatic rollback if health checks fail during deployment"", ""All resources must be tagged with Environment, Project, and CostCenter tags"", ""Network traffic between components must use where available"", ""CloudFormation stack must use nested stacks for modular resource organization""]"
m7r9l9,in_progress,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a template to deploy a PCI-compliant payment processing infrastructure. The configuration must: 1. Create a VPC with CIDR 10.0.0.0/16 spanning 3 AZs with 3 public and 3 private subnets. 2. Deploy an RDS Aurora PostgreSQL cluster with 2 instances, encryption, and automated backups. 3. Create an ECS cluster with Fargate service running a payment API container on port 8080. 4. Set up table 'payment-sessions' with partition key 'sessionId' and TTL attribute. 5. Configure Lambda function 'payment-processor' with 3GB memory and 5-minute timeout. 6. Create S3 bucket for audit logs with server-side encryption and versioning. 7. Deploy distribution with custom error pages and AWS integration. 8. Configure Application Load Balancer with HTTPS listener and health checks. 9. Set up CloudWatch dashboards for monitoring ECS tasks, RDS connections, and Lambda invocations. 10. Create all necessary IAM roles and security groups with minimal required permissions. Expected output: A single JSON template that creates all resources with proper dependencies, security configurations, and outputs for key resource identifiers like ALB DNS name, domain, and RDS endpoint.",A financial services company is establishing a new AWS environment for their payment processing application. The infrastructure must comply with PCI DSS requirements and support high-throughput transaction processing with strict data retention policies.,"""Production-grade AWS environment in us-east-1 region for payment processing workload. Requires VPC with 3 availability zones, each containing public and private subnets. Core services include ECS Fargate for containerized API services, RDS Aurora PostgreSQL Multi-AZ for transaction data, for session management, Lambda for async processing, S3 for audit logs, and with for CDN. Infrastructure must support 10,000 concurrent users with sub-second response times. deployment requires AWS CLI 2.x configured with appropriate IAM permissions for creating VPCs, compute resources, databases, and security configurations.""","[""All RDS instances must use encrypted storage with customer-managed keys"", ""Lambda functions must have reserved concurrent executions set to prevent throttling"", "" tables must use point-in-time recovery and on-demand billing mode"", ""VPC must span exactly 3 availability zones with both public and private subnets"", ""All S3 buckets must have versioning enabled and lifecycle policies for 90-day retention"", "" distributions must use AWS with rate-based rules"", ""ECS tasks must run on Fargate with at least 2GB memory allocation"", ""All IAM roles must follow least-privilege principle with no wildcard actions"", ""CloudWatch Log Groups must have 30-day retention and encryption enabled"", ""All resources must have Cost Allocation Tags with Environment and CostCenter keys""]"
y5o9t1,error,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a multi-environment payment processing infrastructure that ensures consistency across development, staging, and production AWS accounts. MANDATORY REQUIREMENTS (Must complete): 1. Create a master template using nested stacks for VPC, database, and compute resources (CORE: CloudFormation StackSets) 2. Deploy RDS Aurora PostgreSQL cluster with environment-specific instance classes using Mappings (CORE: RDS Aurora) 3. Implement Lambda functions for payment webhook processing with environment-based concurrency limits 4. Configure S3 buckets with environment-specific naming conventions and retention policies 5. Use Conditions to enable/disable CloudWatch detailed monitoring based on environment type 6. Create EventBridge rules to route payment events to appropriate Lambda functions 7. Implement parameter groups with AWS::CloudFormation::Interface for organized input 8. Export critical resource ARNs using Outputs for cross-stack references 9. Use Mappings to define environment-specific values for instance types, storage sizes, and retention periods OPTIONAL ENHANCEMENTS (If time permits):  Add AWS WebACL for production environment only (OPTIONAL: ) - adds security layer for prod  Implement AWS Backup plans with environment-specific schedules (OPTIONAL: Backup) - ensures data protection  Add integration for secrets (OPTIONAL: ) - improves secret management Expected output: A master CloudFormation JSON template with nested stacks that can be deployed to any environment by changing a single Environment parameter, ensuring consistent infrastructure while allowing environment-specific configurations.","A fintech startup needs to maintain identical infrastructure across development, staging, and production environments for their payment processing platform. They require a CloudFormation template that can be deployed consistently across multiple AWS accounts while maintaining environment-specific configurations through parameters.","""Multi-account AWS deployment across three environments (dev, staging, prod) in us-east-1 region. Each environment resides in a separate AWS account with isolated VPCs using non-overlapping CIDR ranges (10.0.0.0/16 for dev, 10.1.0.0/16 for staging, 10.2.0.0/16 for prod). Infrastructure includes RDS Aurora PostgreSQL clusters, Lambda functions for payment processing, S3 buckets for transaction logs, and EventBridge for event routing. Requires CloudFormation with JSON templates, AWS CLI configured with appropriate cross-account assume role permissions. Each environment needs its own KMS keys for encryption.""","[""Use Mappings to define environment-specific values for at least 3 different configuration types"", ""Implement Conditions to control resource creation based on environment type"", ""Use nested stacks to modularize the infrastructure into at least 3 logical components"", ""All S3 buckets must have versioning enabled and lifecycle policies defined"", ""RDS instances must use encrypted storage with AWS KMS CMKs"", ""Lambda functions must use environment-specific reserved concurrent executions"", ""VPC CIDR ranges must not overlap between environments using parameterized values"", ""Implement cross-stack exports for sharing critical resource ARNs"", ""Use AWS::CloudFormation::Interface to organize parameters into logical groups""]"
r7o5p8,in_progress,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a web application infrastructure for a product catalog API. MANDATORY REQUIREMENTS (Must complete): 1. Deploy an Application Load Balancer across 3 availability zones with SSL termination (CORE: ALB). 2. Create an Auto Scaling Group with t3.medium instances running Amazon Linux 2 AMI, min 2, max 8 instances (CORE: ASG/EC2). 3. Configure target group with path-based health checks on /api/v1/health with 30-second intervals. 4. Set up CloudWatch alarms for ASG average CPU utilization triggering scale-out at 70% and scale-in at 30%. 5. Create IAM instance profile allowing EC2 instances to read from Parameter Store and write to CloudWatch Logs. 6. Configure security groups allowing HTTPS (443) from internet to ALB, and HTTP (80) from ALB to instances only. 7. Output the ALB DNS name and target group ARN for integration with deployment pipelines. 8. Tag all resources with Environment=Production and Application=ProductCatalogAPI. OPTIONAL ENHANCEMENTS (If time permits):  Add RDS Aurora PostgreSQL cluster with read replicas (OPTIONAL: RDS) - provides managed database with high availability.  Implement ElastiCache Redis cluster for session storage (OPTIONAL: ElastiCache) - improves API response times.  Configure AWS on ALB (OPTIONAL: ) - adds protection against common web exploits. Expected output: A complete CloudFormation JSON template that provisions the web application infrastructure with auto-scaling capabilities, proper security configurations, and monitoring in place.",A growing e-commerce platform needs to deploy their product catalog API with high availability across multiple availability zones. The API serves millions of requests daily and requires automatic scaling based on traffic patterns. The deployment must support blue-green deployments for zero-downtime updates.,"""Production deployment in us-east-1 region spanning 3 availability zones (us-east-1a, us-east-1b, us-east-1c). Infrastructure includes Application Load Balancer, Auto Scaling Group with EC2 instances, RDS Aurora PostgreSQL cluster, and ElastiCache Redis for session management. VPC configured with public subnets for ALB and private subnets for compute and database tiers. NAT Gateways provide outbound internet access for private instances. Requires AWS CLI 2.x configured with appropriate permissions. Target group stickiness enabled for session persistence.""","[""Use JSON format exclusively for the CloudFormation template"", ""Implement CloudWatch alarms that trigger at 70% CPU utilization"", ""Configure ALB health checks with specific path /api/v1/health"", ""Use Parameter Store for database connection strings only"", ""Ensure all EC2 instances use IMDSv2 for metadata access""]"
a0t4r7,in_progress,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to implement a serverless fraud detection pipeline. The configuration must: 1. Deploy a Lambda function (Python 3.11 runtime, 1GB memory) that processes incoming transactions and calculates risk scores. 2. Create a DynamoDB table with partition key 'transactionId' and sort key 'timestamp' for storing transaction records. 3. Implement a Step Functions state machine that orchestrates the fraud detection workflow with parallel processing branches. 4. Configure an S3 bucket for archiving processed transactions with intelligent tiering enabled. 5. Set up EventBridge rules to trigger the Step Functions execution when new transactions arrive. 6. Create an SNS topic for alerting compliance teams about high-risk transactions. 7. Deploy a second Lambda function for post-processing that moves completed transactions to S3. 8. Implement proper IAM roles with least-privilege policies for all services. 9. Configure CloudWatch Logs retention to 30 days for all Lambda functions. 10. Add stack outputs for the Step Functions ARN, S3 bucket name, and SNS topic ARN. Expected output: A complete CloudFormation JSON template that deploys the entire fraud detection pipeline with all components properly connected and configured according to the specified constraints.","A financial services company needs to build a serverless event processing system for real-time fraud detection. The system must process payment transactions from multiple sources, apply machine learning models for anomaly detection, and notify compliance teams of suspicious activities within seconds.","""Production deployment in us-east-1 region for a serverless fraud detection pipeline. Architecture includes Lambda functions for transaction processing, Step Functions for orchestration, DynamoDB for transaction storage, S3 for long-term archival, and EventBridge for event routing. Requires AWS CLI configured with appropriate IAM permissions. All resources deployed within default VPC using AWS managed services only. CloudFormation stack must support blue-green deployments with minimal downtime.""","[""Lambda functions must use reserved concurrency of exactly 100 to prevent cost overruns"", ""DynamoDB tables must use point-in-time recovery and encryption at rest with AWS managed keys"", ""All Lambda functions must have tracing enabled for compliance auditing"", ""Step Functions state machines must implement exponential backoff with a maximum of 3 retries"", ""EventBridge rules must use content-based filtering to route only high-risk transactions"", ""S3 buckets must have versioning enabled and lifecycle policies to transition objects to Glacier after 90 days""]"
s1u9t3,in_progress,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy an automated CI/CD pipeline for containerized microservices. The configuration must: 1. Create a CodePipeline with source, build, and deploy stages (CORE: CodePipeline). 2. Configure CodeBuild project with Docker support and test execution (CORE: CodeBuild). 3. Set up ECS Fargate service with task definitions for container deployment (CORE: ECS). 4. Implement blue-green deployment using two target groups on ALB. 5. Create CloudWatch alarms monitoring ECS service health and HTTP errors. 6. Configure automatic pipeline rollback when alarms breach thresholds. 7. Store build specifications in buildspec.yml within source repository. 8. Output pipeline URL and ECS service endpoints. Expected output: A complete CloudFormation JSON template that creates a production-ready CI/CD pipeline with automated deployments to ECS Fargate, including health monitoring and automatic rollback functionality.",A software company needs to implement a fully automated CI/CD pipeline for their microservices architecture. The pipeline must support blue-green deployments with automatic rollback capabilities and integration with existing GitHub repositories.,"""Multi-AZ deployment in us-east-1 region using CodePipeline for orchestration, CodeBuild for compilation and testing, ECS Fargate for container hosting with Application Load Balancer for traffic management. Requires AWS CLI configured, Docker for container builds, and GitHub OAuth token stored in . VPC spans 2 availability zones with public subnets for ALB and private subnets for ECS tasks. NAT Gateways enable outbound internet access for private resources.""","[""Use JSON format exclusively for the CloudFormation template"", ""Pipeline must trigger on GitHub webhook events only"", ""Build artifacts must be encrypted with customer-managed KMS keys"", ""Implement automatic rollback if CloudWatch alarms detect errors"", ""Use for all sensitive configuration values"", ""Deploy to ECS Fargate with blue-green deployment strategy"", ""Limit IAM permissions to specific resources without wildcards""]"
t7b5b6,in_progress,CloudFormation,JSON,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CloudFormation template to deploy a multi-region Aurora Global Database with automated health monitoring and DNS-based failover. The configuration must: 1. Deploy Aurora MySQL Global Database with primary cluster in us-east-1 and secondary in eu-west-1. 2. Configure writer and reader endpoints with connection pooling parameters. 3. Implement Lambda-based health checks that monitor cluster endpoints every 30 seconds. 4. Create Route 53 health checks with 10-second intervals and 2-failure threshold. 5. Configure weighted routing policy with automatic failover to secondary region. 6. Set up CloudWatch alarms for replication lag exceeding 1000ms. 7. Enable deletion protection on production clusters only. 8. Implement point-in-time recovery with 7-day backup retention. Expected output: A CloudFormation template in JSON format that creates fault-tolerant Aurora infrastructure with automated regional failover, maintaining RPO < 1 second and RTO < 30 seconds.",A financial services company requires zero-downtime database operations with automatic failover capabilities across multiple regions. Their transaction processing system must maintain sub-second switchover times during regional failures while preserving data consistency.,"""Multi-region AWS deployment spanning us-east-1 (primary) and eu-west-1 (secondary) with Aurora MySQL 5.7 compatible Global Database. Requires VPCs with private subnets in both regions, cross-region VPC peering established. Lambda functions deployed in each region for health monitoring. Route 53 hosted zone for DNS failover management. CloudFormation StackSets enabled for multi-region coordination. Minimum db.r5.large instances for production workloads.""","[""Aurora clusters must use encrypted storage with customer-managed KMS keys"", ""Lambda health check functions must complete within 5 seconds timeout"", ""Route 53 health checks must use HTTPS protocol on port 3306"", ""Secondary region must have at least 2 read replicas for load distribution"", ""Backtrack must be enabled with 24-hour window on primary cluster"", ""Parameter groups must disable binary logging for read replicas"", ""Subnet groups must span at least 3 availability zones per region"", ""CloudWatch Logs must retain Aurora slow query logs for 30 days""]"
g6b0j1,in_progress,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a template to deploy a secure data processing pipeline for financial transactions. The configuration must: 1. Create a KMS key with key rotation enabled for encrypting all resources. 2. Deploy an S3 bucket with SSE-KMS encryption, versioning, and lifecycle policies for compliance data archival. 3. Create a DynamoDB table for transaction records with encryption, point-in-time recovery, and contributor insights enabled. 4. Implement Lambda functions in VPC private subnets for data processing with environment variables encrypted by KMS. 5. Configure API Gateway REST API with request validation, API key requirements, and CloudWatch logging. 6. Set up Secrets Manager to store and rotate RDS database credentials automatically. 7. Create IAM roles with explicit permissions for each service, no wildcards allowed. 8. Implement security groups with minimal required ports and explicit CIDR blocks. 9. Configure CloudWatch Log Groups with KMS encryption for all services. 10. Add cost allocation tags to all resources for compliance tracking. 11. Create for S3, DynamoDB, and Secrets Manager to avoid internet traffic. 12. Implement CloudWatch alarms for failed API requests and Lambda errors. Expected output: A complete JSON template that creates a fully secure data processing infrastructure meeting all financial compliance requirements, with encryption at rest and in transit, automated credential rotation, and comprehensive audit logging.","A financial services company needs to implement a secure data processing pipeline that meets strict compliance requirements for handling sensitive customer information. The pipeline must enforce encryption at all layers, implement fine-grained access controls, and maintain detailed audit logs for regulatory compliance.","""Highly secure multi-AZ deployment in us-east-1 for financial data processing. Core services include Lambda functions in isolated VPCs, DynamoDB with encryption, API Gateway with request validation, Secrets Manager for credential rotation, and KMS for encryption keys. Requires AWS CLI configured with appropriate permissions, JSON template format. VPC spans 3 availability zones with private subnets only, no NAT gateways or internet gateways. All traffic flows through for AWS services.""","[""All S3 buckets must use SSE-KMS encryption with customer-managed keys"", ""Lambda functions must run in isolated VPC environments with no internet access"", ""DynamoDB tables must use point-in-time recovery and encryption at rest"", ""IAM roles must follow least-privilege principle with no wildcard actions"", ""All API Gateway endpoints must require API keys and use request validation"", ""CloudWatch Logs must be encrypted with KMS and have 90-day retention"", ""Security groups must explicitly define all ingress and egress rules"", ""All resources must have cost allocation tags for compliance tracking"", ""Secrets Manager must rotate database credentials every 30 days automatically""]"
n3p3c1,in_progress,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to optimize an existing multi-tier web application infrastructure that currently spans over 2000 lines across multiple files with significant duplication. The configuration must: 1. Consolidate three separate VPC templates into one reusable nested stack with configurable CIDR blocks. 2. Replace 15 hardcoded security group rules with a Mappings section for port configurations. 3. Implement proper parameter validation for instance types limiting to t3.medium, t3.large, or t3.xlarge. 4. Add Conditions to make ElastiCache deployment optional based on environment type. 5. Use Fn::Sub to dynamically generate resource names with environment prefixes. 6. Implement cross-stack exports for VPC ID, subnet IDs, and security group IDs. 7. Add AWS::CloudFormation::Interface metadata to group parameters logically. 8. Ensure all RDS and ElastiCache resources have DeletionPolicy set to Snapshot. 9. Replace explicit DependsOn attributes with proper resource references. 10. Implement a tagging strategy using a common CostCenter parameter. Expected output: A modular CloudFormation template structure with a master stack and at least three nested stacks (VPC, Compute, Data) that eliminates duplication, implements proper validation, and follows AWS best practices for maintainability.","Your company's CloudFormation templates for a multi-tier web application have grown complex and inefficient over two years of development. The templates suffer from hardcoded values, excessive resource duplication, and lack proper parameter validation, causing frequent deployment failures and making maintenance difficult.","""Production multi-tier application infrastructure deployed in us-east-1 across 3 availability zones. Current setup includes Application Load Balancer, Auto Scaling Group with EC2 instances, RDS Aurora MySQL cluster, and ElastiCache Redis cluster. VPC spans 10.0.0.0/16 with public subnets for ALB and private subnets for compute and data tiers. Requires AWS CLI 2.x configured with appropriate IAM permissions for CloudFormation stack operations. Templates must support blue-green deployments.""","[""Must use CloudFormation intrinsic functions to eliminate all hardcoded ARNs and resource names"", ""Implement nested stacks to modularize VPC, compute, and database layers"", ""Add parameter constraints with AllowedValues or AllowedPattern for all user inputs"", ""Use Conditions to handle optional resources without creating empty stacks"", ""Replace duplicate security group rules with reusable mappings"", ""Implement DeletionPolicy and UpdateReplacePolicy on all stateful resources"", ""Add Metadata sections with AWS::CloudFormation::Interface for parameter grouping"", ""Use Outputs with Export names for cross-stack references"", ""Implement DependsOn only where CloudFormation cannot infer dependencies"", ""Add resource tags using a common parameter for cost tracking""]"
m1s5f7,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a template to validate and analyze existing infrastructure templates for compliance and security issues. MANDATORY REQUIREMENTS (Must complete): 1. Define a Lambda function that scans templates for security violations (CORE: Lambda) 2. Create a DynamoDB table to store validation results with partition key 'TemplateId' and sort key 'Timestamp' (CORE: DynamoDB) 3. Configure an EventBridge rule to trigger validation when templates are uploaded to S3 (CORE: EventBridge) 4. Implement IAM roles with minimal permissions for Lambda to read S3 and write to DynamoDB 5. Add CloudWatch Logs group with 30-day retention for Lambda execution logs 6. Create S3 bucket with versioning for storing templates to be analyzed 7. Output the Lambda function ARN and DynamoDB table name for integration 8. Include deletion protection on DynamoDB table set to false for testing OPTIONAL ENHANCEMENTS (If time permits):  Add topic for critical security findings (OPTIONAL: ) - enables alerting  Implement workflow for complex multi-stage validation (OPTIONAL: ) - adds orchestration  Add API Gateway endpoint to trigger manual scans (OPTIONAL: API Gateway) - improves accessibility Expected output: A JSON template that deploys an automated infrastructure validation system capable of scanning templates for security and compliance issues, storing results, and providing actionable insights.",A financial services company recently acquired another firm and discovered their infrastructure was deployed using inconsistent templates. The security team needs to analyze and validate the existing templates against company standards before migration.,"""Infrastructure analysis environment in us-east-1 region for validating templates. Requires AWS CLI configured with read-only access to existing stacks. Analysis covers VPC configurations with public/private subnets, RDS MySQL instances, S3 buckets for application assets, EC2 Auto Scaling groups, and Application Load Balancers. Templates use JSON format and must be validated against best practices and company security policies. Environment includes Guard for policy-as-code validation.""","[""All analysis must be performed using Guard rules or custom validation scripts"", ""Security group rules must not contain 0.0.0.0/0 CIDR blocks for ingress traffic"", ""All S3 buckets must have versioning enabled and encryption at rest configured"", ""IAM roles must follow principle of least privilege with no wildcard (*) actions"", ""All database instances must have automated backups enabled with minimum 7-day retention"", ""Template outputs must expose only non-sensitive information required for cross-stack references""]"
f2f9f9,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy a production-ready Amazon EKS cluster with managed node groups. The configuration must: 1. Create an EKS cluster with Kubernetes version 1.28 in the provided VPC. 2. Configure cluster endpoint access as private-only for enhanced security. 3. Enable CloudWatch logging for all EKS control plane components (api, audit, authenticator, controllerManager, scheduler). 4. Create an OIDC provider for the cluster to enable IRSA functionality. 5. Deploy a managed node group with 3 t4g.medium instances (min: 2, max: 4, desired: 3). 6. Configure node group to use only private subnets provided as parameters. 7. Create necessary IAM roles for both the EKS service and node groups with appropriate managed policies. 8. Implement security groups that allow communication between nodes on required Kubernetes ports. 9. Add required tags for cost allocation and environment identification. 10. Enable encryption at rest using AWS-managed KMS keys. Expected output: A complete CloudFormation template in JSON format that creates a fully functional EKS cluster with managed node groups, proper IAM roles, security groups, and OIDC provider configuration ready for microservices deployment.","A fintech startup needs to deploy a Kubernetes cluster on AWS for their microservices architecture. They require a production-ready EKS cluster with proper networking, security, and node group configuration to support their containerized payment processing applications.","""Production EKS infrastructure deployed in us-east-1 across 3 availability zones. Requires VPC with public and private subnets, NAT Gateways for outbound traffic from private subnets. EKS cluster version 1.28 with managed node groups using t4g.medium instances. OIDC provider configuration for pod-level IAM permissions. CloudWatch Container Insights enabled for monitoring. Security groups configured for inter-node communication on ports 443, 10250, and cluster DNS (53). Requires AWS CLI 2.x and kubectl 1.28+ for validation.""","[""The EKS cluster must use Kubernetes version 1.28 or higher"", ""Node groups must use Graviton-based instances (ARM architecture) for cost optimization"", ""All worker nodes must be deployed in private subnets with no direct internet access"", ""The cluster must have OIDC provider enabled for IRSA (IAM Roles for Service Accounts)"", ""Implement strict security group rules allowing only necessary ports between nodes""]"
r9d3k5,,CloudFormation,JSON,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CloudFormation template to implement an active-passive disaster recovery architecture for a transaction processing application. MANDATORY REQUIREMENTS (Must complete): 1. Create an Aurora Global Database cluster with writer in us-east-1 and read replica in us-west-2 (CORE: Aurora) 2. Deploy Lambda functions in both regions to handle automated failover logic (CORE: Lambda) 3. Configure Route 53 hosted zone with health check-based routing policy 4. Set up Application Load Balancers in both regions with target group health checks 5. Implement IAM roles with cross-region assume permissions for failover operations 6. Configure CloudWatch alarms for database replication lag exceeding 5 seconds 7. Create parameters to store region-specific endpoints 8. Enable deletion protection on production resources with explicit override parameter OPTIONAL ENHANCEMENTS (If time permits):  Add DynamoDB Global Tables for session state replication (OPTIONAL: DynamoDB) - provides stateful failover  Implement Step Functions for complex failover workflows (OPTIONAL: Step Functions) - improves failover orchestration  Add AWS Backup for additional data protection (OPTIONAL: AWS Backup) - enhances recovery options Expected output: A CloudFormation template that deploys a complete multi-region disaster recovery solution with automated failover capabilities, health monitoring, and data replication.","A financial services company requires a disaster recovery solution for their critical transaction processing system. The primary region hosts a web application that processes customer transactions, and they need an automated failover mechanism to a secondary region with minimal data loss and downtime.","""Multi-region deployment across us-east-1 (primary) and us-west-2 (secondary) for disaster recovery. Infrastructure includes Aurora MySQL Global Database cluster, Application Load Balancers in each region, Lambda functions for failover automation, and Route 53 for DNS management. VPCs in both regions with private subnets for database tier and public subnets for ALBs. Requires AWS CLI configured with appropriate permissions for cross-region resource creation. CloudFormation StackSets used for consistent deployment across regions.""","[""Use Route 53 health checks with automatic DNS failover between regions"", ""Implement Aurora Global Database with less than 1 second replication lag"", ""Configure Lambda functions for automated failover orchestration"", ""Enable point-in-time recovery for all databases with 7-day retention"", ""Use Parameter Store for cross-region configuration sharing"", ""Implement CloudWatch cross-region dashboards for unified monitoring""]"
t1t3e6,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to implement a production-ready blue-green deployment system for containerized microservices. MANDATORY REQUIREMENTS (Must complete): 1. Create an ECS cluster with Fargate capacity providers and Container Insights enabled (CORE: ECS) 2. Configure Application Load Balancer with two target groups for blue/green deployments (CORE: ALB) 3. Define ECS service with CodeDeploy integration for traffic shifting between target groups (CORE: CodeDeploy) 4. Set up ECR repository with image scanning and lifecycle rules to retain only last 10 images 5. Create task definition referencing container image from ECR with 2GB memory allocation 6. Implement IAM roles with least-privilege policies for ECS tasks and CodeDeploy 7. Configure CloudWatch Log Groups with 30-day retention for container logs 8. Set up parameters for database connection strings and API keys 9. Define security groups allowing only ALB traffic to containers on port 8080 10. Output ALB DNS name, ECS service ARN, and CodeDeploy application name OPTIONAL ENHANCEMENTS (If time permits):  Add Route 53 alias record for custom domain (OPTIONAL: Route 53) - provides production-ready DNS  Implement Auto Scaling for ECS service based on CPU utilization (OPTIONAL: Application Auto Scaling) - improves cost efficiency  Add CloudWatch alarms for deployment failures (OPTIONAL: CloudWatch Alarms) - enables proactive monitoring Expected output: A CloudFormation JSON template that deploys a complete blue-green container orchestration system with automated traffic shifting, allowing zero-downtime deployments with automatic rollback capabilities.",A financial services company needs to deploy their payment processing microservices with zero-downtime deployments and automatic rollback capabilities. The system must handle production traffic while maintaining strict SLAs during deployment windows. The architecture requires careful orchestration of container updates with traffic shifting between blue and green environments.,"""Production-grade blue-green deployment infrastructure in us-east-1 using ECS Fargate for container orchestration, Application Load Balancer for traffic management, and CodeDeploy for automated deployments. Requires VPC with 3 availability zones, each with public and private subnets. NAT Gateways provide outbound internet access for containers in private subnets. ECR repositories store Docker images with scanning enabled. manages application configurations. CloudWatch Container Insights monitors cluster performance. Route 53 manages DNS with health checks. Deployment pipeline integrates with existing CI/CD systems.""","[""ECS tasks must use Fargate launch type with at least 2GB memory per container"", ""Application Load Balancer must implement path-based routing with weighted target groups"", ""CodeDeploy must be configured for LINEAR_10PERCENT_EVERY_10MINUTES traffic shifting"", ""Container images must be stored in private ECR repositories with lifecycle policies"", ""All resources must use for configuration values""]"
i2p6k9,,CloudFormation,JSON,expert,Provisioning of Infrastructure Environments,Environment Migration,Create a CloudFormation template to orchestrate a zero-downtime migration of an on-premises payment processing system to AWS. MANDATORY REQUIREMENTS (Must complete): 1. Create Aurora MySQL cluster with 2 instances across multiple AZs (CORE: ) 2. Configure DMS replication instance and tasks for continuous data sync (CORE: DMS) 3. Set up Application Load Balancer with target groups for blue-green deployment 4. Implement Route 53 hosted zone with weighted routing policies (0-100% traffic split) 5. Create VPC peering connection between migration VPC (10.0.0.0/16) and production VPC (10.1.0.0/16) 6. Configure DataSync locations and tasks for migration from on-premises NFS 7. Store all sensitive parameters in with encryption 8. Set up CloudWatch dashboard with DMS replication lag and performance metrics OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Lambda for automated traffic shifting based on error rates (OPTIONAL: Lambda) - enables automated rollback  Implement queues for decoupling payment processing (OPTIONAL: ) - improves scalability  Configure AWS Backup for automated cross-region backups (OPTIONAL: Backup) - adds disaster recovery Expected output: A complete CloudFormation template in JSON format that creates all required resources for migrating the payment system with zero downtime. The template should support parameterized traffic shifting percentages and include outputs for monitoring dashboard URL and database endpoints.,A financial services company needs to migrate their payment processing infrastructure from their on-premises datacenter to AWS. The existing system processes credit card transactions through a legacy monolithic application that must be modernized during migration. The company requires zero-downtime migration with the ability to rollback if issues arise.,"""Production environment migration from on-premises to AWS us-east-1 region. Requires Aurora MySQL for database tier, Application Load Balancer for traffic distribution, and DMS for data migration. VPC setup includes 3 availability zones with public and private subnets. Migration VPC (10.0.0.0/16) peers with production VPC (10.1.0.0/16). Requires AWS CLI configured with appropriate IAM permissions for DMS, VPC, and Route 53 services. CloudFormation stack must support phased rollout with traffic shifting capabilities.""","[""Use AWS Database Migration Service (DMS) for continuous data replication"", ""Implement blue-green deployment strategy using Route 53 weighted routing"", ""All database passwords must be stored in "", ""Enable point-in-time recovery for with 7-day retention period"", ""Configure VPC peering between migration and production VPCs"", ""Use AWS DataSync for migrating static assets to "", ""Implement rules to validate compliance during migration"", ""Set up CloudWatch dashboards with custom metrics for migration monitoring""]"
d0y1b9,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy a production VPC with dual-stack networking. The configuration must: 1. Create a VPC with both IPv4 (10.0.0.0/16) and auto-assigned IPv6 CIDR blocks. 2. Deploy 6 subnets across 3 AZs - one public and one private subnet per AZ. 3. Configure Internet Gateway with proper route table associations for public subnets. 4. Deploy NAT Gateways in each public subnet with Elastic IPs for redundancy. 5. Create separate route tables for public and private subnets with appropriate routes. 6. Enable VPC Flow Logs with CloudWatch Logs destination and required IAM role. 7. Implement custom Network ACLs with explicit ingress/egress rules for security. 8. Apply consistent tagging schema across all resources for cost tracking. 9. Use intrinsic functions for dynamic resource naming and cross-references. 10. Configure all resources with DeletionPolicy: Delete for environment cleanup. Expected output: A valid CloudFormation JSON template that creates a fully functional multi-AZ VPC with public/private subnet isolation, redundant NAT Gateways, and comprehensive logging enabled.","Your organization is migrating a legacy on-premises application to AWS and requires a production-grade network foundation. The application consists of web servers, application servers, and database servers that must be isolated in separate network tiers with strict security boundaries. The infrastructure must support both IPv4 and IPv6 traffic while maintaining compliance with corporate security policies.","""Production-grade multi-AZ VPC infrastructure in us-east-1 region spanning 3 availability zones (us-east-1a, us-east-1b, us-east-1c). Requires CloudFormation JSON template with IPv4/IPv6 dual-stack configuration. Infrastructure includes public subnets for load balancers, private subnets for application workloads, NAT Gateways for outbound internet access, and VPC Flow Logs for security compliance. CloudWatch Logs integration required for flow log storage with proper IAM roles and retention policies.""","[""VPC must use CIDR block 10.0.0.0/16 for IPv4 and auto-assign IPv6 CIDR"", ""Each availability zone must have exactly one public and one private subnet"", ""Public subnets must use /24 CIDR blocks starting from 10.0.1.0"", ""Private subnets must use /24 CIDR blocks starting from 10.0.11.0"", ""NAT Gateways must be deployed in each AZ for high availability"", ""All route tables must have explicit names following pattern: {vpc-name}-{subnet-type}-rt-{az}"", ""Network ACLs must deny all traffic by default except explicitly allowed rules"", ""VPC Flow Logs must be enabled and sent to CloudWatch Logs with 7-day retention"", ""All resources must have DeletionPolicy set to Delete for clean teardown"", ""Tags must include Environment, Owner, and CostCenter on all resources""]"
z5k7f9,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy consistent infrastructure across multiple environments. The configuration must: 1. Design a master template that uses nested stacks for VPC, database, and compute resources. 2. Create parameter mappings that define environment-specific values for instance sizes (t3.micro for dev, t3.small for staging, t3.medium for prod). 3. Deploy RDS Aurora PostgreSQL clusters with encryption at rest and automated backups. 4. Configure Lambda functions with 256MB memory for dev/staging and 512MB for production. 5. Set up S3 buckets with intelligent tiering and cross-region replication to us-west-2. 6. Implement VPC peering between environments with appropriate route tables and security groups. 7. Use CloudFormation Conditions to create NAT Gateways only in staging and production. 8. Configure CloudWatch Alarms with SNS topics for critical metrics (RDS CPU > 80%, Lambda errors > 10/min). 9. Output critical resource ARNs and endpoints for use by application deployment pipelines. Expected output: A complete CloudFormation template structure with a master template and at least three nested stack templates that can deploy identical infrastructure across environments while maintaining environment-specific configurations through parameters and mappings.","A financial services company needs to maintain identical infrastructure configurations across development, staging, and production environments to ensure consistency and reduce configuration drift. They require an automated solution that can replicate critical infrastructure components while allowing environment-specific customizations for resource sizing and access controls.","""Multi-environment AWS infrastructure spanning three environments (dev, staging, prod) deployed across us-east-1 as primary region with disaster recovery capability in us-west-2. Core services include VPC with public/private subnets across 2 AZs per environment, RDS Aurora PostgreSQL clusters with read replicas, Lambda functions for data processing, S3 buckets for storage with cross-region replication enabled. Requires AWS CLI configured with appropriate permissions for CloudFormation, S3, RDS, Lambda, VPC, and IAM services. Each environment uses separate AWS accounts linked through AWS Organizations for billing consolidation.""","[""Use nested stacks with a master template that orchestrates environment deployments"", ""Implement parameter mappings for environment-specific values without hardcoding"", ""All S3 buckets must have versioning enabled and lifecycle policies for 30-day transitions to Glacier"", ""RDS instances must use encrypted storage with automated backups retained for 7 days minimum"", ""Lambda functions must use environment variables for configuration, never hardcoded values"", ""VPC CIDR blocks must not overlap between environments (10.0.0.0/16 for dev, 10.1.0.0/16 for staging, 10.2.0.0/16 for prod)"", ""All resources must be tagged with Environment, Project, and CostCenter tags"", ""Use Conditions to control resource creation based on environment type"", ""Implement CloudFormation StackSets for cross-region replication capability""]"
y6k4t1,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a template to deploy a serverless credit scoring web application. MANDATORY REQUIREMENTS (Must complete): 1. Configure Application Load Balancer with HTTPS listener using ACM certificate (CORE: ELB) 2. Deploy Lambda function with Node.js 18 runtime for credit scoring logic (CORE: Lambda) 3. Create Aurora Serverless v2 PostgreSQL cluster with encryption enabled (CORE: Aurora) 4. Implement Lambda function URL with IAM authentication for ALB integration 5. Configure VPC with 3 AZs, public subnets for ALB, private subnets for Lambda/RDS 6. Set up CloudWatch Logs groups with 365-day retention for all components 7. Create KMS key with rotation enabled for database encryption 8. Implement least-privilege IAM roles for Lambda with specific Aurora permissions 9. Configure Aurora automatic backups with 30-day retention 10. Add all required tags to every resource OPTIONAL ENHANCEMENTS (If time permits):  Add distribution for static assets (OPTIONAL: ) - improves global performance  Implement SQS queue for asynchronous score processing (OPTIONAL: SQS) - adds scalability  Configure EventBridge for audit event routing (OPTIONAL: EventBridge) - enhances compliance tracking Expected output: A complete JSON template that provisions the entire infrastructure stack with proper networking, security configurations, and compliance controls. The template should use parameters for environment-specific values and include outputs for key resource identifiers.",A fintech startup needs to deploy their new credit scoring web application with strict compliance requirements. The application must handle sensitive financial data and maintain audit trails for regulatory purposes. Performance and security are critical as the app processes real-time credit decisions.,"""Production deployment in us-east-1 region for a credit scoring web application. Architecture includes Application Load Balancer distributing traffic to Lambda functions via target groups, with Aurora Serverless v2 PostgreSQL for primary data storage. VPC spans 3 availability zones with private subnets for database and Lambda functions. Public subnets host ALB with internet gateway. NAT gateways provide outbound connectivity for Lambda. Requires AWS CLI configured with appropriate permissions for stack creation. KMS key for encryption, CloudWatch Logs for audit trails, and AWS Certificate Manager for TLS certificates.""","[""All data must be encrypted at rest using AWS KMS customer managed keys"", ""Application logs must be retained for exactly 365 days for compliance"", ""Database backups must occur daily with 30-day retention period"", ""All resources must be tagged with CostCenter, Environment, and DataClassification"", ""RDS instances must use encrypted storage with automated minor version patching"", ""Application Load Balancer must enforce TLS 1.2 minimum and use AWS Certificate Manager"", ""Lambda functions must have reserved concurrent executions set to prevent throttling""]"
w4c5f4,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a serverless crypto price alert system. MANDATORY REQUIREMENTS (Must complete): 1. Create a Lambda function 'PriceWebhookProcessor' with 1GB memory to receive price updates from exchanges (CORE: Lambda). 2. Create a DynamoDB table 'CryptoAlerts' with partition key 'userId' and sort key 'alertId' for storing user alerts (CORE: DynamoDB). 3. Create a Lambda function 'AlertMatcher' with 2GB memory that compares prices against user thresholds. 4. Create an EventBridge rule that triggers AlertMatcher every 60 seconds. 5. Configure Lambda destinations to route successful alerts to a 'ProcessedAlerts' Lambda function. 6. Set reserved concurrent executions: PriceWebhookProcessor=100, AlertMatcher=50. 7. Create IAM execution roles with DynamoDB read/write and CloudWatch Logs permissions only. 8. Add CloudFormation outputs for all three Lambda function ARNs. OPTIONAL ENHANCEMENTS (If time permits):  Add SNS topic for sending notifications to users (OPTIONAL: SNS) - enables multi-channel alerts.  Implement SQS FIFO queue between webhook and processor (OPTIONAL: SQS) - guarantees order preservation.  Add API Gateway REST API for manual price updates (OPTIONAL: API Gateway) - allows testing without webhooks. Expected output: A complete CloudFormation JSON template that deploys the serverless alert system with all Lambda functions, DynamoDB table, EventBridge rule, and proper IAM configurations ready for production use.",A fintech startup needs to process cryptocurrency price alerts in real-time. The system must handle spikes during market volatility when thousands of users set price alerts simultaneously. The architecture needs to be cost-effective during quiet periods but scale instantly when crypto markets become volatile.,"""Serverless cryptocurrency alert processing system deployed in us-east-1. Architecture uses Lambda functions with Graviton2 processors for webhook processing and alert matching, DynamoDB for storing user alerts and price thresholds, EventBridge for scheduled price checks every minute. No VPC required as all services are AWS-managed. CloudFormation JSON template deployment via AWS CLI or Console. System scales from 0 to thousands of concurrent executions based on crypto market volatility. CloudWatch Logs for debugging with 3-day retention.""","[""Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""DynamoDB tables must use on-demand billing with point-in-time recovery enabled"", ""All Lambda functions must have reserved concurrent executions set to prevent throttling"", ""Use Lambda destinations instead of DLQ for failed invocations"", ""CloudWatch Logs retention must be exactly 3 days to minimize costs"", ""Lambda functions must use environment variables for configuration, not hardcoded values"", ""EventBridge rules must use rate expressions, not cron expressions"", ""All IAM policies must follow least privilege with no wildcard actions"", ""Lambda timeout must not exceed 5 minutes for any function"", ""Use CloudFormation Outputs to expose all Lambda function ARNs""]"
d1f2f3,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a template to deploy a multi-stage CI/CD pipeline for containerized microservices. MANDATORY REQUIREMENTS (Must complete): 1. Create with source, build, test, and deploy stages for dev/staging/prod (CORE: ) 2. Configure projects for Docker image building and security scanning (CORE: ) 3. Set up S3 bucket with versioning and lifecycle policies for pipeline artifacts 4. Create ECR repository with image scanning on push enabled 5. Implement application for blue/green ECS deployments 6. Configure topic for pipeline failure notifications to ops team 7. Create CloudWatch dashboard showing pipeline metrics and success rates 8. Implement entries for environment-specific configurations 9. Set up rule to trigger pipeline on ECR image pushes 10. Configure all resources with appropriate tags for cost allocation OPTIONAL ENHANCEMENTS (If time permits):  Add Lambda function for automated rollback on metric alarms (OPTIONAL: Lambda) - improves reliability  Implement for complex approval workflows (OPTIONAL: ) - adds flexibility  Add AWS Chatbot for Slack notifications (OPTIONAL: Chatbot) - improves team communication Expected output: A template in JSON format that creates a complete CI/CD pipeline with automated building, testing, security scanning, and multi-environment deployment capabilities for containerized applications.","A financial technology startup needs to establish a CI/CD pipeline for their payment processing microservices. The pipeline must handle automated testing, security scanning, and multi-stage deployments while maintaining PCI compliance requirements.","""AWS CI/CD infrastructure deployed in us-east-2 region using for orchestration, for build and test stages, and for ECS deployments. Requires VPC with private subnets for projects, S3 bucket for artifacts, ECR for container images, and for encryption. Pipeline deploys to three environments (dev, staging, prod) with ECS Fargate clusters in separate VPCs. Each environment has Application Load Balancer and Aurora PostgreSQL database. Security scanning integrated via with Trivy and OWASP dependency check.""","[""All projects must use compute type BUILD_GENERAL1_SMALL to minimize costs"", ""Pipeline artifacts must be encrypted using customer-managed keys"", ""Each deployment stage must have manual approval except for development environment"", "" projects must run in VPC mode with no internet access for security"", ""Pipeline must fail if any security vulnerabilities rated HIGH or CRITICAL are detected"", ""All IAM roles must follow least privilege with no wildcard resource permissions""]"
m3c3w1,,CloudFormation,JSON,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a template to implement a multi-region disaster recovery architecture for a transaction processing system. MANDATORY REQUIREMENTS (Must complete): 1. Create an Aurora Global Database cluster with PostgreSQL 13.7 engine, automated backtrack enabled, and encryption using (CORE: Aurora) 2. Configure Route 53 hosted zone with failover routing policy and health checks for both regions (CORE: Route 53) 3. Deploy Lambda functions in both regions for transaction processing with 3GB memory and 15-minute timeout 4. Set up Application Load Balancers in each region with target groups pointing to Lambda functions 5. Implement S3 buckets with cross-region replication and RTC enabled for transaction logs 6. Configure CloudWatch alarms for Aurora cluster availability, Lambda errors, and ALB target health 7. Create IAM roles with least-privilege policies for Lambda execution and Aurora access 8. Enable deletion protection on Aurora clusters and set backup retention to 7 days 9. Implement StackSets for consistent deployment across both regions 10. Configure for S3 and DynamoDB to keep traffic within AWS network OPTIONAL ENHANCEMENTS (If time permits):  Add DynamoDB Global Tables for session state management (OPTIONAL: DynamoDB) - provides consistent session data across regions  Implement EventBridge rules for automated failover notifications (OPTIONAL: EventBridge) - improves incident response time  Add AWS Backup plans for additional data protection (OPTIONAL: AWS Backup) - enhances compliance posture Expected output: A template using StackSets that deploys identical infrastructure in us-east-1 and us-west-2, with automated failover capabilities achieving RTO under 5 minutes and RPO under 1 minute.",A financial services company needs to implement a disaster recovery solution for their critical transaction processing system. The system must maintain 99.99% availability and automatically failover between regions during outages. Compliance requires all data to be encrypted at rest and in transit.,"""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary). Utilizes Aurora Global Database for PostgreSQL 13.7 with automated backtrack, Route 53 for DNS failover with health checks, S3 with cross-region replication and RTC, Lambda functions for transaction processing, and Application Load Balancers in each region. VPC setup includes 3 private subnets per region for Aurora, 3 public subnets for ALB, VPC peering between regions for data synchronization. Requires AWS CLI 2.x configured with appropriate permissions for StackSets deployment across multiple regions.""","[""Use StackSets to deploy identical infrastructure across primary (us-east-1) and secondary (us-west-2) regions"", ""Implement Route 53 health checks with automatic DNS failover between regions with 30-second TTL"", ""Configure Aurora Global Database with automated backtrack enabled and 1-second RPO"", ""All S3 buckets must use cross-region replication with RTC (Replication Time Control) enabled"", ""Deploy Lambda functions in both regions with identical environment variables and IAM roles""]"
j7d0q1,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a template to implement a security-first infrastructure foundation with automated compliance controls. MANDATORY REQUIREMENTS (Must complete): 1. Create a KMS customer-managed key with automatic rotation enabled and alias 'security/primary' (CORE: KMS) 2. Deploy Secrets Manager secret for database credentials with automatic rotation every 30 days using the KMS key (CORE: Secrets Manager) 3. Define IAM role with AssumeRolePolicyDocument allowing only specific AWS account (123456789012) to assume 4. Implement IAM policy enforcing S3 bucket encryption with customer KMS keys only 5. Create IAM group 'SecurityAuditors' with read-only access to CloudTrail and Config 6. Configure resource-based policy on KMS key allowing only tagged principals (Environment=Production) 7. Set up IAM password policy requiring 14+ characters, uppercase, lowercase, numbers, and symbols 8. Implement SCPs-like IAM boundary policy preventing EC2 instance launch without encryption 9. Create cross-account IAM role for security scanning with external ID requirement OPTIONAL ENHANCEMENTS (If time permits):  Add Config rules for continuous compliance monitoring (OPTIONAL: Config) - automates compliance checks  Implement GuardDuty for threat detection (OPTIONAL: GuardDuty) - adds real-time security monitoring  Create custom controls (OPTIONAL: ) - centralizes security findings Expected output: A JSON template that establishes a hardened security baseline with encryption, least-privilege access controls, and audit capabilities suitable for regulated environments.","A financial services company needs to establish a secure foundation for their AWS infrastructure with strict compliance requirements. They require automated security controls that enforce encryption, access patterns, and audit capabilities across all their workloads. The security team has mandated zero-trust principles with granular permissions and comprehensive key management.","""AWS multi-account security architecture deployed in us-east-1 with KMS for encryption key management, Secrets Manager for credential rotation, and IAM for granular access controls. Requires deployment with appropriate permissions to create IAM resources, KMS keys, and Secrets Manager. No VPC required as this creates account-level security resources. Target account ID: 123456789012 for cross-account access patterns.""","[""All IAM policies must use explicit deny statements for prohibited actions"", ""KMS key policy must enforce separation of duties between key administrators and users"", ""Secrets rotation Lambda must run in isolated environment with no internet access"", ""IAM roles must have unique external IDs with minimum 32 characters"", ""All resource names must follow pattern: CompanyName-Environment-ResourceType-Purpose"", "" stack must export KMS key ARN and IAM role ARNs for cross-stack references"", ""IAM policies must explicitly deny access from non-approved IP ranges (10.0.0.0/8)"", ""Resource tags must include CostCenter, DataClassification, and Owner as mandatory"", ""Template must use Conditions to enable/disable features based on environment parameter""]"
p2e9q6,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to optimize and fix a legacy three-tier web application infrastructure that currently fails deployment validation. The configuration must: 1. Refactor an existing VPC setup that uses hardcoded CIDR blocks into a parameterized template with proper subnet calculations using Fn::Cidr. 2. Fix Auto Scaling Group configuration that references non-existent launch templates and implement proper health check grace periods. 3. Correct RDS Aurora cluster setup where reader endpoints are incorrectly referenced before cluster creation completes. 4. Replace Lambda functions using Node.js 12.x runtime with Node.js 18.x and add proper error handling. 5. Fix S3 bucket policies that currently allow public access and implement least-privilege access patterns. 6. Resolve circular dependency between Application Load Balancer target groups and Auto Scaling Group. 7. Implement proper integration for database credentials instead of plain text parameters. 8. Add CloudWatch alarms for all critical metrics with topic integration for alerts. 9. Fix IAM roles that currently use overly permissive wildcard policies. 10. Implement proper stack outputs with exports for cross-stack references. 11. Add cost allocation tags to all resources following company naming conventions. 12. Ensure all resources support blue-green deployments through proper logical ID management. Expected output: A fully functional CloudFormation JSON template that passes cfn-lint validation, deploys successfully on first attempt, reduces deployment time by 40%, and eliminates all security vulnerabilities identified in the original template.",A financial services company discovered their existing CloudFormation templates are causing deployment failures and excessive costs. The infrastructure team needs to refactor a problematic multi-tier application template that violates AWS limits and contains numerous anti-patterns.,"""Production environment in us-east-1 region requiring high availability across 3 availability zones. Infrastructure includes VPC with public and private subnets, Application Load Balancer, Auto Scaling Group with EC2 instances running Amazon Linux 2, RDS Aurora MySQL cluster, Lambda functions for data processing, and S3 buckets for static assets. Deployment uses CloudFormation StackSets for multi-account governance. Requires AWS CLI 2.x configured with appropriate credentials. All resources must comply with CIS AWS Foundations Benchmark v1.4.0.""","[""Remove all hardcoded resource names and replace with generated names"", ""Fix circular dependencies between Security Groups and EC2 instances"", ""Implement proper parameter constraints with AllowedValues where applicable"", ""Replace inline IAM policies with managed policies or policy documents"", ""Add DeletionPolicy and UpdateReplacePolicy to all stateful resources"", ""Use Fn::Sub instead of multiple nested Fn::Join operations"", ""Implement proper error handling for all Lambda functions"", ""Remove duplicate resource definitions and consolidate using mappings"", ""Fix all references to deprecated runtime versions"", ""Ensure all S3 buckets have encryption and versioning enabled""]"
m0m4b7,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to implement an automated infrastructure compliance analysis system. The configuration must: 1. Deploy AWS Config with custom rules to analyze CloudFormation stack compliance (CORE: Config). 2. Create Lambda functions to process Config evaluations and generate compliance reports (CORE: Lambda). 3. Store compliance reports in S3 with lifecycle policies for 90-day retention. 4. Implement Config rules for mandatory tags (Environment, Owner, CostCenter). 5. Check for encrypted storage on all RDS, S3, and EBS resources. 6. Validate security group rules prohibit 0.0.0.0/0 ingress on ports 22, 3389. 7. Generate SNS notifications for critical non-compliance findings. 8. Create IAM roles with least-privilege access for Config and Lambda execution. Expected output: A CloudFormation JSON template that deploys a fully functional compliance analysis system capable of evaluating infrastructure against defined policies and generating actionable reports.",Your organization needs automated infrastructure compliance checking to ensure all CloudFormation stacks meet security and tagging standards. The compliance team requires real-time analysis of stack deployments with automated remediation capabilities for common violations.,"""AWS compliance monitoring infrastructure deployed in us-east-1 using AWS Config for continuous configuration assessment, Lambda for custom rule evaluation and report generation, S3 for compliance report storage. Requires AWS CLI configured with appropriate permissions, CloudFormation deployment capabilities. Single-region deployment with cross-account analysis support. Config aggregator setup for organization-wide visibility. SNS topic in same region for alert distribution.""","[""Config rules must evaluate resources within 15 minutes of deployment"", ""Lambda functions must use Python 3.11 runtime with 256MB memory allocation"", ""S3 bucket must have versioning enabled and block public access"", ""Config delivery channel must use SSE-S3 encryption for configuration snapshots"", ""Lambda execution roles must not use AWS managed policies"", ""Config rules must support custom remediation actions via SSM documents"", ""All Lambda functions must have reserved concurrent executions set to prevent throttling"", ""SNS topic must use KMS encryption with customer-managed keys""]"
z5b3t6,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy a multi-OS EKS cluster with enhanced security controls.

MANDATORY REQUIREMENTS (Must complete):
1. Create an EKS cluster with private API endpoint only (CORE: EKS)
2. Deploy two managed node groups: one for Linux workloads (t3.medium, min 2, max 10) and one for Windows workloads (t3.large, min 1, max 5) (CORE: EC2)
3. Configure KMS encryption for EKS secrets and control plane logs
4. Enable all EKS control plane log types (api, audit, authenticator, controllerManager, scheduler)
5. Create custom launch templates for both node groups with IMDSv2 required and HttpPutResponseHopLimit set to 1
6. Configure Spot instance usage with OnDemandPercentageAboveBaseCapacity set to 50
7. Set up OIDC provider for IRSA functionality
8. Configure VPC CNI addon with ENABLE_PREFIX_DELEGATION set to true
9. Tag all resources with Environment=Production and ManagedBy=CloudFormation
10. Output cluster endpoint, OIDC issuer URL, and node group ARNs

OPTIONAL ENHANCEMENTS (If time permits):
 Add AWS Load Balancer Controller using IAM role and service account (OPTIONAL: ElasticLoadBalancing) - enables ingress management
 Configure Fluent Bit for log forwarding to CloudWatch (OPTIONAL: CloudWatch) - centralizes logging
 Add EBS CSI driver with encrypted volume support (OPTIONAL: EBS) - enables persistent storage

Expected output: A CloudFormation JSON template that creates a production-ready EKS cluster supporting both Linux and Windows containers, with private access only, Spot instance integration, and IRSA configured for secure workload permissions.","A financial services company needs to deploy a production-ready Kubernetes cluster on AWS for their microservices architecture. The cluster must support both Linux and Windows workloads, with strict security requirements including private endpoint access and encryption at rest.","""Production EKS cluster deployment in us-east-1 across 3 availability zones. Infrastructure includes EKS control plane with private endpoint, managed node groups for both Linux (Amazon Linux 2) and Windows Server 2022 workloads, VPC with private subnets only, NAT gateways for outbound connectivity, and KMS encryption. Requires AWS CLI 2.x configured with appropriate IAM permissions for EKS, EC2, IAM, and KMS services. The cluster will host approximately 50 microservices with auto-scaling requirements.""","[""EKS cluster must use Kubernetes version 1.28 or higher"", ""Control plane logging must be enabled for all log types with encryption"", ""Node groups must use Spot instances for at least 50% of capacity"", ""Windows node group must use Windows Server 2022 AMI"", ""All worker nodes must use custom launch templates with IMDSv2 enforced"", ""Cluster must use IRSA (IAM Roles for Service Accounts) for pod-level permissions"", ""VPC CNI addon must be configured with custom settings for IP address management""]"
v8l8h7,,CloudFormation,JSON,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CloudFormation template to deploy a multi-region disaster recovery solution for a transaction processing system. MANDATORY REQUIREMENTS (Must complete): 1. Configure DynamoDB Global Tables with on-demand billing mode and point-in-time recovery (CORE: DynamoDB) 2. Set up S3 buckets in both regions with cross-region replication and versioning enabled (CORE: S3) 3. Implement Route 53 hosted zone with failover routing policy using health checks 4. Create Lambda functions in both regions for transaction processing with environment variables for region-specific configuration 5. Configure keys in each region with alias 'alias/transaction-encryption' 6. Set up CloudWatch alarms for DynamoDB throttling, S3 replication lag, and Lambda errors 7. Create topics in both regions for operational alerts 8. Implement IAM roles with cross-region assume role permissions 9. Configure CloudWatch Logs with cross-region log group subscriptions 10. Add stack outputs for primary and secondary region endpoints OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Backup for automated cross-region backups (OPTIONAL: AWS Backup) - provides additional data protection layer  Implement EventBridge rules for automated failover triggers (OPTIONAL: EventBridge) - enables event-driven DR automation  Add for configuration management (OPTIONAL: ) - centralizes multi-region configuration Expected output: A single CloudFormation JSON template that deploys all resources in both regions with proper cross-region references, automated health monitoring, and failover capabilities. The template should use nested stacks or StackSets for multi-region deployment coordination.","A financial services company needs to implement a disaster recovery solution for their critical transaction processing system. The primary region experienced a 6-hour outage last quarter, resulting in significant revenue loss. They require an active-passive multi-region setup with automated failover capabilities.","""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary). Utilizes DynamoDB Global Tables for transaction data, S3 with cross-region replication for document storage, Lambda functions for transaction processing, and Route 53 for DNS failover. Requires AWS CLI 2.x configured with appropriate IAM permissions for multi-region deployments. VPCs in both regions with private subnets and VPC peering connection. keys in each region for encryption. CloudWatch cross-region monitoring with centralized dashboard. Total infrastructure supports 10,000 TPS with sub-second latency requirements.""","[""Use JSON format exclusively for the CloudFormation template"", ""Primary region must be us-east-1 with failover to us-west-2"", ""RTO must be under 15 minutes and RPO under 5 minutes"", ""All data must be encrypted at rest using CMKs"", ""Route 53 health checks must monitor both regions continuously"", ""DynamoDB global tables must have point-in-time recovery enabled"", ""Lambda functions must use reserved concurrency of at least 100"", ""Cross-region replication must use S3 Transfer Acceleration"", ""All resources must have DeletionPolicy set to Retain"", ""CloudWatch alarms must trigger notifications for any failover events""]"
m3h6t7,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a containerized batch processing system using Amazon ECS. The configuration must: 1. Define an ECS cluster with Container Insights enabled and capacity providers for Fargate. 2. Create task definitions for three processing stages: data-ingestion, transaction-processing, and report-generation, each with 2GB memory and 1 vCPU. 3. Configure ECS services for each stage with exactly 2 running tasks and circuit breaker deployment configuration. 4. Set up an Application Load Balancer with target groups for the report-generation service only. 5. Implement task IAM roles with permissions to read from Secrets Manager and write to S3 buckets. 6. Configure daemon as sidecar containers in each task definition for distributed tracing. 7. Create CloudWatch log groups with 30-day retention for each service. 8. Define service auto-scaling policies based on CPU utilization (scale up at 70%, scale down at 30%). 9. Implement health checks with 30-second intervals and 3 retry attempts. 10. Configure task placement constraints to spread tasks across availability zones. Expected output: A complete CloudFormation JSON template that deploys the entire ECS infrastructure with three interdependent services, proper networking, security configurations, and monitoring capabilities.",A financial services company needs to modernize their batch processing system for daily transaction reconciliation. The current monolithic application causes bottlenecks during peak hours and lacks proper resource isolation between different processing stages.,"""Production environment deployed in us-east-2 region using Amazon ECS with Fargate launch type for container orchestration. Infrastructure includes VPC with 3 availability zones, private subnets for ECS tasks, NAT gateways for outbound connectivity, and Application Load Balancer for traffic distribution. Requires CloudFormation JSON templates, AWS CLI 2.x configured with appropriate permissions. ECR repositories must be pre-created with container images. Secrets Manager contains database credentials and API keys. CloudWatch Container Insights and enabled for full observability.""","[""ECS tasks must run in FARGATE launch type only"", ""Container images must be pulled from ECR repositories in the same region"", ""Each ECS service must have exactly 2 tasks for redundancy"", ""Task definitions must use awsvpc network mode"", ""CloudWatch Container Insights must be enabled at the cluster level"", ""All ECS tasks must have sidecar containers for distributed tracing"", ""Secrets must be injected from AWS Secrets Manager, not environment variables""]"
z4p0x6,,CloudFormation,JSON,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a CloudFormation template to orchestrate migration of on-premises PostgreSQL databases to Aurora using Database Migration Service. The configuration must: 1. Create source and target DMS endpoints with SSL encryption enabled. 2. Deploy DMS replication instance in private subnet with t3.medium instance class. 3. Configure DMS migration task for full load plus CDC with validation enabled. 4. Create Aurora PostgreSQL cluster with 2 reader instances across multiple AZs. 5. Implement Route 53 hosted zone with weighted routing policies for gradual traffic shift. 6. Store database passwords in Parameter Store with SecureString type. 7. Configure CloudWatch dashboard showing replication metrics and lag time. 8. Create topic for alerting when replication lag exceeds threshold. 9. Output DMS task ARN, Aurora cluster endpoint, and Route 53 hosted zone ID. Expected output: A complete CloudFormation template in JSON format that automates the database migration infrastructure setup, enabling zero-downtime cutover from on-premises PostgreSQL to Aurora PostgreSQL with continuous replication monitoring.",A financial services company is migrating their legacy monolithic application from on-premises infrastructure to AWS. The application currently runs on physical servers with local PostgreSQL databases and file storage. The migration must maintain data integrity while minimizing downtime during the cutover period.,"""Production migration environment deployed in us-east-1 with multi-AZ Aurora PostgreSQL cluster, DMS replication instances in private subnets, and Application Load Balancer in public subnets. VPC spans 3 availability zones with separate subnet tiers for web, application, and database layers. Requires AWS CLI configured with appropriate IAM permissions for CloudFormation, DMS, and Route 53. Target architecture includes Auto Scaling groups for instances running the migrated application, with buckets replacing local file storage. Network configuration includes VPC peering or Direct Connect to on-premises datacenter for DMS replication.""","[""Use AWS Database Migration Service (DMS) for continuous data replication"", ""Implement blue-green deployment strategy with Route 53 weighted routing"", ""All databases must use encrypted storage with customer-managed keys"", ""Configure DMS endpoints with SSL/TLS encryption for data in transit"", ""Use Parameter Store for database credentials"", ""Enable CloudWatch alarms for DMS replication lag exceeding 300 seconds"", ""Set DeletionPolicy to Snapshot for all instances""]"
i6v9t9,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy a production-grade VPC network architecture. MANDATORY REQUIREMENTS (Must complete): 1. Create a VPC with CIDR 10.0.0.0/16 and enable DNS hostnames (CORE: VPC) 2. Deploy 6 subnets across 3 AZs - one public and one private per AZ (CORE: EC2 Subnets) 3. Configure Internet Gateway attached to VPC with proper route tables 4. Deploy NAT Gateways in each public subnet with Elastic IPs 5. Create separate route tables for public and private subnets with appropriate routes 6. Enable VPC Flow Logs to CloudWatch Logs with IAM role and 30-day retention 7. Implement custom Network ACLs with explicit inbound/outbound rules 8. Use Mappings section to define subnet CIDRs for different regions 9. Apply consistent tagging with Environment and Department keys on all resources OPTIONAL ENHANCEMENTS (If time permits):  Add for S3 and DynamoDB (OPTIONAL: ) - reduces NAT Gateway costs  Create Transit Gateway attachment (OPTIONAL: Transit Gateway) - enables multi-VPC connectivity  Add AWS Network Firewall (OPTIONAL: Network Firewall) - provides advanced threat protection Expected output: A complete CloudFormation JSON template that deploys a production-ready VPC with high-availability networking components across 3 availability zones, including all routing, security, and logging configurations.",Your organization is establishing a new AWS presence for a financial services application that requires strict network isolation and compliance controls. The infrastructure must support both public-facing web services and private backend systems with controlled internet access. You need to build a production-ready network foundation that can support future growth while maintaining security standards.,"""Production multi-AZ network foundation deployed in us-east-1 across 3 availability zones. Core networking components include VPC with custom CIDR, Internet Gateway for public access, NAT Gateways for private subnet outbound traffic, and VPC Flow Logs for compliance. Requires AWS CLI configured with appropriate permissions for VPC, EC2, and CloudWatch services. Infrastructure designed for high availability with separate public and private subnet tiers, supporting both internet-facing and internal workloads with strict security controls.""","[""VPC CIDR must be 10.0.0.0/16 with no overlapping ranges"", ""Each availability zone must have exactly one public and one private subnet"", ""NAT Gateways must be deployed in high-availability mode across all AZs"", ""All private subnets must route outbound traffic through NAT Gateways in the same AZ"", ""VPC Flow Logs must be enabled and sent to CloudWatch Logs with 30-day retention"", ""Network ACLs must explicitly deny all traffic except required ports (80, 443, 22 from specific IPs)"", ""All resources must use Cost Allocation Tags with Environment and Department keys"", ""Internet Gateway and NAT Gateway names must follow pattern: {ResourceType}-{AZ}-{Environment}"", ""Template must use Mappings for subnet CIDR blocks to ensure consistency across regions""]"
v1u4g8,,CloudFormation,JSON,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a CloudFormation template to establish a multi-environment database replication system. MANDATORY REQUIREMENTS (Must complete): 1. Deploy RDS Aurora MySQL clusters in three separate VPCs representing dev, staging, and prod environments (CORE: RDS Aurora) 2. Implement Lambda functions to synchronize schema changes and reference data between environments (CORE: Lambda) 3. Create buckets for storing database migration scripts with versioning enabled (CORE: ) 4. Configure cross-account assume roles for Lambda to access resources in different environments 5. Set up VPC peering connections between environments with appropriate route tables 6. Implement CloudWatch alarms for replication lag exceeding 60 seconds 7. Use Parameter Store for storing database connection strings 8. Enable encryption at rest for all databases using AWS KMS with separate keys per environment 9. Configure automated backups with 7-day retention for all Aurora clusters OPTIONAL ENHANCEMENTS (If time permits):  Add EventBridge rules to trigger sync on schedule (OPTIONAL: EventBridge) - enables automated synchronization  Implement notifications for sync failures (OPTIONAL: ) - improves incident response  Add Step Functions for orchestrating complex migrations (OPTIONAL: Step Functions) - handles multi-step workflows Expected output: A CloudFormation template that deploys a complete multi-environment database replication infrastructure with automated synchronization capabilities while maintaining strict environment isolation.","A financial services company needs to maintain identical database schemas and reference data across development, staging, and production environments. They require automated synchronization of non-sensitive configuration data while ensuring production data remains isolated.","""Multi-environment AWS deployment across three separate accounts (dev, staging, prod) in us-east-1 region. Each environment requires its own VPC with private subnets across 2 AZs, RDS Aurora MySQL 5.7 compatible clusters, Lambda functions with Python 3.9 runtime, and buckets for migration artifacts. VPC CIDR blocks: dev (10.1.0.0/16), staging (10.2.0.0/16), prod (10.3.0.0/16). Requires AWS CLI configured with cross-account access, CloudFormation StackSets permissions. KMS keys must be created separately in each account before deployment.""","[""Aurora clusters must use db.r5.large instances minimum for consistent performance"", ""Lambda functions must complete synchronization within 5-minute timeout"", ""VPC peering connections must restrict traffic to MySQL port 3306 only"", "" buckets must have lifecycle policies to delete migration scripts older than 30 days"", ""Parameter Store values must be encrypted using KMS customer managed keys"", ""Lambda functions must log all synchronization activities to CloudWatch Logs"", ""Database passwords must be generated using with automatic rotation"", ""Cross-account roles must follow least-privilege principle with explicit resource ARNs"", ""Template must use Conditions to handle environment-specific configurations""]"
p6a2z0,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a template to deploy a production-ready multi-tier web application infrastructure. The configuration must: 1. Create a VPC with 3 public subnets, 3 private subnets, and 3 database subnets across 3 AZs. 2. Deploy an Application Load Balancer in public subnets with path-based routing to different target groups. 3. Configure Auto Scaling Groups in private subnets with Launch Templates using Amazon Linux 2 AMI. 4. Set up Aurora MySQL cluster with one writer and two reader instances in database subnets. 5. Implement ElastiCache Redis cluster with multi-AZ failover for session storage. 6. Create CloudWatch dashboards displaying application metrics and custom alarms. 7. Configure integration for application configuration. 8. Implement AWS Secrets Manager for database credentials with automatic rotation. 9. Set up VPC Flow Logs and AWS Config rules for compliance monitoring. Expected output: A single JSON template with proper parameters, mappings, conditions, and outputs that deploys the complete infrastructure stack with all security and compliance requirements met.",A financial technology company needs to deploy a multi-tier web application that processes payment transactions. The application requires high availability across multiple availability zones with strict security requirements for PCI compliance. The infrastructure must support auto-scaling based on transaction volume and provide complete isolation between web and application tiers.,"""Production multi-tier infrastructure deployed in us-east-1 across 3 availability zones. Core services include Application Load Balancer for web tier, Auto Scaling Groups with EC2 instances running Amazon Linux 2, Aurora MySQL cluster for database tier, ElastiCache Redis for session management. VPC with public subnets for ALB, private subnets for EC2 instances, and database subnets for /ElastiCache. NAT Gateways in each AZ for outbound internet access from private subnets. Requires AWS CLI 2.x configured with appropriate IAM permissions. Stack designed for high-availability payment processing workloads with PCI compliance requirements.""","[""All resources must use specific naming convention: {Environment}-{ResourceType}-{Purpose}"", "" instances must use encrypted storage with customer-managed KMS keys"", ""Auto Scaling Groups must scale based on custom CloudWatch metrics, not default CPU"", ""All EC2 instances must use IMDSv2 (Instance Metadata Service Version 2)"", ""Security groups must follow least-privilege with no inbound rules from 0.0.0.0/0"", ""Load balancers must terminate SSL/TLS with ACM certificates only"", ""All logs must be centralized to a single CloudWatch Log Group with 30-day retention"", ""Stack must support blue-green deployments via parameter switches"", ""All IAM roles must use external ID for cross-account access scenarios""]"
b2u4w3,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a serverless cryptocurrency price alert system. MANDATORY REQUIREMENTS (Must complete): 1. Create a DynamoDB table named 'PriceAlerts' with partition key 'userId' (String) and sort key 'alertId' (String) (CORE: DynamoDB) 2. Deploy a Lambda function 'ProcessPriceChecks' with Node.js 18 runtime on ARM architecture, 512MB memory (CORE: Lambda) 3. Configure the Lambda with 100 reserved concurrent executions 4. Create an SNS topic 'PriceAlertNotifications' with server-side encryption 5. Set up CloudWatch Logs group with 30-day retention for the Lambda function 6. Create a customer-managed KMS key for encrypting Lambda environment variables 7. Enable point-in-time recovery on the DynamoDB table 8. Implement IAM roles with explicit resource ARNs (no wildcards) 9. Add CloudFormation outputs for Lambda function ARN, DynamoDB table name, and SNS topic ARN 10. Tag all resources with 'Environment': 'Production' and 'Service': 'PriceAlerts' OPTIONAL ENHANCEMENTS (If time permits):  Add SQS queue for buffering high-volume price updates (OPTIONAL: SQS) - improves reliability during traffic spikes  Implement API Gateway for manual alert management (OPTIONAL: API Gateway) - enables user self-service  Add rule for scheduled price checks (OPTIONAL: ) - automates periodic monitoring Expected output: A complete CloudFormation JSON template that deploys the serverless price alert infrastructure with all security controls, proper IAM permissions, and monitoring configured. The stack should be production-ready with encryption, logging, and recovery mechanisms in place.",A financial technology startup needs to process cryptocurrency price alerts in real-time. The system must handle thousands of price threshold checks per minute while maintaining strict latency requirements for alert notifications.,"""Production serverless infrastructure deployed in us-east-1 for cryptocurrency price alert processing. Core services include Lambda functions for price monitoring and alert processing, DynamoDB for storing user preferences and alert thresholds, SNS for multi-channel notifications. No VPC required as all services are AWS-managed. Lambda functions use Node.js 18 runtime on ARM architecture. Customer-managed KMS keys for encryption. CloudWatch Logs for monitoring with 30-day retention. Deployment via CloudFormation JSON templates with drift detection enabled.""","[""Lambda functions must have reserved concurrent executions set to prevent throttling"", ""DynamoDB tables must use pay-per-request billing mode"", ""All Lambda environment variables must be encrypted with customer-managed KMS keys"", ""SNS topics must have server-side encryption enabled"", ""Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""CloudWatch Logs retention must be set to exactly 30 days"", ""DynamoDB point-in-time recovery must be enabled"", ""All IAM roles must follow least privilege with no wildcard resource permissions""]"
f4t3a0,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a complete CI/CD pipeline for Node.js microservices. MANDATORY REQUIREMENTS (Must complete): 1. Configure with GitHub source, build, and deploy stages (CORE: ) 2. Create project with ARM compute type and Node.js 18 runtime (CORE: ) 3. Set up S3 artifact bucket with customer-managed KMS encryption 4. Implement build caching for node_modules using S3 cache location 5. Add manual approval action between staging and production deployments 6. Configure rule to send notifications on pipeline failures 7. Create IAM roles with specific resource ARNs (no wildcards) 8. Set CloudWatch Logs retention to 30 days for all log groups 9. Support parallel builds by accepting microservice names as parameters OPTIONAL ENHANCEMENTS (If time permits):  Add for blue/green ECS deployments (OPTIONAL: ) - enables zero-downtime deployments  Implement Lambda function for custom notifications (OPTIONAL: Lambda) - provides richer notification formatting  Add for build secrets (OPTIONAL: ) - improves secret management Expected output: A CloudFormation JSON template that creates a fully functional CI/CD pipeline with GitHub integration, automated builds, manual approval gates, and deployment capabilities to ECS Fargate environments.","A software development team needs automated CI/CD infrastructure to build, test, and deploy their Node.js microservices. The pipeline must integrate with GitHub webhooks, run automated tests, and deploy to ECS Fargate containers across development and production environments.","""Multi-environment CI/CD infrastructure deployed in us-east-1 region using for orchestration, for build/test execution, and for ECS Fargate deployments. Requires VPC with private subnets for build environments, NAT Gateway for outbound traffic. S3 buckets for artifact storage with KMS encryption. for pipeline notifications. GitHub integration via OAuth. Development and production environments with separate ECS clusters.""","["" must use GitHub OAuth connection, not personal access tokens"", "" projects must use ARM-based compute for cost optimization"", ""All build artifacts must be encrypted with customer-managed KMS keys"", ""Pipeline must include manual approval stage before production deployment"", "" must cache node_modules using S3 to reduce build times"", ""Use rules to trigger notifications on pipeline state changes"", ""Implement least-privilege IAM policies with no wildcard resource permissions"", ""All logs must be retained for exactly 30 days for compliance"", ""Pipeline must support parallel builds for multiple microservices""]"
v6m5d4,,CloudFormation,JSON,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CloudFormation template to deploy a multi-region disaster recovery infrastructure for payment processing. MANDATORY REQUIREMENTS (Must complete): 1. Deploy Aurora Global Database cluster in us-east-1 as primary with read replica in us-west-2 (CORE: Aurora) 2. Create Lambda functions in both regions for payment processing with 1GB memory allocation (CORE: Lambda) 3. Configure Route 53 hosted zone with health checks and failover routing policies 4. Set up CloudWatch alarms for database replication lag exceeding 1 second 5. Implement SNS topics in both regions for failover notifications 6. Create IAM roles with least-privilege access for Lambda to Aurora 7. Configure Lambda reserved concurrent executions at 100 to guarantee capacity 8. Enable point-in-time recovery for Aurora with 7-day retention 9. Set deletion_protection to false for all database resources 10. Output primary and secondary endpoints for application configuration OPTIONAL ENHANCEMENTS (If time permits):  Add DynamoDB Global Tables for session state management (OPTIONAL: DynamoDB) - provides stateful failover  Implement for orchestrating complex payment workflows (OPTIONAL: ) - adds transaction orchestration  Configure AWS Backup for additional protection (OPTIONAL: AWS Backup) - provides centralized backup management Expected output: A CloudFormation template that deploys a complete multi-region disaster recovery solution with automated failover capabilities, meeting the 15-minute RTO and near-zero RPO requirements.","A financial services company requires a disaster recovery solution for their critical payment processing application. The application must maintain 99.99% availability and recover from regional failures within 15 minutes. The company processes over 10,000 transactions per hour and cannot afford data loss.","""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary). Utilizes Aurora Global Database for near-instantaneous data replication, Lambda functions for payment processing logic, and Route 53 for DNS failover. VPC in each region with private subnets across 3 availability zones. Requires AWS CLI configured with appropriate permissions. CloudWatch cross-region monitoring with centralized alarm management. Total infrastructure supports 10,000+ transactions per hour with sub-second latency requirements.""","[""RTO (Recovery Time Objective) must be under 15 minutes"", ""RPO (Recovery Point Objective) must be near-zero with cross-region replication"", ""Primary region must be us-east-1 with failover to us-west-2"", ""Database must use Aurora Global Database with automated backups"", ""Lambda functions must be deployed in both regions with identical configurations"", ""Route 53 health checks must trigger automatic failover"", ""All resources must have deletion protection disabled for testing purposes"", ""CloudWatch alarms must notify on failover events via SNS""]"
l8f0y2,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy a secure data processing pipeline for financial transactions. The configuration must: 1. Create a VPC with 3 private subnets across different AZs and no public subnets. 2. Deploy an RDS Aurora PostgreSQL cluster with encryption enabled and SSL enforcement. 3. Create Lambda functions that process data from and write to RDS (use inline Python code). 4. Configure buckets with SSE-KMS encryption using customer-managed CMKs. 5. Set up Secrets Manager to store RDS credentials with automatic rotation disabled. 6. Create for and DynamoDB services. 7. Implement CloudWatch Log groups with KMS encryption for all Lambda logs. 8. Configure IAM roles with explicit permissions (no wildcards) for Lambda execution. 9. Set up security groups allowing only necessary traffic between components. 10. Add CloudFormation outputs for resource ARNs and endpoints. Expected output: A complete CloudFormation template in JSON format that creates a fully functional, highly secure data processing environment meeting all PCI-DSS related requirements with comprehensive encryption and access controls.","A financial services company requires a secure data processing environment for handling sensitive customer information. The infrastructure must comply with PCI-DSS requirements and implement defense-in-depth security controls. All data must be encrypted at rest and in transit, with comprehensive logging for audit purposes.","""Highly secure financial data processing environment deployed in us-east-1 across 3 availability zones. Uses RDS Aurora PostgreSQL with encryption, Lambda functions in private subnets, and for encrypted data storage. VPC with only private subnets, for and DynamoDB, NAT instances for controlled outbound traffic. Requires AWS CLI configured with appropriate permissions. All resources must be tagged with Environment, DataClassification, and CostCenter tags. CloudFormation must create KMS keys for encryption, Secrets Manager secrets, and comprehensive CloudWatch log groups with encryption.""","[""All buckets must use SSE-KMS encryption with customer-managed CMKs"", ""VPC must have no public subnets - all resources must be private"", ""RDS instances must use encrypted storage and enforce SSL connections"", ""Lambda functions must use for AWS service calls"", ""CloudWatch Logs must be encrypted with KMS keys"", ""All IAM roles must follow least-privilege with no wildcard permissions"", ""Security groups must use explicit port ranges with no 0.0.0.0/0 ingress"", ""All resources must have deletion protection disabled for testing"", ""Must use AWS Secrets Manager for all database credentials""]"
h8p7g8,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to refactor an existing transaction processing infrastructure. The configuration must: 1. Convert hardcoded Lambda memory (3008MB) and timeout (900s) values to parameters with validation rules. 2. Replace polling Lambda that checks S3 every minute with EventBridge rule triggered by S3 events. 3. Fix circular dependency where Lambda needs S3 bucket ARN and S3 notification needs Lambda ARN. 4. Consolidate three separate inline IAM policies into one managed policy with least-privilege permissions. 5. Implement environment-based conditional logic to use on-demand DynamoDB for dev/staging and provisioned for production. 6. Replace custom Lambda that deletes old S3 objects with native S3 lifecycle configuration. 7. Add comprehensive tagging using a parameter-based tag mapping for cost allocation. 8. Ensure all stateful resources have Retain DeletionPolicy and proper UpdateReplacePolicy. 9. Parameterize table names, bucket names, and function names with environment prefixes. Expected output: An optimized CloudFormation JSON template that maintains the same infrastructure functionality while implementing all refactoring requirements, with proper parameters, conditions, and outputs sections.","A financial services company has an existing CloudFormation stack that was hastily assembled during a proof-of-concept phase. The stack deploys a transaction processing system but suffers from inefficiencies, hardcoded values, and lack of proper parameterization. The DevOps team needs to refactor this infrastructure code to meet production standards while maintaining the same functionality.","""Production AWS environment in us-east-1 region with existing CloudFormation stack managing transaction processing infrastructure. Current stack includes Lambda functions, DynamoDB tables, S3 buckets, and EventBridge rules. The environment spans three AWS accounts (dev, staging, prod) with cross-account IAM roles. VPC already exists with private subnets across 3 availability zones. Stack currently uses CloudFormation JSON format and needs optimization while maintaining backward compatibility with existing resources.""","[""Must convert all hardcoded values to parameters with appropriate constraints and defaults"", ""Implement proper resource tagging strategy using a tag mapping parameter"", ""Replace inefficient Lambda polling with EventBridge rule triggers"", ""Consolidate duplicate IAM policies into reusable managed policies"", ""Fix circular dependency between S3 bucket and Lambda function"", ""Implement conditional resource creation based on environment type parameter"", ""Replace custom Lambda for S3 lifecycle with native S3 lifecycle rules"", ""Optimize DynamoDB capacity settings using on-demand billing for non-production"", ""Add proper DeletionPolicy and UpdateReplacePolicy to all stateful resources""]"
m4n9o3,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a template to deploy an infrastructure compliance analysis system. The configuration must: 1. Set up AWS Config with custom rules to check for required tags (Cost Center, Environment) on all EC2 instances and RDS databases. 2. Create Lambda functions to analyze security group rules for overly permissive ingress (0.0.0.0/0). 3. Deploy DynamoDB table with on-demand billing to store compliance scan results with 30-day TTL. 4. Configure S3 bucket with versioning and lifecycle policies to archive reports after 90 days. 5. Set up distribution with OAI for secure report access. 6. Implement EventBridge rule to trigger daily compliance scans at 2 AM UTC. 7. Create cross-account IAM roles for scanning dev, staging, and prod accounts. 8. Configure topic with email subscription for critical violations. 9. Generate JSON-formatted compliance reports with pass/fail status per resource. 10. Implement Lambda function to aggregate results and calculate compliance percentage. Expected output: A JSON template that deploys a fully automated compliance analysis system capable of scanning multiple AWS accounts, identifying policy violations, and generating distributable compliance reports with historical tracking.","A financial services company needs to audit their AWS infrastructure for compliance with internal security policies. They require an automated system that analyzes deployed resources, identifies configuration drift, and generates compliance reports. The audit must check for proper tagging, encryption settings, and resource configurations across multiple AWS accounts.","""Multi-account AWS infrastructure analysis environment deployed in us-east-1. Uses AWS Config for resource discovery, Lambda for custom compliance rules, DynamoDB for storing compliance history, S3 for report storage, and for report distribution. Requires cross-account IAM roles for scanning production, staging, and development accounts. VPC not required as services are serverless. EventBridge schedules daily scans at 2 AM UTC. sends alerts to security team for critical violations. All data encrypted with customer-managed KMS keys.""","[""Use AWS Config for resource inventory and compliance tracking"", ""Implement Lambda functions in Python 3.9 for custom compliance rules"", ""Store compliance results in DynamoDB with point-in-time recovery enabled"", ""Generate reports using S3 and for secure distribution"", ""Use EventBridge for scheduling daily compliance scans"", ""Implement cross-account role assumption for multi-account scanning"", ""All Lambda functions must have 256MB memory allocation"", ""Use KMS customer-managed keys for all encryption"", ""Configure topics for critical compliance violations"", ""All resources must have Cost Center and Environment tags""]"
z2g1m7,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy an Amazon EKS cluster with hybrid node group architecture.

MANDATORY REQUIREMENTS (Must complete):
1. Create an EKS cluster version 1.28 with private API endpoint access only (CORE: EKS)
2. Deploy one managed node group with t3.medium instances (min: 2, max: 6) (CORE: EC2)
3. Deploy one self-managed node group using Launch Templates with m5.large instances
4. Configure OIDC provider for IRSA (IAM Roles for Service Accounts)
5. Enable envelope encryption using AWS KMS customer-managed key
6. Create dedicated VPC with 3 private subnets across different AZs
7. Configure cluster logging for api, audit, and controllerManager
8. Tag all resources with Environment=production and ManagedBy=CloudFormation

OPTIONAL ENHANCEMENTS (If time permits):
 Add AWS Load Balancer Controller using IAM role (OPTIONAL: IAM) - enables Kubernetes Ingress
 Configure Fargate profile for system workloads (OPTIONAL: Fargate) - reduces node management overhead
 Add CloudWatch Container Insights (OPTIONAL: CloudWatch) - provides cluster-level monitoring

Expected output: A CloudFormation JSON template that creates a production-ready EKS cluster with both managed and self-managed node groups, proper networking isolation, and security configurations meeting compliance requirements.",A fintech startup needs to deploy a production-grade Kubernetes cluster on AWS to host their microservices architecture. The cluster must support both managed and self-managed node groups with specific instance types and scaling policies. Security compliance requires private endpoint access and encryption at rest.,"""Production EKS infrastructure deployed in us-east-1 across 3 availability zones. Requires EKS cluster v1.28 with mixed node group strategy - managed nodes using t3.medium and self-managed nodes using m5.large EC2 instances. VPC with private subnets only, no internet-facing resources. KMS encryption enabled for ethos storage. CloudFormation stack requires IAM permissions for EKS, EC2, VPC, KMS, and IAM service operations. OIDC provider configuration enables pod-level AWS service access.""","[""All node groups must use Amazon Linux 2 EKS-optimized AMIs"", ""The KMS key must have automatic rotation enabled"", ""Self-managed nodes must use IMDSv2 with hop limit of 1"", ""Security groups must deny all ingress except from within the VPC CIDR"", ""The cluster service role must not have AdministratorAccess policy"", ""Node instance profiles must use least-privilege policies without wildcards""]"
l4f0r8,,CloudFormation,JSON,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CloudFormation template to implement an active-passive disaster recovery architecture for a PostgreSQL database across two AWS regions. MANDATORY REQUIREMENTS (Must complete): 1. Deploy Aurora PostgreSQL Global Database with primary cluster in us-east-1 and secondary in us-west-2 (CORE: Aurora) 2. Configure Route53 health checks with automated DNS failover between regions (CORE: Route53) 3. Create Lambda function to promote secondary cluster during failover events (CORE: Lambda) 4. Enable automated backups with 7-day retention and point-in-time recovery 5. Configure cross-region read replicas with lag monitoring alarms 6. Implement parameter groups ensuring synchronous replication priority 7. Set up topics in both regions for failover notifications 8. Configure IAM roles with cross-region assume permissions for Lambda 9. Enable encryption at rest using AWS-managed keys in each region 10. Tag all resources with Environment=DR and CostCenter=Infrastructure OPTIONAL ENHANCEMENTS (If time permits):  Add EventBridge rules to automate failback procedures (OPTIONAL: EventBridge) - enables automated recovery workflows  Implement for orchestrated failover testing (OPTIONAL: ) - provides controlled DR testing  Deploy for global database endpoint caching (OPTIONAL: ) - reduces latency for read operations Expected output: CloudFormation template in JSON format that deploys a fully automated disaster recovery solution with primary database in us-east-1 and standby in us-west-2, including health monitoring and automatic DNS failover capabilities.",A financial services company requires a disaster recovery solution for their critical PostgreSQL database that can withstand regional AWS outages. The system must maintain RPO of 1 hour and RTO of 15 minutes while minimizing costs during normal operations.,"""Multi-region AWS deployment spanning us-east-1 (primary) and us-west-2 (secondary) for disaster recovery. Infrastructure includes Aurora PostgreSQL Global Database clusters, Route53 hosted zones with health checks, Lambda functions for failover automation. Requires AWS CLI configured with credentials having permissions for multi-region resource creation. VPCs in both regions with private database subnets across 3 AZs each, VPC peering for cross-region replication traffic. keys for encryption, topics for alerting, and CloudWatch alarms for monitoring replication lag. Minimum AWS account limits: 2 VPCs per region, 10 instances total.""","[""Must use Aurora Global Database feature for cross-region replication"", ""Route53 health checks must detect database failures within 30 seconds"", ""Lambda function code must be inline within the template, not referenced from S3"", ""Database subnet groups must span at least 3 availability zones in each region"", ""All database connections must use SSL/TLS encryption"", ""CloudFormation stack must be deployable using standard AWS CLI without custom resources"", ""Parameter groups must enforce synchronous_commit for critical write operations"", ""Backup window must not overlap with peak business hours (9 AM - 5 PM EST)"", ""Security groups must restrict database access to application subnets only"", ""Total monthly cost estimate must stay under $2000 for the complete DR setup""]"
t6t5u9,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,Create a CloudFormation template to deploy a containerized payment processing system using ECS Fargate. MANDATORY REQUIREMENTS (Must complete): 1. Create an ECS cluster with capacity providers set to FARGATE and FARGATE_SPOT (CORE: ECS) 2. Deploy RDS Aurora PostgreSQL Serverless v2 cluster with minimum 0.5 ACUs for transaction storage (CORE: RDS) 3. Configure Application Load Balancer with target group health checks every 30 seconds (CORE: ALB) 4. Define ECS task definition with 2 vCPU and 4GB memory for payment processor container 5. Create ECS service with desired count of 3 tasks across multiple AZs 6. Implement VPC with private subnets in 3 AZs for ECS tasks and database 7. Configure service auto-scaling policy to scale between 3-10 tasks based on CPU utilization 8. Set up CloudWatch log groups with 30-day retention for container logs 9. Create task execution role with permissions for ECR image pull and CloudWatch logging 10. Enable deletion protection on RDS cluster and set retention to 7 days 11. Configure security groups allowing only ALB to communicate with ECS tasks on port 8080 12. Output the ALB DNS name and RDS cluster endpoint for application configuration OPTIONAL ENHANCEMENTS (If time permits):  Add integration for database credentials (OPTIONAL: ) - improves security posture  Implement sidecar container for distributed tracing (OPTIONAL: ) - enhances debugging capabilities  Configure topic for auto-scaling notifications (OPTIONAL: ) - provides operational visibility Expected output: A complete CloudFormation JSON template that provisions the entire ECS Fargate infrastructure with Aurora Serverless backend. The template should be deployment-ready and follow AWS best practices for container orchestration.,A fintech startup needs to deploy their payment processing microservices on AWS using container orchestration. The system processes sensitive financial transactions and requires strict network isolation between components. They've chosen ECS Fargate for serverless container management to reduce operational overhead.,"""Production-grade container orchestration infrastructure deployed in us-east-1 across 3 availability zones. Core services include ECS Fargate for container management, Aurora Serverless v2 PostgreSQL for data persistence, and Application Load Balancer for traffic distribution. VPC spans 10.0.0.0/16 with 6 subnets (3 public for ALB, 3 private for ECS/RDS). Requires AWS CLI configured with appropriate permissions. Task containers pulled from ECR repository. Auto-scaling configured for handling variable transaction loads between 3-10 container instances.""","[""Use only FARGATE and FARGATE_SPOT capacity providers - no EC2 instances"", ""RDS Aurora cluster must use Serverless v2 with PostgreSQL 15.x engine"", ""All ECS tasks must run in private subnets with no direct internet access"", ""Container images must be referenced using ECR ARN format, not Docker Hub"", ""Task definition must specify both memory and CPU at the task level, not container level"", ""Security groups must follow least-privilege principle with explicit port mappings"", ""Use CloudFormation intrinsic functions for all cross-resource references"", ""Enable ContainerInsights on the ECS cluster for enhanced monitoring""]"
s0a4d2,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy a centralized observability platform for multi-account monitoring. The configuration must: 1. Create a CloudWatch Logs destination in the central account that accepts logs from 5 specified AWS accounts (CORE: CloudWatch Logs). 2. Deploy Lambda functions to process incoming log streams and extract custom metrics from JSON payloads (CORE: Lambda). 3. Configure cross-account IAM roles with least-privilege policies for log streaming. 4. Set up CloudWatch composite alarms that monitor multiple metrics across accounts. 5. Create SNS topics with email and HTTPS subscriptions for critical alerts. 6. Implement CloudWatch dashboards showing aggregated metrics from all accounts. 7. Configure metric math expressions to calculate business KPIs from raw metrics. 8. Set up log retention policies that vary by log group pattern (7 days for debug, 30 days for errors). 9. Create custom CloudWatch metrics using the embedded metric format from Lambda. 10. Implement dead letter queues for failed metric processing. Expected output: A complete CloudFormation JSON template that establishes a production-ready observability platform capable of monitoring distributed applications across multiple AWS accounts with automated alerting and visualization.","A financial services company needs centralized observability for their distributed microservices architecture. The platform must aggregate metrics, logs, and traces from multiple AWS accounts while maintaining strict data retention policies for compliance. The solution should provide real-time alerting for critical business metrics and support custom dashboards for different teams.","""Multi-account observability infrastructure deployed in us-east-1 as the central monitoring region. Uses CloudWatch for metrics aggregation, CloudWatch Logs for centralized logging with cross-account log destinations, Lambda for metric processing, SNS for alerting, and KMS for encryption. Requires CloudFormation JSON templates, AWS CLI 2.x configured with appropriate permissions. The infrastructure spans 5 production accounts plus 1 central monitoring account within an AWS Organization. VPC endpoints required for private CloudWatch API access from isolated subnets.""","[""All CloudWatch Logs must use KMS encryption with customer-managed keys"", ""Metric data retention must be exactly 15 days for standard resolution and 90 days for high-resolution metrics"", ""Cross-account access must use AWS Organizations and not require manual IAM role creation"", ""SNS topics must have server-side encryption enabled using AWS managed keys"", ""Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""All resources must be tagged with CostCenter, Environment, and Owner tags"", ""CloudWatch dashboards must auto-refresh every 5 minutes during business hours only"", ""Metric filters must extract numerical values from JSON-formatted log entries"", ""Alarms must implement a 3-strike evaluation policy before triggering notifications"", ""Log groups must have a specific naming convention: /aws/application/{account-id}/{service-name}""]"
q5f6i6,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,Create a CloudFormation template to migrate a containerized payment processing system from single-AZ development to multi-AZ production environment. MANDATORY REQUIREMENTS (Must complete): 1. Deploy ECS Fargate cluster with services across 3 availability zones for production (CORE: ECS). 2. Create RDS Aurora PostgreSQL cluster with read replicas and automated failover (CORE: RDS). 3. Implement blue-green deployment strategy using for zero-downtime migration (CORE: ). 4. Configure parameter-based environment switching (dev/staging/prod) using mappings and conditions. 5. Set up cross-environment data migration with AWS Database Migration Service task. 6. Create separate security groups for each environment with strict ingress rules. 7. Implement CloudWatch alarms for migration monitoring with notifications. 8. Configure automated backups with 30-day retention for production database. OPTIONAL ENHANCEMENTS (If time permits):  Add for centralized management (OPTIONAL: ) - improves disaster recovery.  Implement health checks for automatic failover (OPTIONAL: ) - adds DNS-level resilience.  Configure for integration (OPTIONAL: ) - centralizes configuration management. Expected output: A parameterized CloudFormation template that supports environment-based deployment with built-in migration capabilities and rollback support.,A financial services company needs to migrate their payment processing system from development to production with zero downtime. The system consists of containerized microservices and a PostgreSQL database currently running in a single-AZ development environment.,"""Multi-AZ infrastructure deployment across us-east-1 region with ECS Fargate for container orchestration, RDS Aurora PostgreSQL Multi-AZ cluster, and for blue-green deployments. Requires AWS CLI 2.x configured with appropriate IAM permissions. VPC spans 3 availability zones with public and private subnets. NAT Gateways in each AZ for high availability. Application Load Balancer distributes traffic across ECS tasks. Production environment uses dedicated placement groups and enhanced monitoring.""","[""Template must use CloudFormation conditions to deploy different configurations based on Environment parameter"", ""All resources must be tagged with Environment, CostCenter, and MigrationPhase tags"", ""Database migration must preserve all indexes and constraints during the transfer"", ""Security groups must restrict database access to only ECS tasks within the same VPC"", ""Template must include DeletionPolicy: Snapshot for all stateful resources"", ""Blue-green deployment must maintain session affinity during traffic shifting""]"
t0o3t9,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a template to deploy a three-tier loan processing application with strict security requirements. MANDATORY REQUIREMENTS (Must complete): 1. Create a VPC with 3 public and 3 private subnets across 3 AZs (CORE: VPC) 2. Deploy an Auto Scaling Group with 2-4 EC2 instances for the web tier in private subnets 3. Deploy an Auto Scaling Group with 2-6 EC2 instances for the API tier in private subnets 4. Create an Aurora MySQL cluster with one writer and one reader instance (CORE: ) 5. Configure an Application Load Balancer in public subnets with target groups for web and API tiers 6. Implement CloudWatch Logs for application logging with 30-day retention 7. Create buckets for static assets and audit logs with versioning enabled 8. Configure IAM roles for EC2 instances with permissions for , CloudWatch, and Secrets Manager 9. Set up for and 10. Implement all constraints listed above OPTIONAL ENHANCEMENTS (If time permits):  Add AWS WAF to the Application Load Balancer (OPTIONAL: WAF) - provides protection against common web exploits  Implement Redis cluster for session management (OPTIONAL: ) - improves application performance  Add AWS Config rules for compliance monitoring (OPTIONAL: Config) - ensures ongoing compliance with security policies Expected output: A complete JSON template that deploys all infrastructure components with proper dependencies, outputs for key resource identifiers, and parameters for customizable values like instance types and database passwords.","A financial technology startup needs to deploy their loan processing application in AWS with strict compliance requirements for data isolation and audit logging. The application consists of a web frontend, API backend, and batch processing components that must run in separate security zones.","""Production-grade multi-tier infrastructure deployed in us-east-1 region across 3 availability zones. Core services include VPC with public/private subnet tiers, Application Load Balancer, Auto Scaling Groups for web and API tiers, Aurora MySQL cluster, for static assets and logs, CloudWatch for monitoring. Requires AWS CLI configured with appropriate permissions, CLI tools installed. Network architecture includes NAT Gateways for outbound internet access from private subnets, for and to avoid internet routing for AWS service calls.""","[""Use JSON format exclusively for the template"", ""Deploy all compute resources in private subnets only"", ""Implement separate security groups for each application tier with least-privilege rules"", ""Use Session Manager for EC2 access (no SSH keys)"", ""Enable VPC Flow Logs for all network traffic"", ""Configure bucket lifecycle policies to transition logs to Glacier after 90 days"", ""Use AWS Secrets Manager for all database credentials"", ""Implement CloudWatch Alarms for CPU utilization above 80%"", ""Enable deletion protection on the instance"", ""Use only AWS-managed encryption keys (no custom KMS keys)""]"
m1j7v9,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy infrastructure that can be replicated across multiple environments with consistent configurations. MANDATORY REQUIREMENTS (Must complete): 1. Create an RDS MySQL instance with Multi-AZ deployment and automated backups (CORE: RDS). 2. Deploy a Lambda function for data processing with configurable memory based on environment (CORE: Lambda). 3. Set up a DynamoDB table for configuration storage with on-demand billing (CORE: DynamoDB). 4. Use nested stacks with a master template that accepts environment parameters. 5. Implement parameter mappings for environment-specific values (dev/staging/prod). 6. Configure cross-stack exports for resource sharing between stacks. 7. Enable encryption at rest for all data storage services. 8. Set up CloudWatch Logs with environment-specific retention periods. 9. Create condition functions to control resource creation based on environment. 10. Implement stack tags that propagate to all child resources. OPTIONAL ENHANCEMENTS (If time permits):  Add SNS topic for cross-environment notifications (OPTIONAL: SNS) - enables environment status alerts  Implement Systems Manager Parameter Store for secrets (OPTIONAL: SSM) - centralizes configuration management  Add CloudWatch Dashboards for environment comparison (OPTIONAL: CloudWatch) - improves visibility across environments. Expected output: A master CloudFormation template with nested stacks that can deploy identical infrastructure across multiple environments, with environment-specific parameters controlling resource sizing and configurations.","A financial services company needs to maintain identical infrastructure across development, staging, and production environments. They require automated environment replication with consistent configurations while allowing environment-specific parameters for database sizing and Lambda memory allocations.","""Multi-environment AWS deployment across us-east-1 (production), us-west-2 (staging), and eu-west-1 (development). Uses RDS MySQL 8.0 with Multi-AZ, Lambda with Python 3.9 runtime, and DynamoDB tables. Requires AWS CLI 2.x configured with appropriate IAM permissions for multi-region deployment. VPC configuration with public and private subnets in each region, with VPC peering for cross-environment communication. Each environment requires its own AWS account with cross-account IAM roles for centralized deployment. Minimum 3 availability zones per region for high availability.""","[""Master template must use nested stacks with at least 3 child stacks"", ""Environment parameter must accept only 'dev', 'staging', or 'prod' values"", ""RDS instance class must vary by environment: db.t3.micro (dev), db.t3.small (staging), db.m5.large (prod)"", ""Lambda memory must be 128MB (dev), 256MB (staging), 512MB (prod)"", ""All resources must have DeletionPolicy set to 'Retain' for production environment"", ""Stack outputs must include ARNs and endpoints for all major resources"", ""Use Fn::ImportValue for at least 2 cross-stack references"", ""CloudWatch Log retention must be 7 days (dev), 30 days (staging), 90 days (prod)"", ""All IAM roles must follow least-privilege principle with no wildcard actions"", ""Template must validate successfully in all three target regions""]"
j2w5y0,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a highly available payment processing web application. MANDATORY REQUIREMENTS (Must complete): 1. Deploy ECS Fargate service with auto-scaling (2-10 tasks) based on CPU utilization (CORE: ECS) 2. Create Aurora Serverless v2 PostgreSQL cluster with encrypted storage (CORE: RDS Aurora) 3. Configure Application Load Balancer with path-based routing and health checks 4. Implement WebACL with rate limiting rules (100 requests per 5 minutes per IP) 5. Create VPC with 3 AZs, private subnets for ECS/RDS, public subnets for ALB 6. Configure for database credentials 7. Implement CloudFormation custom resource for post-deployment health validation 8. Create CloudWatch dashboard with ECS, ALB, and Aurora metrics OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Cognito User Pool for authentication (OPTIONAL: Cognito) - provides managed user authentication  Implement Redis cluster for session storage (OPTIONAL: ) - improves application performance  Add for distributed tracing (OPTIONAL: ) - enhances debugging capabilities Expected output: A complete CloudFormation JSON template with nested stacks that deploys the entire payment processing infrastructure. The template should include outputs for ALB DNS name, CloudWatch dashboard URL, and stack deployment status.",A fintech startup needs to deploy their payment processing web application with strict compliance requirements for PCI-DSS. The application must handle high-volume transactions during peak hours while maintaining audit trails and ensuring data encryption at rest and in transit.,"""Production deployment in us-east-1 as primary region with us-west-2 configured for disaster recovery readiness. Infrastructure includes Application Load Balancer with integration, ECS Fargate cluster running containerized Node.js application, Aurora Serverless v2 PostgreSQL database with encryption enabled. VPC spans 3 availability zones with private subnets for ECS tasks and database, public subnets for ALB. NAT Gateways provide outbound internet access. Requires AWS CLI 2.x configured with administrative permissions. CloudFormation StackSets must be enabled in the account.""","[""Use JSON format exclusively for the CloudFormation template"", ""Implement CloudFormation custom resources for Lambda-based health checks"", ""Configure ALB with WebACL integration for DDoS protection"", ""Use Aurora Serverless v2 with minimum 0.5 ACUs and maximum 4 ACUs"", ""Implement CloudFormation StackSets for multi-region failover preparation"", ""Create nested stacks for modular resource organization"", ""Use CloudFormation drift detection compatible configurations only"", ""Implement CloudFormation wait conditions for ECS service stabilization""]"
n0y6d3,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy a serverless ETL pipeline for processing financial transaction data. MANDATORY REQUIREMENTS (Must complete): 1. Create state machine with parallel processing branches for CSV validation and transformation (CORE: ) 2. Deploy three Lambda functions: FileValidator (2GB memory), DataTransformer (3GB memory), and ReportGenerator (1GB memory) with Graviton2 architecture 3. Configure DynamoDB table 'TransactionState' with partition key 'jobId' and sort key 'timestamp' for tracking processing status (CORE: DynamoDB) 4. Set up S3 bucket with event notifications triggering the execution on CSV upload 5. Implement CloudWatch Logs groups with KMS encryption for all Lambda functions 6. Configure IAM execution roles with least-privilege policies for each service 7. Add CloudWatch alarms for execution failures and Lambda throttling 8. Enable tracing on and all Lambda functions for debugging OPTIONAL ENHANCEMENTS (If time permits):  Add SNS topic for failure notifications (OPTIONAL: SNS) - enables real-time alerting  Implement EventBridge rule for scheduled daily report generation (OPTIONAL: EventBridge) - automates reporting  Add FIFO queue for ordered transaction processing (OPTIONAL: ) - ensures processing order Expected output: A complete CloudFormation JSON template that deploys the serverless ETL pipeline with all Lambda functions, state machine, DynamoDB table, S3 bucket with notifications, and proper IAM configurations. The solution should handle concurrent file uploads and process transactions within 5-minute SLA windows.","A financial analytics company needs to process daily transaction reports from multiple bank partners. Each partner uploads CSV files to S3 containing millions of transaction records that must be validated, transformed, and aggregated for regulatory compliance reporting within strict SLA windows.","""Production ETL pipeline infrastructure deployed in us-east-1 across 3 availability zones. Core services include for orchestration, Lambda functions for data processing, DynamoDB for state management and S3 for data storage. configured for private S3 and DynamoDB access. Requires AWS CLI 2.x configured with appropriate IAM permissions for CloudFormation stack deployment. Infrastructure supports processing 50GB+ daily transaction volumes with sub-5-minute processing SLAs.""","[""Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""DynamoDB tables must use on-demand billing mode with point-in-time recovery enabled"", ""All Lambda functions must have reserved concurrent executions configured to prevent throttling"", ""S3 buckets must enable versioning and configure lifecycle policies for 90-day archival"", ""CloudWatch Logs must use KMS encryption with customer-managed keys"", "" state machine must implement exponential backoff retry logic for all tasks""]"
j2b8q9,,CloudFormation,JSON,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CloudFormation template to implement a multi-region disaster recovery architecture for transaction processing. MANDATORY REQUIREMENTS (Must complete): 1. Deploy Aurora PostgreSQL Global Database with primary cluster in us-east-1 (CORE: Aurora) 2. Create secondary Aurora cluster in us-west-2 with read replicas (CORE: Aurora) 3. Configure Route 53 hosted zone with health check-based failover routing 4. Deploy Lambda functions in both regions with environment-specific configurations 5. Set up cross-region replication monitoring with CloudWatch custom metrics 6. Create SNS topics for alerting on replication lag exceeding thresholds 7. Implement automated database failover with promotion of secondary to primary 8. Configure VPC peering between regions for secure database connectivity 9. Use for cross-region configuration sharing 10. Enable point-in-time recovery with 7-day retention on all databases OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Backup for additional database backup automation (OPTIONAL: AWS Backup) - provides centralized backup management  Implement EventBridge rules for automated failover orchestration (OPTIONAL: EventBridge) - enables event-driven recovery  Add for complex failover workflows (OPTIONAL: ) - orchestrates multi-step recovery processes Expected output: A CloudFormation template that creates a complete multi-region disaster recovery infrastructure with automated failover capabilities, meeting RTO/RPO requirements through Aurora Global Database and Route 53 health checks.",A financial services company requires a disaster recovery solution for their critical transaction processing system. They need automated failover capabilities with RTO under 5 minutes and RPO under 1 minute to meet regulatory compliance.,"""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (DR). Deploys Aurora PostgreSQL 15.x Global Database with encrypted storage, Lambda functions for transaction processing, and Route 53 failover routing. VPCs in both regions with private subnets across 3 AZs each, VPC peering for cross-region connectivity. Requires AWS CLI configured with appropriate permissions for multi-region deployments. CloudWatch cross-region monitoring with custom metrics for replication lag tracking.""","[""Use Aurora Global Database for cross-region replication with automated failover"", ""Implement Route 53 health checks with failover routing policy"", ""All databases must use encrypted storage with customer-managed KMS keys"", ""Lambda functions must have reserved concurrency of at least 100"", ""Cross-region replication lag monitoring must trigger SNS alerts if > 30 seconds""]"
y3v8k5,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy a secure data processing pipeline for handling sensitive payment card data. MANDATORY REQUIREMENTS (Must complete): 1. Create a VPC with private subnets across 3 availability zones (CORE: VPC) 2. Deploy Lambda function with 1GB memory for data validation in private subnet (CORE: Lambda) 3. Configure S3 bucket with SSE-KMS encryption using customer-managed CMK 4. Implement for S3 and KMS services 5. Create IAM execution role for Lambda with minimal permissions to read/write S3 6. Enable VPC flow logs with 90-day retention in CloudWatch Logs 7. Configure security groups allowing only HTTPS traffic between components 8. Add stack-level termination protection in template 9. Include mandatory tags DataClassification=PCI and ComplianceScope=Payment 10. Set DeletionPolicy to Retain for KMS key and S3 bucket OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Config rules for compliance monitoring (OPTIONAL: Config) - automates compliance checks  Implement SNS topic for security alerts (OPTIONAL: SNS) - enables real-time notifications  Add Parameter Store for secure config (OPTIONAL: ) - centralizes secret management Expected output: A CloudFormation JSON template that deploys a PCI-compliant data processing infrastructure with end-to-end encryption, network isolation, and comprehensive audit logging.","A financial services company needs to implement a secure data processing pipeline that complies with PCI-DSS requirements. The architecture must encrypt data at rest and in transit, enforce strict access controls, and maintain audit logs for all data access attempts.","""Highly secure multi-AZ deployment in us-east-1 for PCI-DSS compliant data processing. Infrastructure includes VPC with private subnets across 3 availability zones, S3 buckets with KMS encryption, Lambda functions in private subnets, and comprehensive CloudWatch logging. Requires AWS CLI configured with appropriate permissions, CloudFormation JSON template deployment capability. Network architecture includes for S3 and KMS to avoid internet traversal, NAT instances for controlled outbound access, and strict security group rules.""","[""All S3 buckets must use SSE-KMS encryption with customer-managed keys"", ""Lambda functions must run in private subnets with no direct internet access"", ""All IAM roles must follow least privilege principle with no wildcard actions"", ""VPC flow logs must be enabled and stored for 90 days minimum"", ""All security groups must have explicit egress rules with no 0.0.0.0/0 destinations"", ""CloudFormation stack must have termination protection enabled"", ""All resources must be tagged with DataClassification and ComplianceScope tags""]"
z0n1u1,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to optimize and fix an existing transaction processing infrastructure that suffers from deployment timeouts and update failures. The configuration must: 1. Split the monolithic template into nested stacks with clear separation of concerns (networking, compute, database). 2. Implement CloudFormation StackSets for consistent multi-region deployments. 3. Fix circular dependencies between RDS Aurora and Lambda functions using proper DependsOn logic. 4. Convert Lambda deployment packages to container images stored in ECR for faster updates. 5. Add rollback triggers monitoring RDS connection count and Lambda error rates. 6. Implement custom resources with 5-minute timeouts for database schema migrations. 7. Use CloudFormation conditions to skip unchanged resources during updates. 8. Configure deletion policies to retain RDS snapshots for 30 days. 9. Add CloudFormation outputs for all resource ARNs needed by other stacks. 10. Implement parameter validation using AllowedValues and constraint descriptions. Expected output: Optimized CloudFormation templates that deploy in under 15 minutes with proper error handling and rollback capabilities. The solution should demonstrate clear stack boundaries, efficient update mechanisms, and production-ready deployment patterns.","A financial services company deployed their transaction processing system using CloudFormation but is experiencing template update failures and resource drift. The current templates take over 45 minutes to deploy and frequently timeout during updates, causing production deployment delays.","""Production environment spanning us-east-1 and eu-west-1 regions for transaction processing workload. Current infrastructure includes RDS Aurora MySQL clusters, Lambda functions for transaction validation, DynamoDB tables for session management, and S3 buckets for audit logs. VPC setup with 3 availability zones per region, private subnets for databases, and VPC endpoints for AWS services. Requires AWS CLI 2.x and CloudFormation CLI for testing. Stack deployment uses CodePipeline with cross-region replication enabled.""","[""Must use JSON format for all CloudFormation templates"", ""Stack updates must complete within 15 minutes"", ""Use CloudFormation StackSets for multi-region deployments"", ""Implement proper DependsOn attributes to prevent circular dependencies"", ""All Lambda functions must use container images stored in ECR"", ""Database snapshots must be retained for 30 days after stack deletion"", ""Use CloudFormation Drift Detection compatible resources only"", ""Implement rollback triggers for failed deployments"", "" must be used for all sensitive configuration values"", ""Custom resources must have proper timeout and error handling""]"
k6z6r1,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to analyze and validate infrastructure compliance for a production environment. The configuration must: 1. Deploy Lambda functions to scan existing EC2 instances for unencrypted EBS volumes and non-compliant security group rules. 2. Create an RDS MySQL instance to store compliance scan results with automated backups enabled. 3. Implement CloudWatch Events to trigger compliance scans every 6 hours. 4. Generate SNS notifications when non-compliant resources are detected. 5. Create IAM roles with read-only permissions to analyze resources across the account. 6. Output a compliance dashboard URL using CloudWatch custom metrics. 7. Tag all resources with CostCenter, Environment, and ComplianceLevel tags. 8. Implement stack policies to prevent accidental deletion of analysis resources. Expected output: A CloudFormation template that deploys an automated compliance analysis system capable of identifying and reporting infrastructure violations across EC2 and RDS resources.","A financial services company needs to audit their existing CloudFormation stacks for compliance with new regulatory requirements. The infrastructure team must create an analysis template that validates resource configurations, identifies potential security risks, and generates compliance reports for their multi-tier application architecture.","""Production AWS environment in us-east-1 region hosting critical financial applications. Infrastructure includes 50+ EC2 instances across multiple VPCs, RDS Multi-AZ deployments, and Lambda-based microservices. Requires AWS CLI 2.x configured with appropriate permissions. Analysis must cover resources in private subnets behind NAT gateways. Compliance checks must validate encryption at rest, security group configurations, and backup policies. CloudFormation stack will be deployed in a dedicated monitoring VPC with cross-VPC access configured.""","[""Lambda functions must have 3GB memory allocation and 15-minute timeout"", ""RDS instance must use db.t3.medium with 100GB encrypted storage"", ""All IAM policies must follow least privilege principle with no wildcard actions"", ""CloudWatch Logs retention must be set to 30 days for audit trails"", ""Lambda functions must use Python 3.11 runtime"", ""SNS topic must have email subscription endpoint pre-configured"", ""Stack must include DeletionPolicy: Retain for RDS instance"", ""Custom resource must validate at least 10 compliance rules""]"
r9g4b2,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy a production-ready EKS cluster with managed node groups. The configuration must: 1. Create an EKS cluster version 1.28 with private API endpoint access only. 2. Configure IAM role for the EKS service with required AWS managed policies. 3. Deploy a managed node group with t3.large instances across 3 AZs. 4. Enable control plane logging for api, audit, authenticator, controllerManager, and scheduler. 5. Create and associate an OIDC identity provider for the cluster. 6. Configure auto-scaling for the node group with min 2, max 10, desired 4 nodes. 7. Set up IAM role for worker nodes with required policies for EKS operations. 8. Apply consistent tagging across all resources. 9. Configure CloudWatch Logs retention to 30 days for all EKS log groups. 10. Output the cluster endpoint, OIDC issuer URL, and node group ARN. Expected output: A single CloudFormation JSON template that creates a fully functional EKS cluster with managed node groups, proper IAM configurations, logging enabled, and OIDC provider configured for service account integration.","A fintech startup needs to deploy a production-ready Kubernetes environment for their microservices architecture. The infrastructure must support automated deployments, handle varying traffic loads, and maintain strict security compliance for financial data processing.","""Production EKS infrastructure deployed in us-east-1 across 3 availability zones. Uses Amazon EKS 1.28 with managed node groups running Amazon Linux 2. VPC with private subnets for worker nodes and control plane endpoint. Requires AWS CLI 2.x configured with appropriate permissions. CloudFormation JSON templates deployed via AWS Console or CLI. NAT Gateways in each AZ for outbound connectivity. Security groups configured for pod-to-pod communication and ingress controllers.""","[""EKS cluster must use managed node groups with t3.large instances only"", ""Node groups must span exactly 3 availability zones with auto-scaling between 2-10 nodes"", ""Control plane logging must be enabled for all log types with 30-day retention"", ""OIDC provider must be configured for IRSA (IAM Roles for Service Accounts)"", ""Private endpoint access only - no public API endpoint exposure"", ""All resources must use consistent tagging: Environment, Owner, CostCenter"", ""CloudFormation stack must complete deployment within 25 minutes""]"
r6g7q7,,CloudFormation,JSON,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CloudFormation template to implement a multi-region disaster recovery solution for a payment processing system. The configuration must: 1. Define a CloudFormation StackSet that deploys resources to us-east-1 (primary) and us-west-2 (secondary) regions. 2. Create an Aurora Global Database cluster with one primary cluster in us-east-1 and one secondary in us-west-2, using db.r6g.large instances. 3. Configure Route 53 hosted zone with primary and secondary record sets using failover routing policy. 4. Deploy Lambda functions in both regions for payment processing with 3GB memory and 15-minute timeout. 5. Set up S3 buckets with cross-region replication and RTC enabled for transaction logs. 6. Implement DynamoDB global tables for session state with on-demand capacity. 7. Create CloudWatch alarms for database CPU > 80% and Lambda errors > 5 per minute. 8. Configure topics in both regions for alarm notifications. 9. Establish VPC peering between regions for secure replication traffic. 10. Define IAM roles with least-privilege access for all services. 11. Enable encryption at rest for all data stores using AWS-managed KMS keys. 12. Add stack outputs for primary and secondary endpoint URLs. Expected output: A CloudFormation template in JSON format that creates a complete multi-region DR infrastructure with automated failover capabilities, meeting all RTO/RPO requirements and supporting immediate regional failover through Route 53.",A financial services company requires a disaster recovery solution for their critical payment processing application. The application must maintain 99.99% availability with automated failover capabilities between regions. Recent regulatory changes mandate that financial data must be recoverable within 15 minutes with zero data loss.,"""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary). Deployment uses CloudFormation StackSets to manage resources across both regions. Core services include Aurora Global Database for PostgreSQL 15.4, Lambda functions for payment processing, DynamoDB global tables for session management, and S3 with cross-region replication. Route 53 manages DNS failover with health checks. VPCs in each region with 3 private subnets across availability zones. NAT Gateways provide outbound connectivity. Requires AWS CLI 2.x configured with appropriate IAM permissions for StackSets operations.""","[""Use AWS CloudFormation StackSets for multi-region deployment"", ""Primary region must be us-east-1 with failover to us-west-2"", "" Aurora Global Database with automated backups every 5 minutes"", ""Route 53 health checks must trigger failover within 60 seconds"", ""Lambda functions must be replicated in both regions with identical configurations"", ""S3 buckets must use cross-region replication with RTC (Replication Time Control)"", ""DynamoDB global tables required for session state management"", ""All resources must be tagged with Environment=DR and CostCenter=Finance"", ""CloudWatch alarms must notify topic when failover occurs"", ""Maximum RTO of 5 minutes and RPO of 1 minute""]"
j4n3a6,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a containerized batch processing system using ECS Fargate. The configuration must: 1. Define an ECS cluster with Fargate capacity providers and managed termination protection. 2. Create task definitions for three different batch job types (data-ingestion, risk-calculation, report-generation) with specific CPU/memory configurations. 3. Configure ECR repositories with lifecycle policies to retain only the last 10 images and enable scan-on-push. 4. Set up CloudWatch log groups with KMS encryption and 30-day retention for each task type. 5. Implement ECS services with circuit breaker deployment configuration and health check grace period of 120 seconds. 6. Create task execution roles with permissions limited to specific ECR repositories and S3 buckets. 7. Define CloudWatch alarms for task failures exceeding 5% threshold over 10-minute periods. 8. Configure ECS task placement constraints to ensure tasks run in different availability zones. 9. Set up rules to trigger tasks based on S3 object creation events. 10. Implement service auto-scaling policies targeting 70% CPU utilization with 5-minute cooldown. Expected output: A complete CloudFormation JSON template that deploys the entire ECS infrastructure with proper security controls, monitoring, and automation for reliable batch processing operations.","A financial services company needs to modernize their batch processing system by containerizing their risk calculation workloads. The system processes large datasets overnight and must complete calculations before market open, with strict requirements for resource isolation and audit logging.","""Production environment in us-east-1 with ECS Fargate cluster for batch processing workloads. Infrastructure includes VPC with 3 private subnets across availability zones, NAT gateways for outbound connectivity, and ECR repositories for container images. CloudWatch Logs with KMS encryption for audit compliance. ECS service auto-scaling based on CPU/memory metrics. Requires CloudFormation with JSON templates, AWS CLI 2.x configured with appropriate IAM permissions. Integration with existing S3 buckets for data input/output and SNS for job notifications.""","[""ECS tasks must use Fargate launch type with platform version 1.4.0 or higher"", ""Each container must have dedicated CPU and memory allocations with no oversubscription"", ""Task definitions must specify both task-level and container-level resource limits"", ""All container logs must be encrypted at rest using KMS customer-managed keys"", ""ECS cluster must use capacity providers with managed scaling enabled"", ""Container images must be scanned for vulnerabilities using ECR scanning on push""]"
s4x6u5,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a template to deploy an observability stack for payment processing infrastructure. The configuration must: 1. Create encrypted CloudWatch Log Groups with 30-day retention and KMS encryption. 2. Deploy CloudWatch Dashboard with widgets for API Gateway latency, Lambda errors, and custom metrics. 3. Configure service map and sampling rules at 10% for production traffic. 4. Set up composite CloudWatch Alarms combining multiple metrics (API 5XX errors + Lambda timeouts). 5. Create SNS topics with email subscriptions for critical alerts. 6. Store dashboard JSON configuration in Parameter Store for version control. 7. Implement CloudWatch Logs Insights queries as saved queries for common troubleshooting. 8. Configure metric filters on log groups to extract custom metrics from application logs. 9. Set up cross-region metric streams to replicate metrics between regions. 10. Create IAM roles with least privilege for CloudWatch agent on EC2 instances. Expected output: A JSON template that creates a complete observability solution with encrypted logging, multi-region dashboards, intelligent alerting, and distributed tracing capabilities suitable for financial compliance requirements.","A financial services company needs to implement comprehensive observability for their payment processing system to meet compliance requirements. The system must capture detailed metrics, logs, and traces while maintaining data residency in specific regions for regulatory purposes.","""Multi-region deployment across us-east-1 (primary) and eu-west-1 (secondary) for financial services compliance. Requires CloudWatch, and Systems Manager Parameter Store. for CloudWatch and to keep traffic private. Existing Lambda functions and API Gateway need instrumentation. KMS keys pre-created in both regions. Cross-region replication for dashboard configurations via Parameter Store. Deployment uses StackSets for multi-region consistency.""","[""All CloudWatch Logs must use KMS encryption with customer-managed keys"", ""Metrics data retention must be exactly 90 days for compliance"", ""Log groups must have resource policies limiting access to specific IAM roles"", ""CloudWatch Dashboards must be deployed in at least 2 regions"", ""Alarms must use composite alarms for reducing false positives"", ""All resources must use consistent tagging with Environment, Owner, and CostCenter tags"", ""Parameter Store must be used for storing dashboard configurations""]"
z5d1l3,,CloudFormation,JSON,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a CloudFormation template to orchestrate a cross-region migration from us-east-1 to eu-central-1 for a trading analytics platform. MANDATORY REQUIREMENTS (Must complete): 1. Define a StackSet template that deploys infrastructure in both regions simultaneously (CORE: CloudFormation StackSets) 2. Configure buckets with cross-region replication rules and lifecycle policies (CORE: ) 3. Set up DynamoDB global tables with auto-scaling for both regions 4. Create Lambda functions that can operate in either region with environment-specific configurations 5. Implement parameters for storing region-agnostic application settings 6. Define custom CloudFormation resources to track migration progress and validate data consistency 7. Configure CloudWatch dashboards in both regions to monitor migration metrics 8. Set up topics for migration event notifications with cross-region subscriptions 9. Create IAM roles with cross-region trust relationships for migration operations 10. Implement CloudFormation conditions to handle region-specific resource variations OPTIONAL ENHANCEMENTS (If time permits):  Add Route 53 health checks with failover routing (OPTIONAL: Route 53) - enables automatic traffic switching  Implement EventBridge rules for migration workflow orchestration (OPTIONAL: EventBridge) - improves automation  Configure AWS Backup for cross-region backup verification (OPTIONAL: AWS Backup) - adds data protection Expected output: A CloudFormation StackSet template in JSON format that can be deployed to orchestrate the complete migration process, including infrastructure replication, data synchronization, and cutover capabilities with rollback support.","A financial services company needs to migrate their trading analytics platform from us-east-1 to eu-central-1 due to new regulatory requirements. The existing infrastructure includes data processing pipelines, real-time dashboards, and historical data storage that must maintain operational continuity during the migration.","""Multi-region AWS deployment spanning us-east-1 (source) and eu-central-1 (target) for financial trading platform migration. Infrastructure includes buckets with 500TB of historical data, DynamoDB tables processing 10K TPS, Lambda functions for real-time analytics, and Data Streams for market data ingestion. Requires CloudFormation StackSets enabled, cross-region IAM roles configured, and AWS CLI v2 installed. VPCs in both regions with VPC peering for secure data transfer during migration phase.""","[""Use CloudFormation StackSets for multi-region deployment"", ""Implement cross-region replication for buckets with versioning enabled"", ""Configure DynamoDB global tables for seamless data migration"", ""Use Parameter Store for region-agnostic configuration"", ""Implement CloudFormation custom resources for migration state tracking"", ""Ensure zero data loss during the migration window"", ""Maintain read access to historical data during migration"", ""Use CloudFormation outputs to expose region-specific endpoints""]"
k7x5r2,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a production-ready VPC infrastructure for a financial services platform.

MANDATORY REQUIREMENTS (Must complete):
1. Create VPC with CIDR 10.0.0.0/16 and enable DNS hostnames (CORE: VPC)
2. Deploy 3 public subnets (10.0.1.0/24, 10.0.2.0/24, 10.0.3.0/24) across 3 AZs
3. Deploy 3 private subnets (10.0.11.0/24, 10.0.12.0/24, 10.0.13.0/24) across 3 AZs
4. Create Internet Gateway and attach to VPC
5. Launch NAT instances using t3.micro with latest Amazon Linux 2 AMI (CORE: EC2)
6. Configure route tables: public subnets route to IGW, private subnets route to NAT instances
7. Create security groups for NAT instances allowing only HTTPS/HTTP from private subnets
8. Implement stack outputs for VPC ID, subnet IDs, and NAT instance IDs
9. Add Parameters for environment name and cost center with allowed values
10. Set DeletionPolicy to Delete for all resources

OPTIONAL ENHANCEMENTS (If time permits):
 Add VPC Flow Logs to S3 bucket (OPTIONAL: S3) - improves security monitoring
 Implement Systems Manager Session Manager for NAT instances (OPTIONAL: SSM) - enables secure access without SSH
 Add CloudWatch alarms for NAT instance health (OPTIONAL: CloudWatch) - provides operational visibility

Expected output: A valid CloudFormation JSON template that creates a complete VPC infrastructure with proper network segmentation, NAT-based internet access for private resources, and parameterized configuration for multi-environment deployment.","A financial services company needs to establish a secure, isolated network foundation in AWS for their new digital banking platform. The infrastructure must support both public-facing web applications and private backend services while maintaining strict security boundaries and compliance requirements.","""AWS us-east-1 region deployment for production-grade network infrastructure. Requires CloudFormation JSON template creating a complete VPC with public and private subnets across 3 availability zones. Infrastructure includes custom NAT instances on Amazon Linux 2, route tables, internet gateway, and network ACLs. Must support both internet-facing and internal-only workloads with strict security boundaries. Deployment requires AWS CLI configured with appropriate permissions for VPC, EC2, and CloudFormation services.""","[""VPC must use CIDR block 10.0.0.0/16 and span exactly 3 availability zones"", ""All private subnets must route through NAT instances (not NAT Gateway) for cost control"", ""Security groups must explicitly deny all traffic by default with only necessary ports opened"", ""All resources must be tagged with Environment, Project, and CostCenter tags"", ""CloudFormation stack must support rollback with DeletionPolicy set to Delete for all resources""]"
q4g6h3,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy payment processing infrastructure consistently across multiple AWS accounts. The configuration must: 1. Define a StackSet template that deploys Lambda functions for payment validation and processing. 2. Create DynamoDB tables with consistent partition keys, sort keys, and two Global Secondary Indexes. 3. Set up Application Load Balancer with target groups pointing to Lambda functions. 4. Configure Step Functions state machine for payment workflow orchestration. 5. Implement CloudWatch alarms with environment-specific SNS topic endpoints. 6. Use Parameters for environment-specific values (account IDs, domain names, SNS emails). 7. Apply Conditions to add production-only features (enhanced monitoring, reserved concurrency). 8. Create outputs that expose critical resource ARNs for cross-stack references. 9. Include stack update policy to prevent accidental deletion of stateful resources. Expected output: A CloudFormation JSON template ready for StackSet deployment that ensures infrastructure consistency across all environments while allowing minimal, controlled variations through parameters.","A fintech startup needs to ensure their payment processing infrastructure remains identical across development, staging, and production environments. They've experienced configuration drift that led to a production incident when a critical Lambda timeout setting differed between environments.","""Multi-account AWS deployment spanning three environments: development (account 123456789012), staging (account 234567890123), and production (account 345678901234) in us-east-1 region. Each environment requires identical infrastructure with VPCs containing 3 availability zones, private subnets for Lambda and DynamoDB endpoints, and public subnets for ALB. StackSets deployed from a central management account (456789012345) with OrganizationAccountAccessRole permissions. Requires AWS CLI 2.x configured with appropriate cross-account assume role capabilities.""","[""Use CloudFormation StackSets to deploy identical stacks across 3 AWS accounts"", ""Parameter overrides must be limited to environment-specific values only (account IDs, domain names)"", ""All Lambda functions must use identical runtime versions and memory allocations"", ""DynamoDB tables must have identical capacity settings and GSI configurations"", ""Use CloudFormation Conditions to handle environment-specific tagging requirements"", ""Implement drift detection using CloudFormation drift detection APIs"", ""Stack update operations must validate consistency before deployment"", ""Use nested stacks for modular resource organization"", ""All IAM roles must reference resources using CloudFormation intrinsic functions only""]"
w1s5a3,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,Create a CloudFormation template to deploy a containerized loan processing application. The configuration must: 1. Define a VPC with public and private subnets across 3 availability zones. 2. Deploy an ECS Fargate cluster running the application container (CORE: ECS). 3. Configure Aurora PostgreSQL Serverless v2 with minimum 0.5 ACUs and maximum 4 ACUs (CORE: Aurora). 4. Implement an Application Load Balancer with HTTPS listener using ACM certificate. 5. Create S3 bucket for document storage with server-side encryption. 6. Configure auto-scaling for ECS tasks based on ALB request count metric. 7. Set up CloudWatch Log Groups with specified retention periods. 8. Implement least-privilege IAM roles for all services. Expected output: A CloudFormation JSON template that deploys the complete infrastructure with proper security configurations and compliance-ready logging.,A fintech startup needs to deploy their loan processing web application with strict compliance requirements. The application requires real-time data processing capabilities and must maintain audit logs for regulatory purposes. All infrastructure must be defined as code to ensure reproducibility across environments.,"""Production infrastructure deployed in us-east-2 region for loan processing web application. Architecture includes ALB in public subnets, ECS Fargate containers in private subnets across 3 AZs, Aurora PostgreSQL Multi-AZ cluster, S3 for document storage. VPC with 10.0.0.0/16 CIDR, NAT Gateways for outbound traffic. Requires AWS CLI configured with appropriate IAM permissions. CloudFormation stack will use nested stacks for modularity. Application handles sensitive financial data requiring encryption at rest and in transit.""","[""All compute resources must run in private subnets with no direct internet access"", ""Database backups must be encrypted using customer-managed KMS keys"", ""Application logs must be retained for exactly 365 days for compliance"", ""Auto-scaling must trigger based on custom CloudWatch metrics, not default CPU/memory"", ""All S3 buckets must have versioning enabled and lifecycle policies for cost optimization""]"
t8f4t2,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a serverless trade processing system. MANDATORY REQUIREMENTS (Must complete): 1. Create a state machine that orchestrates the trade processing workflow with parallel validation and enrichment states (CORE: ) 2. Deploy three Lambda functions using container images: trade validator, metadata enricher, and compliance recorder (CORE: Lambda) 3. Configure DynamoDB global table spanning us-east-1 and eu-west-1 for trade storage 4. Set up rules to trigger the state machine on trade events from three different source systems 5. Implement dead letter queues for all Lambda functions with CloudWatch alarms on DLQ depth 6. Create parameters for API endpoints and processing thresholds 7. Configure Lambda Insights for all functions with custom metrics 8. Set up cross-region replication for the ECR repository containing Lambda images OPTIONAL ENHANCEMENTS (If time permits):  Add AppSync GraphQL API for real-time trade queries (OPTIONAL: AppSync) - enables client subscriptions  Implement Kinesis Data Firehose for archiving to S3 (OPTIONAL: Kinesis Firehose) - adds long-term storage  Add tracing with custom segments (OPTIONAL: ) - improves distributed tracing Expected output: A complete CloudFormation JSON template that deploys the serverless trade processing infrastructure with all mandatory components properly configured for high availability and compliance requirements.","A financial services company needs to process real-time stock market data feeds. They receive JSON payloads containing trade information that must be validated, enriched with market metadata, and stored for compliance reporting. The system must handle variable load patterns during market hours and maintain audit trails for regulatory purposes.","""Production serverless infrastructure deployed in us-east-1 with multi-region failover capability to eu-west-1. Core services include Lambda container functions for data processing, DynamoDB global tables for trade storage, for orchestration workflow, and for event routing. required for private connectivity to AWS services. Deployment uses CloudFormation JSON templates with cross-region stack sets. Environment requires AWS CLI 2.x configured with appropriate IAM permissions for global table creation and Lambda container registry access.""","[""Use JSON format exclusively for the CloudFormation template"", ""Lambda functions must use container images instead of zip deployments"", ""DynamoDB tables must use on-demand billing mode with point-in-time recovery enabled"", ""All Lambda functions must have reserved concurrent executions configured"", ""Use for all configuration values"", ""Implement custom CloudFormation resource for DynamoDB global table configuration"", ""Dead letter queues must have message retention of exactly 14 days"", ""Lambda functions must use ARM-based Graviton2 processors""]"
v6o0c6,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a complete CI/CD pipeline for containerized applications. MANDATORY REQUIREMENTS (Must complete): 1. Set up with source, build, and deploy stages (CORE: ) 2. Configure project to build Docker images and push to ECR (CORE: ) 3. Create S3 bucket for pipeline artifacts with AES256 encryption 4. Define IAM roles with least-privilege policies for pipeline services 5. Implement manual approval action between staging and production stages 6. Configure CloudWatch Events to trigger pipeline on CodeCommit pushes 7. Output pipeline ARN and execution role ARN 8. Set DeletionPolicy to Delete for all resources OPTIONAL ENHANCEMENTS (If time permits):  Add for blue/green ECS deployments (OPTIONAL: ) - enables zero-downtime deployments  Implement SNS notifications for pipeline state changes (OPTIONAL: SNS) - improves team visibility  Add CloudWatch Synthetics for post-deployment testing (OPTIONAL: Synthetics) - automates validation Expected output: A CloudFormation template that creates a fully automated CI/CD pipeline with proper staging gates and security controls.","A DevOps team needs to establish a standardized CI/CD pipeline for their Node.js microservices. They require infrastructure that automatically builds, tests, and deploys code changes while maintaining separate staging and production environments with proper approval gates.","""AWS multi-account setup in us-east-1 region with separate staging (123456789012) and production (987654321098) accounts. Infrastructure includes orchestrating CodeCommit, and services. ECS Fargate clusters in both accounts with ALB for load balancing. VPC with private subnets for containers and public subnets for ALB. S3 bucket for artifacts with KMS encryption. IAM roles for cross-account deployments. Requires AWS CLI 2.x configured with appropriate credentials.""","[""Use CodeCommit for source control with branch-based triggers"", "" projects must use compute type BUILD_GENERAL1_SMALL"", ""Deploy to ECS Fargate with rolling updates enabled"", ""Manual approval required between staging and production deployments"", ""All build artifacts must be encrypted at rest in S3""]"
t1x0d4,,CloudFormation,JSON,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CloudFormation template to deploy a highly available payment processing infrastructure with automated failover capabilities. The configuration must: 1. Set up Aurora PostgreSQL cluster with one writer and two reader instances across different AZs. 2. Deploy ECS Fargate service running 6 tasks distributed evenly across 3 AZs. 3. Configure Application Load Balancer with target group health checks and connection draining. 4. Implement Route 53 failover routing between primary and secondary ALB endpoints. 5. Create Auto Scaling policies that maintain exactly 6 ECS tasks during AZ failures. 6. Set up CloudWatch alarms for failover events, ECS task failures, and ALB unhealthy targets. 7. Configure topic for critical alerts with email subscription. 8. Implement automated snapshot backup with 7-day retention. 9. Create CloudWatch dashboard showing real-time failover metrics. 10. Use CloudFormation stack sets to deploy identical standby stack in us-west-2. Expected output: Complete CloudFormation template in JSON format that creates all resources with proper dependencies, automated failover mechanisms, and monitoring. The infrastructure should automatically recover from single AZ failures without manual intervention.",A financial services company needs to ensure their critical transaction processing system can survive AWS availability zone failures. The system processes time-sensitive payment authorizations and must maintain 99.99% uptime. Any downtime directly impacts revenue and customer trust.,"""Production multi-AZ deployment in us-east-1 region across 3 availability zones (us-east-1a, us-east-1b, us-east-1c). Infrastructure includes Aurora PostgreSQL Multi-AZ cluster, ECS Fargate for containerized services, Application Load Balancer with cross-zone load balancing enabled. VPC spans all 3 AZs with public subnets for ALB and private subnets for ECS tasks and . NAT Gateways in each AZ for high availability. for configuration management. CloudWatch for monitoring with custom metrics. Route 53 for DNS failover. Requires AWS CLI configured with appropriate permissions.""","["" instances must use Aurora PostgreSQL with automated failover capability"", ""ECS tasks must be distributed across at least 3 availability zones"", ""Application Load Balancer must perform health checks every 5 seconds"", ""Auto Scaling Group must maintain exactly 6 running instances at all times"", ""All data must be encrypted at rest using AWS KMS customer managed keys"", ""CloudWatch alarms must trigger within 60 seconds of any service degradation"", ""Route 53 health checks must use failover routing policy with 30-second TTL"", ""ECS service must use rolling update with minimum healthy percent of 100"", "" read replicas must be in different AZs than the primary instance"", ""All resources must use DeletionPolicy: Retain for production safety""]"
o0n3f4,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy a secure payment card data processing infrastructure. The configuration must: 1. Create a KMS key with automatic rotation enabled for encrypting all resources. 2. Deploy a Lambda function in private subnets that processes encrypted payment files from S3. 3. Configure DynamoDB table with encryption at rest using the KMS key and point-in-time recovery. 4. Set up S3 bucket with SSE-KMS encryption, versioning, and block all public access. 5. Implement VPC endpoints for S3 and DynamoDB to avoid internet routing. 6. Create IAM roles with minimal permissions following least privilege principle. 7. Enable CloudWatch Logs encryption for Lambda with 30-day retention. 8. Configure bucket policies that deny unencrypted uploads and enforce HTTPS. 9. Add Lambda environment variables encrypted with KMS. 10. Implement CloudTrail logging to a separate encrypted S3 bucket. Expected output: A CloudFormation template that creates a fully encrypted, PCI-compliant infrastructure with all data encrypted at rest and in transit, comprehensive audit logging, and strict access controls.","A financial services company requires a secure data processing pipeline that meets PCI-DSS compliance standards. The infrastructure must encrypt all data at rest and in transit, implement strict access controls, and maintain detailed audit logs for regulatory compliance.","""Secure multi-AZ deployment in us-east-1 region for PCI-DSS compliant data processing. Uses Lambda for compute, DynamoDB for transaction storage, S3 for encrypted file storage, and KMS for key management. Requires VPC with private subnets across 3 AZs, VPC endpoints for S3 and DynamoDB, and NAT instances for controlled outbound traffic. All components must be deployed in isolated security zones with CloudWatch Logs encryption and comprehensive IAM policies.""","[""All S3 buckets must use SSE-KMS encryption with customer-managed keys"", ""Lambda functions must run in private subnets with no direct internet access"", ""DynamoDB tables must have point-in-time recovery enabled"", ""All IAM roles must follow least privilege with no wildcard permissions"", ""CloudWatch Logs must have encryption enabled using KMS"", ""VPC endpoints must be used for all AWS service communications""]"
x3s7n5,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy an automated infrastructure compliance analysis system. The configuration must: 1. Set up AWS Config with custom rules to monitor CloudFormation stack drift and compliance violations. 2. Deploy Lambda functions (Python 3.9 runtime, 256MB memory) that validate resource configurations against security policies. 3. Create S3 bucket with versioning enabled and lifecycle rules to transition compliance reports to Glacier after 30 days. 4. Configure EventBridge rules to capture AWS Config compliance changes and trigger Lambda validation functions. 5. Implement SNS topic with email subscriptions for security team notifications on non-compliant resources. 6. Create CloudWatch dashboard displaying compliance metrics and drift detection results. 7. Set up Parameter Store entries for storing approved AMI IDs and security group rules. 8. Implement automated tagging compliance checks ensuring all resources have required tags (Environment, Owner, CostCenter). 9. Configure AWS Config aggregator to collect compliance data from multiple AWS accounts if present. 10. Create IAM roles and policies following least privilege principle for all services. Expected output: A CloudFormation template that deploys a complete infrastructure compliance monitoring system capable of detecting configuration drift, validating security policies, and alerting on non-compliant resources across CloudFormation stacks.",A financial services company has discovered configuration drift in their production environment after a security audit. They need to implement automated infrastructure compliance scanning to ensure their CloudFormation stacks remain compliant with corporate security policies and AWS best practices.,"""Infrastructure compliance monitoring system deployed in us-east-1 region. Uses AWS Config for continuous monitoring of CloudFormation stacks, Lambda functions for custom compliance checks, S3 for report storage with 90-day retention. EventBridge rules trigger on configuration changes. SNS topics distribute alerts to security team. Requires AWS CLI 2.x configured, Python 3.9+ for Lambda runtime. VPC not required as all services are managed. Parameter Store holds compliance thresholds and approved AMI lists. CloudWatch Logs aggregates Lambda execution logs with 30-day retention.""","[""Use AWS Config rules to monitor CloudFormation stack compliance"", ""Implement custom Lambda functions for policy validation"", ""Store compliance reports in S3 with lifecycle policies"", ""Use EventBridge for real-time drift detection"", ""Implement SNS notifications for non-compliant resources"", ""Create IAM roles with least privilege access"", ""Enable CloudWatch Logs for all Lambda functions"", ""Use Parameter Store for configuration values"", ""Implement resource tagging standards validation""]"
q4c8r7,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a production-ready EKS cluster with managed node groups.

MANDATORY REQUIREMENTS (Must complete):
1. Create an EKS cluster with private API endpoint and public access disabled (CORE: EKS)
2. Configure two managed node groups: one for general workloads (t3.large, 2-6 nodes) and one for compute-intensive workloads (c5.xlarge, 1-4 nodes) (CORE: EC2)
3. Set up OIDC identity provider for the cluster to enable IRSA
4. Create IAM roles for node groups with required AWS managed policies (AmazonEKSWorkerNodePolicy, AmazonEKS_CNI_Policy, AmazonEC2ContainerRegistryReadOnly)
5. Configure cluster security group to allow ingress only from specific CIDR blocks (10.0.0.0/8)
6. Enable CloudWatch logging for all EKS control plane components (api, audit, authenticator, controllerManager, scheduler)
7. Tag all resources with Environment=Production and ManagedBy=CloudFormation
8. Set DeletionPolicy to Retain for the EKS cluster resource

OPTIONAL ENHANCEMENTS (If time permits):
 Add AWS Load Balancer Controller IRSA role (OPTIONAL: IAM) - enables Kubernetes Ingress with ALB/NLB
 Configure Fargate profile for system pods (OPTIONAL: Fargate) - reduces operational overhead for cluster add-ons
 Add EBS CSI driver IRSA role (OPTIONAL: EBS) - enables persistent volume support for stateful workloads

Expected output: A CloudFormation JSON template that creates a secure EKS cluster with two managed node groups, proper IAM roles, OIDC provider configuration, and CloudWatch logging enabled. The template should output the cluster endpoint, OIDC issuer URL, and node group ARNs for use by deployment pipelines.",A fintech startup needs to deploy their containerized microservices architecture on AWS. They require a managed Kubernetes environment with strict security controls and automated node management. The infrastructure must support both stateless API services and stateful database workloads.,"""Production-grade EKS cluster deployed in us-east-1 across 3 availability zones. VPC with public subnets for load balancers and private subnets for worker nodes. NAT gateways in each AZ for outbound internet access. Requires AWS CLI 2.x configured with appropriate permissions. Infrastructure includes EKS control plane, managed node groups with t3.large instances, VPC CNI plugin, CoreDNS, and kube-proxy add-ons. Security groups restrict inter-node communication and control plane access.""","[""EKS cluster must use Kubernetes version 1.28 or higher"", ""Node groups must use Amazon Linux 2 EKS optimized AMIs only"", ""All worker nodes must be deployed in private subnets with no direct internet access"", ""OIDC provider must be configured for IRSA (IAM Roles for Service Accounts)"", ""Cluster endpoint must be private with whitelisted CIDR blocks for access"", ""Node groups must have auto-scaling enabled with minimum 2 and maximum 10 nodes"", ""All IAM roles must follow least privilege principle with no wildcard actions""]"
j5u5j7,,CloudFormation,JSON,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CloudFormation template to implement a multi-region disaster recovery solution for a payment processing application. MANDATORY REQUIREMENTS (Must complete): 1. Deploy Aurora Global Database cluster with writer in us-east-1 and reader in us-west-2 (CORE: Aurora) 2. Configure Route 53 hosted zone with health checks and failover routing between regions (CORE: Route 53) 3. Create Application Load Balancers in both regions with target groups 4. Deploy Lambda functions in both regions for payment processing logic 5. Set up S3 buckets with cross-region replication for transaction logs 6. Configure CloudWatch alarms to monitor Aurora replication lag (threshold: 5 seconds) 7. Create SNS topics in both regions for alert notifications 8. Implement IAM roles with cross-region assume permissions for failover automation 9. Enable deletion protection on production resources using DeletionPolicy: Retain 10. Configure VPC peering between regions for secure database communication OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Backup for point-in-time recovery (OPTIONAL: AWS Backup) - provides additional recovery options  Implement for orchestrated failover workflow (OPTIONAL: ) - automates complex failover sequences  Deploy distribution with origin failover (OPTIONAL: ) - improves global performance and availability Expected output: A CloudFormation template in JSON format that creates all required resources across both regions, with proper dependencies and cross-region references. The template should support stack creation in the primary region and reference resources in the DR region through parameters or exports.","A financial services company requires a disaster recovery solution for their critical payment processing application. The primary region recently experienced a 4-hour outage, resulting in significant revenue loss. Management has mandated implementation of an active-passive DR strategy with automated failover capabilities.","""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (DR). Utilizes Aurora Global Database for data replication, Route 53 for DNS failover, and Lambda functions for business logic. VPCs in both regions with private subnets for databases and public subnets for ALBs. Requires AWS CLI configured with appropriate permissions. CloudWatch cross-region dashboards monitor health status. S3 buckets with cross-region replication for static assets.""","[""RTO (Recovery Time Objective) must be under 15 minutes"", ""RPO (Recovery Point Objective) must be less than 5 minutes"", ""Use Route 53 health checks with failover routing policy"", ""Primary region must be us-east-1, DR region must be us-west-2"", ""Database replication must use Aurora Global Database"", ""All resources must have DeletionPolicy set to Retain"", ""Cost optimization: DR region runs minimal capacity until failover"", ""CloudWatch alarms must trigger SNS notifications for failover events"", ""Lambda functions must be replicated with identical environment variables""]"
x6z9l1,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a blue-green capable ECS cluster with automated rollback capabilities. The configuration must: 1. Create an ECS cluster with Container Insights enabled and capacity providers configured for Fargate and Fargate Spot. 2. Deploy two ECS services (blue and green) running identical task definitions with 3 desired tasks each. 3. Configure an Application Load Balancer with two target groups and weighted routing between blue/green services. 4. Implement auto-scaling policies that scale between 3-10 tasks based on both CPU and memory metrics. 5. Set up CloudWatch alarms for unhealthy targets that trigger SNS notifications when 2+ tasks fail health checks. 6. Create task execution roles with permissions to pull from ECR, write logs, and read secrets from . 7. Configure service discovery using with private DNS namespace for inter-service communication. 8. Implement Circuit Breaker settings with 50% rollback threshold and 10-minute evaluation period. Expected output: A complete CloudFormation template in JSON format that provisions the entire ECS infrastructure with blue-green deployment capabilities, including all networking, security, monitoring, and auto-scaling configurations required for production-grade container orchestration.","A fintech startup needs to deploy their microservices architecture on AWS ECS with blue-green deployment capabilities. The system processes sensitive financial transactions and requires strict network isolation, automated scaling based on CPU and memory metrics, and seamless rollback mechanisms.","""Production-grade container orchestration infrastructure deployed in us-east-1 across 3 availability zones. Core services include ECS Fargate for container orchestration, Application Load Balancer for traffic distribution, Auto Scaling for dynamic capacity management. VPC configured with public subnets for ALB and private subnets for ECS tasks. NAT Gateways provide outbound internet access for containers. CloudWatch Container Insights enabled for monitoring. Requires AWS CLI 2.x configured with appropriate IAM permissions for ECS, EC2, and CloudWatch services.""","[""ECS services must use Fargate launch type with platform version 1.4.0"", ""Task definitions must specify both CPU and memory limits with at least 1 vCPU and 2GB RAM"", ""Application Load Balancer must use path-based routing with health check intervals of 15 seconds"", ""Auto-scaling policies must trigger at 70% CPU or 80% memory utilization"", ""All container logs must be streamed to CloudWatch Logs with 30-day retention"", ""Secrets must be stored in and injected as environment variables"", ""Network ACLs must explicitly deny all traffic except ports 80, 443, and 8080"", ""Each service must have dedicated target groups with deregistration delay of 30 seconds""]"
m5b4c1,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy an enterprise observability platform for distributed microservices. The configuration must: 1. Set up CloudWatch Container Insights for ECS clusters with enhanced monitoring enabled. 2. Create service map with sampling rules for 10% of requests and 100% of errors. 3. Deploy CloudWatch Synthetics canaries for login, checkout, and payment API endpoints. 4. Configure CloudWatch Anomaly Detector for transaction volume and latency metrics. 5. Create Contributor Insights rules to identify top API consumers by IP address. 6. Set up composite alarms using metric math for 99.9% availability SLO. 7. Implement CloudWatch RUM application with custom page grouping rules. 8. Create centralized CloudWatch dashboard with cross-region widgets. 9. Configure topics with email and Slack webhook subscriptions for alerts. 10. Set up rules to trigger Lambda functions for auto-remediation. 11. Enable cross-account sharing for security and compliance team access. 12. Tag all resources with CostCenter, Environment, and Owner tags. Expected output: A CloudFormation JSON template that creates a complete observability platform with real-time monitoring, distributed tracing, synthetic testing, and anomaly detection capabilities across multiple AWS accounts and regions.","A financial services company needs centralized observability for their microservices architecture to meet regulatory audit requirements. The monitoring solution must capture application metrics, distributed traces, and logs with automated anomaly detection for suspicious transaction patterns.","""Production observability infrastructure deployed in us-east-1 with cross-region aggregation from us-west-2 and eu-west-1. Monitors ECS Fargate microservices, Lambda functions, and API Gateway endpoints. Requires CloudWatch cross-account observability enabled in AWS Organizations management account. VPC flow logs and AWS logs streamed to CloudWatch Logs. Integration with existing Kinesis Data Firehose for long-term storage in S3. CloudWatch dashboard accessible from corporate network via AWS SSO. Minimum retention of 30 days for metrics and 90 days for logs to meet compliance requirements.""","[""Use service map for distributed tracing across all microservices"", ""Configure CloudWatch Container Insights for ECS task-level metrics"", ""Implement metric math expressions for composite SLI calculations"", ""Set up CloudWatch Synthetics canaries for critical user journeys"", ""Create CloudWatch Contributor Insights rules for high-cardinality analysis"", ""Use CloudWatch Anomaly Detector with custom training periods"", ""Configure cross-account observability with AWS Organizations"", ""Implement CloudWatch RUM for real user monitoring data collection""]"
i4c3p7,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a template to migrate a payment processing application from on-premises to AWS. The configuration must: 1. Create a VPC with public, private, and database subnet tiers across 3 AZs. 2. Deploy an Auto Scaling group with min 2, max 6 EC2 instances behind an ALB. 3. Set up Aurora MySQL cluster with one writer and one reader instance. 4. Configure security groups allowing HTTPS (443) from internet to ALB, HTTP (80) from ALB to EC2, and MySQL (3306) from EC2 to . 5. Create IAM roles with policies for EC2 instances to access Parameter Store and . 6. Set up CloudWatch Log Groups for application logs with 30-day retention. 7. Implement Auto Scaling policies based on target tracking (CPU 70%). 8. Configure bucket for application artifacts with versioning enabled. Expected output: A single JSON template that creates all infrastructure components with proper dependencies, outputs for ALB DNS name, and parameter inputs for environment-specific values.","A financial services company is migrating their legacy on-premises payment processing system to AWS. The current system handles 50,000 transactions daily and requires strict compliance with PCI DSS standards. They need a phased migration approach to minimize downtime during the transition.","""Production migration environment in us-east-1 across 3 availability zones. VPC with CIDR 10.0.0.0/16, public subnets (10.0.1.0/24, 10.0.2.0/24, 10.0.3.0/24) for ALB, private subnets (10.0.11.0/24, 10.0.12.0/24, 10.0.13.0/24) for EC2 instances, and database subnets (10.0.21.0/24, 10.0.22.0/24, 10.0.23.0/24) for Aurora. NAT Gateways in each AZ for outbound connectivity. Requires AWS CLI 2.x configured with appropriate IAM permissions.""","[""Use Parameter Store for all sensitive configuration values"", ""Implement AWS WAF with rate limiting rules set to 2000 requests per 5-minute window"", ""Configure Aurora MySQL with encryption at rest using customer-managed KMS keys"", ""Enable point-in-time recovery with a 7-day retention period"", ""Use Application Load Balancer with SSL termination and TLS 1.2 minimum"", ""Deploy EC2 instances in private subnets with no direct internet access"", ""Configure VPC Flow Logs to with 90-day lifecycle policy"", ""Implement CloudWatch alarms for CPU > 80% and database connections > 100"", ""Use t3.large instances with gp3 EBS volumes (100GB, 3000 IOPS)"", ""Tag all resources with Environment, CostCenter, and MigrationPhase tags""]"
r9q6o8,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a template to deploy a production-ready payment processing environment. MANDATORY REQUIREMENTS (Must complete): 1. Configure VPC with 6 subnets (3 public, 3 private) across 3 AZs with NAT Gateways (CORE: VPC). 2. Deploy RDS Aurora PostgreSQL cluster with Multi-AZ replicas and encryption (CORE: RDS). 3. Create Auto Scaling Group with Launch Template for t3.large instances. 4. Configure Application Load Balancer with HTTPS listener and target group. 5. Implement least-privilege IAM roles for EC2 instances to access S3 and RDS. 6. Set up CloudWatch Log Groups with 30-day retention for application logs. 7. Create S3 buckets for static assets and VPC Flow Logs with encryption. 8. Configure all security groups with specific port rules (no wildcards). 9. Add CloudWatch alarms for CPU, memory, and database connections. 10. Enable deletion protection on RDS cluster and set DeletionPolicy. OPTIONAL ENHANCEMENTS (If time permits):  Add AWS WAF rules to ALB for SQL injection protection (OPTIONAL: WAF) - improves security posture  Implement distribution for static assets (OPTIONAL: ) - reduces latency globally  Configure for automated RDS snapshots (OPTIONAL: ) - ensures data recovery. Expected output: A single JSON template that creates the complete infrastructure stack with all security controls, monitoring, and high availability features configured according to requirements.","A fintech startup needs to establish a secure cloud environment for their payment processing application. The infrastructure must comply with PCI-DSS standards and support 50,000 daily transactions with sub-second response times.","""Production-grade infrastructure in us-east-1 region spanning 3 availability zones. Core services include Application Load Balancer, Auto Scaling Group with EC2 instances, RDS Aurora PostgreSQL cluster, and S3 for static assets and logs. VPC with public subnets for ALB, private subnets for compute and database tiers. NAT Gateways in each AZ for outbound connectivity. JSON templates required with AWS CLI 2.x configured. Environment supports high-throughput payment processing with strict security requirements.""","[""All resources must be deployed across 3 availability zones"", ""RDS instances must use encrypted storage with customer-managed KMS keys"", ""Application Load Balancer must terminate SSL with ACM certificate"", ""Auto Scaling Group must maintain minimum 6 instances during business hours"", ""All EC2 instances must use IMDSv2 for metadata access"", ""VPC Flow Logs must be enabled and sent to S3 bucket"", ""Security Groups must follow least-privilege with no 0.0.0.0/0 inbound rules"", ""All S3 buckets must have versioning and lifecycle policies enabled"", ""CloudWatch alarms must trigger when CPU exceeds 80% for 5 minutes"", ""DeletionPolicy must be set to Retain for all stateful resources""]"
o3a2r3,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a transaction processing system that can be consistently replicated across dev, staging, and production environments. MANDATORY REQUIREMENTS (Must complete): 1. Define an RDS Aurora MySQL cluster with read replicas, using Conditions to vary instance types by environment (CORE: RDS Aurora) 2. Create Lambda functions for transaction processing with environment-specific memory allocations (CORE: Lambda) 3. Configure S3 buckets with environment-prefixed names and cross-region replication for prod only 4. Implement ALB with path-based routing to Lambda functions 5. Use Parameters for environment name, instance sizes, and retention periods 6. Define Outputs for database endpoints, bucket names, and ALB DNS names 7. Create IAM roles with least-privilege policies for Lambda execution 8. Configure CloudWatch Log Groups with environment-specific retention OPTIONAL ENHANCEMENTS (If time permits):  Add DynamoDB tables for session management (OPTIONAL: DynamoDB) - improves stateless processing  Implement topics for transaction notifications (OPTIONAL: ) - enables real-time alerting  Add CloudFront distribution for static assets (OPTIONAL: CloudFront) - reduces latency globally Expected output: A CloudFormation JSON template with parameters, conditions, and resources that can be deployed via StackSets to create identical infrastructure across multiple environments with appropriate variations.","A financial services company needs to replicate their transaction processing infrastructure across three environments (dev, staging, prod) with consistent configurations but environment-specific parameters. The infrastructure must maintain strict data isolation between environments while ensuring configuration drift is minimized through a single source of truth.","""Multi-environment AWS deployment spanning dev (us-east-1), staging (us-east-2), and production (us-west-2) regions. Each environment requires isolated VPCs with 2 public and 2 private subnets across 2 AZs. Infrastructure includes RDS Aurora MySQL, Lambda functions for transaction processing, S3 buckets for data storage, and Application Load Balancers. with separate accounts per environment. CloudFormation StackSets deployed from management account. KMS keys for encryption per environment. for configuration management.""","[""Use CloudFormation StackSets for multi-environment deployment orchestration"", ""Implement parameter overrides using environment-specific parameter files"", ""All S3 buckets must have versioning enabled and lifecycle policies defined"", ""RDS instances must use encrypted storage with environment-specific KMS keys"", ""Lambda functions must have environment variables injected via "", ""Use CloudFormation Conditions to handle environment-specific resource variations""]"
n3v6p3,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a template to deploy a highly available web application infrastructure for financial transaction processing. MANDATORY REQUIREMENTS (Must complete): 1. Configure an Application Load Balancer with target group health checks every 30 seconds (CORE: ELB/ALB) 2. Deploy ECS Fargate service running containerized web application with 2 desired tasks (CORE: ECS) 3. Create RDS Aurora MySQL cluster with one writer and one reader instance (CORE: RDS) 4. Set up VPC with 3 public and 3 private subnets across availability zones 5. Configure security groups allowing HTTPS (443) inbound to ALB and appropriate ports between services 6. Create bucket for storing application logs with lifecycle policy moving logs to Glacier after 90 days 7. Implement CloudWatch alarms for ECS CPU utilization above 80% and RDS connection count above 100 8. Configure auto-scaling for ECS service based on average CPU utilization 9. Set up IAM roles and policies for ECS task execution and log access 10. Enable CloudWatch Container Insights for ECS cluster monitoring OPTIONAL ENHANCEMENTS (If time permits):  Add distribution for static content caching (OPTIONAL: ) - improves global performance  Implement AWS WAF on ALB for protection against common attacks (OPTIONAL: WAF) - enhances security posture  Add Route 53 health checks with failover routing (OPTIONAL: Route 53) - provides DNS-level redundancy Expected output: A complete JSON template that provisions all mandatory infrastructure components with proper security configurations, monitoring, and auto-scaling capabilities suitable for a production financial application.",A fintech startup needs to deploy a customer-facing web application that processes financial transactions with strict compliance requirements. The application must handle variable traffic patterns during market hours and maintain audit logs for regulatory purposes.,"""Production environment deployed in us-east-1 region across 3 availability zones. Infrastructure includes Application Load Balancer for traffic distribution, ECS Fargate for containerized web application hosting, and RDS Aurora MySQL for transactional data storage. VPC configured with public subnets for ALB and private subnets for ECS tasks and RDS instances. NAT gateways in each AZ for outbound internet access from private subnets. Requires AWS CLI configured with appropriate credentials and permissions. Environment supports auto-scaling based on CPU utilization with minimum 2 and maximum 10 ECS tasks.""","[""All buckets must have versioning enabled and server-side encryption with AWS managed keys"", ""ALB must use HTTPS listeners only with SSL redirect enabled"", ""RDS instances must have automated backups with 7-day retention and encryption at rest"", ""ECS task definitions must specify exact memory and CPU limits without using wildcards"", ""CloudWatch log groups must have 30-day retention for compliance"", ""All resources must have deletion policies set to 'Retain' for production data"", ""IAM roles must follow least privilege with no wildcard actions on production resources""]"
i1a5l7,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,Create a CloudFormation template to deploy a serverless payment webhook processing system. MANDATORY REQUIREMENTS (Must complete): 1. Create Lambda function with 3GB memory and 5-minute timeout for webhook processing (CORE: Lambda) 2. Configure DynamoDB table with GSI for querying by payment_provider and timestamp (CORE: DynamoDB) 3. Set up SQS queue with visibility timeout of 6 times Lambda timeout (CORE: SQS) 4. Create API Gateway REST API with request validation and API key authentication 5. Implement Lambda environment variables using references 6. Configure CloudWatch Log Groups with 30-day retention for all Lambda functions 7. Create DLQ for failed webhook processing with redrive policy 8. Set up rule to trigger DLQ processor every 15 minutes 9. Implement least-privilege IAM roles with no AdminAccess policies 10. Add CloudFormation outputs for API endpoint URL and queue URLs OPTIONAL ENHANCEMENTS (If time permits):  Add SNS topic for critical failure notifications (OPTIONAL: SNS) - enables real-time alerting  Implement for complex payment workflows (OPTIONAL: ) - adds orchestration capabilities  Add CloudWatch dashboard with custom metrics (OPTIONAL: CloudWatch) - improves operational visibility Expected output: Complete CloudFormation JSON template that deploys all required resources with proper dependencies and configurations. The stack should be deployable in any AWS account without manual intervention.,A fintech startup needs to process millions of daily payment webhook notifications from multiple payment providers. The system must handle burst traffic during sales events and maintain transaction integrity while minimizing operational costs.,"""Production serverless infrastructure deployed in us-east-1 region using Lambda for webhook processing, DynamoDB for transaction storage, and SQS for asynchronous processing. Architecture includes API Gateway REST API with custom domain, dead letter queues for failure handling, and CloudWatch dashboards for monitoring. Requires AWS CLI 2.x configured with appropriate IAM permissions for CloudFormation stack creation. for DynamoDB and S3 to reduce data transfer costs. Multi-AZ deployment not required as serverless services provide built-in redundancy.""","[""Lambda functions must use reserved concurrency to prevent throttling other workloads"", ""DynamoDB tables must use point-in-time recovery for compliance requirements"", ""All Lambda functions must have tracing enabled for debugging"", ""Use for storing configuration values"", ""Implement exponential backoff in DLQ processing with maximum 5 retry attempts""]"
f0z8p0,,CloudFormation,JSON,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CloudFormation template to implement a multi-region disaster recovery architecture for a critical healthcare application. MANDATORY REQUIREMENTS (Must complete): 1. Configure Route 53 hosted zone with failover routing policy using health checks (CORE: Route 53) 2. Deploy DynamoDB global tables across us-east-1 and us-west-2 with point-in-time recovery (CORE: DynamoDB) 3. Create Lambda functions in both regions to process patient records with environment variables for region-specific endpoints 4. Set up Route 53 health checks monitoring HTTPS endpoints in both regions with 30-second intervals 5. Configure CloudWatch alarms for DynamoDB throttling and Lambda errors with SNS notifications 6. Implement least-privilege IAM roles for all services with explicit deny for resource deletion 7. Enable CloudWatch Logs for Lambda functions with 30-day retention period 8. Add resource tags for cost allocation: Environment=Production, DR-Tier=Critical OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Backup for automated DynamoDB backups across regions (OPTIONAL: AWS Backup) - provides additional data protection  Implement EventBridge rules for cross-region event replication (OPTIONAL: EventBridge) - enables event-driven failover automation  Add for configuration management (OPTIONAL: ) - centralizes configuration across regions Expected output: A single CloudFormation template in JSON format that creates all primary region resources and outputs the necessary values for the secondary region stack. The template should include proper error handling, rollback triggers, and clear documentation for each resource.","A healthcare SaaS company needs to implement a disaster recovery solution for their patient record management system. The current single-region deployment has experienced outages affecting thousands of patients, and regulatory compliance requires 99.99% uptime with automated failover capabilities.","""Multi-region active-passive disaster recovery deployment spanning us-east-1 (primary) and us-west-2 (secondary). Uses Route 53 for DNS failover, DynamoDB global tables for data replication, Lambda for business logic, and for global content delivery. Requires AWS CLI configured with appropriate permissions, CloudFormation StackSets enabled for cross-region deployments. VPCs in both regions with private subnets for compute resources, NAT gateways for outbound connectivity. KMS keys must be pre-created in both regions for encryption requirements.""","[""Primary region must be us-east-1 with failover to us-west-2"", ""RTO (Recovery Time Objective) must be under 5 minutes"", ""RPO (Recovery Point Objective) must be under 1 minute"", ""All data must be encrypted at rest using AWS KMS with customer-managed keys"", ""Route 53 health checks must monitor both regions every 30 seconds"", ""DynamoDB global tables must use on-demand billing mode"", ""Lambda functions must have reserved concurrent executions set to 100"", "" distribution must use custom SSL certificate from ACM""]"
g3w3l6,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy a secure payment card data processing pipeline with end-to-end encryption.

MANDATORY REQUIREMENTS (Must complete):
1. Create an S3 bucket with AES-256 encryption using customer-managed KMS key (CORE: S3)
2. Deploy Lambda function in VPC private subnets for PCI data transformation (CORE: Lambda)
3. Configure VPC endpoints for S3 and KMS to avoid internet routing
4. Implement bucket policies that enforce encryption in transit (aws:SecureTransport)
5. Create Lambda execution role with minimal permissions following least privilege
6. Enable S3 bucket versioning with MFA delete protection
7. Configure S3 event notifications to trigger Lambda on object creation
8. Set up CloudWatch Logs with KMS encryption for Lambda function logs
9. Create KMS key with automatic rotation enabled
10. Implement S3 lifecycle policies to transition data to Glacier after 90 days

OPTIONAL ENHANCEMENTS (If time permits):
 Add AWS Config rules for compliance monitoring (OPTIONAL: Config) - automates compliance checks
 Implement CloudTrail logging for audit trails (OPTIONAL: CloudTrail alternative using S3 access logs) - improves audit capabilities
 Add SNS topic for security alert notifications (OPTIONAL: SNS) - enables real-time alerting

Expected output: A complete CloudFormation JSON template that creates a PCI-compliant data processing pipeline with customer-managed encryption, VPC isolation, and comprehensive security controls.",A financial services company needs to implement a secure data processing pipeline that meets PCI-DSS compliance requirements. The pipeline must process sensitive payment card data while maintaining strict security boundaries and audit trails. All data must be encrypted at rest and in transit with customer-managed encryption keys.,"""Secure multi-AZ deployment in us-east-1 for PCI-DSS compliant payment processing. Uses Lambda for data transformation, S3 for encrypted storage with customer-managed KMS keys, and VPC endpoints for private connectivity. Requires AWS CLI 2.x configured with appropriate permissions. Infrastructure spans 3 availability zones with private subnets only, no NAT gateways or internet gateways. All traffic routes through VPC endpoints for S3 and KMS services.""","[""Use only customer-managed KMS keys for all encryption operations"", ""Implement strict network isolation with no direct internet access"", ""Enable versioning and MFA delete on all S3 buckets"", ""Configure Lambda functions with maximum 3-minute timeout"", ""Deploy all resources with CloudFormation stack termination protection disabled""]"
p1a5e7,,CloudFormation,JSON,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CloudFormation template to refactor and optimize an existing ECS-based microservices architecture by fixing critical issues and improving resource efficiency. MANDATORY REQUIREMENTS (Must complete): 1. Restructure template to eliminate circular dependencies between ECS services and database (CORE: ECS) 2. Convert hardcoded subnet IDs and security groups to dynamic references using Fn::ImportValue (CORE: Aurora) 3. Implement conditional logic to support both development (t3.micro) and production (m5.large) instance types 4. Replace inline IAM policies with managed policies and fix overly permissive ""*"" resource permissions 5. Add DependsOn attributes to ensure proper resource creation order for ECS task definitions 6. Configure Aurora with proper backup retention (7 days) and enable deletion protection for production 7. Fix CloudWatch Log Groups that are missing retention policies causing unbounded storage costs 8. Implement proper parameter validation with AllowedValues and ConstraintDescription 9. Add resource tagging strategy with Environment, CostCenter, and Application tags OPTIONAL ENHANCEMENTS (If time permits):  Add for database credentials (OPTIONAL: ) - improves security posture  Implement CloudFormation StackSets for multi-region deployment (OPTIONAL: StackSets) - enables disaster recovery  Add rules for compliance checking (OPTIONAL: ) - ensures ongoing compliance Expected output: An optimized CloudFormation JSON template that resolves all circular dependencies, implements proper parameterization, and reduces deployment time by 50% while maintaining security best practices.","A financial services company has inherited a poorly performing CloudFormation stack that manages their transaction processing system. The current template suffers from circular dependencies, hardcoded values, and inefficient resource configurations causing frequent deployment failures and high operational costs.","""Production infrastructure in us-east-1 region hosting critical transaction processing workloads. Current stack includes ECS Fargate cluster with 3 microservices, Aurora MySQL cluster (db.r5.large), and Application Load Balancer. VPC spans 2 availability zones with public and private subnets. Requires AWS CLI 2.x configured with appropriate IAM permissions. Stack must support blue-green deployments and handle 10,000 concurrent transactions. Development environment mirrors production but uses smaller instance types.""","[""Template must be valid JSON format compatible with CloudFormation"", ""All resources must use CloudFormation intrinsic functions instead of hardcoded values"", ""Stack creation must complete within 15 minutes in both environments"", ""No use of custom resources or Lambda-backed custom providers"", ""Template size must not exceed 51,200 bytes limit"", ""All IAM roles must follow principle of least privilege with no wildcard actions"", ""Database passwords must not be stored in template parameters"", ""ECS task definitions must specify exact CPU and memory allocations"", ""Template must be backwards compatible with existing stack updates""]"
q3y8r7,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy an infrastructure analysis system that validates CloudFormation templates for security compliance. The configuration must: 1. Deploy a Lambda function (Python 3.11, 1024MB memory) that reads CloudFormation templates from S3 and analyzes them for security issues. 2. Create a DynamoDB table with on-demand billing to store analysis results with template name, timestamp, and compliance findings. 3. Configure an S3 bucket with versioning enabled and public access blocked to store CloudFormation templates for analysis. 4. Implement IAM roles for Lambda with permissions to read from S3, write to DynamoDB, and create CloudWatch Logs. 5. Set up CloudWatch Logs group with 30-day retention for Lambda function logs. 6. Configure Lambda environment variables for DynamoDB table name and S3 bucket name. 7. Add resource tags for Environment: qa-validation and Purpose: compliance-analysis. 8. Ensure all resources have DeletionPolicy set to Delete for test environment cleanup. Expected output: A complete CloudFormation JSON template that deploys a functional infrastructure analysis system capable of parsing CloudFormation templates from S3, performing security compliance checks, and storing detailed results in DynamoDB with proper logging and monitoring.","A financial services company needs to validate their CloudFormation templates against security compliance standards before deployment. They require an automated infrastructure analysis tool that can parse templates, identify security misconfigurations, and generate detailed compliance reports for their audit team.","""AWS infrastructure deployed in us-east-1 for CloudFormation template analysis and compliance validation. Core services include Lambda for template processing, DynamoDB for storing analysis results, and S3 for template storage. Requires CloudFormation JSON templates, AWS CLI configured with appropriate permissions. VPC not required as all services are managed. Lambda runtime Python 3.11 with boto3 SDK. DynamoDB configured with on-demand capacity for variable workloads. CloudWatch Logs for Lambda execution monitoring and audit trails.""","[""Must analyze CloudFormation templates stored in S3 for security compliance issues"", ""Lambda function must process templates within 30 seconds timeout limit"", ""DynamoDB table must use on-demand billing mode for cost optimization"", ""All IAM roles must follow least-privilege principle with no wildcard permissions"", ""Lambda must generate JSON-formatted compliance reports with severity levels"", ""S3 buckets must have versioning enabled and block all public access"", ""CloudWatch Logs retention must be set to 30 days for audit requirements"", ""Lambda must validate against at least 10 predefined security rules"", ""All resources must have deletion protection disabled for testing environments""]"
z7m6b6,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to deploy a production-ready EKS cluster for microservices workloads. MANDATORY REQUIREMENTS (Must complete): 1. Create an EKS cluster with Kubernetes 1.28+ in private subnets across 3 AZs (CORE: EKS). 2. Configure a managed node group with 3-6 t3.medium instances using Auto Scaling (CORE: EC2 Auto Scaling). 3. Enable all EKS control plane logging types to CloudWatch Logs. 4. Set up OIDC provider for IAM Roles for Service Accounts (IRSA). 5. Create node security groups allowing only ports 443, 10250, and 53 for pod communication. 6. Configure KMS key for EKS secrets encryption at rest. 7. Implement least-privilege IAM roles for cluster and node groups. 8. Tag all resources with Environment=Production and ManagedBy=CloudFormation. 9. Set DeletionPolicy to Delete for all resources to enable clean teardown. OPTIONAL ENHANCEMENTS (If time permits):  Add EKS add-ons for CoreDNS, kube-proxy, and vpc-cni (OPTIONAL: EKS Add-ons) - standardizes cluster networking.  Implement AWS Load Balancer Controller using IRSA (OPTIONAL: ELB) - enables ingress management.  Configure Container Insights for cluster monitoring (OPTIONAL: CloudWatch Container Insights) - provides deep observability. Expected output: A complete CloudFormation JSON template that deploys a secure, production-grade EKS cluster with managed node groups, proper networking isolation, comprehensive logging, and IRSA capability for workload authentication.",A financial services company needs to modernize their monolithic application architecture by deploying microservices on Kubernetes. They require a managed EKS cluster with strict security controls and automated node management to support their containerized workloads while maintaining compliance with financial regulations.,"""Production EKS infrastructure deployed in us-east-1 region using AWS EKS for Kubernetes orchestration, EC2 Auto Scaling Groups for worker nodes, and VPC with private subnets across 3 availability zones. Requires CloudFormation JSON templates, AWS CLI 2.x configured with appropriate permissions. Infrastructure includes EKS control plane, managed node groups with t3.medium instances, KMS encryption, CloudWatch logging, and OIDC provider for pod-level IAM authentication. VPC must have private subnets with NAT gateways for outbound connectivity.""","[""EKS cluster must use Kubernetes version 1.28 or higher"", ""Control plane logging must be enabled for all log types (api, audit, authenticator, controllerManager, scheduler)"", ""Node groups must use Amazon Linux 2 EKS optimized AMIs only"", ""All worker nodes must be deployed across at least 3 availability zones"", ""EKS cluster endpoint must be private with no public access allowed"", ""OIDC provider must be configured for the cluster to enable IRSA (IAM Roles for Service Accounts)"", ""Node groups must have encryption at rest enabled using AWS managed KMS keys"", ""Security groups must restrict pod-to-pod communication to necessary ports only"", ""All IAM roles must follow least privilege principle with no wildcard permissions""]"
d9e7w2,,CloudFormation,JSON,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a CloudFormation template to implement a multi-region disaster recovery architecture for a payment processing system. The configuration must: 1. Deploy Aurora Global Database with a primary cluster in us-east-1 and secondary in us-west-2. 2. Configure DynamoDB Global Tables for transaction state management across both regions. 3. Set up Lambda functions in both regions for payment webhook processing. 4. Implement S3 buckets with cross-region replication for payment receipts and audit logs. 5. Create Route53 hosted zone with failover routing policies between regions. 6. Configure health checks that validate database connectivity and Lambda function availability. 7. Store all database endpoints and connection strings in with KMS encryption. 8. Use nested stacks to separate network, compute, storage, and database layers. 9. Output primary and secondary endpoint URLs for application configuration. 10. Include CloudWatch alarms for RTO/RPO monitoring with notifications. Expected output: A complete CloudFormation JSON template that deploys identical infrastructure in both regions with automated failover capabilities, meeting the 15-minute RPO and 30-minute RTO requirements.","A financial services company requires a disaster recovery solution for their critical payment processing application. The primary region experienced a 4-hour outage last quarter, resulting in significant revenue loss. Management has mandated a multi-region DR strategy with RPO of 15 minutes and RTO of 30 minutes.","""Multi-region AWS deployment spanning us-east-1 (primary) and us-west-2 (disaster recovery). Infrastructure includes Aurora Global Database (MySQL 8.0), DynamoDB Global Tables, Lambda functions for payment processing, S3 buckets with cross-region replication, and Route53 failover routing. Requires AWS CLI 2.x configured with appropriate IAM permissions for CloudFormation, DynamoDB, Lambda, S3, Route53, and . VPCs in both regions with private subnets across 3 availability zones, VPC peering for cross-region connectivity.""","[""Use JSON format exclusively for the CloudFormation template"", ""Primary region must be us-east-1 and DR region must be us-west-2"", "" Aurora MySQL must use Global Database with automated backups"", ""Lambda functions must be packaged as inline code, not S3 references"", ""All S3 buckets must have versioning enabled and cross-region replication"", ""Route53 health checks must monitor both regions with 30-second intervals"", ""DynamoDB global tables must have point-in-time recovery enabled"", ""Use for storing database endpoints"", ""CloudFormation stacks must use nested stacks for modularity"", ""All resources must have DeletionPolicy set to Retain for production data""]"
r4o3d3,,CloudFormation,JSON,expert,Application Deployment,Web Application Deployment,"Create a CloudFormation template to deploy a containerized fraud detection system with blue-green deployment capabilities. The configuration must: 1. Create an ECS cluster with Container Insights enabled. 2. Define task definitions for fraud-detector service with X-Ray sidecar. 3. Configure ECS service using FARGATE with desired count of 4 tasks. 4. Set up Application Load Balancer with target groups for blue and green environments. 5. Implement CodeDeploy application and deployment group for ECS blue-green deployments. 6. Create CloudWatch alarms monitoring ALB 5XX error rate with 5% threshold. 7. Configure automatic rollback in CodeDeploy based on CloudWatch alarms. 8. Output the ALB DNS name and CodeDeploy deployment group name. Expected output: A complete CloudFormation JSON template that deploys the ECS infrastructure with automated blue-green deployment pipeline, including all necessary IAM roles, security groups, and monitoring configuration for production-ready container orchestration.",A financial services company needs to deploy their fraud detection microservices on AWS ECS with blue-green deployment capabilities. The system processes real-time transaction data and requires zero-downtime deployments with automatic rollback on failure. The architecture must support high-frequency updates while maintaining strict isolation between production and staging environments.,"""Multi-AZ ECS Fargate cluster deployed in us-east-1 across 3 availability zones. Uses Application Load Balancer for traffic distribution, CodeDeploy for blue-green deployments, and ECR for container image storage. VPC configured with public subnets for ALB and private subnets for ECS tasks. NAT Gateways provide outbound internet access. CloudWatch Container Insights and X-Ray enabled for full observability. Requires AWS CLI 2.x configured with appropriate permissions for ECS, CodeDeploy, and CloudFormation operations.""","[""Use ECS Service with FARGATE launch type and platform version 1.4.0"", ""Implement CloudWatch Container Insights for all ECS tasks"", ""Configure task definitions with exactly 2048 CPU units and 4096 memory"", ""Use Application Load Balancer with path-based routing for /api/* and /health"", ""Set up CodeDeploy for blue-green deployments with 5-minute traffic shift"", ""Enable AWS X-Ray sidecar container in each task definition"", ""Implement automatic rollback based on CloudWatch alarms for 5XX errors > 5%""]"
r5u3h5,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a CloudFormation template to implement a centralized observability platform for multi-account AWS environments. The configuration must: 1. Deploy a CloudWatch Logs destination in the monitoring account with KMS encryption and resource policy allowing specific OUs. 2. Create subscription filters in member accounts to forward logs to the central destination. 3. Configure CloudWatch Contributor Insights rules to analyze high-cardinality metrics from ECS tasks. 4. Set up service map with encryption and sampling rules for 10% of requests. 5. Implement CloudWatch Synthetics canaries to monitor critical API endpoints every 5 minutes. 6. Create a CloudWatch dashboard with cross-account widgets displaying service health metrics. 7. Configure EventBridge rules to capture CloudTrail API calls for security monitoring. 8. Deploy Parameter Store entries for storing monitoring configuration. 9. Set up CloudWatch Anomaly Detector for ECS task count and ALB request metrics. 10. Create composite alarms that aggregate health signals across multiple services. Expected output: A parameterized CloudFormation template that deploys a complete observability solution with cross-account log aggregation, distributed tracing, synthetic monitoring, and automated anomaly detection capabilities.","A financial services company needs centralized observability for their distributed microservices architecture. The infrastructure must capture metrics, logs, and traces from multiple AWS services while maintaining compliance with financial regulations for data retention and access control.","""Multi-account AWS environment in us-east-1 region with enabled. Central monitoring account aggregates logs and metrics from 15+ production accounts across 3 organizational units (Production, Staging, Development). Uses CloudWatch Logs, CloudWatch Metrics, for tracing, and for configuration. VPC flow logs enabled across all accounts. Requires CloudFormation StackSets for multi-account deployment. Each account runs 10-20 microservices on ECS Fargate with Application Load Balancers.""","[""All CloudWatch Log Groups must have KMS encryption using a customer-managed key"", ""Metric filters must trigger alarms for error rates exceeding 1% over 5-minute periods"", ""Cross-account log aggregation must use with specific OU targeting"", ""All IAM roles must use external ID for assume role policies with minimum 16-character complexity"", ""Log retention must be exactly 2555 days (7 years) for compliance with no manual override allowed""]"
l4x3u0,,CloudFormation,JSON,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a CloudFormation template to orchestrate a zero-downtime database migration from on-premises MySQL to AWS Aurora. MANDATORY REQUIREMENTS (Must complete): 1. Create Aurora MySQL cluster with writer and reader endpoints (CORE: Aurora) 2. Configure DMS replication instance and migration task (CORE: DMS) 3. Set up VPC with public/private subnets across 3 AZs 4. Implement Route 53 weighted routing between legacy and new endpoints 5. Configure security groups allowing DMS access to source database 6. Create CloudWatch dashboard displaying replication metrics 7. Output migration task ARN and cluster endpoints OPTIONAL ENHANCEMENTS (If time permits):  Add Lambda function for automated cutover (OPTIONAL: Lambda) - enables one-click migration completion  Implement notifications for replication events (OPTIONAL: ) - improves monitoring visibility  Add parameters for connection strings (OPTIONAL: ) - simplifies application configuration updates Expected output: A CloudFormation template that creates all infrastructure for database migration, enables continuous replication from on-premises MySQL to Aurora, and provides monitoring capabilities to track migration progress.","A financial services company needs to migrate their legacy on-premises payment processing system to AWS. The system currently handles 50,000 transactions daily and requires zero downtime during the migration phase.","""Production environment in us-east-1 region spanning 3 availability zones. Requires VPC with public and private subnets, NAT gateways for outbound traffic, and VPN connection to on-premises datacenter. Uses Aurora MySQL for primary database, DMS for migration, Route 53 for DNS management, and Application Load Balancer for traffic distribution. AWS CLI and CloudFormation CLI must be configured with appropriate IAM permissions for DMS, VPC, and Route 53 services.""","[""Use AWS Database Migration Service (DMS) for continuous data replication"", ""Implement blue-green deployment strategy with weighted routing"", ""Configure read replicas in at least two availability zones"", ""Set up automated failover with RTO under 60 seconds"", ""Use for all database credentials"", ""Enable point-in-time recovery with 7-day retention"", ""Implement CloudWatch alarms for replication lag monitoring""]"
c1r4z6,,CloudFormation,JSON,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a template to deploy a production-ready EKS cluster for microservices migration. MANDATORY REQUIREMENTS (Must complete): 1. Create an EKS cluster with Kubernetes 1.28+ and enable control plane logging (CORE: EKS) 2. Deploy managed node groups using Bottlerocket AMI across 3 AZs (CORE: EC2) 3. Configure OIDC identity provider for the cluster 4. Create IAM roles with IRSA capability for pod-level permissions 5. Attach AWS Load Balancer Controller IAM policy to node instance role 6. Configure GP3 encrypted EBS CSI driver with appropriate IAM permissions 7. Set up CloudWatch Logs retention to 30 days for control plane logs 8. Implement pod security standards with restricted baseline 9. Output cluster endpoint, OIDC provider URL, and node role ARN OPTIONAL ENHANCEMENTS (If time permits):  Add Fargate profile for system workloads (OPTIONAL: Fargate) - reduces operational overhead  Configure access for node troubleshooting (OPTIONAL: ) - improves debugging  Implement for ECR and (OPTIONAL: ) - reduces data transfer costs Expected output: A complete JSON template that creates a production-grade EKS cluster with managed node groups, proper IAM configuration, and security controls. The template should include all necessary outputs for kubectl configuration and application deployment.",A financial services company needs to modernize their monolithic application by transitioning to microservices on Kubernetes. They require a production-grade EKS cluster with strict security requirements for PCI DSS compliance. The infrastructure must support both stateless API services and stateful database workloads with appropriate storage classes.,"""Production EKS environment deployed in us-east-1 with managed node groups across 3 AZs. Uses EKS 1.28, VPC with private subnets (10.0.0.0/16), NAT Gateways for outbound traffic. Requires JSON format, AWS CLI 2.x configured with appropriate permissions. Integrates with Route53 for DNS, ACM for certificates. Node groups use m5.large instances with Bottlerocket AMI. CloudWatch Container Insights enabled for monitoring.""","[""EKS cluster must use Kubernetes version 1.28 or higher"", ""Node groups must use Bottlerocket AMI for enhanced security"", ""Implement IRSA (IAM Roles for Service Accounts) for pod-level permissions"", ""Enable control plane logging for audit, authenticator, and scheduler"", ""Configure OIDC provider for external authentication integration"", ""Use GP3 EBS volumes with encryption enabled for persistent storage"", ""Implement pod security standards with restricted baseline"", ""Node groups must span exactly 3 availability zones"", ""Enable AWS Load Balancer Controller via IAM policy attachment""]"
t0l8v1,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to orchestrate a zero-downtime migration from on-premises infrastructure to AWS cloud. MANDATORY REQUIREMENTS (Must complete): 1. Set up Aurora PostgreSQL cluster with read replicas across two regions (CORE: Aurora) 2. Deploy services with blue-green deployment capability (CORE: ) 3. Implement AWS DMS tasks for continuous data replication from on-premises database 4. Create Lambda functions to validate data consistency between environments 5. Configure Application Load Balancers with weighted target groups for traffic shifting 6. Implement custom Pulumi ComponentResource for migration orchestration 7. Set up CloudWatch dashboards to monitor replication lag and service health 8. Create rollback mechanism using Pulumi stack exports and automation API 9. Implement feature flags using 10. Configure Route 53 health checks with automated DNS failover OPTIONAL ENHANCEMENTS (If time permits):  Add Step Functions for complex migration workflows (OPTIONAL: Step Functions) - improves orchestration visibility  Implement rules for migration events (OPTIONAL: ) - enables event-driven automation  Add for point-in-time recovery (OPTIONAL: ) - provides additional data protection Expected output: A complete Pulumi TypeScript program with modular components for each migration phase, custom providers for legacy integration, comprehensive error handling, and automated rollback capabilities. The solution should support incremental migration with real-time synchronization and zero-downtime cutover.","A fintech startup is migrating their payment processing infrastructure from a legacy on-premises setup to AWS. The existing system processes 50,000 transactions daily and requires zero downtime during migration. The architecture must support blue-green deployments for seamless cutover.","""Multi-environment AWS infrastructure spanning us-east-1 and us-west-2 regions for migration from on-premises to cloud. Includes Aurora PostgreSQL clusters, services, Application Load Balancers, and Lambda functions for data synchronization. VPC setup with Transit Gateway connecting legacy data center via Direct Connect. Requires Pulumi 3.x with TypeScript, AWS CLI v2 configured with appropriate IAM roles. Migration involves three environments: legacy (on-prem), staging (AWS), and production (AWS). Network configuration includes private subnets across 3 AZs per region with NAT instances for cost optimization.""","[""Use AWS SDK v3 for all AWS service interactions"", ""Implement custom resource providers for migration state tracking"", ""Database migration must use AWS DMS with CDC enabled"", ""All secrets must be stored in with automatic rotation"", ""Use Pulumi stack references for cross-environment dependencies"", ""Implement retry logic with exponential backoff for all API calls"", ""Resource names must follow pattern: {env}-{service}-{resource}-{random}"", ""Use TypeScript strict mode with no implicit any types"", ""All Lambda functions must use ARM64 architecture"", ""Implement custom Pulumi dynamic providers for legacy system integration""]"
y1j2r2,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement a multi-region disaster recovery infrastructure. MANDATORY REQUIREMENTS (Must complete): 1. Set up Aurora Global Database with a primary cluster in us-east-1 and secondary in us-west-2 (CORE: ) 2. Deploy ECS Fargate services in both regions with ALB and target groups (CORE: ECS) 3. Configure Route53 hosted zone with health checks and failover routing policy 4. Implement S3 bucket replication from primary to DR region with versioning enabled 5. Create EventBridge rules in both regions for cross-region event forwarding 6. Set up CloudWatch dashboards showing metrics from both regions 7. Configure VPC peering between regions with proper route tables 8. Export critical resource ARNs and endpoints using stack outputs OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Backup for automated cross-region backup management (OPTIONAL: AWS Backup) - provides centralized backup governance  Implement Lambda functions for automated failover testing (OPTIONAL: Lambda) - enables regular DR drills  Configure replication (OPTIONAL: ) - synchronizes configuration across regions Expected output: A modular Pulumi TypeScript program with separate components for each service, utilizing Pulumi's ComponentResource pattern. The program should deploy infrastructure in both regions with proper dependencies and output the primary endpoint URL and failover endpoint URL.",A healthcare SaaS platform requires a multi-region disaster recovery setup to meet strict RPO/RTO requirements of 15 minutes. The primary region handles patient data processing while the secondary region must maintain a warm standby ready for automatic failover.,"""Multi-region AWS deployment spanning us-east-1 (primary) and us-west-2 (DR) regions. Architecture includes ECS Fargate containers running Node.js microservices, Aurora PostgreSQL Global Database, S3 buckets for static assets, and Route53 for DNS management. VPCs in both regions with private subnets connected via VPC peering. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 16+, and AWS credentials with permissions for multi-region resource creation. NAT Gateways in each region for outbound connectivity.""","[""Use Route53 health checks with failover routing policy for automatic DNS failover"", ""Deploy identical ECS services in both regions with cross-region container registry replication"", ""Configure Aurora Global Database with automated backups and point-in-time recovery"", ""Implement cross-region S3 bucket replication with lifecycle policies"", ""Use EventBridge Global Endpoints for event routing between regions"", ""Set up CloudWatch cross-region dashboard for unified monitoring"", ""All resources must be tagged with Environment, Region, and DR-Role tags"", ""Use Pulumi stack references to share outputs between regional stacks""]"
k7y7h2,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready three-tier architecture with strict security controls. The configuration must: 1. Create a VPC with 10.0.0.0/16 CIDR spanning 3 AZs with public (10.0.1-3.0/24), private (10.0.11-13.0/24), and data (10.0.21-23.0/24) subnets. 2. Deploy an Application Load Balancer in public subnets with HTTPS listener using ACM certificate. 3. Configure an Auto Scaling Group with 2-6 t3.medium instances in private subnets running Ubuntu 22.04 AMI. 4. Set up RDS Aurora PostgreSQL 15.x cluster with one writer and one reader instance in data subnets. 5. Create S3 bucket for static content with distribution for global delivery. 6. Implement VPC Flow Logs to S3 with 5-minute intervals for network monitoring. 7. Configure Systems Manager Session Manager for secure instance access without SSH. 8. Deploy all resources with consistent tagging and enable AWS Config for compliance tracking. 9. Create CloudWatch dashboard with key metrics for ALB, ASG, RDS, and S3. 10. Implement budget alerts at $500 and $1000 thresholds. Expected output: A complete Pulumi program that provisions all infrastructure components with proper dependencies, exports critical resource IDs and endpoints, and includes a comprehensive README with deployment instructions and architecture diagram reference.",A financial services startup needs to establish a secure cloud foundation for their payment processing platform. The infrastructure must support strict compliance requirements with network isolation between tiers and comprehensive audit logging. The platform will process sensitive customer data requiring encryption at rest and in transit.,"""Production-grade multi-tier infrastructure deployed in us-east-1 across 3 availability zones. Core services include VPC with public/private/data subnets, ALB for load balancing, Auto Scaling Group with EC2 instances running Ubuntu 22.04, RDS Aurora PostgreSQL cluster in Multi-AZ configuration, and S3 for static assets. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate credentials. Network architecture includes Internet Gateway for public subnets, NAT Gateways in each AZ for private subnet outbound traffic, and VPC endpoints for S3 access from private subnets.""","[""VPC must use 10.0.0.0/16 CIDR block with exactly 3 availability zones"", ""Each tier (web, app, data) must have dedicated subnets with non-overlapping CIDR ranges"", ""NAT Gateways must be deployed in each AZ for high availability"", ""All EC2 instances must use IMDSv2 with hop limit of 1"", ""RDS instances must have automated backups with 7-day retention and encryption enabled"", ""ALB must use AWS Certificate Manager for TLS termination"", ""Security groups must follow least privilege with no 0.0.0.0/0 ingress rules except ALB port 443"", ""All resources must be tagged with Environment, Project, and CostCenter tags"", ""S3 buckets must have versioning enabled and block all public access""]"
h8q3i4,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster with container orchestration capabilities. The configuration must: 1. Create an EKS cluster version 1.28 with OIDC provider enabled for IRSA. 2. Configure two managed node groups: one using t3.large Spot instances (min 2, max 10) for general workloads, another using t3.xlarge On-Demand (min 1, max 5) for critical services. 3. Set up Fargate profiles for kube-system and monitoring namespaces only. 4. Create private ECR repositories with lifecycle policies to retain only the last 10 images. 5. Deploy AWS Load Balancer Controller using Helm with proper IRSA configuration. 6. Install Cluster Autoscaler as an EKS managed add-on with appropriate scaling policies. 7. Configure KMS key for envelope encryption of Kubernetes secrets with automatic rotation. 8. Implement Calico network policies to enforce namespace isolation and restrict egress to AWS services only. 9. Deploy metrics-server and container insights agent for monitoring. 10. Output cluster endpoint, OIDC issuer URL, and kubeconfig for kubectl access. Expected output: A fully functional EKS cluster with automated scaling, secure container orchestration, and monitoring capabilities ready for microservices deployment.",A fintech startup needs to deploy their microservices architecture on AWS EKS with strict security requirements and automated scaling. The platform must support both stateless API services and stateful data processing workloads with different resource requirements and isolation boundaries.,"""Production-grade EKS infrastructure in us-east-2 with multi-AZ deployment across 3 availability zones. Requires Pulumi CLI 3.x with TypeScript support, AWS CLI configured with appropriate permissions, kubectl 1.28+, and Node.js 18+. VPC spans 10.0.0.0/16 with dedicated subnets for EKS nodes, pods, and Fargate profiles. Integration with AWS Load Balancer Controller for ingress management. Container insights enabled for CloudWatch monitoring.""","[""EKS cluster must use managed node groups with Spot instances for cost optimization"", ""Implement IRSA (IAM Roles for Service Accounts) for pod-level AWS permissions"", ""Use Fargate profiles for system-critical workloads only"", ""All container images must be scanned and stored in private ECR repositories"", ""Network policies must enforce namespace isolation and egress restrictions"", ""Cluster autoscaler and metrics server must be deployed as managed add-ons"", ""Enable envelope encryption for Kubernetes secrets using AWS KMS""]"
i5g0d0,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a payment processing infrastructure that maintains consistency across three environments (dev, staging, prod). The configuration must: 1. Define a reusable Pulumi ComponentResource class for the payment processing stack that accepts environment-specific configurations. 2. Create Lambda functions with ARM64 architecture for webhook processing and transaction validation (CORE: Lambda). 3. Deploy DynamoDB tables with on-demand billing for transaction storage with environment-specific read/write capacity (CORE: DynamoDB). 4. Implement SQS queues with different message retention periods per environment (CORE: SQS). 5. Configure VPCs with appropriate subnet configurations based on environment (multi-AZ for prod, single-AZ for dev/staging). 6. Use Pulumi configuration files (Pulumi.dev.yaml, Pulumi.staging.yaml, Pulumi.prod.yaml) to manage environment-specific values. 7. Implement consistent resource naming with environment prefixes (e.g., prod-payment-processor-lambda). 8. Create CloudWatch dashboards that automatically reflect the deployed environment's metrics. 9. Ensure all resources are tagged with mandatory tags: Owner, Environment, CostCenter, and DeploymentMethod. 10. Export stack outputs including Lambda function ARNs, DynamoDB table names, and SQS queue URLs for each environment. Expected output: A Pulumi TypeScript program with a reusable ComponentResource that deploys identical infrastructure across environments, using configuration files to manage environment-specific variations while maintaining consistency in architecture and security configurations.","A fintech startup needs to maintain identical infrastructure across development, staging, and production environments for their payment processing platform. The infrastructure must support automatic environment replication with consistent configurations while allowing environment-specific overrides for database sizes and compute capacity.","""Multi-environment AWS deployment across us-east-1 (production), us-west-2 (staging), and eu-west-1 (development). Each environment requires VPC with public and private subnets, Lambda functions for payment processing, DynamoDB for transaction storage, and SQS for async processing. Setup requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate credentials. Network architecture includes NAT gateways in production only, with dev/staging using NAT instances for cost savings.""","[""Use Pulumi configuration files to manage environment-specific values with strong typing"", ""Implement a single stack that can deploy to any environment based on configuration"", ""All Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""DynamoDB tables must use point-in-time recovery and encryption at rest"", ""Environment names must be prefixed to all resource names for clear identification"", ""Implement automated tagging strategy with Owner, Environment, and CostCenter tags"", ""Use Pulumi ComponentResource pattern for reusable infrastructure modules"", ""All IAM roles must follow principle of least privilege with no wildcard actions"", ""Production environment must use multi-AZ deployments while dev/staging use single-AZ""]"
m1b6o1,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to build an observability platform for distributed microservices. MANDATORY REQUIREMENTS (Must complete): 1. Deploy CloudWatch Log Groups with 365-day retention and KMS encryption for 5 microservices (CORE: CloudWatch) 2. Create service map with custom segments and subsegments for distributed tracing (CORE: ) 3. Configure SNS topics with email subscriptions for critical alerts using customer-managed KMS keys (CORE: SNS) 4. Implement metric filters on log groups to extract custom metrics for error rates and latencies 5. Create CloudWatch dashboards with widgets for service health, error rates, and P99 latencies 6. Set up CloudWatch alarms for each service with compound conditions (CPU > 80% AND errors > 5%) 7. Configure cross-account EventBridge rules to aggregate events from multiple AWS accounts 8. Deploy Lambda function to process and enrich metrics before publishing to CloudWatch 9. Implement IAM roles with explicit deny policies for unauthorized metric deletion 10. Enable CloudWatch Logs Insights queries with saved query definitions OPTIONAL ENHANCEMENTS (If time permits):  Add Kinesis Data Firehose for log archival to S3 (OPTIONAL: Kinesis Firehose) - enables long-term analytics  Implement OpsCenter integration (OPTIONAL: ) - centralizes operational issues  Add CloudWatch Synthetics canaries for endpoint monitoring (OPTIONAL: CloudWatch Synthetics) - proactive monitoring Expected output: A Pulumi program that deploys a complete observability platform with centralized logging, distributed tracing, custom metrics, and automated alerting for a microservices architecture.","A financial services company needs centralized observability for their microservices architecture. They require real-time monitoring, distributed tracing, and custom metrics collection to meet strict SLA requirements and regulatory compliance for transaction processing systems.","""Production observability infrastructure deployed in us-east-1 for financial microservices monitoring. Uses CloudWatch for metrics and logs, for distributed tracing, SNS for alerting, and Lambda for custom metric processing. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate permissions. required for private communication with AWS services. Multi-account setup with centralized logging account and cross-account roles for metric collection.""","[""All Lambda functions must have tracing enabled with custom segments for database queries"", ""CloudWatch Logs retention must be set to 365 days for compliance with financial regulations"", ""Custom metrics must be published to CloudWatch with dimensions for service_name, environment, and region"", ""SNS topics for alerts must use KMS encryption with customer-managed keys"", ""All IAM roles must follow least-privilege principle with explicit deny statements for sensitive operations""]"
q5o5d7,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a high-performance trading dashboard web application. MANDATORY REQUIREMENTS (Must complete): 1. Deploy React frontend to S3 with CloudFront distribution and OAI (CORE: CloudFront/S3) 2. Create Application Load Balancer with target group for Lambda functions (CORE: ALB/Lambda) 3. Implement 3 Lambda functions: /api/quotes, /api/portfolio, /api/orders with 512MB memory 4. Configure DynamoDB table 'user-sessions' with userId partition key 5. Set up VPC with 2 public and 2 private subnets across 2 AZs 6. Configure security groups allowing HTTPS (443) inbound only 7. Create IAM roles with least-privilege policies for each Lambda function 8. Output CloudFront distribution URL and ALB DNS name OPTIONAL ENHANCEMENTS (If time permits):  Add ElastiCache Redis cluster for quote caching (OPTIONAL: ElastiCache) - reduces API latency  Implement API Gateway WebSocket API for real-time updates (OPTIONAL: API Gateway) - enables push notifications  Add hosted zone with custom domain (OPTIONAL: ) - improves branding Expected output: Complete Pulumi TypeScript program that deploys the trading dashboard with frontend accessible via CloudFront and backend APIs through ALB. All resources should be properly tagged with Environment=Production and Project=TradingDashboard.",A fintech startup needs to deploy their real-time trading dashboard application with strict latency requirements. The application consists of a React frontend and Node.js API backend that connects to financial data feeds and requires sub-100ms response times for user interactions.,"""Deploy to AWS us-east-1 region for proximity to NYSE data feeds. Infrastructure includes CloudFront distribution with custom domain, S3 bucket for React static assets, Lambda functions behind Application Load Balancer for API endpoints, DynamoDB for user sessions and watchlists, and ElastiCache Redis cluster for market data caching. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate credentials. VPC spans 2 AZs with private subnets for compute resources and public subnets for ALB. NAT Gateways provide outbound internet access for Lambda functions.""","[""API responses must be cached with TTL of exactly 5 seconds for market data endpoints"", ""WebSocket connections must auto-reconnect within 3 seconds of disconnection"", ""All data must be encrypted at rest using customer-managed KMS keys"", ""Lambda functions must use reserved concurrency of exactly 10 instances"", ""DynamoDB tables must use on-demand billing mode with point-in-time recovery enabled""]"
j6y6d4,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to implement a zero-downtime migration infrastructure for a payment processing system moving from on-premises to AWS. MANDATORY REQUIREMENTS (Must complete): 1. Create a VPC with public and private subnets across 3 AZs with NAT gateways (CORE: VPC) 2. Deploy ECS Fargate cluster with task definitions for payment processor containers (CORE: ECS) 3. Configure Aurora PostgreSQL with Multi-AZ, encryption, and automated backups (CORE: ) 4. Set up Application Load Balancer with target groups for blue-green deployment 5. Implement AWS DMS replication instance and tasks for database migration 6. Create CloudWatch dashboards showing migration progress metrics 7. Configure topic and subscriptions for migration status alerts 8. Export stack outputs for blue environment endpoints and green environment endpoints OPTIONAL ENHANCEMENTS (If time permits):  Add AWS WAF rules on ALB for PCI compliance (OPTIONAL: WAF) - improves security posture  Implement weighted routing for gradual traffic shifting (OPTIONAL: ) - enables canary deployments  Add AWS Secrets Manager for database credentials rotation (OPTIONAL: Secrets Manager) - enhances credential security Expected output: Complete Pulumi TypeScript program with modular components for VPC, compute, database, and migration resources. The program should support parameterized deployment for both blue and green environments with clear separation of concerns and proper error handling.",A financial services company needs to migrate their monolithic payment processing application from on-premises infrastructure to AWS. The application currently runs on VMware with Oracle Database and requires zero-downtime migration to meet regulatory requirements. The new cloud architecture must maintain strict PCI DSS compliance while modernizing the deployment model.,"""Production migration environment in us-east-1 with multi-AZ deployment across 3 availability zones. Infrastructure includes VPC with public/private subnet tiers, NAT gateways for outbound connectivity, Application Load Balancer in public subnets, ECS Fargate cluster in private subnets, Aurora PostgreSQL Multi-AZ cluster with encrypted storage. Requires Pulumi 3.x with TypeScript, AWS CLI v2 configured, Node.js 18+, and Docker for container builds. Migration tooling includes AWS Database Migration Service for data replication.""","[""Migration must support blue-green deployment pattern for zero-downtime cutover"", "" Aurora PostgreSQL must use encrypted storage with customer-managed KMS keys"", ""All compute resources must run in private subnets with no direct internet access"", ""Database migration must preserve transaction logs during the cutover window"", ""ALB must terminate SSL with ACM certificates and enforce TLS 1.2 minimum"", ""ECS tasks must use Fargate Spot instances where possible to reduce costs"", ""CloudWatch alarms must trigger notifications for any migration failures"", ""All resources must be tagged with Environment, MigrationPhase, and CostCenter""]"
n5h4q8,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a serverless webhook processing system for payment notifications. The configuration must: 1. Deploy an API Gateway REST API with a /webhooks endpoint that accepts POST requests. 2. Create a Lambda function (Node.js 18, ARM64, 1GB memory) to validate and process incoming webhooks. 3. Implement SQS queue for reliable message processing with visibility timeout of 6 minutes. 4. Configure a DLQ for failed messages with maximum receive count of 3 attempts. 5. Set up DynamoDB table 'payment-events' with partition key 'transactionId' and sort key 'timestamp'. 6. Create SNS topic for alerting on processing failures with email subscription endpoint. 7. Implement CloudWatch alarms for Lambda errors exceeding 1% error rate over 5 minutes. 8. Configure tracing for all Lambda functions with sampling rate of 10%. 9. Set up CloudWatch Logs Insights queries for analyzing payment processing patterns. 10. Create S3 bucket for CloudWatch Logs export with lifecycle policy to transition to Glacier after 90 days. Expected output: A complete Pulumi TypeScript program that creates a production-ready serverless webhook processing system with monitoring, alerting, and compliance features fully configured.",A financial technology startup needs to process millions of daily payment webhooks from various payment providers while maintaining strict compliance with PCI DSS requirements. The system must handle variable traffic patterns with cost optimization and provide detailed audit trails for regulatory compliance.,"""AWS multi-region deployment with primary in us-east-1 and disaster recovery in us-west-2. Infrastructure includes API Gateway REST API, Lambda functions with Node.js 18 runtime, DynamoDB global tables, SQS queues with DLQ configuration, SNS for notifications, and S3 for log archival. for private communication with AWS services. Requires Pulumi 3.x with TypeScript, AWS CLI v2 configured with appropriate credentials, Node.js 18+ and npm installed. The architecture spans three availability zones in each region with cross-region replication for critical data.""","[""Lambda functions must use ARM64 architecture for cost optimization"", ""All data at rest must be encrypted using AWS KMS customer-managed keys"", ""Lambda environment variables containing sensitive data must use "", ""Dead letter queues must have message retention of exactly 14 days"", ""API Gateway must implement request throttling at 1000 requests per second"", ""DynamoDB tables must use point-in-time recovery with 35-day retention"", ""CloudWatch Logs must be exported to S3 for long-term archival after 30 days"", ""Lambda functions must implement structured logging with correlation IDs"", ""All IAM policies must follow least privilege with no wildcard resource permissions"", ""SNS topics must use server-side encryption with AWS managed keys""]"
g1n6g8,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a multi-environment AWS infrastructure with centralized networking and security services. The configuration must: 1. Define reusable VPC components that create public/private subnets across 3 AZs. 2. Deploy three separate VPCs (dev, staging, prod) using stack-specific CIDR blocks. 3. Create an AWS Transit Gateway and attach all VPCs with appropriate route tables. 4. Configure AWS Secrets Manager to store database credentials with 30-day automatic rotation. 5. Implement ACM certificate provisioning with Route53 DNS validation for *.example.com. 6. Use Pulumi stack outputs to export VPC IDs, subnet IDs, and Transit Gateway attachment IDs. 7. Apply consistent tagging strategy with Environment, Project, and ManagedBy tags. 8. Configure VPC Flow Logs to CloudWatch Logs with 7-day retention. 9. Implement least-privilege security groups allowing only necessary inter-VPC communication. 10. Create Lambda function for Secrets Manager rotation using AWS-provided templates. Expected output: A modular Pulumi program with separate files for VPC, Transit Gateway, and security components, deployable across multiple stacks representing different environments with shared transit connectivity.","A fintech startup needs to establish isolated AWS environments for development, staging, and production workloads. Each environment requires strict network segmentation, centralized secrets management, and automated certificate provisioning to meet PCI-DSS compliance requirements.","""Multi-environment AWS infrastructure spanning us-east-1 and us-west-2 regions. Each environment (dev, staging, prod) requires isolated VPCs with public/private subnets across 3 availability zones. Centralized networking through AWS Transit Gateway enables secure inter-VPC communication. AWS Secrets Manager handles sensitive configuration with Lambda-based rotation. ACM manages SSL certificates with Route53 DNS validation. Requires Pulumi 3.x with TypeScript, Node.js 16+, AWS CLI configured with appropriate IAM permissions for VPC, Transit Gateway, Secrets Manager, ACM, and Route53 operations.""","[""Use Pulumi's stack configuration system to parameterize differences between environments"", ""Implement cross-stack references for shared resources like transit gateways"", ""All inter-VPC traffic must flow through AWS Transit Gateway with route tables"", ""Secrets must be stored in AWS Secrets Manager with automatic rotation enabled"", ""SSL/TLS certificates must be provisioned through AWS Certificate Manager with DNS validation""]"
y9a2t2,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a complete CI/CD pipeline for serverless microservices. MANDATORY REQUIREMENTS (Must complete): 1. Create a CodePipeline with 4 stages connecting GitHub source to blue-green deployment (CORE: CodePipeline) 2. Configure CodeBuild project with TypeScript compilation and Jest unit tests (CORE: CodeBuild) 3. Set up two Lambda functions (blue and green) with 512MB memory and Node.js 18 runtime 4. Create DynamoDB table for storing deployment history with partition key 'deploymentId' 5. Implement CodeDeploy application for Lambda blue-green deployments with automatic rollback 6. Configure S3 bucket for pipeline artifacts with server-side encryption using AWS managed keys 7. Create CloudWatch alarm monitoring Lambda error rates with SNS notifications 8. Output the pipeline execution URL and deployment status table name OPTIONAL ENHANCEMENTS (If time permits):  Add CodeArtifact repository for npm package caching (OPTIONAL: CodeArtifact) - reduces build times  Implement rule to trigger pipeline on git tags (OPTIONAL: ) - enables tag-based releases  Add tracing to Lambda functions (OPTIONAL: ) - improves deployment debugging Expected output: A Pulumi TypeScript program that creates a fully functional CI/CD pipeline with blue-green deployment capability, automated testing, and rollback mechanisms. The infrastructure should be idempotent and support multiple deployment environments through Pulumi stacks.",A fintech startup needs to implement a fully automated CI/CD pipeline for their payment processing microservices. The pipeline must support blue-green deployments with automatic rollback capabilities and integrate with their existing GitHub repository structure.,"""Production CI/CD infrastructure deployed in us-east-1 region. Uses CodePipeline for orchestration, CodeBuild for compilation and testing, CodeDeploy for blue-green deployments to Lambda functions. DynamoDB tables store deployment metadata and application state. S3 buckets host build artifacts and deployment packages. CloudWatch monitors deployment health with automated rollback triggers. VPC not required as all services are managed. Requires Pulumi CLI 3.x, Node.js 18+, TypeScript 5.x, AWS CLI configured with appropriate permissions. Multi-stage pipeline with automated testing and gradual traffic shifting between blue and green environments.""","[""Use CodePipeline with exactly 4 stages: Source, Build, Deploy-Blue, and Switch-Traffic"", ""CodeBuild projects must use compute type BUILD_GENERAL1_SMALL for cost optimization"", ""All Lambda functions must have reserved concurrent executions set to 100"", ""DynamoDB tables must use PAY_PER_REQUEST billing mode with point-in-time recovery enabled"", ""CodeDeploy must use LINEAR_10PERCENT_EVERY_10MINUTES deployment configuration"", ""All S3 buckets must have versioning enabled and lifecycle rules to delete old versions after 30 days"", ""CloudWatch alarms must trigger rollback if error rate exceeds 5% for 2 consecutive periods"", ""IAM roles must follow principle of least privilege with no inline policies allowed""]"
m9d7s9,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy identical payment processing infrastructure across three environments (dev, staging, prod) with environment-specific configurations. MANDATORY REQUIREMENTS (Must complete): 1. Create a reusable ComponentResource class that encapsulates the entire payment infrastructure (CORE: Multiple AWS services) 2. Deploy RDS Aurora PostgreSQL clusters with encrypted storage and automatic failover (CORE: RDS) 3. Implement Lambda functions using container images for payment webhook processing (CORE: Lambda) 4. Configure VPC with proper subnet isolation and security groups for each environment 5. Use Pulumi configuration to manage environment-specific values (instance sizes, retention periods) 6. Export critical outputs (endpoints, connection strings) for cross-stack references 7. Implement a validation script using Pulumi's automation API to compare configurations 8. Create separate stack configurations for dev, staging, and prod environments 9. Ensure all resources follow consistent naming conventions with environment prefixes OPTIONAL ENHANCEMENTS (If time permits):  Add for database credentials rotation (OPTIONAL: ) - improves security posture  Implement CloudFront distribution for static assets (OPTIONAL: CloudFront) - reduces latency globally  Add for complex payment workflows (OPTIONAL: ) - enables visual workflow management Expected output: A complete Pulumi TypeScript project with reusable components, environment-specific configurations, and automation scripts that ensure consistent infrastructure deployment across all three environments.","A financial services company needs to ensure their payment processing infrastructure is identical across development, staging, and production environments. They've experienced configuration drift issues where manual changes in production weren't reflected in lower environments, causing deployment failures.","""Multi-environment AWS deployment across us-east-1 (production), us-west-2 (staging), and eu-west-1 (development). Each environment requires isolated VPCs with 3 availability zones, private subnets for compute resources, and public subnets for load balancers. Infrastructure includes ECS Fargate for containerized services, Lambda functions for event processing, and RDS Aurora PostgreSQL for data persistence. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, and AWS CLI configured with appropriate credentials. Each environment maintains separate state backends in S3 with DynamoDB table locking.""","[""Use Pulumi configuration files to manage environment-specific values without hardcoding"", ""Implement stack references to share outputs between environments"", ""All Lambda functions must use ARM64 architecture for cost optimization"", ""RDS instances must use encrypted storage with customer-managed keys"", ""Implement automatic failover for RDS with read replicas in different AZs"", ""Use Pulumi's automation API to validate configuration consistency across stacks"", ""All resources must be tagged with Environment, CostCenter, and Owner tags"", ""Lambda functions must use container images stored in private ECR repositories"", ""Network traffic between services must stay within private subnets""]"
i3m3e9,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement automated cross-region database failover with health monitoring. The configuration must: 1. Deploy RDS MySQL instances in us-east-1 (primary) and us-west-2 (read replica) with encryption enabled. 2. Configure Route53 health checks monitoring primary database endpoint on port 3306. 3. Create Lambda functions in both regions to validate database connectivity and replication lag. 4. Implement Route53 failover routing policy with 60-second TTL for database CNAME records. 5. Set up CloudWatch alarms triggering when replication lag exceeds 5 seconds. 6. Configure SNS topics for failover notifications to ops-alerts@company.com. 7. Enable automated backups with 7-day retention and point-in-time recovery. 8. Implement IAM roles with minimal permissions for Lambda execution and RDS operations. 9. Tag all resources with Environment=production and CostCenter=trading. Expected output: A Pulumi program that provisions a complete multi-region database infrastructure with automatic failover capabilities, ensuring less than 2 minutes of downtime during primary region failures.",A financial services company requires zero-downtime database failover capabilities for their trading platform. The system must detect primary database failures and automatically promote read replicas while updating DNS records to redirect traffic seamlessly.,"""Multi-region AWS deployment spanning us-east-1 (primary) and us-west-2 (secondary) for high availability database infrastructure. Uses RDS MySQL 8.0 with encrypted storage, Route53 for DNS failover routing, and Lambda functions for health monitoring. Requires Pulumi 3.x with TypeScript, AWS CLI configured with cross-region permissions, Node.js 16+. VPCs in both regions with private subnets for RDS instances, security groups allowing MySQL traffic between regions. Production account with Route53 hosted zone already configured.""","[""RDS instances must use db.r5.large instance class with 100GB encrypted gp3 storage"", ""Lambda functions must complete health checks within 10 seconds timeout"", ""Route53 health checks must use HTTPS protocol with custom health check path"", ""Cross-region replication must use VPC peering with encrypted transit"", ""All passwords and connection strings must be stored in AWS Secrets Manager"", ""CloudWatch Logs retention must be set to 30 days for all Lambda functions"", ""RDS parameter groups must enforce SSL connections only"", ""SNS topics must use KMS encryption for messages at rest"", ""Pulumi stack exports must include primary and secondary database endpoints""]"
v5s1q7,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a highly available web application for payment processing. The configuration must: MANDATORY REQUIREMENTS (Must complete): 1. Deploy an ECS Fargate cluster across 3 availability zones with auto-scaling based on CPU utilization (CORE: ECS) 2. Create an Aurora PostgreSQL Serverless v2 cluster with encryption at rest and automatic backups (CORE: Aurora) 3. Configure an Application Load Balancer with integration and SSL/TLS termination 4. Implement blue-green deployment capability using ECS service deployment configurations 5. Create separate VPCs for production and staging with VPC peering for secure communication 6. Configure secrets rotation for database credentials using with 30-day rotation 7. Set up CloudWatch dashboards with custom metrics for transaction processing latency 8. Implement least-privilege IAM roles with session tags for temporary access 9. Configure automated SSL certificate renewal using ACM with DNS validation 10. Enable container insights and tracing for all ECS tasks OPTIONAL ENHANCEMENTS (If time permits):  Add API Gateway with request throttling for public API access (OPTIONAL: API Gateway) - provides rate limiting and API management  Implement queues for asynchronous payment processing (OPTIONAL: ) - decouples payment processing from web tier  Add ElastiCache Redis for session management (OPTIONAL: ElastiCache) - improves application performance Expected output: A complete Pulumi TypeScript program that provisions the entire infrastructure stack with proper resource dependencies, exports critical endpoints and connection strings, and includes stack configuration for both staging and production environments.",A fintech startup needs to deploy their payment processing web application with strict compliance requirements. The application must handle sensitive financial data with zero downtime deployments and automated failover capabilities.,"""Multi-region AWS deployment primarily in us-east-1 with disaster recovery in us-west-2. Uses ECS Fargate for container orchestration, Aurora PostgreSQL Serverless v2 for database, Application Load Balancer with for traffic distribution. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions, Node.js 18+. Infrastructure spans multiple VPCs with for inter-VPC communication. Implements for account separation between environments.""","[""All resources must be tagged with Environment, Application, and CostCenter tags for compliance tracking"", ""Database connections must use SSL/TLS with certificate validation enabled"", ""ECS task definitions must specify exact container image tags, no 'latest' tags allowed"", ""All S3 buckets must have versioning enabled and lifecycle policies for 90-day retention"", ""CloudWatch log groups must have encryption with customer-managed keys"", ""Network traffic between services must traverse private subnets only, no direct internet routing"", ""Implement stack policies to prevent accidental deletion of critical resources"", ""Use Pulumi configuration secrets for all sensitive values, no hardcoded credentials"", ""ALB access logs must be stored in S3 with server-side encryption"", ""Deploy monitoring alarms for 5XX errors, database connections, and ECS task failures""]"
b9z0m7,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a zero-trust security infrastructure for payment processing. MANDATORY REQUIREMENTS (Must complete): 1. Create a VPC with private subnets across 3 AZs and for , DynamoDB, Secrets Manager (CORE: VPC) 2. Deploy Aurora PostgreSQL cluster with encryption using customer-managed KMS keys (CORE: ) 3. Create function in private subnet for payment validation with 1GB memory (CORE: ) 4. Configure with mutual TLS using uploaded client certificates 5. Set up DynamoDB table for session storage with point-in-time recovery enabled 6. Implement IAM roles with explicit deny statements for :DeleteBucket and :DeleteDBCluster 7. Enable VPC flow logs to bucket with 90-day lifecycle policy 8. Configure Secrets Manager to rotate credentials every 30 days 9. Create security groups allowing only specific port ranges with no wildcard sources OPTIONAL ENHANCEMENTS (If time permits):  Add AWS WAF with rate limiting rules (OPTIONAL: WAF) - provides DDoS protection  Implement GuardDuty for threat detection (OPTIONAL: GuardDuty) - adds security monitoring  Add Config rules for compliance checking (OPTIONAL: Config) - ensures ongoing compliance Expected output: Complete Pulumi TypeScript program that creates a zero-trust infrastructure meeting all PCI-DSS requirements with proper encryption, network isolation, and audit logging.","A financial services company needs to implement a zero-trust security architecture for their payment processing workload. The infrastructure must meet PCI-DSS compliance requirements with strict network isolation, encryption at rest and in transit, and comprehensive audit logging. All sensitive data must be isolated in private subnets with no direct internet access.","""Highly secure multi-AZ deployment in us-east-1 region for PCI-DSS compliant payment processing. Uses Aurora PostgreSQL with encryption, in private subnets, with mTLS, DynamoDB for session data. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions. VPC spans 3 AZs with private subnets only, for , DynamoDB, and Secrets Manager. No NAT gateways or internet gateways allowed. All traffic must flow through endpoints.""","[""All instances must use encrypted storage with customer-managed KMS keys"", "" functions must run in private subnets with for AWS services"", "" must enforce mutual TLS authentication with client certificates"", ""DynamoDB tables must use point-in-time recovery and encryption with separate KMS keys"", ""Security groups must follow least-privilege with no 0.0.0.0/0 ingress rules"", ""All IAM roles must include explicit deny statements for sensitive operations"", ""VPC flow logs must be enabled and sent to with lifecycle policies"", ""Secrets Manager must rotate database credentials every 30 days automatically"", ""CloudTrail must log all API calls to a dedicated bucket with MFA delete enabled""]"
n6o1g2,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a serverless payment webhook processing system. The configuration must: 1. Deploy an API Gateway REST API with /webhooks/{provider} endpoint accepting POST requests. 2. Create a Lambda function to validate webhook signatures and parse payment data. 3. Set up DynamoDB table 'payment-events' with partition key 'transactionId' and sort key 'timestamp'. 4. Configure EventBridge custom event bus 'payment-bus' to route processed events. 5. Implement Step Functions state machine for payment workflow orchestration. 6. Create Lambda function for payment processing logic triggered by EventBridge. 7. Set up CloudWatch Log Groups with 30-day retention for all Lambda functions. 8. Configure IAM roles with least-privilege policies for each service. 9. Enable service map for the entire payment processing flow. 10. Implement error handling with SQS dead letter queues for failed events. Expected output: A complete Pulumi TypeScript program that deploys a production-ready serverless payment processing system with proper error handling, monitoring, and security configurations.","A fintech startup needs a serverless event processing system to handle real-time payment notifications from multiple payment providers. The system must process webhook events, validate signatures, and trigger downstream workflows while maintaining PCI compliance standards.","""Production serverless infrastructure in us-east-1 region using API Gateway, Lambda, DynamoDB, EventBridge, and Step Functions. Requires Pulumi CLI 3.x with TypeScript, Node.js 18.x, and AWS CLI configured with appropriate IAM permissions. Architecture includes REST API endpoints, event-driven processing with EventBridge, DynamoDB for state management, and Step Functions for orchestration. for DynamoDB and S3 to keep traffic within AWS network. Multi-AZ deployment for high availability.""","[""Lambda functions must use Node.js 18.x runtime with 1GB memory allocation"", ""DynamoDB tables must use on-demand billing mode with point-in-time recovery enabled"", ""All Lambda functions must have tracing enabled for distributed debugging"", ""API Gateway must implement request throttling at 1000 requests per second"", ""EventBridge rules must include dead letter queues for failed event deliveries"", ""Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""All resources must be tagged with CostCenter, Environment, and Owner tags"", ""Lambda functions must have reserved concurrent executions set to prevent throttling"", ""DynamoDB streams must trigger Lambda functions with parallelization factor of 10""]"
q7r4q5,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to optimize an existing ECS Fargate infrastructure for cost and performance. The configuration must: 1. Refactor existing ECS services to use mixed Fargate and Fargate Spot capacity providers with weighted targets (30% on-demand, 70% spot). 2. Implement precise CPU and memory configurations based on p95 utilization metrics (512 CPU units with 1GB memory for API service, 256 CPU units with 512MB for workers). 3. Configure auto-scaling policies using target tracking on custom CloudWatch metrics for memory utilization with scale-out at 70% and scale-in at 40%. 4. Optimize task definitions with streamlined environment variables and secrets from Parameter Store instead of hardcoded values. 5. Implement proper health check configurations with initial delay of 30 seconds and interval of 15 seconds. 6. Configure deployment circuit breaker with failure threshold of 2 and rollback enabled. 7. Set up CloudWatch Container Insights with custom namespace 'PaymentSystem' and enhanced monitoring. 8. Implement service discovery using Cloud Map with TTL of 60 seconds and health check enabled. 9. Configure task placement constraints to spread across availability zones with distinct instance attribute. 10. Add cost allocation tags including 'Environment', 'CostCenter', 'Application', and 'ManagedBy' for all resources. Expected output: Optimized Pulumi TypeScript code that reduces infrastructure costs by 40% while maintaining 99.9% availability SLA, with detailed CloudWatch dashboards showing resource utilization and cost metrics.","A financial services company is experiencing high infrastructure costs and performance issues with their containerized payment processing system. The current ECS Fargate deployment lacks proper resource optimization, causing over-provisioning and inefficient scaling patterns. The DevOps team needs to refactor the existing Pulumi code to reduce costs by 40% while maintaining performance SLAs.","""Production ECS cluster deployed in us-east-1 across 3 availability zones using Fargate and Fargate Spot capacity providers. Application Load Balancer with target group health checks. Aurora PostgreSQL Multi-AZ database cluster. VPC with private subnets and NAT gateways. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate IAM permissions. CloudWatch Container Insights enabled for monitoring. Current monthly cost: $8,500, target: $5,100.""","[""Must use ECS Fargate Spot instances for at least 70% of non-critical workloads"", ""Container memory limits must be precisely tuned based on CloudWatch metrics data"", ""Auto-scaling policies must use target tracking with custom metrics"", ""Task definitions must implement health check grace periods of exactly 60 seconds"", ""All container images must use multi-stage builds with distroless base images"", ""Network mode must be awsvpc with IPv6 enabled for future compatibility"", ""Service discovery must use AWS Cloud Map with health checks"", ""Container insights must be enabled with custom metrics for memory pressure"", ""Task placement strategies must use spread across availability zones"", ""Circuit breaker deployment configuration must be enabled with rollback triggers""]"
b9h4q2,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy an enterprise-grade CI/CD pipeline infrastructure. The configuration must: 1. Create a with source, build, and deploy stages for processing code from CodeCommit. 2. Configure CodeBuild projects with TypeScript/Node.js build environment and Docker image creation capabilities. 3. Implement blue-green deployment to ECS Fargate using CodeDeploy with automatic rollback on failures. 4. Create S3 buckets for build artifacts with versioning and lifecycle policies (90-day expiration). 5. Set up customer-managed KMS keys for encrypting pipeline artifacts and build outputs. 6. Configure CloudWatch Logs groups with 30-day retention for all build and deployment logs. 7. Implement SNS topics for pipeline state change notifications with email subscriptions. 8. Create least-privilege IAM roles for each service with explicit resource ARNs. 9. Enable CloudWatch Events to trigger pipeline on CodeCommit pushes to main branch. 10. Configure CodeBuild webhook for pull request validation builds. 11. Implement integration for storing build-time secrets. 12. Set up tracing for pipeline execution monitoring. Expected output: A complete Pulumi program that provisions the entire CI/CD infrastructure with proper error handling, resource dependencies, and stack outputs showing pipeline URL and artifact bucket names.","A software development team needs to establish a fully automated CI/CD pipeline for their microservices architecture. The pipeline must support parallel builds, automated testing, and blue-green deployments while maintaining strict security controls and audit trails.","""AWS us-east-1 region hosting a multi-service CI/CD infrastructure using for orchestration, CodeBuild for builds, and CodeDeploy for blue-green deployments to ECS Fargate. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured. VPC with private subnets for build environments, S3 for artifact storage with versioning enabled. Customer-managed KMS keys for encryption at rest. CloudWatch Logs for centralized logging with custom log groups per service.""","[""All CodeBuild projects must use compute type BUILD_GENERAL1_SMALL to minimize costs"", "" must enforce manual approval only for production deployments"", ""All artifacts must be encrypted with customer-managed KMS keys"", ""Build logs must be streamed to CloudWatch Logs with 30-day retention"", ""IAM roles must follow least privilege with no wildcard resource permissions""]"
t5d4m9,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to deploy a multi-region disaster recovery infrastructure for a financial transaction processing system.

MANDATORY REQUIREMENTS (Must complete):
1. Create Aurora PostgreSQL Global Database cluster in us-east-1 as primary with one writer instance (CORE: Aurora)
2. Add Aurora read replica cluster in us-west-2 as secondary region
3. Deploy ECS Fargate services in both regions running a containerized application (CORE: ECS)
4. Configure Application Load Balancers in both regions with health checks
5. Create DynamoDB global tables spanning both regions for session state (CORE: DynamoDB)
6. Implement Route 53 hosted zone with failover routing policy
7. Configure health checks to monitor ALB endpoints in both regions
8. Set up VPCs with private subnets in both regions
9. Enable Aurora automated backups with 7-day retention
10. Apply consistent tagging across all resources

OPTIONAL ENHANCEMENTS (If time permits):
 Add Lambda functions for automated failover orchestration (OPTIONAL: Lambda) - enables custom failover logic
 Implement CloudWatch Synthetics for application-level health monitoring (OPTIONAL: CloudWatch Synthetics) - improves detection accuracy
 Configure AWS Backup for centralized backup management (OPTIONAL: AWS Backup) - simplifies recovery procedures

Expected output: A Pulumi TypeScript program that provisions complete multi-region infrastructure with automated failover capabilities, meeting the 60-second RTO requirement through Route 53 health check-based routing.","A global financial services company requires a disaster recovery solution for their critical transaction processing system. The system must maintain 99.99% availability and support automatic failover between AWS regions within 60 seconds of a regional outage. The architecture needs to handle 50,000 concurrent connections during peak trading hours.","""Multi-region deployment across us-east-1 (primary) and us-west-2 (secondary) for disaster recovery. Infrastructure includes Aurora PostgreSQL Global Database with automated backups, Application Load Balancers with cross-zone load balancing enabled, ECS Fargate services running containerized applications, DynamoDB global tables for distributed session management, and Route 53 with health check-based failover routing. VPCs in both regions with private subnets across 3 availability zones each, connected via VPC peering. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate IAM permissions, Node.js 18+ and npm installed. The environment supports automatic failover with sub-minute RTO requirements.""","[""Use Aurora Global Database with at least one read replica in the secondary region"", ""Implement Route 53 health checks with failover routing policy"", ""Deploy ALB in both regions with identical target group configurations"", ""Configure DynamoDB global tables for session state replication"", ""Set RTO (Recovery Time Objective) to 60 seconds maximum"", ""Ensure all resources are tagged with Environment=Production and CostCenter=FinanceOps""]"
e7t1z2,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster for a trading platform. MANDATORY REQUIREMENTS (Must complete): 1. Create an EKS cluster with Kubernetes 1.28+ in a custom VPC with 3 private and 3 public subnets (CORE: EKS) 2. Configure two managed node groups: one with t4g.medium instances for general workloads, another with t4g.large spot instances for batch processing (CORE: EC2) 3. Enable all EKS control plane logging types (api, audit, authenticator, controllerManager, scheduler) with 30-day retention 4. Implement IRSA for Cluster Autoscaler with proper IAM policy for node group scaling 5. Deploy AWS Load Balancer Controller using Helm with IRSA configuration 6. Install and configure EBS CSI driver with encrypted GP3 storage class as default 7. Apply Kubernetes pod security standards with 'baseline' as default enforcement 8. Configure VPC CNI for pod-to-pod encryption using SecurityGroupPolicy 9. Tag all resources with Environment=production and ManagedBy=pulumi 10. Export cluster endpoint, OIDC provider URL, and kubeconfig for cluster access OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Systems Manager Session Manager for node access (OPTIONAL: Systems Manager) - improves security by eliminating SSH keys  Integrate with AWS Secrets Manager for application secrets (OPTIONAL: Secrets Manager) - centralizes secret management  Configure Amazon GuardDuty for EKS threat detection (OPTIONAL: GuardDuty) - adds security monitoring Expected output: A fully functional Pulumi TypeScript program that deploys an EKS cluster with proper networking, security configurations, essential add-ons installed via Helm, and outputs for cluster access. The infrastructure should be production-ready with high availability across multiple AZs.","A financial services company needs to migrate their containerized trading platform to AWS EKS. The platform requires strict network isolation between different trading environments, automated cluster scaling based on market hours, and integration with existing connections for secure trader access.","""Production EKS infrastructure deployed in us-east-1 region across 3 availability zones. Uses EKS managed control plane with multiple node groups, AWS Load Balancer Controller for ingress, EBS CSI driver for storage, and Cluster Autoscaler for scaling. VPC configuration includes private subnets for worker nodes and public subnets for load balancers. Requires Pulumi CLI 3.x with TypeScript, AWS CLI configured with appropriate permissions, kubectl 1.28+, and helm 3.x for add-on deployments. Integration with existing gateway in the same VPC for secure access.""","[""EKS cluster must use Kubernetes version 1.28 or higher"", ""Node groups must use Graviton3 (ARM64) instances for cost optimization"", ""Implement pod-to-pod encryption using AWS VPC CNI with SecurityGroupPolicy"", ""Configure IRSA (IAM Roles for Service Accounts) for all workload identities"", ""Enable EKS cluster logging for all log types to CloudWatch"", ""Use managed node groups with spot instances for non-critical workloads"", ""Implement Cluster Autoscaler with proper IRSA configuration"", ""Configure AWS Load Balancer Controller for ingress management"", ""Set up EBS CSI driver with encryption enabled for persistent volumes"", ""Implement pod security standards with baseline enforcement mode""]"
w7t5a8,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to implement a zero-trust security architecture for processing sensitive financial data. The configuration must: 1. Create a KMS key with key rotation enabled and alias 'financial-data-key'. 2. Deploy an bucket with server-side encryption using the KMS key, versioning enabled, and block all public access. 3. Configure a Lambda function that processes data from , running in a private subnet with 1024MB memory. 4. Set up DynamoDB table 'audit-logs' with encryption using the same KMS key and on-demand billing. 5. Create VPC with 3 private subnets across different AZs, no public subnets allowed. 6. Implement for , DynamoDB, and KMS services. 7. Configure security groups that allow only necessary internal communication between Lambda and AWS services. 8. Create CloudWatch Log Group with KMS encryption for Lambda logs. 9. Define IAM roles and policies that grant minimal required permissions for each service. 10. Enable AWS Config rules to monitor encryption compliance. 11. Output the KMS key ARN, bucket name, and Lambda function ARN. Expected output: A complete Pulumi TypeScript program that creates a fully isolated, encrypted infrastructure where sensitive data never traverses the public internet and all access is logged and monitored.","A financial services company needs to implement a secure data processing pipeline that meets PCI-DSS compliance requirements. The system must enforce encryption at rest and in transit, implement strict access controls, and maintain audit trails for all data access attempts.","""Production-grade security infrastructure deployed in us-east-1 across 3 availability zones. Core services include for encrypted data storage, Lambda for processing in isolated VPC, DynamoDB for audit logs, and KMS for key management. Requires Pulumi 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate permissions. VPC spans 10.0.0.0/16 with private subnets only, for , DynamoDB, and KMS. No NAT Gateway or Internet Gateway allowed. All inter-service communication must use AWS PrivateLink.""","[""All buckets must use AES-256 encryption with customer-managed KMS keys"", ""Lambda functions must run in private subnets with no direct internet access"", ""DynamoDB tables must enable point-in-time recovery and encryption at rest"", "" must be used for all AWS service communications"", ""IAM roles must follow least-privilege principle with no wildcard permissions"", ""All resources must be tagged with 'Environment', 'DataClassification', and 'Owner' tags"", ""CloudWatch Logs must be encrypted and retain logs for exactly 90 days"", ""Security group rules must explicitly define source IPs with no 0.0.0.0/0 rules""]"
x2f8q6,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,Create a Pulumi TypeScript program to deploy a multi-region disaster recovery infrastructure for a PostgreSQL database. MANDATORY REQUIREMENTS (Must complete): 1. Deploy RDS Aurora PostgreSQL Global Database spanning us-east-1 (primary) and eu-west-1 (secondary) (CORE: RDS Aurora) 2. Configure Route53 health checks and failover routing between regions (CORE: Route53) 3. Implement Lambda function in each region to monitor database health and trigger failover (CORE: Lambda) 4. Set up cross-region VPC peering for secure replication traffic 5. Configure automated backups with point-in-time recovery enabled 6. Create IAM roles with least privilege for Lambda execution 7. Implement CloudWatch alarms for replication lag exceeding 300 seconds 8. Tag all resources with Environment=DR and CostCenter=Finance 9. Enable deletion protection on production clusters only OPTIONAL ENHANCEMENTS (If time permits):  Add SNS topics for failover notifications (OPTIONAL: SNS) - improves incident response  Implement for orchestrated failover workflow (OPTIONAL: ) - reduces manual steps  Add AWS Backup for additional protection (OPTIONAL: AWS Backup) - provides centralized backup management Expected output: Complete Pulumi TypeScript program that creates a functioning multi-region DR setup with automated health checks and failover capabilities. The solution should demonstrate proper state management and resource dependencies.,A financial services company needs to implement a disaster recovery solution for their critical transaction database. The system must maintain RPO of 1 hour and RTO of 15 minutes across AWS regions. Active-passive DR pattern is required with automated failover capabilities.,"""Multi-region AWS deployment spanning us-east-1 (primary) and eu-west-1 (secondary) using RDS Aurora Global Database, Route53 for DNS failover, and Lambda for health monitoring. Requires Pulumi CLI 3.x with TypeScript, Node.js 16+, and AWS CLI configured with cross-region permissions. VPCs in both regions with private subnets for database clusters, VPC peering for replication. Each region needs 3 AZs for Aurora cluster deployment.""","[""Aurora cluster must use db.r6g.large instances minimum for production workloads"", ""Route53 health check intervals must be 30 seconds with 3 consecutive failures threshold"", ""Lambda functions must complete health checks within 10 seconds timeout"", ""VPC peering must restrict traffic to PostgreSQL port 5432 only"", ""Backup retention must be 7 days minimum with encrypted snapshots"", ""CloudWatch Logs retention for Lambda must be 30 days"", ""All inter-region traffic must use AWS PrivateLink or VPC peering"", ""Database passwords must be stored in "", ""Pulumi stack exports must include primary and secondary cluster endpoints""]"
w2q3q9,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster for containerized trading applications. The configuration must: 1. Create an EKS cluster with OIDC provider enabled for IRSA. 2. Deploy mixed node groups with 70% spot and 30% on-demand instances across 3 AZs. 3. Install AWS Load Balancer Controller using Helm with proper IAM role. 4. Configure cluster autoscaler with IRSA permissions to scale nodes 2-10. 5. Deploy metrics-server for HPA functionality. 6. Create three namespaces: trading-frontend, trading-backend, trading-analytics. 7. Implement NetworkPolicies restricting backend pods to only accept traffic from frontend. 8. Configure ResourceQuotas limiting each namespace to 10 pods and 16Gi memory. 9. Set up pod security standards enforcing restricted baseline. 10. Export cluster endpoint, certificate authority data, and kubeconfig. Expected output: A fully functional EKS cluster with container orchestration capabilities, automated scaling, network isolation, and security policies ready for deploying containerized trading applications.",A financial services company needs to modernize their monolithic trading application by containerizing it and deploying to EKS. They require strict network isolation between different trading components and automated deployment pipelines. The infrastructure must support both spot and on-demand instances for cost optimization while maintaining high availability.,"""Production EKS cluster deployed in us-east-2 region across 3 availability zones. Infrastructure includes EKS 1.28+ with managed node groups, Application Load Balancer for ingress, ECR for container images. VPC with private subnets for worker nodes and public subnets for load balancers. Requires Pulumi 3.x with TypeScript, kubectl, helm CLI tools installed. AWS CLI configured with appropriate permissions for EKS, EC2, IAM, and VPC management.""","[""EKS cluster must use Kubernetes version 1.28 or higher"", ""Node groups must mix spot instances (70%) and on-demand instances (30%)"", ""Implement pod security standards with restricted baseline"", ""Use AWS Load Balancer Controller for ingress management"", ""Configure Horizontal Pod Autoscaler with custom metrics"", ""Enable EKS managed node groups with automatic updates"", ""Implement network policies for pod-to-pod communication restrictions"", ""Use IRSA (IAM Roles for Service Accounts) for pod-level permissions"", ""Configure cluster autoscaler with proper IAM permissions"", ""Deploy metrics-server and configure resource quotas per namespace""]"
s4m6l3,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to orchestrate a phased migration from on-premises infrastructure to AWS cloud. The configuration must: 1. Establish Transit Gateway with attachments to VPC and VPN for hybrid connectivity. 2. Create Site-to-Site VPN with two tunnels using customer gateway configuration. 3. Deploy Aurora MySQL cluster with encryption and automated backups. 4. Configure AWS DMS replication instance and tasks for continuous data sync. 5. Set up ECS Fargate service with ALB and target group for blue-green deployments. 6. Implement Route 53 hosted zone with weighted routing policies for traffic migration. 7. Create buckets for application artifacts with versioning and replication. 8. Configure for database credentials with automatic rotation. 9. Deploy CloudWatch dashboard with widgets for DMS lag, ECS health, and VPN status. 10. Implement AWS Config rules to validate security group configurations. Expected output: A Pulumi program that creates all infrastructure components with proper dependencies, outputs critical resource IDs and endpoints, and supports phased execution through stack configuration. The program should enable safe rollback at any migration phase and provide clear visibility into migration progress through CloudWatch metrics.","A financial services company needs to migrate their legacy monolithic application from on-premises infrastructure to AWS. The application currently serves 50,000 daily users and processes sensitive payment data. They require a phased migration approach that allows parallel running of both environments during the transition period.","""Hybrid cloud environment spanning on-premises datacenter and AWS us-east-1 region. Requires AWS Transit Gateway for network connectivity, Site-to-Site VPN with BGP routing, ECS Fargate for containerized workloads, Aurora MySQL for database tier, AWS DMS for data replication. VPC with 3 availability zones, public and private subnets, NAT gateways for outbound connectivity. Pulumi TypeScript stack with AWS provider v6.x, Node.js 18+, Docker for container builds. Migration spans 4 phases: connectivity setup, database replication, application deployment, traffic cutover.""","[""Use AWS Transit Gateway for network connectivity between on-premises and cloud environments"", ""Implement database migration using AWS DMS with ongoing replication enabled"", ""Configure VPN connection with redundant tunnels for secure hybrid connectivity"", ""Use Route 53 weighted routing for gradual traffic migration (0% to 100%)"", ""Deploy application on ECS Fargate with blue-green deployment capability"", ""Implement rotation for database credentials every 30 days"", ""Configure CloudWatch dashboards with custom metrics for migration progress monitoring"", ""Use bucket versioning for application artifacts with lifecycle policies"", ""Implement AWS Config rules to ensure compliance during migration phases""]"
i0d5m0,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to deploy a multi-region active-passive architecture with automatic failover capabilities. The configuration must: 1. Set up RDS Aurora Global Database with a primary cluster in us-east-1 and secondary in eu-west-1, using db.r6g.large instances. 2. Deploy Lambda functions in both regions for transaction processing, with environment variables for database endpoints. 3. Configure Route 53 health checks and failover routing policies with weighted routing (100% primary, 0% secondary). 4. Create for Lambda and RDS services in both regions to ensure private connectivity. 5. Implement CloudWatch alarms for database lag, Lambda errors, and health check failures. 6. Set up topics in both regions for failover notifications with email subscriptions. 7. Configure Aurora Global Database with 1-second RPO using Aurora's native replication. 8. Create CloudWatch dashboards showing cross-region replication lag and system health metrics. 9. Implement automated database endpoint updates in Lambda environment variables during failover. 10. Set backup retention to 7 days in primary and 3 days in secondary region. Expected output: A Pulumi program that deploys a fully functional multi-region architecture with automated failover capabilities, where Route 53 automatically redirects traffic to the secondary region when primary region health checks fail.","A financial services company requires a multi-region active-passive architecture for their critical transaction processing system. The system must handle automatic failover between regions within 60 seconds of primary region failure, while maintaining strict data consistency and compliance with financial regulations.","""Multi-region deployment spanning us-east-1 (primary) and eu-west-1 (secondary) for financial transaction processing. Infrastructure includes RDS Aurora Global Database with PostgreSQL 15.4, Lambda functions for transaction processing, and Route 53 for DNS failover. Each region has its own VPC with 3 availability zones, private subnets for compute and database tiers, and for AWS services. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate IAM permissions for multi-region deployments. KMS keys must be created in advance in both regions.""","[""Route 53 health checks must monitor both application and database endpoints with 10-second intervals"", ""RDS Aurora Global Database must use encrypted storage with customer-managed KMS keys in each region"", ""Lambda functions must have reserved concurrency of exactly 100 in primary and 50 in secondary region"", ""All inter-region traffic must flow through AWS PrivateLink endpoints, not over public internet"", ""CloudWatch alarms must trigger notifications for any failover event within 30 seconds"", ""Secondary region resources must use 50% capacity of primary to minimize costs during standby"", ""All IAM roles must follow principle of least privilege with no wildcard resource permissions""]"
t7s2x0,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a multi-environment payment processing system with automatic replication and consistency checks. The configuration must: 1. Define a reusable PaymentProcessor ComponentResource that encapsulates Lambda, DynamoDB, and SNS resources. 2. Deploy identical infrastructure to dev, staging, and prod environments using different Pulumi stacks. 3. Implement environment-specific scaling (dev: 1 Lambda concurrent execution, staging: 10, prod: 100). 4. Create DynamoDB tables with point-in-time recovery enabled only in staging and prod. 5. Configure SNS topics with email subscriptions using environment-specific addresses from config. 6. Use stack references to propagate DynamoDB table ARNs from lower to higher environments. 7. Implement a drift detection function using Pulumi Automation API that compares staging and prod. 8. Create Lambda functions with 512MB memory in dev, 1GB in staging, and 2GB in prod. 9. Set up dead letter queues for Lambda functions with different retry counts per environment. 10. Export a comparison report showing configuration differences between environments. Expected output: A Pulumi TypeScript project with separate stack configurations for each environment, a reusable ComponentResource class, and an automation script that validates consistency between staging and production while allowing controlled variations in scaling parameters.","A fintech startup needs to replicate their payment processing infrastructure across three environments (dev, staging, prod) with consistent configurations but environment-specific scaling. They require automated environment promotion workflows and drift detection to ensure production mirrors staging exactly, with controlled parameter variations.","""Multi-account AWS deployment across us-east-1 (dev), us-west-2 (staging), and eu-west-1 (prod) regions. Each environment runs in separate AWS accounts with cross-account IAM roles. Core infrastructure includes Lambda functions for payment processing, DynamoDB tables for transaction records, and SNS topics for notifications. VPC setup with private subnets for Lambda, VPC endpoints for DynamoDB and SNS. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI with profiles for each account configured. Environment promotion workflow using Pulumi Automation API for controlled deployments.""","[""Use Pulumi stack references to share outputs between environments"", ""Implement custom ComponentResource classes for reusable infrastructure patterns"", ""Deploy to exactly 3 AWS accounts using cross-account assume role"", ""Use Pulumi configuration files with environment-specific overrides"", ""Implement automated drift detection using Pulumi Automation API"", ""All Lambda functions must use ARM64 architecture for cost optimization""]"
o2o5s4,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a containerized payment gateway application using ECS Fargate and Aurora Serverless. MANDATORY REQUIREMENTS (Must complete): 1. Create a custom VPC with private subnets across 3 AZs for ECS tasks (CORE: VPC) 2. Deploy Aurora Serverless v2 PostgreSQL cluster with encryption enabled (CORE: Aurora) 3. Configure ECS Fargate service with auto-scaling based on CPU utilization (CORE: ECS) 4. Set up Application Load Balancer with path-based routing to /api/* endpoints 5. Implement blue-green deployment strategy using ECS service deployment configuration 6. Configure CloudWatch Log Groups with 30-day retention for application logs 7. Create secret for database credentials with automatic rotation 8. Set up CloudWatch alarms for ECS task failures and database connection errors 9. Implement stack outputs for ALB DNS name and database endpoint 10. Use Pulumi's ComponentResource pattern for reusable infrastructure modules OPTIONAL ENHANCEMENTS (If time permits):  Add AWS rules to ALB for SQL injection protection (OPTIONAL: ) - improves security posture  Implement tracing for distributed request tracking (OPTIONAL: ) - enhances debugging capabilities  Configure health checks with failover routing (OPTIONAL: ) - adds DNS-level resilience Expected output: A complete Pulumi TypeScript program with modular components that deploys a production-ready payment gateway infrastructure. The program should include proper error handling, use async/await patterns, and export all necessary endpoints for application configuration.","A fintech startup needs to deploy their payment gateway web application with strict compliance requirements for PCI DSS. The application processes credit card transactions and must maintain audit logs, implement zero-trust networking, and support blue-green deployments for zero-downtime updates.","""Production-grade infrastructure in us-east-1 with multi-AZ deployment across 3 availability zones. Uses ECS Fargate for container orchestration, Aurora Serverless v2 PostgreSQL for database, Application Load Balancer for traffic distribution. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, AWS CLI configured with appropriate permissions. VPC spans 10.0.0.0/16 with isolated subnets for web, application, and database tiers. NAT gateways in each AZ for outbound connectivity. AWS Organizations with separate accounts for dev, staging, and production environments.""","[""All resources must be tagged with Environment, CostCenter, and Compliance tags"", ""ECS tasks must use Fargate Spot for non-production workloads to reduce costs"", ""Application logs must be encrypted at rest using AWS KMS customer-managed keys"", ""ALB must enforce TLS 1.2 minimum with specific cipher suites for PCI compliance"", ""Database connections must use IAM authentication instead of passwords"", ""Container images must be scanned for vulnerabilities before deployment"", ""Network traffic between services must flow through AWS PrivateLink endpoints"", ""All IAM roles must follow least-privilege with no wildcard resource permissions"", ""Deployment must support automatic rollback on health check failures""]"
x2j9i5,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to build a serverless fraud detection pipeline. The configuration must: 1. Create a DynamoDB table named 'transactions' with partition key 'transactionId' (string) and sort key 'timestamp' (number), with on-demand billing mode. 2. Create a Lambda function 'transaction-processor' with 3008MB memory that receives transaction events and stores them in DynamoDB. 3. Create a second Lambda function 'fraud-detector' with 1024MB memory that analyzes transactions using pattern matching. 4. Set up EventBridge custom event bus 'fraud-detection-bus' to route events between Lambda functions. 5. Configure EventBridge rules to trigger fraud-detector when transaction-processor publishes events with amount > 10000. 6. Create SNS topic 'fraud-alerts' with email subscription for notifications when fraud is detected. 7. Implement proper IAM roles with least privilege for each Lambda function. 8. Tag all resources with Environment='production' and Service='fraud-detection'. 9. Export the EventBridge bus ARN and SNS topic ARN for external integrations. Expected output: A complete Pulumi TypeScript program that deploys a functional fraud detection pipeline with proper event routing, processing capabilities, and alerting mechanisms.","A fintech startup needs a serverless event processing system to handle real-time transaction fraud detection. The system must process incoming transaction events, run fraud detection algorithms, and notify relevant parties while maintaining strict latency requirements under 500ms.","""Production environment in us-east-1 region for low-latency fraud detection system. Uses Lambda functions with Graviton2 processors for transaction processing, DynamoDB for storing transaction history and fraud patterns, EventBridge for event routing between services. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18.x, AWS CLI configured with appropriate permissions. VPC endpoints configured for DynamoDB and S3 to reduce data transfer costs. Multi-AZ deployment not required as Lambda and DynamoDB handle availability automatically.""","[""Lambda functions must have reserved concurrent executions set to prevent cold starts"", ""DynamoDB tables must use point-in-time recovery and encryption at rest"", ""All Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""EventBridge rules must include dead-letter queue configuration for failed invocations"", ""Lambda functions must use environment variables from with KMS encryption"", ""CloudWatch Logs retention must be set to exactly 30 days for compliance""]"
p3b9o7,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a multi-stage CI/CD pipeline with integrated security scanning and blue-green ECS deployments. MANDATORY REQUIREMENTS (Must complete): 1. Create with source, build, test, security-scan, and deploy stages (CORE: ) 2. Configure CodeBuild projects for each pipeline stage with appropriate compute types (CORE: CodeBuild) 3. Deploy ECS Fargate service with blue-green deployment configuration using CODE_DEPLOY controller (CORE: ECS) 4. Set up ECR repository with lifecycle policies to retain only last 10 images per tag 5. Configure Application Load Balancer with target group switching for blue-green deployments 6. Implement application and deployment group for ECS blue-green deployments 7. Create S3 bucket for pipeline artifacts with versioning and server-side encryption 8. Define IAM roles with least-privilege policies for all pipeline components 9. Configure CloudWatch alarms for pipeline failures and deployment rollbacks 10. Export pipeline ARN, webhook URL, and ECS service endpoint as stack outputs OPTIONAL ENHANCEMENTS (If time permits):  Add CodeGuru Reviewer integration for automated code quality checks (OPTIONAL: CodeGuru) - improves code quality  Implement rules for pipeline notifications to Slack (OPTIONAL: ) - enhances team communication  Configure tracing for deployed ECS services (OPTIONAL: ) - enables distributed tracing Expected output: A complete Pulumi TypeScript program that deploys a production-ready CI/CD pipeline with automated testing, security scanning, and zero-downtime deployments to ECS Fargate.","A software development team needs to implement a multi-stage CI/CD pipeline for their Node.js microservices. The pipeline must support parallel testing, automated security scanning, and blue-green deployments to ECS Fargate across development, staging, and production environments.","""Multi-region AWS deployment spanning us-east-1 (primary) and us-west-2 (DR). Infrastructure includes for orchestration, CodeBuild for CI/CD stages, ECS Fargate for container hosting, ECR for image storage, and Application Load Balancer for traffic management. VPC configuration with private subnets across 3 AZs per region, NAT Gateways for outbound connectivity. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, Docker Desktop, and AWS CLI v2 configured with appropriate IAM permissions for creating CI/CD resources.""","[""Pipeline must complete all stages within 15 minutes for standard deployments"", ""Security scanning must block deployment if critical vulnerabilities are detected"", ""Blue-green deployments must maintain zero downtime with automatic rollback capability"", ""All pipeline artifacts must be encrypted at rest using customer-managed KMS keys"", ""Pipeline must support manual approval gates only for production deployments""]"
d7z3a7,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to deploy a multi-region disaster recovery architecture for document processing. The configuration must: 1. Create Aurora Global Database cluster with writer in us-east-1 and read replica in us-west-2. 2. Configure S3 buckets in both regions with cross-region replication and lifecycle policies. 3. Deploy Lambda functions for document processing in both regions with identical configurations. 4. Set up EventBridge global endpoints to route events to the active region. 5. Implement Route53 hosted zone with health checks and failover routing between regions. 6. Create parameters to store regional endpoints and configuration. 7. Configure AWS Backup plans with cross-region copy for Aurora and S3. 8. Set up CloudWatch dashboards in both regions with cross-region metrics. 9. Implement automated failover testing using . 10. Configure SNS topics for disaster recovery notifications. 11. Enable Aurora backtrack with 72-hour window. 12. Create disaster recovery runbooks as documents. Expected output: A Pulumi program that deploys a complete multi-region DR solution with automated failover capabilities, comprehensive monitoring, and the ability to perform regular DR drills without impacting production traffic.",A financial services company needs to deploy a highly available document processing system that can survive regional failures. The system must maintain 99.99% uptime for critical transaction documents and automatically failover between regions within 60 seconds.,"""Multi-region deployment across us-east-1 (primary) and us-west-2 (secondary) for disaster recovery. Uses Aurora PostgreSQL 14.x with global database, S3 with cross-region replication, Lambda for document processing, EventBridge for event routing, and Route53 for DNS failover. VPC peering between regions with private subnets in 3 AZs per region. Requires Pulumi 3.x with TypeScript, AWS CLI v2 configured with appropriate IAM permissions. Implementation includes automated health checks and failover testing capabilities.""","[""Use Aurora Global Database with automatic failover between primary and secondary regions"", ""Implement Route53 health checks with failover routing policies"", ""Configure S3 cross-region replication with RTC (Replication Time Control)"", ""Set up EventBridge global endpoints for multi-region event routing"", ""Use for centralized configuration management"", ""Enable point-in-time recovery for all stateful resources"", ""Configure CloudWatch cross-region dashboards for unified monitoring"", ""Implement AWS Backup for automated cross-region backup scheduling""]"
v5z6d0,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,Create a Pulumi TypeScript program to deploy a zero-trust security infrastructure for microservices. The configuration must: 1. Create a VPC with only private subnets across 3 AZs and for AWS services. 2. Deploy an ECS Fargate cluster with task definitions that enforce non-root users. 3. Configure AWS Secrets Manager with automatic rotation Lambda for database credentials and API keys. 4. Implement Network Load Balancer with ACM certificates for internal mTLS communication. 5. Create security groups that deny all traffic by default and explicitly allow only required ports. 6. Set up CloudWatch Logs with encryption at rest and 90-day retention for audit trails. 7. Configure IAM roles using ABAC tags for fine-grained access control between services. 8. Deploy for non-sensitive configuration with encryption. 9. Implement AWS WAF rules on the internal load balancer to prevent OWASP Top 10 attacks. 10. Create CloudWatch alarms for failed authentication attempts exceeding thresholds. Expected output: A complete Pulumi program that provisions zero-trust infrastructure where services can only communicate through authenticated channels with full audit logging.,"A financial services company needs to implement zero-trust security architecture for their microservices platform. The infrastructure must enforce strict network segmentation, certificate-based authentication, and automated secret rotation while maintaining compliance with PCI-DSS requirements.","""Zero-trust security infrastructure deployed in us-east-1 using ECS Fargate for container orchestration, AWS Secrets Manager for credential management, and AWS Certificate Manager for TLS certificates. Requires Pulumi 3.x with TypeScript, AWS CLI v2 configured with appropriate permissions. VPC with private subnets across 3 availability zones, no IGW, all outbound traffic through endpoints. Network Load Balancer for internal service discovery with TLS termination.""","[""All secrets must be stored in AWS Secrets Manager with automatic rotation enabled every 30 days"", ""Network traffic between services must use mTLS with certificates managed by AWS Certificate Manager"", ""Each microservice must run in isolated security groups with no direct internet access"", ""CloudTrail alternatives using CloudWatch Logs must capture all API calls for compliance"", ""IAM roles must follow least privilege with no inline policies or wildcard permissions""]"
g6k0m5,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to optimize an existing ECS Fargate service by fixing performance and cost issues. The configuration must: 1. Define an ECS task definition with optimized CPU (256) and memory (512MB) allocations based on profiling data. 2. Configure the task to use both Fargate Spot and regular Fargate capacity providers with proper weights (Spot: 70%, Regular: 30%). 3. Create an ECS service with exactly 2 desired tasks and proper placement strategies for AZ distribution. 4. Implement auto-scaling policies that scale from 2 to 8 tasks based on CPU utilization (scale-out at 70%, scale-in at 30%). 5. Configure ALB target group with HTTP health checks on /health endpoint, 5-second intervals, and 2 healthy threshold. 6. Set target group deregistration delay to 30 seconds to speed up deployments. 7. Configure CloudWatch Logs with 14-day retention for container logs using awslogs driver. 8. Apply proper IAM task execution role with minimal permissions for ECR access and CloudWatch Logs. 9. Use stack configuration to make the ECR repository URI and VPC/subnet IDs parameterizable. Expected output: A corrected Pulumi program that reduces costs by 40% through Fargate Spot usage and improves response times by optimizing task sizing and load balancer configuration.","A fintech startup's containerized fraud detection service is experiencing performance bottlenecks and high costs in their ECS deployment. The current infrastructure lacks proper auto-scaling, has oversized task definitions, and inefficient load balancing configuration causing frequent timeouts during peak transaction periods.","""Production environment in us-east-1 with existing VPC (vpc-0abc123def456789) containing private subnets in 3 availability zones. ECS Fargate cluster named 'fraud-detection-cluster' already exists. Application Load Balancer deployed in public subnets. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate credentials. ECR repository 'fraud-detector' contains Docker images tagged with semantic versions. CloudWatch Container Insights enabled for cluster-level monitoring.""","[""Task CPU and memory must be right-sized based on actual usage patterns (256 CPU units, 512MB memory)"", ""Auto-scaling policies must respond within 30 seconds to CPU utilization changes"", ""Load balancer health checks must use HTTP endpoint /health with 5-second intervals"", ""All container logs must be centralized in CloudWatch Logs with 14-day retention"", ""Service must maintain exactly 2 tasks during normal operations, scale to 8 during peaks"", ""Target group deregistration delay must be reduced to 30 seconds for faster deployments"", ""Tasks must use Fargate Spot for cost optimization with FARGATE as fallback"", ""Container images must be pulled from private ECR repository with lifecycle policies""]"
m2q0x8,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to implement an infrastructure compliance validation framework that analyzes resource configurations before deployment. The configuration must: 1. Define a PolicyPack with at least 5 custom policy rules for AWS resources. 2. Validate that all S3 buckets have versioning enabled and use AWS KMS encryption. 3. Check that Lambda functions have memory configurations between 512MB and 3008MB. 4. Ensure all DynamoDB tables have point-in-time recovery enabled. 5. Verify IAM roles follow least-privilege principles with no wildcard (*) actions. 6. Implement a custom validation for Lambda functions to have reserved concurrent executions configured. 7. Create enforcement levels (advisory, mandatory) for different policy violations. 8. Generate a compliance report showing pass/fail status for each resource. 9. Export validation results as structured JSON for integration with CI/CD pipelines. 10. Include unit tests for at least 3 policy rules using Pulumi's testing framework. Expected output: A complete Pulumi PolicyPack that can be published and enforced across all infrastructure deployments, preventing non-compliant resources from being created while providing detailed compliance reports.",A financial services company needs automated infrastructure compliance validation for their payment processing systems. Their security team requires continuous verification that all deployed resources meet strict regulatory standards and internal security policies before promotion to production.,"""AWS infrastructure validation framework deployed in us-east-2 for analyzing payment processing components including Lambda functions for transaction processing, S3 buckets for audit logs, and DynamoDB tables for transaction records. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate IAM permissions for resource inspection. The validation runs in a CI/CD pipeline using GitHub Actions before any infrastructure changes are applied to staging or production environments. No actual resources are deployed during validation phase.""","[""All validation checks must run as part of the Pulumi preview operation without deploying resources"", ""Policy violations must fail the deployment with clear error messages indicating the specific non-compliant resource"", ""The validation framework must support custom policy rules written in TypeScript"", ""All S3 buckets must have versioning enabled and server-side encryption with AWS KMS"", ""Lambda functions must have reserved concurrent executions set between 10-100 to prevent runaway scaling""]"
k4y1b3,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster for microservices architecture. The configuration must: 1. Create an EKS cluster version 1.28 with OIDC provider enabled for IRSA. 2. Configure managed node groups with Bottlerocket AMI, minimum 2 nodes, maximum 10 nodes across 3 AZs. 3. Set up Fargate profiles for pods labeled with 'workload-type=critical' in 'payment-processing' namespace. 4. Deploy ECR repositories with vulnerability scanning enabled for storing microservice images. 5. Configure EKS add-ons: VPC-CNI, CoreDNS, kube-proxy with latest compatible versions. 6. Implement CloudWatch Container Insights for cluster monitoring and logging. 7. Create namespace 'payment-processing' with resource quotas limiting CPU to 100 cores and memory to 200Gi. 8. Deploy AWS Load Balancer Controller using Helm for ingress management. 9. Configure cluster autoscaler with proper IRSA permissions for node group scaling. 10. Enable control plane audit logging to CloudWatch Logs with 30-day retention. 11. Create IAM roles for service accounts (IRSA) for DynamoDB and S3 access patterns. 12. Output cluster endpoint, certificate authority data, and kubectl configuration commands. Expected output: A complete Pulumi program that provisions a production-ready EKS cluster with enhanced security controls, automatic scaling capabilities, and comprehensive observability features suitable for running critical financial microservices.","A fintech startup needs to migrate their monolithic payment processing application to a microservices architecture on Kubernetes. The application handles sensitive financial transactions and requires strict security controls, automated scaling based on transaction volume, and comprehensive observability for regulatory compliance.","""Production-grade EKS cluster deployed in us-east-1 across 3 availability zones. Uses EKS 1.28 with managed node groups running Bottlerocket OS, Fargate profiles for critical workloads, and ECR for container registry. Requires Pulumi CLI 3.x, TypeScript 5.x, kubectl 1.28+, AWS CLI v2 configured with appropriate permissions. VPC with private subnets for worker nodes, public subnets for load balancers, and dedicated subnets for Fargate pods. Integration with AWS Systems Manager for node access without SSH.""","[""EKS cluster must use managed node groups with Bottlerocket AMI for enhanced security"", ""All container images must be scanned for vulnerabilities using ECR scanning before deployment"", ""Network policies must enforce zero-trust communication between microservices"", ""Implement pod-level IAM roles using IRSA for fine-grained AWS service access"", ""Enable EKS control plane audit logging to CloudWatch for compliance requirements"", ""Use Fargate profiles for critical payment processing pods to ensure isolation"", ""Implement automatic node scaling based on pending pod metrics using Cluster Autoscaler""]"
j8g3q9,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement a multi-region disaster recovery infrastructure for a payment processing application. MANDATORY REQUIREMENTS (Must complete): 1. Create Aurora Global Database cluster with primary in us-east-1 and secondary in us-west-2 (CORE: Aurora) 2. Set up DynamoDB global tables for session state with point-in-time recovery enabled (CORE: DynamoDB) 3. Configure Route 53 failover routing between regions with health checks every 30 seconds 4. Implement Lambda functions to monitor database health and publish custom metrics 5. Create VPCs in both regions with for cross-region connectivity 6. Configure S3 buckets with cross-region replication for application artifacts 7. Set up CloudWatch dashboards that aggregate metrics from both regions 8. Implement SNS topics with cross-region subscriptions for incident notifications 9. Configure AWS Backup plans with 15-minute RPO for Aurora clusters 10. Use to manage database endpoints and configuration OPTIONAL ENHANCEMENTS (If time permits):  Add for automated failover orchestration (OPTIONAL: ) - streamlines recovery procedures  Implement EventBridge rules for automated incident response (OPTIONAL: EventBridge) - reduces manual intervention  Add rules for compliance monitoring (OPTIONAL: ) - ensures DR readiness Expected output: Complete Pulumi TypeScript program that creates a fault-tolerant multi-region infrastructure with automated failover capabilities, meeting 15-minute RPO and 30-minute RTO requirements.","A financial services company requires a disaster recovery solution for their critical payment processing application. The system must maintain RPO of 15 minutes and RTO of 30 minutes, with automatic failover capabilities between regions. The architecture needs to synchronize data across regions while minimizing cross-region transfer costs.","""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (DR). Deploys Aurora Global Database cluster with writer in us-east-1 and read replica in us-west-2, Lambda functions for health monitoring, Route 53 failover routing policies, DynamoDB global tables for session management. Each region has VPC with 3 availability zones, private subnets for databases, public subnets for ALB. connects both regions. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions. Infrastructure includes CloudWatch cross-region dashboards, SNS alerting, and AWS Backup for automated recovery point management.""","[""Use Aurora Global Database with PostgreSQL 14 engine for multi-region data replication"", ""Configure Route 53 health checks with 30-second intervals for automatic failover"", ""Implement Lambda functions for health monitoring with custom CloudWatch metrics"", ""Set up cross-region VPC peering with for secure communication"", ""Configure DynamoDB global tables for session state management"", ""Use S3 cross-region replication with lifecycle policies for static assets"", ""Implement AWS Backup for automated snapshots with 7-day retention"", ""Configure CloudWatch cross-region dashboards for unified monitoring"", ""Set up SNS topics in both regions with cross-region subscriptions for alerts"", ""Use for centralized configuration management""]"
u4r3j9,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster with advanced container orchestration features.

MANDATORY REQUIREMENTS (Must complete):
1. Create an EKS cluster version 1.28 with OIDC provider enabled (CORE: EKS)
2. Configure App Mesh with virtual nodes and services for microservices communication (CORE: App Mesh)
3. Deploy managed node groups with Spot instances (70% spot, 30% on-demand mix)
4. Implement IRSA with fine-grained IAM policies for service accounts
5. Install Calico CNI using Helm chart for network policy enforcement
6. Configure Horizontal Pod Autoscaler with CloudWatch custom metrics
7. Deploy Fluent Bit DaemonSet for log aggregation to CloudWatch Logs
8. Enable cluster autoscaler with proper IAM permissions and node group tags

OPTIONAL ENHANCEMENTS (If time permits):
 Add AWS Load Balancer Controller for advanced ingress (OPTIONAL: ALB) - enables path-based routing
 Implement Karpenter for advanced node provisioning (OPTIONAL: EC2) - improves scaling efficiency
 Configure AWS Secrets Manager CSI driver (OPTIONAL: Secrets Manager) - enhances secret management

Expected output: Complete Pulumi TypeScript program that provisions a production-ready EKS cluster with service mesh, autoscaling, observability, and security features properly configured.",A fintech startup needs to deploy their microservices architecture on Kubernetes with strict security and compliance requirements. The platform must handle payment processing workloads with zero-downtime deployments and automated scaling based on transaction volume.,"""Production-grade EKS cluster deployed in us-east-1 across 3 availability zones. Requires Pulumi CLI 3.x with TypeScript, AWS CLI configured with admin access, kubectl 1.28+, and helm 3.x installed. Infrastructure includes EKS 1.28 with managed node groups using t3.medium and t3.large Spot instances, App Mesh for service mesh, Application Load Balancer for ingress, and CloudWatch Container Insights for monitoring. VPC with private subnets for worker nodes and public subnets for load balancers.""","[""EKS cluster must use managed node groups with Spot instances for cost optimization"", ""Implement pod-to-pod encryption using AWS App Mesh service mesh"", ""Configure Horizontal Pod Autoscaler with custom metrics from CloudWatch"", ""Use IRSA (IAM Roles for Service Accounts) for pod-level AWS permissions"", ""Deploy Fluent Bit as DaemonSet for centralized logging to CloudWatch"", ""Enable cluster autoscaling with mixed instance types across 3 AZs"", ""Implement network policies using Calico CNI plugin"", ""Configure OIDC provider integration for kubectl access management""]"
e0p9w5,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy an observability platform for containerized microservices. The configuration must: 1. Set up CloudWatch Container Insights for an existing ECS cluster named 'prod-microservices'. 2. Configure X-Ray service map with 10% sampling rate and encryption using AWS managed key. 3. Create CloudWatch Logs groups for each microservice with 30-day retention and metric filters for error rates. 4. Deploy CloudWatch Synthetics canaries to monitor 3 critical API endpoints every 5 minutes. 5. Configure CloudWatch Anomaly Detector for CPU and memory metrics with 2-week training period. 6. Create SNS topics for 'critical', 'warning', and 'info' alerts with email subscriptions. 7. Set up CloudWatch Contributor Insights rules to track top API consumers and throttled requests. 8. Create CloudWatch dashboard with widgets for service health, error rates, and latency percentiles. 9. Configure CloudWatch alarms for P99 latency > 500ms and error rate > 1%. 10. Implement CloudWatch Events rules to trigger Lambda functions for automated remediation. 11. Apply consistent tagging across all resources with 'Environment', 'Service', and 'Team'. 12. Export stack outputs for dashboard URL, SNS topic ARNs, and X-Ray trace URL. Expected output: A complete Pulumi TypeScript program that provisions a production-ready observability platform with real-time monitoring, distributed tracing, synthetic monitoring, and automated alerting capabilities for ECS-based microservices.","A fintech startup needs centralized observability for their microservices running on ECS Fargate. The platform must collect metrics, logs, and traces from multiple services while enabling real-time alerting for SLA violations and anomaly detection.","""Production observability infrastructure deployed in eu-central-1 across 3 availability zones. Uses ECS Fargate for container workloads, CloudWatch for metrics and logs, X-Ray for distributed tracing, SNS for alerting. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate permissions. VPC with private subnets for ECS tasks, NAT Gateway for outbound connectivity. CloudWatch Logs retention set to 30 days for compliance.""","[""Use CloudWatch Container Insights for ECS cluster monitoring"", ""Configure X-Ray tracing with sampling rate of 10%"", ""Create SNS topics for critical alerts with email and SMS subscriptions"", ""Implement CloudWatch Synthetics canaries for endpoint monitoring"", ""Set up CloudWatch Contributor Insights rules for API throttling analysis"", ""Configure metric filters on CloudWatch Logs for custom application metrics"", ""Use CloudWatch Anomaly Detector for baseline deviations"", ""Implement tagging strategy with 'Environment', 'Service', and 'Team' tags""]"
z8i4q2,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to orchestrate a phased migration of payment processing infrastructure from on-premises to AWS. MANDATORY REQUIREMENTS (Must complete): 1. Deploy Aurora PostgreSQL Global Database with primary in us-east-1 and read replicas in eu-west-1 and ap-southeast-1 (CORE: Aurora) 2. Configure AWS DMS replication instances in each region to sync data from on-premises Oracle databases (CORE: DMS) 3. Create services for payment processing API with blue-green deployment configuration 4. Implement Site-to-Site VPN connection with customer gateway configuration for 10.0.0.0/8 network 5. Set up Application Load Balancers with weighted target groups for gradual traffic shifting 6. Create table to track migration state with attributes: resourceId, timestamp, phase, status 7. Configure with hierarchical paths: /migration/db/*, /migration/app/* 8. Implement CloudWatch dashboards showing migration progress metrics from DMS and application health 9. Create Lambda function to validate data consistency between source and target databases 10. Set up topics for migration phase notifications with email subscriptions OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Backup plans for Aurora clusters (OPTIONAL: AWS Backup) - ensures data protection during migration  Implement Route 53 health checks with failover routing (OPTIONAL: Route 53) - enables automatic failback to on-premises  Configure AWS Config rules for migration compliance (OPTIONAL: AWS Config) - validates security standards are met Expected output: A complete Pulumi TypeScript program with stack outputs showing VPN connection status, DMS replication endpoints, Aurora cluster endpoints, and migration tracking table name. The program should support phased execution with clear separation between infrastructure provisioning and data migration phases.","A fintech company is migrating their payment processing infrastructure from a legacy on-premises datacenter to AWS. The current system handles 50,000 daily transactions across three geographic regions. They need to replicate their existing environment in AWS while ensuring zero downtime during the cutover phase.","""Multi-AZ deployment across us-east-1, eu-west-1, and ap-southeast-1 regions for global payment processing. Infrastructure includes Application Load Balancers, containers running payment services, Aurora PostgreSQL Global Database clusters, and DMS replication instances. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate credentials, Node.js 18+. VPC peering between regions with Transit Gateway for inter-region communication. Site-to-Site VPN connection to on-premises datacenter (10.0.0.0/8). Each region has public and private subnets across 3 AZs with NAT Gateways for outbound traffic.""","[""Must support blue-green deployment strategy for zero-downtime migration"", ""Database migration must use AWS DMS with continuous replication enabled"", ""All resources must be tagged with Environment, MigrationPhase, and CostCenter"", ""Network traffic between on-premises and AWS must use Site-to-Site VPN"", ""Implement automated rollback capability if migration validation fails"", ""Use for all configuration values"", ""Migration state must be tracked in with timestamp and status""]"
d2t9g6,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to establish cross-account VPC peering with strict security controls. The configuration must: 1. Create two VPCs in separate AWS accounts (use different provider instances). 2. Configure 3 public subnets (/24) and 3 private subnets (/22) per VPC across different AZs. 3. Establish VPC peering connection with DNS resolution enabled bidirectionally. 4. Create custom route tables for each subnet type with explicit peering routes. 5. Deploy NAT instances (t3.micro) in public subnets with source/dest check disabled. 6. Configure security groups allowing only HTTPS (443) and PostgreSQL (5432) traffic across peering. 7. Enable VPC Flow Logs with custom format capturing srcaddr, dstaddr, srcport, dstport, protocol, packets, bytes, action. 8. Create S3 bucket with versioning, lifecycle rules (transition to Glacier after 30 days), and AES256 encryption. 9. Implement CloudWatch metric filters on Flow Logs for rejected connections. 10. Output peering connection ID, VPC IDs, subnet IDs, and Flow Logs S3 bucket ARN. Expected output: Pulumi program that provisions complete cross-account VPC infrastructure with peering, custom routing, NAT instances, Flow Logs to S3, and CloudWatch monitoring. The stack should be idempotent and handle cross-account permissions correctly.",A financial services company needs to establish secure connectivity between their payment processing VPC and analytics VPC in different AWS accounts. The infrastructure must support cross-account resource sharing while maintaining strict network isolation and compliance requirements.,"""Multi-account AWS deployment across us-east-1 and us-west-2 regions. Payment VPC (10.1.0.0/16) in production account hosts ECS services. Analytics VPC (10.2.0.0/16) in data account runs EMR clusters and Redshift. Requires Pulumi 3.x with TypeScript, AWS CLI with profiles for both accounts configured. Each VPC has 3 availability zones with public and private subnets. NAT instances in public subnets for cost optimization over NAT Gateways. VPC Flow Logs stored in dedicated S3 bucket with 30-day retention.""","[""VPCs must be in different AWS accounts with separate provider configurations"", ""CIDR blocks must not overlap between any VPCs or subnets"", ""All route tables must explicitly define peering routes without using 0.0.0.0/0"", ""Security groups must use least-privilege rules with specific port ranges"", ""VPC Flow Logs must be enabled with custom format including packet-level details"", ""S3 bucket for Flow Logs must have lifecycle policies and encryption at rest"", ""IAM roles must use AssumeRole with external ID for cross-account access"", ""All resources must use consistent tagging with Environment, Owner, and CostCenter"", ""Subnet CIDR allocations must follow /24 for public and /22 for private subnets"", ""Route table associations must be explicit without relying on main route table""]"
f1s5g2,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a consistent payment processing infrastructure across dev, staging, and production environments.

MANDATORY REQUIREMENTS (Must complete):
1. Define a reusable ComponentResource class for the payment processing stack (CORE: Lambda)
2. Create DynamoDB tables with environment-specific capacity modes (CORE: DynamoDB)
3. Deploy Lambda functions with environment-specific memory: dev=512MB, staging=1024MB, prod=2048MB
4. Configure S3 buckets with environment-specific lifecycle rules: dev=30 days, staging=90 days, prod=365 days
5. Implement Pulumi stack configurations for environment-specific variables
6. Use Pulumi secrets for database connection strings and API keys
7. Create stack outputs that can be referenced by other stacks
8. Ensure all IAM roles follow least-privilege principles with no wildcard permissions
9. Tag all resources with Environment, Project, and ManagedBy tags
10. Implement proper error handling and rollback capabilities

OPTIONAL ENHANCEMENTS (If time permits):
 Add API Gateway with environment-specific rate limits (OPTIONAL: API Gateway) - enables direct API testing
 Implement SQS queues for asynchronous processing (OPTIONAL: SQS) - improves system resilience
 Add CloudFront distribution for static assets (OPTIONAL: CloudFront) - enhances performance

Expected output: A Pulumi TypeScript program with a main index.ts file and reusable component classes that can deploy identical infrastructure across multiple environments using stack configurations. The program should handle environment differences through configuration rather than code changes.","A financial services company needs to maintain identical infrastructure across development, staging, and production environments for their payment processing system. Each environment must have the same resource configurations but with environment-specific parameters and isolated state management.","""Multi-environment AWS infrastructure spanning three separate accounts for dev, staging, and production in us-east-1 region. Each environment requires isolated VPCs with public and private subnets across 2 AZs. Core services include Lambda functions for payment processing, DynamoDB for transaction records, and S3 for audit logs. Requires Pulumi CLI 3.x, Node.js 18+, TypeScript 5.x, and AWS credentials configured for each environment. Production uses dedicated tenancy with enhanced monitoring, while dev/staging use shared infrastructure. Network architecture includes NAT gateways in production but NAT instances in lower environments for cost optimization.""","[""Use Pulumi stack configurations to manage environment-specific variables"", ""Implement custom ComponentResource classes for reusable infrastructure patterns"", ""Deploy Lambda functions with identical code but environment-specific memory allocations"", ""DynamoDB tables must use on-demand billing in dev/staging but provisioned capacity in production"", ""All S3 buckets must have versioning enabled and lifecycle rules that vary by environment"", ""Use Pulumi's secret management for sensitive configuration values"", ""Implement stack references to share outputs between dependent stacks""]"
n1k6l5,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,Create a Pulumi TypeScript program to deploy a production-ready web application infrastructure for a payment processing system. MANDATORY REQUIREMENTS (Must complete): 1. Deploy ECS Fargate service with Node.js API container using rolling deployments (CORE: ECS) 2. Create Aurora PostgreSQL Serverless v2 cluster with automated backups (CORE: RDS Aurora) 3. Configure Application Load Balancer with path-based routing and health checks 4. Set up S3 bucket with CloudFront distribution for React frontend hosting 5. Implement VPC with proper subnet isolation (public/private) across 3 AZs 6. Store database credentials in with automatic rotation 7. Configure rules to block common SQL injection patterns 8. Create CloudWatch dashboards with custom metrics for API response times 9. Implement least-privilege IAM policies with no wildcard actions 10. Use Pulumi stack outputs for all endpoint URLs and resource ARNs OPTIONAL ENHANCEMENTS (If time permits):  Add hosted zone with custom domain and SSL certificate (OPTIONAL: ) - enables professional domain branding  Implement queue for asynchronous payment processing (OPTIONAL: ) - improves API response times  Add Redis cluster for session management (OPTIONAL: ) - reduces database load Expected output: Complete Pulumi TypeScript program with modular components that deploys the entire infrastructure stack. The program should use Pulumi's Component Resources pattern to organize related resources and include proper error handling and rollback capabilities.,A fintech startup needs to deploy their payment processing web application with strict compliance requirements. The application consists of a React frontend and Node.js API that processes sensitive financial data. They require infrastructure that meets PCI-DSS standards with proper network isolation and audit trails.,"""Production-grade web application infrastructure deployed in us-east-1 across 3 availability zones. Uses ECS Fargate for containerized Node.js API, Aurora PostgreSQL Serverless v2 for database, S3 and CloudFront for React frontend hosting. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI v2 configured with appropriate permissions. VPC with public subnets for ALB, private subnets for ECS tasks and database. NAT Gateways in each AZ for outbound connectivity. for database credentials and API keys.""","[""All database connections must use SSL/TLS encryption with certificate validation"", ""API endpoints must implement rate limiting of 100 requests per minute per IP"", ""Frontend assets must be served through CloudFront with custom cache behaviors for static and dynamic content"", ""Application logs must be encrypted at rest and retained for exactly 90 days"", ""Auto-scaling must trigger based on custom CloudWatch metrics, not default CPU/memory"", ""All IAM roles must use external ID for cross-account access assumptions""]"
d6w4i3,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a serverless cryptocurrency price alert system. The configuration must: 1. Create a Lambda function to receive webhook events from crypto exchanges with 256MB memory and ARM64 architecture. 2. Set up a DynamoDB table to store user alert configurations with partition key 'userId' and sort key 'alertId'. 3. Implement a second Lambda function to process alerts and check price thresholds against stored configurations. 4. Configure an SQS queue between the webhook receiver and processor functions with visibility timeout of 5 minutes. 5. Create an SNS topic for sending notifications with email subscriptions filtered by crypto symbols. 6. Set up a dead letter queue for failed message processing with max receive count of 3. 7. Configure X-Ray tracing on both Lambda functions with active tracing mode. 8. Implement CloudWatch Logs groups with 30-day retention for all Lambda functions. 9. Create least-privilege IAM roles allowing only required DynamoDB operations and SNS publishing. 10. Add a Lambda function for SNS message formatting with 256MB memory. 11. Configure reserved concurrent executions of 100 for all Lambda functions. 12. Ensure all DynamoDB tables have deletion protection disabled and point-in-time recovery enabled. Expected output: A complete Pulumi TypeScript program that deploys all serverless components with proper configurations, IAM policies, and monitoring setup. The system should handle webhook ingestion, asynchronous processing, and user notifications efficiently.","A fintech startup needs a serverless architecture to process cryptocurrency price alerts. The system must handle webhook events from various exchanges, process them asynchronously, and notify users via SNS when price thresholds are triggered.","""AWS serverless infrastructure deployed in us-east-1 region using Lambda for event processing, DynamoDB for state storage, SNS for notifications, and SQS for message queuing. Requires Node.js 18+, Pulumi CLI 3.x, AWS CLI configured with appropriate credentials. Architecture includes dead letter queues for failed messages, X-Ray tracing for distributed debugging, and CloudWatch Logs for centralized logging. No VPC required as all services are managed.""","[""Lambda functions must use Node.js 18.x runtime with 256MB memory allocation"", ""DynamoDB tables must use on-demand billing mode with point-in-time recovery enabled"", ""All Lambda functions must have X-Ray tracing enabled for debugging"", ""SNS topics must include email subscription filter policies based on crypto symbols"", ""Lambda functions must have reserved concurrent executions set to 100"", ""CloudWatch Logs retention must be set to 30 days for all function logs"", ""DynamoDB tables must have deletion protection disabled for testing environments"", ""Lambda functions must use ARM64 architecture for cost optimization"", ""All IAM roles must follow least-privilege principle with no wildcard actions""]"
u7o3g4,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a CI/CD pipeline infrastructure.

MANDATORY REQUIREMENTS (Must complete):
1. Create an S3 bucket for pipeline artifacts with versioning enabled and lifecycle rules to delete objects after 30 days (CORE: S3)
2. Set up CodeBuild projects for building Docker images with compute type BUILD_GENERAL1_MEDIUM and Linux container (CORE: CodeBuild)
3. Configure CodePipeline with source, build, test, and deploy stages including manual approval action before production (CORE: CodePipeline)
4. Implement IAM roles with least-privilege policies for each service - no wildcard actions or resources
5. Configure all CodeBuild projects to use VPC mode with private subnets for security
6. Enable CloudWatch Logs for build outputs with 14-day retention
7. Tag all resources with Environment, Project, and Owner tags
8. Set up SNS topic for pipeline notifications with email subscription endpoint
9. Configure pipeline to trigger on source repository changes via CloudWatch Events
10. Implement separate pipelines for dev and prod environments using stack configuration

OPTIONAL ENHANCEMENTS (If time permits):
 Add Lambda function for custom pipeline actions (OPTIONAL: Lambda) - enables advanced automation
 Implement ECR repositories for Docker image storage (OPTIONAL: ECR) - centralizes container management
 Add Step Functions for complex deployment orchestration (OPTIONAL: Step Functions) - improves workflow control

Expected output: A Pulumi TypeScript program that creates a complete CI/CD pipeline infrastructure with proper separation of concerns, security configurations, and multi-environment support. The solution should use Pulumi's component resources to encapsulate pipeline logic and support easy replication across environments.","Your organization needs to implement a multi-stage CI/CD pipeline that builds and deploys containerized microservices to different environments. The pipeline must support parallel builds, automated testing, and manual approval gates for production deployments.","""AWS multi-account setup deployed in us-east-1 with CodePipeline orchestrating builds across development and production accounts. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate credentials, Node.js 18+, and Docker installed. VPC with private subnets in 2 AZs for CodeBuild projects. S3 buckets for artifact storage with cross-account access policies. SNS topics for pipeline notifications. CloudWatch Logs for build output monitoring. IAM roles with cross-account assume role permissions for pipeline execution.""","[""Use Pulumi Component Resources to create reusable pipeline abstractions"", ""Implement stack configurations for environment-specific parameters without hardcoding"", ""CodeBuild projects must use custom Docker images stored in a private registry"", ""All S3 buckets must have encryption at rest using AWS-managed keys (SSE-S3)"", ""Pipeline must support both GitHub and CodeCommit as source providers through configuration"", ""Implement retry logic for flaky test stages with exponential backoff using Step Functions""]"
w3f9b5,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to deploy a multi-region disaster recovery infrastructure. The configuration must: 1. Set up Aurora Global Database cluster in us-east-1 with read replica in us-west-2. 2. Configure automatic promotion of secondary region during failover scenarios. 3. Create DynamoDB global tables for session data with consistent replication. 4. Implement Route 53 hosted zone with health checks and automatic DNS failover. 5. Deploy Lambda functions to verify backup integrity every 6 hours. 6. Configure EventBridge rules to replicate critical events across regions. 7. Set up CloudWatch dashboards in both regions with cross-region metrics. 8. Implement AWS Backup plans with 7-day retention and cross-region copying. 9. Create SNS topics for disaster recovery notifications in both regions. 10. Configure all resources with appropriate tags and enable deletion protection for production resources. Expected output: A Pulumi TypeScript program that deploys a complete multi-region disaster recovery solution with automated failover capabilities, achieving RTO under 5 minutes and RPO under 1 minute.","A financial services company needs to implement a disaster recovery solution for their critical transaction processing system. The current system experiences 2-3 hours of downtime during regional failures, resulting in significant revenue loss and regulatory compliance issues.","""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary). Utilizes Aurora Global Database for PostgreSQL 15.x with automated backups, DynamoDB global tables for session management, and EventBridge for event-driven architecture. Route 53 manages DNS failover with health checks. AWS Backup orchestrates cross-region backup policies. Requires Pulumi 3.x with TypeScript, AWS CLI v2 configured with appropriate credentials. VPCs in both regions with private subnets, VPC peering for secure communication. Lambda functions for backup verification and automated recovery testing.""","[""Use Aurora Global Database with automated failover between primary and secondary regions"", ""Implement Route 53 health checks with failover routing policies"", ""Configure DynamoDB global tables with point-in-time recovery enabled"", ""Set RTO (Recovery Time Objective) to less than 5 minutes"", ""Use EventBridge for cross-region event replication"", ""Implement automated backup verification using Lambda functions"", ""Configure CloudWatch cross-region dashboards for unified monitoring"", ""Use AWS Backup for centralized backup management across regions"", ""Implement least-privilege IAM roles with MFA enforcement for disaster recovery operations"", ""Tag all resources with 'Environment', 'DR-Priority', and 'Cost-Center' tags""]"
g2s7k8,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a secure secrets management infrastructure. The configuration must: 1. Create a KMS key with automatic rotation enabled and explicit deny for root account. 2. Deploy Secrets Manager secrets for database credentials, API keys, and certificates with 30-day rotation. 3. Create Lambda functions in TypeScript for custom rotation logic with VPC configuration. 4. Configure IAM roles for 5 different microservices with scoped access to specific secrets only. 5. Set up VPC endpoints for Secrets Manager, KMS, and Lambda services. 6. Implement CloudWatch log groups with 90-day retention for all secret access events. 7. Create resource policies on secrets that enforce access only through VPC endpoints. 8. Deploy SNS topic for rotation failure alerts with email subscription. 9. Tag all resources with Department, Environment, and CostCenter tags. 10. Output the ARNs of created secrets and IAM role names for microservice configuration. Expected output: A Pulumi program that creates a complete secrets management infrastructure with automated rotation, strict access controls, and full audit capabilities meeting financial compliance requirements.","A financial services company needs to implement a secure secrets management system for their microservices architecture. The system must provide automated secret rotation, fine-grained access controls, and comprehensive audit logging while meeting PCI-DSS compliance requirements.","""Production security infrastructure deployed in us-east-1 across 3 availability zones. Uses AWS Secrets Manager for secret storage, KMS for encryption, Lambda for rotation logic, and VPC endpoints for private connectivity. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate permissions. VPC with private subnets only, no internet gateway. All traffic remains within AWS backbone network. Deployment targets a dedicated security account with cross-account assume role permissions.""","[""All secrets must be encrypted at rest using customer-managed KMS keys"", ""Secret rotation must occur every 30 days with zero downtime"", ""Each microservice must have its own IAM role with least-privilege access"", ""All secret access attempts must be logged to CloudWatch with 90-day retention"", ""VPC endpoints must be used for all AWS service communication"", ""Secrets must be versioned with ability to rollback to previous versions"", ""Lambda functions accessing secrets must use VPC networking"", ""KMS key policies must explicitly deny root account access"", ""All resources must have cost allocation tags for department billing""]"
q0a8x1,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to refactor an existing data processing pipeline infrastructure for improved efficiency and cost optimization. The configuration must: 1. Define a reusable Component Resource class for the data processing pipeline that encapsulates Lambda, S3, and DynamoDB resources. 2. Implement CrossGuard policies to enforce mandatory tags (Environment, CostCenter, Owner) on all resources. 3. Convert existing x86 Lambda functions to ARM64 architecture with 512MB memory allocation. 4. Add S3 lifecycle policies to automatically transition objects older than 90 days to Glacier storage class. 5. Refactor DynamoDB tables from provisioned capacity (5 RCU/5 WCU) to on-demand billing mode. 6. Create stack references to share VPC and security group IDs between networking and application stacks. 7. Implement dynamic resource naming using the specified pattern with Pulumi's random provider. 8. Configure all IAM roles with 3600-second maximum session duration. 9. Add automation to destroy all resources in development stacks after 12 hours using Pulumi Automation API. 10. Set up SNS topic with SQS subscription for processing notifications with proper dead-letter queues. 11. Ensure all Lambda functions have environment variables for configuration and X-Ray tracing enabled. 12. Output the refactored infrastructure cost estimates and deployment time improvements. Expected output: A fully refactored Pulumi TypeScript program with modular components, automated policies, and optimized resource configurations that reduces infrastructure costs by at least 30% and deployment times by 50%.","TechFlow Solutions has an existing Pulumi TypeScript program managing their data processing pipeline on AWS. The current implementation suffers from excessive resource costs, slow deployment times, and difficulty managing different environments. The DevOps team needs to refactor the code to improve efficiency and maintainability.","""AWS multi-environment setup deployed across us-east-1 for production and us-west-2 for development/staging. Infrastructure includes Lambda functions for data processing, S3 buckets for storage, DynamoDB tables for metadata, and SNS/SQS for messaging. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, and AWS CLI configured. Each environment runs in isolated VPCs with private subnets. The production environment processes approximately 10GB of data daily through the pipeline.""","[""Must use Pulumi CrossGuard policies to enforce tagging standards"", ""Lambda functions must use ARM64 architecture for cost optimization"", ""All S3 buckets must have lifecycle policies to transition objects to Glacier after 90 days"", ""DynamoDB tables must use on-demand billing mode instead of provisioned capacity"", ""Must implement Pulumi stack references to share outputs between stacks"", ""Resource names must follow the pattern: {environment}-{service}-{resource-type}-{random}"", ""Must use Pulumi Component Resources to encapsulate related infrastructure"", ""All IAM roles must have explicit session duration limits of 1 hour"", ""Must implement automatic resource pruning for development environments after 12 hours""]"
z9k0p6,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to build an automated infrastructure compliance scanner. The configuration must: 1. Deploy a Lambda function that uses AWS Config and APIs to scan resources across three regions (us-east-1, eu-west-1, ap-southeast-1). 2. Create a DynamoDB table to track scan history with partition key 'scanId' and sort key 'timestamp'. 3. Set up an bucket with versioning enabled to store detailed compliance reports in JSON format. 4. Configure EventBridge rules to trigger scans daily at 2 AM UTC and on-demand via custom events. 5. Implement a second Lambda function to analyze scan results and identify critical violations (unencrypted buckets, public RDS instances, overly permissive security groups). 6. Create topics for alerting on critical findings with email subscriptions for the security team. 7. Generate CloudWatch dashboards showing compliance trends, resource coverage, and violation counts by service. 8. Export scan results to CloudWatch Logs Insights-compatible format for advanced querying. 9. Implement automated remediation suggestions in scan reports based on AWS best practices. 10. Create Lambda layers for shared analysis logic and AWS SDK optimizations. 11. Set up CloudWatch alarms for scanner failures and performance degradation. 12. Tag all resources with mandatory compliance tags and cost allocation tags. Expected output: A Pulumi TypeScript program that deploys a complete infrastructure analysis system capable of scanning multi-region AWS resources, identifying compliance violations, storing historical data, and providing actionable insights through dashboards and reports. The system should run automated daily scans and support on-demand analysis requests.","A financial services company has discovered configuration drift and compliance violations across their multi-region AWS infrastructure. They need an automated analysis tool that can scan their existing resources, identify security misconfigurations, and generate actionable reports for their DevOps team to remediate issues before their upcoming SOC2 audit.","""Multi-region AWS infrastructure analysis environment spanning us-east-1, eu-west-1, and ap-southeast-1. Requires Pulumi 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate credentials. The infrastructure includes existing VPCs, EC2 instances, RDS databases, buckets, and Lambda functions across all regions that need compliance scanning. Analysis results are centralized in us-east-1 with cross-region data aggregation. DynamoDB global tables for storing scan metadata and bucket with lifecycle policies for report archival.""","[""Must use Pulumi's stack references to analyze resources across multiple existing stacks"", ""Analysis results must be stored in with server-side encryption using KMS"", ""Lambda functions must have tracing enabled for performance monitoring"", ""All IAM roles must follow least-privilege principles with no wildcard actions"", ""DynamoDB tables must use on-demand billing mode to control costs"", ""CloudWatch Logs retention must be set to exactly 30 days for compliance"", ""Must implement error handling with custom CloudWatch metrics for failed scans"", ""Resource tags must include 'Environment', 'Owner', and 'CostCenter' on all resources"", ""Analysis Lambda must complete execution within 5 minutes timeout limit""]"
m4y8k2,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy an EKS cluster with Istio service mesh for a microservices platform. The configuration must: 1. Create an EKS cluster with managed node groups using Bottlerocket AMI across 3 AZs. 2. Configure cluster autoscaling with minimum 3 nodes and maximum 15 nodes. 3. Deploy Istio service mesh using Helm with automatic sidecar injection enabled. 4. Set up AWS Load Balancer Controller for ALB/NLB provisioning. 5. Configure OIDC provider and create IRSA roles for cluster autoscaler and load balancer controller. 6. Implement KMS encryption for all Kubernetes secrets with automatic key rotation. 7. Create Istio VirtualService and Gateway resources for a sample application. 8. Configure Istio PeerAuthentication for strict mTLS between services. 9. Set up CloudWatch Container Insights with Fluent Bit for log aggregation. 10. Export cluster endpoint, OIDC issuer URL, and kubeconfig as stack outputs. Expected output: A complete Pulumi program that provisions a production-ready EKS cluster with Istio service mesh, demonstrating secure microservices communication patterns with proper observability and scaling capabilities.","A financial services company needs to deploy a production-grade Kubernetes cluster for their microservices architecture. The platform must support service mesh capabilities for enhanced security and observability, with strict compliance requirements for encryption and access control.","""Production EKS infrastructure deployed in us-east-2 across 3 availability zones. Requires Pulumi 3.x with TypeScript, kubectl 1.28+, helm 3.x, and AWS CLI v2 configured. VPC with private subnets for worker nodes, public subnets for load balancers. EKS control plane with private endpoint access only. Integration with AWS Systems Manager for node access. Istio service mesh for traffic management and security. Application load balancers for ingress traffic. CloudWatch Container Insights for monitoring.""","[""EKS cluster must use Kubernetes version 1.28 or higher"", ""All node groups must use Bottlerocket AMI for enhanced security"", ""Istio service mesh must be deployed using official Helm charts version 1.20+"", ""Cluster autoscaler must scale between 3-15 nodes based on CPU/memory metrics"", ""All secrets must be encrypted using AWS KMS with customer-managed keys"", ""Pod-to-pod communication must use mTLS via Istio sidecar injection"", ""OIDC provider must be configured for IRSA (IAM Roles for Service Accounts)"", ""Network policies must restrict egress to only required AWS services""]"
v6s3r0,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement multi-region disaster recovery for a financial transaction system. MANDATORY REQUIREMENTS (Must complete): 1. Deploy RDS Aurora PostgreSQL Global Database cluster spanning us-east-1 (primary) and us-west-2 (secondary) with encryption at rest (CORE: RDS Aurora) 2. Configure Route53 health checks and failover routing policies for database endpoints with 30-second intervals (CORE: Route53) 3. Create Lambda functions in both regions to monitor replication lag and trigger alerts when lag exceeds 5 seconds (CORE: Lambda) 4. Implement cross-region VPC peering between application VPCs in both regions 5. Configure automated backups with point-in-time recovery enabled and 7-day retention 6. Set up CloudWatch alarms for database CPU above 80% and storage above 85% 7. Create IAM roles with cross-account assume role permissions for disaster recovery operations 8. Tag all resources with Environment=production and DR-Role=primary/secondary OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Backup for centralized backup management (OPTIONAL: AWS Backup) - provides unified backup policies  Implement EventBridge rules for automated failover orchestration (OPTIONAL: EventBridge) - enables event-driven DR  Add automation documents for failover procedures (OPTIONAL: ) - standardizes failover process Expected output: Complete Pulumi TypeScript program that provisions a multi-region Aurora Global Database with automated health monitoring, DNS failover routing, and cross-region networking. The solution should enable sub-minute RTO for database failover scenarios.",A fintech company requires zero-downtime database failover capabilities across regions to meet regulatory compliance for transaction processing availability. Their payment processing system must maintain 99.99% uptime with automatic failover between us-east-1 (primary) and us-west-2 (secondary) regions.,"""Multi-region AWS deployment spanning us-east-1 (primary) and us-west-2 (secondary) using RDS Aurora PostgreSQL Global Database, Route53 for DNS failover, Lambda for monitoring. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions. VPCs in both regions with private subnets across 3 AZs each, VPC peering for cross-region connectivity. Aurora cluster uses db.r5.large instances with encryption enabled. Route53 private hosted zone for internal DNS resolution.""","[""Aurora Global Database must use PostgreSQL 15.x with encryption using AWS-managed KMS keys"", ""Route53 health checks must evaluate both database endpoint availability and Lambda-reported replication lag"", ""VPC peering connection must use non-overlapping CIDR ranges: 10.0.0.0/16 (us-east-1) and 10.1.0.0/16 (us-west-2)"", ""Lambda functions must be deployed with reserved concurrent executions of 5 to prevent throttling during monitoring"", ""All resources must have explicit DeletionPolicy set to prevent accidental deletion during stack updates""]"
k6u4c8,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster for containerized microservices. The configuration must: 1. Create an EKS cluster version 1.28 with OIDC provider enabled. 2. Configure 2 managed node groups: 'critical' (3-5 t3.large instances) and 'standard' (2-10 t3.medium instances with spot instances). 3. Deploy AWS Load Balancer Controller using Helm with proper IRSA configuration. 4. Install Istio service mesh for mTLS between services. 5. Create ECR repositories for 'payment-api', 'user-service', and 'notification-service'. 6. Configure Fargate profile for kube-system and istio-system namespaces. 7. Implement cluster autoscaler with IRSA permissions. 8. Set up FluentBit DaemonSet for log forwarding to CloudWatch. 9. Create namespace 'production' with resource quotas (100 CPU, 200Gi memory). 10. Configure pod security standards with 'restricted' level for production namespace. Expected output: A fully functional EKS cluster accessible via kubectl with all components verified through health checks, supporting zero-downtime deployments and automatic scaling based on load.","A fintech startup needs to migrate their monolithic payment processing application to a microservices architecture. The application handles sensitive financial transactions and must maintain strict compliance with PCI-DSS requirements while supporting 10,000+ concurrent users during peak hours.","""Production-grade EKS infrastructure deployed in us-east-1 across 3 availability zones. Uses EKS 1.28 with Fargate profiles for system pods and managed node groups for application workloads. VPC with private subnets, NAT gateways for outbound traffic. Requires Pulumi 3.x with TypeScript, kubectl 1.28+, AWS CLI v2 configured. Integration with ECR for container registry, AWS Load Balancer Controller for ingress, and Secrets Manager for sensitive data. Minimum t3.medium instances for worker nodes.""","[""EKS cluster must use private node groups only with no direct internet access"", ""All container images must be scanned for vulnerabilities before deployment"", ""Pod-to-pod communication must be encrypted using service mesh"", ""Implement pod disruption budgets to ensure minimum 2 replicas during updates"", ""Use IRSA (IAM Roles for Service Accounts) for all AWS service access""]"
q7c6w1,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a comprehensive observability stack for transaction monitoring. The configuration must: 1. Create CloudWatch log groups with KMS encryption for Lambda and ECS services. 2. Configure X-Ray service map with custom sampling rules (10% for successful, 100% for errors). 3. Deploy CloudWatch Synthetics canaries to monitor /health, /transactions, and /audit endpoints. 4. Set up Container Insights for ECS cluster with detailed task-level metrics. 5. Create custom CloudWatch dashboards showing transaction volume, latency percentiles, and error rates. 6. Implement metric filters on logs to extract transaction processing times and failure reasons. 7. Configure composite alarms combining multiple metrics (CPU, memory, error rate). 8. Set up SNS topic with email and webhook subscriptions for critical alerts. 9. Enable detailed monitoring for all resources with 1-minute granularity. 10. Create IAM roles with least-privilege access for all monitoring components. Expected output: A Pulumi program that provisions a complete observability solution with distributed tracing, synthetic monitoring, container insights, and automated alerting for financial transaction processing infrastructure.","A fintech startup processes millions of transactions daily through distributed microservices. They need centralized observability to track performance bottlenecks, debug failures, and ensure compliance with financial regulations requiring detailed audit trails.","""Production observability infrastructure deployed in us-east-1 for financial transaction monitoring. Uses CloudWatch Logs with KMS encryption, X-Ray for distributed tracing across Lambda and ECS Fargate services, CloudWatch Synthetics for endpoint monitoring, and Container Insights for ECS metrics. Requires Pulumi 3.x with TypeScript, AWS SDK v3, and Node.js 18+. Multi-AZ deployment with centralized logging in dedicated security account. SNS topics for alarm notifications to PagerDuty integration.""","[""All Lambda functions must have X-Ray tracing enabled with custom subsegments"", ""CloudWatch Logs must use KMS encryption with customer-managed keys"", ""Implement custom CloudWatch metrics with dimensions for service, operation, and environment"", ""Use CloudWatch Synthetics canaries to monitor critical API endpoints every 5 minutes"", ""Configure CloudWatch alarms with SNS notifications for error rates above 1%"", ""Enable Container Insights for ECS tasks with enhanced monitoring"", ""Implement log aggregation with metric filters for transaction processing times""]"
c6i7t2,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to migrate a payment processing system from on-premises to AWS cloud infrastructure. MANDATORY REQUIREMENTS (Must complete): 1. Set up Aurora PostgreSQL Multi-Master cluster with automated backups and point-in-time recovery (CORE: Aurora) 2. Deploy payment processing service on with auto-scaling based on queue depth (CORE: ) 3. Create DMS replication instance to continuously sync data from on-premises database during migration 4. Implement blue-green deployment capability using target groups and weighted routing 5. Configure App Mesh service mesh with virtual nodes for each microservice 6. Set up Lambda functions for transaction validation and fraud detection 7. Create buckets with cross-region replication for transaction logs and audit trails 8. Implement CloudWatch dashboards showing migration progress and system health metrics 9. Configure topics for alerting on migration issues or service degradation 10. Use for all database credentials and API keys OPTIONAL ENHANCEMENTS (If time permits):  Add API Gateway with usage plans for external partner integrations (OPTIONAL: API Gateway) - enables controlled API access  Implement FIFO queues for transaction ordering guarantees (OPTIONAL: ) - ensures message ordering  Configure for real-time transaction event routing (OPTIONAL: ) - improves event-driven architecture Expected output: Complete Pulumi TypeScript program that creates all infrastructure components with proper dependencies, outputs migration endpoints, and includes rollback capabilities. The program should support parameterized deployment across multiple environments.","A fintech startup needs to migrate their payment processing infrastructure from a legacy on-premises setup to AWS. The current system handles 50,000 daily transactions and must maintain zero downtime during the migration. The infrastructure must support blue-green deployments for future updates.","""Production-ready infrastructure deployed across us-east-1 (primary) and us-west-2 (DR) regions. Core services include for containerized microservices, Aurora PostgreSQL Multi-Master for database tier, and Lambda for event processing. VPC setup with 3 availability zones per region, private subnets for compute resources, public subnets for ALB only. Transit Gateway connects both regions with encrypted VPN backup. Requires Node.js 18+, Pulumi 3.x, Docker, and AWS CLI v2 configured with appropriate IAM permissions.""","[""Use AWS DMS for database migration with continuous replication enabled"", ""Implement circuit breaker pattern using AWS App Mesh for service resilience"", ""All Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""Configure cross-region replication for buckets with lifecycle policies"", ""Use for all configuration values, no hardcoded secrets""]"
o7n6g4,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to deploy a multi-VPC architecture with Transit Gateway connectivity. MANDATORY REQUIREMENTS (Must complete): 1. Create production VPC (10.0.0.0/16) and development VPC (10.1.0.0/16) with 3 tiers each: public, private, database (CORE: VPC) 2. Deploy Transit Gateway and attach both VPCs with custom route tables (CORE: Transit Gateway) 3. Configure NAT Gateways in each AZ for private subnet outbound traffic 4. Implement VPC Flow Logs for both VPCs, storing in S3 bucket with lifecycle policy 5. Create for S3 and ECR in both VPCs 6. Configure security groups: allow HTTPS (443) between VPCs, PostgreSQL (5432) within VPC only 7. Output Transit Gateway route table IDs and VPC endpoint DNS names 8. Tag all resources with Environment and CostCenter tags 9. Enable DNS hostnames and DNS resolution in both VPCs 10. Create Internet Gateways and route tables for public subnets OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Network Firewall for centralized traffic inspection (OPTIONAL: Network Firewall) - enhances security posture  Implement VPC peering as backup connectivity (OPTIONAL: VPC Peering) - adds redundancy  Deploy Route 53 Resolver endpoints for hybrid DNS (OPTIONAL: Route 53 Resolver) - enables on-premises integration Expected output: Pulumi program that creates a production-ready multi-VPC architecture with Transit Gateway hub, proper network segmentation, and security controls. The stack should output all VPC IDs, subnet IDs, Transit Gateway attachment IDs, and VPC endpoint DNS names for application teams to reference.",A fintech startup needs to establish a secure AWS foundation for their payment processing platform. The infrastructure must comply with PCI-DSS requirements for network segmentation and provide isolated environments for development and production workloads. The setup should support both containerized microservices and managed databases while maintaining strict security boundaries.,"""Multi-VPC architecture deployed in us-east-1 region with Transit Gateway for inter-VPC communication. Each VPC contains public subnets for load balancers, private subnets for ECS tasks, and isolated database subnets for RDS Aurora PostgreSQL. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate credentials. Infrastructure spans 3 availability zones (us-east-1a, us-east-1b, us-east-1c) with centralized egress through Transit Gateway. required for S3 and ECR to minimize data transfer costs.""","[""VPC CIDR blocks must not overlap: production uses 10.0.0.0/16, development uses 10.1.0.0/16"", ""All database subnets must be private with no direct internet access"", ""NAT Gateways must be deployed in high availability mode across multiple AZs"", ""VPC Flow Logs must be enabled and stored in S3 with 90-day retention"", ""Transit Gateway attachment must use specific route tables for each environment""]"
i6d2a0,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy consistent payment processing infrastructure across three environments. The configuration must: 1. Define reusable ComponentResource classes for VPC, database, and container infrastructure. 2. Implement environment-specific configuration using Pulumi config files for dev, staging, and prod. 3. Create Aurora PostgreSQL clusters with Multi-AZ deployment (single instance for dev). 4. Deploy ECS Fargate services running containerized payment API with auto-scaling. 5. Configure Application Load Balancer with health checks and SSL termination. 6. Implement cross-account IAM roles for deployment automation. 7. Set up CloudWatch Log Groups with environment-specific retention periods. 8. Create parameter validation to ensure staging matches production configurations. 9. Output environment endpoints, database connection strings, and deployment metadata. 10. Include stack references for sharing VPC and security group IDs between stacks. Expected output: A Pulumi TypeScript project with separate configuration files per environment, reusable components, and deployment scripts that ensure infrastructure consistency while allowing controlled variations for cost optimization.","A financial services company needs to maintain identical infrastructure across development, staging, and production environments for their payment processing system. Each environment must be isolated but deploy the exact same resource configurations to ensure consistent testing and validation before production releases.","""Multi-account AWS deployment across three environments (dev, staging, prod) in us-east-1. Each environment requires isolated VPCs with 3 availability zones, private and public subnets, NAT gateways for outbound traffic. Uses Aurora PostgreSQL for database tier, ECS Fargate for application containers, Application Load Balancer for traffic distribution. Requires Pulumi CLI 3.x, Node.js 18+, TypeScript 5.x, AWS CLI configured with profiles for each account. State stored in environment-specific S3 buckets with DynamoDB for locking.""","[""All environments must use separate AWS accounts with cross-account assume role permissions"", ""Resource naming must follow pattern: {env}-{service}-{resource-type}-{region}"", ""Each environment must have its own state backend in S3 with versioning enabled"", ""Production environment must enforce additional tagging for cost allocation"", ""Staging environment must mirror production's instance sizes and configurations exactly"", ""Development environment can use smaller instance sizes but must maintain same architecture""]"
a3i1s8,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a containerized loan processing web application with strict compliance requirements. MANDATORY REQUIREMENTS (Must complete): 1. Create VPC with 3 public and 3 private subnets across 3 AZs with NAT Gateways (CORE: VPC) 2. Deploy ECS Fargate cluster with service running 3 tasks minimum (CORE: ECS) 3. Configure RDS PostgreSQL instance with Multi-AZ deployment and automated backups (CORE: RDS) 4. Set up Application Load Balancer with target group pointing to ECS service 5. Implement CloudWatch Log Groups with 7-day retention for ECS container logs 6. Create bucket for ALB access logs with lifecycle policy transitioning to Glacier after 90 days 7. Configure IAM roles with least-privilege policies for ECS task execution 8. Set up security groups allowing only HTTPS traffic to ALB and PostgreSQL traffic from ECS 9. Enable RDS encryption with customer-managed KMS key 10. Configure ECS service auto-scaling based on CPU utilization (target 70%) OPTIONAL ENHANCEMENTS (If time permits):  Add AWS WAF to ALB for protection against common web exploits (OPTIONAL: WAF) - improves security posture  Implement health checks with failover routing (OPTIONAL: ) - enhances availability  Add ElastiCache Redis cluster for session management (OPTIONAL: ElastiCache) - improves performance Expected output: A complete Pulumi TypeScript program that provisions all required AWS resources with proper networking, security, and compliance configurations. The program should use Pulumi's strong typing and include proper error handling and resource dependencies.",A fintech startup needs to deploy their loan processing web application with strict compliance requirements for data residency and audit trails. The application handles sensitive financial data and must maintain detailed logs for regulatory audits while ensuring high availability across multiple availability zones.,"""Production-grade infrastructure deployed in us-east-1 across 3 availability zones. Core services include ECS Fargate for containerized web application, RDS PostgreSQL Multi-AZ for primary database, Application Load Balancer for traffic distribution. VPC with 3 public subnets for ALB and 3 private subnets for ECS tasks and RDS. NAT Gateways in each AZ for outbound connectivity. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, AWS CLI configured with appropriate IAM permissions. CloudWatch Logs for centralized logging with custom retention policies.""","[""All RDS backups must be encrypted with customer-managed KMS keys"", ""ALB access logs must be stored in with lifecycle policies for 7-year retention"", ""ECS task definitions must use specific CPU and memory limits (256 CPU units, 512MB memory)"", ""All inter-service communication must occur over private subnets only"", ""CloudWatch Logs must use custom log groups with specific naming convention: /ecs/fintech/{service-name}"", "" buckets must have versioning enabled and MFA delete protection"", ""Security groups must explicitly deny all traffic except required ports (80, 443, 5432)""]"
j0e9x3,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a serverless cryptocurrency price alert system. MANDATORY REQUIREMENTS (Must complete): 1. Deploy a Lambda function (ARM64, 1GB memory) to process webhook payloads from crypto exchanges (CORE: Lambda) 2. Create a DynamoDB table 'crypto-alerts' with partition key 'userId' and sort key 'alertId', with point-in-time recovery enabled (CORE: DynamoDB) 3. Implement an SNS topic 'price-alerts' with server-side encryption for sending SMS notifications (CORE: SNS) 4. Configure an SQS queue 'alert-processor' with visibility timeout of 300 seconds and dead letter queue 5. Set up API Gateway REST API with POST endpoint '/webhook' that triggers the Lambda function 6. Create a CloudWatch Events rule to trigger a price-check Lambda every 5 minutes 7. Configure tracing on all Lambda functions for distributed tracing 8. Create a custom KMS key for encrypting Lambda environment variables containing exchange API keys 9. Implement least-privilege IAM roles with no wildcard permissions 10. Set all resources with deletion protection disabled for easy cleanup 11. Add tags 'Environment=production' and 'Service=crypto-alerts' to all resources OPTIONAL ENHANCEMENTS (If time permits):  Add CloudWatch dashboard for monitoring Lambda invocations and errors (OPTIONAL: CloudWatch) - improves operational visibility  Implement for complex alert processing workflows (OPTIONAL: ) - adds orchestration capabilities  Add EventBridge for routing webhooks to different processors (OPTIONAL: EventBridge) - improves scalability Expected output: A complete Pulumi TypeScript program that deploys all infrastructure components with proper IAM permissions, encryption, and monitoring. The system should handle webhook ingestion, process alerts based on price thresholds stored in DynamoDB, and send SMS notifications via SNS when conditions are met.","A fintech startup needs to process cryptocurrency price alerts in real-time. Their system must handle webhook notifications from multiple exchanges, validate price thresholds, and notify users via SMS when their alerts are triggered. The infrastructure must be cost-effective during low-volume periods but scale automatically during market volatility.","""Serverless infrastructure deployed in us-east-1 region using Lambda functions for webhook processing, DynamoDB for alert storage, SNS for SMS notifications, and SQS for message queuing. Architecture includes API Gateway for webhook endpoints, CloudWatch Events for scheduled price checks, and for distributed tracing. Requires Node.js 18+, Pulumi 3.x, AWS CLI configured with appropriate permissions. All resources deployed within default VPC using managed services only, no custom networking required. KMS custom key for encrypting sensitive Lambda environment variables.""","[""Lambda functions must use ARM64 architecture for cost optimization"", ""DynamoDB tables must use point-in-time recovery enabled"", ""All Lambda functions must have tracing enabled"", ""SNS topics must have server-side encryption using AWS managed keys"", ""Lambda environment variables containing API keys must be encrypted with a custom KMS key"", ""Dead letter queues must be configured with a maximum receive count of 3""]"
r6t6f9,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement blue-green deployment infrastructure for high-availability transaction processing. MANDATORY REQUIREMENTS (Must complete): 1. Deploy Aurora MySQL cluster with one writer and two reader instances across multiple AZs (CORE: RDS Aurora). 2. Create Application Load Balancer with two target groups (blue/green) and health checks every 15 seconds (CORE: ALB). 3. Configure Route53 weighted routing policy to control traffic distribution between blue and green environments. 4. Implement CloudWatch alarms for database CPU utilization and ALB unhealthy target count. 5. Set up SNS topic with email notification for all critical alarms. 6. Create separate security groups for blue and green application tiers with identical ingress/egress rules. 7. Configure Aurora automated backups with 7-day retention period. 8. Implement connection draining on ALB with 30-second timeout. 9. Export stack outputs showing current active environment, endpoint URLs, and health status. 10. Add snapshot creation logic before switching between blue and green environments. OPTIONAL ENHANCEMENTS (If time permits):  Add Lambda function for automated blue-green switching based on health metrics (OPTIONAL: Lambda) - enables automatic failover.  Implement CloudWatch Synthetics for end-to-end transaction monitoring (OPTIONAL: CloudWatch Synthetics) - improves failure detection.  Add AWS Backup for cross-region Aurora cluster replication (OPTIONAL: AWS Backup) - provides disaster recovery capability. Expected output: Complete Pulumi TypeScript program that deploys fault-tolerant blue-green infrastructure with automated failover capabilities, health monitoring, and zero-downtime deployment support.","A financial services company requires zero-downtime deployment capabilities for their critical transaction processing system. The current infrastructure experiences 5-10 minutes of downtime during updates, causing transaction failures and customer complaints. They need an automated blue-green deployment strategy with instant failover capabilities.","""Multi-AZ deployment in us-east-1 region using Aurora MySQL cluster with 2 read replicas, Application Load Balancer with target groups, Route53 hosted zone for DNS management. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions. VPC spans 3 availability zones with private subnets for database tier and public subnets for load balancers. Blue-green environments run in isolated security groups but share the same VPC and Aurora cluster.""","[""Use Aurora MySQL with automated failover to secondary instances"", ""Implement Route53 weighted routing with health checks for zero-downtime switches"", ""Configure ALB with connection draining set to exactly 30 seconds"", ""Set Aurora backup retention to 7 days with point-in-time recovery enabled"", ""Use separate security groups for blue and green environments with identical rules"", ""Implement CloudWatch alarms for database CPU > 80% and ALB unhealthy targets"", ""Configure SNS topic for all critical alerts with email subscription"", ""Use Pulumi stack outputs to expose blue/green endpoint URLs and health status"", ""Set RDS deletion protection to false for testing environments only"", ""Implement automated database snapshot before each blue-green switch""]"
d5e3a3,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to implement an automated secrets management system with custom rotation logic. The configuration must: 1. Create a KMS key with automatic rotation enabled every 90 days (CORE: KMS). 2. Deploy AWS Secrets Manager secrets for database credentials with custom Lambda rotation (CORE: Secrets Manager). 3. Implement Lambda function for custom rotation logic supporting PostgreSQL RDS instances (CORE: Lambda). 4. Configure resource policies allowing only specific IAM roles to retrieve secrets. 5. Enable cross-account secret sharing with explicit deny for root accounts. 6. Set up VPC endpoints for Secrets Manager to prevent internet exposure. 7. Create IAM roles with session policies limiting secret access to specific IP ranges. 8. Configure secret versioning with automatic cleanup of versions older than 30 days. 9. Implement CloudWatch alarms for failed rotation attempts. 10. Tag all resources with CostCenter and DataClassification tags. 11. Ensure all IAM policies follow least-privilege with no Action wildcards. 12. Set stack outputs to include secret ARNs but not actual values. Expected output: A Pulumi TypeScript program that creates a complete secrets management infrastructure with automated rotation, strict access controls, and compliance-ready audit capabilities.",A financial services company requires a secure secrets rotation system for their production database credentials and API keys. The system must enforce strict access controls and automated rotation policies while maintaining audit trails for compliance requirements.,"""AWS multi-account setup deployed in us-east-1 with Secrets Manager, KMS, and Lambda for automated secret rotation. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions. VPC with private subnets and VPC endpoints for Secrets Manager and KMS. Cross-account trust relationships configured for secret sharing. PostgreSQL RDS instances in private subnets requiring credential rotation.""","[""KMS key policy must explicitly deny all root account access including the key creator"", ""Lambda rotation function must complete within 60 seconds timeout limit"", ""Secret resource policies must include conditions for aws:SecureTransport and aws:SourceIp"", ""VPC endpoints must use private DNS and security groups allowing only HTTPS traffic"", ""All IAM roles must have ExternalId requirements for assume role operations"", ""Secret versions must be tagged with RotationDate and retain minimum 3 versions"", ""Cross-account sharing must use resource-based policies not IAM trust relationships""]"
o1j7e9,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to optimize an existing fraud detection infrastructure by refactoring inefficient code patterns and implementing best practices. The configuration must: 1. Refactor 12 Lambda functions currently declared individually into a reusable component that accepts configuration arrays. 2. Consolidate 3 separate DynamoDB table declarations that share similar schemas into a parameterized factory function. 3. Replace hardcoded AWS provider instances for us-east-1 and eu-west-1 with dynamic provider generation based on a regions array. 4. Extract all inline IAM policy documents into separate managed policies referenced by ARN. 5. Implement proper resource tagging using a centralized tagging strategy with environment, cost-center, and team tags. 6. Convert synchronous resource creation loops to use Promise.all() for parallel deployment. 7. Add explicit dependsOn declarations between Kinesis streams and their consumer Lambda functions. 8. Create stack outputs that aggregate resource ARNs by service type for easier reference. 9. Implement resource naming using template literals instead of string concatenation. Expected output: An optimized Pulumi TypeScript program with component resources, factory functions, and parallel deployments that reduces infrastructure deployment time by at least 66% while maintaining all existing functionality and improving code maintainability.","A financial services company has an existing Pulumi TypeScript infrastructure that manages their real-time fraud detection system. The current implementation has performance bottlenecks, redundant resource declarations, and inefficient state management causing deployment times to exceed 45 minutes. The infrastructure team needs to optimize the codebase while maintaining all existing functionality.","""Production fraud detection system deployed across us-east-1 and eu-west-1 regions using Lambda functions for real-time processing, DynamoDB for transaction storage, and Kinesis Data Streams for event ingestion. Current setup includes 12 Lambda functions with varying memory configurations (512MB-3008MB), 3 DynamoDB tables with on-demand billing, and 2 Kinesis streams with 10 shards each. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with multi-region access. VPC configuration spans 3 availability zones with private subnets for Lambda execution.""","[""Deployment time must be reduced to under 15 minutes"", ""All existing Lambda functions must maintain their current memory and timeout configurations"", ""DynamoDB table partition keys cannot be modified"", ""Resource naming conventions must follow pattern: {environment}-{service}-{resource}"", ""Stack outputs must remain backward compatible with dependent systems"", ""Component resources must be used to encapsulate related infrastructure"", ""Dynamic provider configurations must replace hardcoded region values"", ""All inline IAM policies must be converted to managed policies"", ""Resource dependencies must be explicitly declared to prevent race conditions""]"
p3w8p3,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy an automated infrastructure compliance scanning system. The configuration must: 1. Deploy AWS Config with custom rules that trigger Lambda-based compliance evaluators for EC2, RDS, and resources. 2. Create three Lambda functions: one for EC2 instance compliance (checking encryption, IMDSv2), one for RDS compliance (checking encryption, backup settings), and one for compliance (checking versioning, encryption, public access). 3. Configure DynamoDB table with global secondary index on compliance status and timestamp for efficient querying. 4. Set up EventBridge event bus with rules routing compliance findings to different targets based on severity (Critical  , High  , Medium/Low  CloudWatch Logs). 5. Implement cross-account IAM roles with trust policies allowing Config to assume roles in target accounts using external ID. 6. Create topic with email subscription for critical compliance violations. 7. Configure CloudWatch dashboard displaying compliance metrics with custom widgets for pass/fail rates per service. 8. Enable Config configuration recorder with bucket for configuration snapshots, using SSE- encryption. 9. Set up Lambda Dead Letter Queue (DLQ) using for failed compliance evaluations. 10. Export stack outputs including Config rule ARNs, Lambda function names, and EventBridge rule IDs for integration with external tools. Expected output: A fully deployed compliance scanning infrastructure that automatically evaluates resources across multiple accounts, stores findings in DynamoDB, routes events through EventBridge, and provides real-time alerting for critical violations. The system should support adding new compliance rules without modifying core infrastructure.","A financial services company needs automated infrastructure compliance scanning across multiple AWS accounts. The solution must detect configuration drift, policy violations, and generate audit reports for quarterly compliance reviews. The system should integrate with existing SIEM tools via standardized event formats.","""Multi-account AWS infrastructure deployed across us-east-1 and us-west-2 regions for compliance scanning. Uses AWS Config for resource inventory, Lambda functions for custom compliance evaluation, DynamoDB for findings storage, EventBridge for event routing, and for notifications. Cross-account IAM roles enable scanning of 10+ AWS accounts. Requires Pulumi 3.x with TypeScript, AWS SDK v3, Node.js 18+. configured for private API access. Assumes existing setup with delegated administrator account.""","[""Use AWS Config rules with custom Lambda evaluators for PCI-DSS compliance checks"", ""Store compliance findings in DynamoDB with partition keys based on account ID and timestamp"", ""Implement cross-account assume role pattern with external ID validation"", ""Lambda functions must use ARM64 architecture and 1GB memory allocation"", ""Enable point-in-time recovery on DynamoDB tables with 7-day backup retention"", ""Use EventBridge for real-time compliance event routing with at least 3 rule patterns"", ""All Lambda functions must have tracing enabled with custom segments""]"
m8l9z5,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster with advanced networking and security configurations.

MANDATORY REQUIREMENTS (Must complete):
1. Create VPC with 3 private subnets for nodes and 3 public subnets for load balancers (CORE: VPC)
2. Deploy EKS cluster version 1.28 with OIDC provider and VPC CNI addon (CORE: EKS)
3. Configure two managed node groups: 'system' (t3.medium, min=2, max=4) and 'workload' (m5.large, min=3, max=10)
4. Enable pod-to-pod encryption by configuring SecurityGroupPolicy for VPC CNI
5. Deploy EBS CSI driver as EKS addon with IRSA permissions for dynamic volume provisioning
6. Create IAM roles for cluster autoscaler and AWS Load Balancer Controller with IRSA
7. Configure custom launch templates enforcing IMDSv2, 80GB encrypted gp3 volumes, and SSM agent
8. Tag all resources with Environment=production, ManagedBy=pulumi, CostCenter=analytics
9. Export cluster endpoint, OIDC issuer URL, and kubeconfig for kubectl access
10. Set node group update config with maxUnavailable=1 for rolling updates

OPTIONAL ENHANCEMENTS (If time permits):
 Deploy cluster autoscaler with priority expander config (OPTIONAL: Auto Scaling) - optimizes node utilization
 Add AWS Load Balancer Controller for ingress (OPTIONAL: Elastic Load Balancing) - enables ALB/NLB ingress
 Configure Fluent Bit for log forwarding (OPTIONAL: CloudWatch) - centralizes container logs

Expected output: Complete Pulumi program that provisions a production-ready EKS cluster with enhanced security, automated scaling, and proper resource tagging. The cluster should be immediately usable with kubectl and support both stateless and stateful workloads.",A financial services company needs to deploy a Kubernetes-based microservices platform for their trading analytics workloads. The platform must support both CPU-intensive batch processing and real-time streaming applications with strict network isolation between different workload types.,"""Production-grade EKS infrastructure deployed in us-east-2 region across 3 availability zones. Uses EKS 1.28 with managed node groups, VPC CNI plugin for networking, and EBS CSI driver for persistent storage. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, kubectl, and AWS CLI configured with appropriate permissions. VPC spans 10.0.0.0/16 with dedicated private subnets for EKS nodes, public subnets for load balancers, and database subnets for future RDS integration. Implements AWS Systems Manager for node access without SSH keys.""","[""EKS cluster must use managed node groups with mixed instance types (t3.medium for system pods, m5.large for workloads)"", ""Implement pod-to-pod encryption using AWS VPC CNI with SecurityGroupPolicy enabled"", ""Node groups must use custom launch templates with IMDSv2 enforced and 80GB encrypted EBS volumes"", ""OIDC provider must be configured with IAM roles for service accounts (IRSA) for pod-level AWS permissions"", ""Cluster autoscaler must be deployed with priority-based scaling (system pods get priority)""]"
k9g3n3,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to deploy a multi-region disaster recovery infrastructure for a payment processing application. MANDATORY REQUIREMENTS (Must complete): 1. Create Aurora PostgreSQL Global Database cluster with primary in us-east-1 and secondary in us-west-2 (CORE: Aurora) 2. Deploy ECS services in both regions with ALB and auto-scaling (2-10 tasks) (CORE: ECS) 3. Configure Route 53 hosted zone with health checks and failover routing between regions (CORE: Route 53) 4. Set up DynamoDB global tables for session data with point-in-time recovery 5. Implement S3 buckets in both regions with cross-region replication rules 6. Create Lambda functions to monitor Aurora lag and trigger failover alarms 7. Configure CloudWatch dashboards showing replication lag and system health metrics 8. Implement topics in both regions for operational alerts 9. Set up VPC peering between regions with appropriate security groups 10. Enable deletion protection on production resources with proper tagging OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Backup for automated cross-region backup management (OPTIONAL: AWS Backup) - improves backup consistency  Implement for orchestrated failover procedures (OPTIONAL: ) - automates complex failover logic  Add EventBridge rules for automated incident response (OPTIONAL: EventBridge) - enables event-driven automation Expected output: A Pulumi TypeScript program that deploys complete disaster recovery infrastructure with automated failover capabilities, monitoring, and alerting across two AWS regions.","A financial services company requires a disaster recovery solution for their critical payment processing application. The system must maintain RPO of less than 1 minute and RTO of less than 15 minutes, with automatic failover capabilities across AWS regions to ensure business continuity during regional outages.","""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary) regions. Utilizes Aurora PostgreSQL Global Database for transactional data, DynamoDB global tables for session storage, ECS for containerized applications, and S3 with cross-region replication for static assets. Route 53 manages DNS failover with health checks. Lambda functions monitor system health and trigger automated failover procedures. VPCs in both regions with private subnets across 3 AZs each, connected via VPC peering. Requires Pulumi 3.x with TypeScript, AWS CLI configured with multi-region access, Node.js 16+, and Docker for container builds.""","[""Use Aurora Global Database with automated backups and point-in-time recovery enabled"", ""Implement Route 53 health checks with failover routing policy for automatic DNS failover"", ""Deploy identical ECS services in both primary and secondary regions"", ""Configure DynamoDB global tables with on-demand billing and encryption at rest"", ""Set up cross-region replication for S3 buckets with versioning enabled"", ""Implement Lambda functions for health monitoring with CloudWatch alarms triggering notifications""]"
x9n2p1,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster for containerized microservices. The configuration must: 1. Create an EKS cluster version 1.28 with OIDC provider enabled for IRSA. 2. Deploy two managed node groups: 'compute-optimized' with 3-5 t3a.large instances and 'memory-optimized' with 2-4 r6g.xlarge instances. 3. Install AWS Load Balancer Controller using Helm with proper IRSA configuration. 4. Create three namespaces: 'trading-core', 'market-data', and 'analytics' with appropriate resource quotas. 5. Deploy EBS CSI driver and create a gp3-based StorageClass with 3000 IOPS. 6. Configure Kubernetes RBAC with separate service accounts per namespace linked to IAM roles. 7. Implement NetworkPolicies allowing only specific inter-namespace communication patterns. 8. Deploy sample workloads in each namespace with HPA targeting 70% CPU utilization. 9. Configure PodDisruptionBudgets ensuring minimum 2 replicas always available. 10. Set up cluster autoscaling with proper node group taints and tolerations. 11. Enable control plane logging for api, audit, authenticator, controllerManager. 12. Tag all resources with Environment=production, ManagedBy=pulumi, CostCenter=trading. Expected output: A fully functional EKS cluster with microservices-ready configuration including autoscaling, persistent storage, network isolation, and proper IAM integration. The cluster should handle rolling updates without service disruption and scale based on workload demands.",A financial services company needs to modernize their monolithic trading platform by migrating to a microservices architecture on Kubernetes. The platform processes real-time market data and must maintain sub-second latency requirements while ensuring zero data loss during pod restarts.,"""Production EKS cluster deployed in us-east-1 across 3 availability zones using Pulumi TypeScript. Requires Pulumi 3.x, Node.js 16+, kubectl, and AWS CLI configured. VPC with private subnets for worker nodes and public subnets for load balancers. Managed node groups with t3a.large Graviton2 instances. Container images stored in ECR. Application requires persistent volumes for stateful services and ALB ingress for external traffic routing.""","[""EKS cluster must use managed node groups with Graviton2 instances for cost optimization"", ""Implement pod disruption budgets to maintain 80% availability during updates"", ""Configure horizontal pod autoscaling based on custom metrics from application endpoints"", ""Use IRSA (IAM Roles for Service Accounts) for granular AWS permissions per microservice"", ""Deploy ALB Ingress Controller using Helm charts within Pulumi code"", ""Implement Kubernetes network policies to restrict inter-service communication"", ""Configure EBS CSI driver for persistent volume provisioning with gp3 storage class"", ""Set resource quotas per namespace limiting CPU to 100 cores and memory to 200Gi""]"
g2v5d6,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy an observability stack for a serverless payment processing system.

MANDATORY REQUIREMENTS (Must complete):
1. Deploy AWS X-Ray with custom service map for payment flow visualization (CORE: X-Ray)
2. Configure CloudWatch Synthetics canaries to monitor /health and /process endpoints every 5 minutes (CORE: CloudWatch Synthetics)
3. Create custom CloudWatch metrics for transaction_success_rate and average_processing_time
4. Set up SNS topics with email and webhook subscriptions for critical alerts
5. Build CloudWatch dashboard with widgets for API latency, error rates, and business metrics
6. Enable X-Ray tracing on all Lambda functions with custom annotations for payment_id
7. Configure CloudWatch Logs Insights with saved queries for error investigation
8. Create metric filters to extract ERROR and TIMEOUT patterns from Lambda logs

OPTIONAL ENHANCEMENTS (If time permits):
 Add CloudWatch Contributor Insights for top API consumers (OPTIONAL: Contributor Insights) - identifies heavy users
 Implement CloudWatch Anomaly Detector for transaction volumes (OPTIONAL: Anomaly Detector) - proactive alerting
 Create Systems Manager OpsCenter for incident management (OPTIONAL: Systems Manager) - streamlines operations

Expected output: Complete Pulumi TypeScript program that deploys a production-ready observability stack with distributed tracing, synthetic monitoring, custom metrics, and multi-channel alerting capabilities.","A fintech startup needs comprehensive application monitoring for their payment processing platform. The system must track API performance, capture custom business metrics, and provide actionable alerts for their DevOps team. Real-time visibility into transaction flows and system health is critical for maintaining SLA compliance.","""Multi-AZ deployment in us-east-1 with Lambda functions processing payment webhooks, API Gateway REST endpoints, and DynamoDB for transaction storage. Requires Pulumi 3.x with TypeScript, Node.js 18+, and AWS CLI configured. VPC with private subnets for Lambda execution. CloudWatch Logs retention set to 30 days. X-Ray service map visualization enabled. SNS topics configured for multi-channel alerting.""","[""Use AWS X-Ray for distributed tracing with custom segments for payment workflows"", ""CloudWatch Synthetics canaries must run every 5 minutes against production endpoints"", ""Custom CloudWatch metrics must track transaction success rates and processing times"", ""SNS topics must route alerts to both email and Slack webhook endpoints"", ""CloudWatch dashboards must display real-time metrics with 1-minute granularity"", ""All Lambda functions must have X-Ray tracing enabled with custom annotations"", ""CloudWatch Logs Insights queries must be pre-configured for common troubleshooting scenarios"", ""Metric filters must extract error patterns from Lambda logs with alarm thresholds""]"
n9b3h8,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to orchestrate a blue-green migration strategy for transitioning from on-premises to AWS cloud infrastructure. MANDATORY REQUIREMENTS (Must complete): 1. Deploy Aurora PostgreSQL cluster with Multi-AZ configuration and encrypted storage (CORE: ) 2. Create ECS service running containerized application with auto-scaling (2-10 tasks) (CORE: ECS) 3. Configure Application Load Balancer with weighted target groups for traffic shifting (CORE: ALB) 4. Implement database migration using Data Migration Service with CDC enabled 5. Set up VPC with 3 availability zones, private subnets for /ECS, public subnets for ALB 6. Create parameter store entries for database credentials with encryption 7. Configure CloudWatch dashboards showing migration progress metrics 8. Implement rollback mechanism using Pulumi stack exports for critical resource IDs 9. Enable point-in-time recovery for with 7-day retention 10. Tag all resources with Environment, MigrationPhase, and CostCenter tags OPTIONAL ENHANCEMENTS (If time permits):  Add Route 53 weighted routing for DNS-based traffic control (OPTIONAL: Route 53) - enables zero-downtime cutover  Implement Lambda function for post-migration data validation (OPTIONAL: Lambda) - automates integrity checks  Configure EventBridge rules for migration event notifications (OPTIONAL: EventBridge) - improves operational visibility Expected output: Complete Pulumi TypeScript program that provisions parallel AWS infrastructure, orchestrates data migration from on-premises PostgreSQL to Aurora, and enables controlled traffic shifting with rollback capabilities.","A financial services company needs to migrate their monolithic application from a legacy on-premises PostgreSQL database to AWS cloud infrastructure. The migration must maintain data integrity while enabling gradual cutover with minimal downtime. The existing application serves 50,000 daily active users processing payment transactions.","""AWS multi-region deployment targeting us-east-1 primary and us-west-2 for disaster recovery. Requires Pulumi 3.x with TypeScript, AWS CLI configured with administrative access, Node.js 18+, and Docker for local testing. Infrastructure spans 3 AZs with VPC CIDR 10.0.0.0/16, using Aurora PostgreSQL 15.x, ECS with Linux containers, and Application Load Balancer for traffic distribution. Migration tooling includes AWS Database Migration Service for continuous data replication.""","["" Aurora cluster must use db.r6g.large instances with encryption at rest using customer-managed keys"", ""ECS tasks must run in private subnets with no direct internet access, using for outbound traffic"", ""All inter-service communication must use endpoints to avoid internet exposure"", ""Database migration must achieve less than 5 minutes of total downtime during final cutover"", ""Pulumi program must use strong typing with no 'any' types and implement proper error handling for all AWS API calls""]"
w7z6d3,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a multi-region financial trading platform infrastructure. The configuration must: 1. Create VPCs in us-east-1 and eu-west-1 with 3 private subnets each across different AZs. 2. Deploy Aurora Global Database cluster with PostgreSQL 14.6 in us-east-1 as primary. 3. Configure Aurora read replica cluster in eu-west-1 with automatic promotion enabled. 4. Set up ECS Fargate services in both regions running the trading application. 5. Configure Application Load Balancers with target groups pointing to ECS services. 6. Implement AWS Global Accelerator with endpoints in both regions. 7. Create Route 53 hosted zone with health check-based routing policies. 8. Configure Secrets Manager entries in both regions with 30-day rotation schedules. 9. Set up CloudWatch dashboards that aggregate metrics from both regions. 10. Implement AWS Config with rules for security compliance monitoring. 11. Configure VPC peering between regions with appropriate route tables. 12. Apply consistent tagging strategy with Environment, Region, and CostCenter tags. Expected output: A fully functional Pulumi TypeScript program that provisions a multi-region infrastructure with automatic failover capabilities, meeting all specified requirements for high availability and data consistency.",A financial services company needs to establish a multi-region presence for their trading platform to ensure low latency access for global customers. The infrastructure must support automatic failover between regions while maintaining strict data consistency requirements for transaction records.,"""Multi-region AWS deployment spanning us-east-1 (primary) and eu-west-1 (secondary) regions for a high-availability trading platform. Infrastructure includes Aurora Global Database with PostgreSQL 14.6, ECS Fargate for containerized services, Application Load Balancers in each region, and AWS Global Accelerator for traffic management. VPCs in both regions with private subnets across 3 availability zones each, connected via VPC peering. Requires Node.js 18+, Pulumi CLI 3.x, AWS CLI configured with appropriate IAM permissions for multi-region deployment. CloudWatch Logs with cross-region log groups for centralized monitoring.""","[""Deploy infrastructure in exactly two AWS regions: us-east-1 (primary) and eu-west-1 (secondary)"", ""Use AWS Global Accelerator for intelligent traffic routing between regions"", ""Implement Aurora Global Database with PostgreSQL 14.6 compatibility"", ""Configure read replicas in the secondary region with automatic promotion capability"", ""Set RTO (Recovery Time Objective) of less than 60 seconds for regional failover"", ""Use Route 53 health checks with 10-second intervals for endpoint monitoring"", ""Implement AWS Secrets Manager for database credentials with automatic rotation every 30 days"", ""Configure CloudWatch cross-region dashboards for unified monitoring"", ""Enable AWS Config rules for compliance tracking across both regions"", ""All resources must be tagged with Environment, Region, and CostCenter tags""]"
d6v7f1,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy infrastructure across three environments (dev, staging, prod) with automated configuration synchronization. MANDATORY REQUIREMENTS (Must complete): 1. Create RDS Aurora MySQL clusters in each environment with automated backups (CORE: RDS Aurora) 2. Deploy DynamoDB global tables spanning all three regions for configuration storage (CORE: DynamoDB) 3. Implement Lambda functions to sync database schemas between environments (CORE: Lambda) 4. Use Pulumi stack references to share outputs between environments 5. Configure parameter groups with environment-specific values using Pulumi config 6. Implement cross-region VPC peering for secure inter-environment communication 7. Create SNS topics for change notification across environments 8. Use tags to track deployment versions and environment metadata 9. Implement rollback capabilities using Pulumi automation API OPTIONAL ENHANCEMENTS (If time permits):  Add for orchestrating multi-environment deployments (OPTIONAL: ) - improves deployment coordination  Implement EventBridge rules for automated environment sync triggers (OPTIONAL: EventBridge) - enables event-driven synchronization  Add Secrets Manager for cross-environment credential rotation (OPTIONAL: Secrets Manager) - enhances security posture Expected output: Pulumi TypeScript program with multiple stacks that maintains infrastructure consistency across environments, including automated synchronization mechanisms and proper isolation between environments.","A financial services company needs to maintain identical infrastructure across development, staging, and production environments while ensuring data consistency between environments. The solution must support automated promotion of configuration changes through environments with proper validation gates.","""Multi-region AWS deployment across us-east-1 (dev), us-west-2 (staging), and eu-west-1 (prod) using RDS Aurora MySQL 8.0, DynamoDB global tables, and Lambda functions for synchronization. Requires Pulumi 3.x with TypeScript, AWS CLI configured with multi-region access. Each environment has isolated VPCs with private subnets, cross-region VPC peering for secure communication. Stack configurations use Pulumi Config for environment-specific parameters. Infrastructure versioning tracked through tags and Pulumi stack outputs.""","[""Each environment must use different AWS regions to simulate geographic distribution"", ""RDS Aurora clusters must have automated backup retention of 7 days minimum"", ""DynamoDB global tables must have point-in-time recovery enabled"", ""Lambda functions must use container images for consistency across environments"", ""VPC peering connections must use non-overlapping CIDR blocks"", ""All inter-environment communication must go through VPC peering, not public internet"", ""Stack outputs must be typed using Pulumi's Output<T> types for type safety"", ""Configuration changes must be validated in dev before promoting to staging"", ""Production deployments must require explicit approval through Pulumi automation API""]"
v7n8a9,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production-ready e-commerce API infrastructure. The configuration must: 1. Deploy a VPC with 3 public and 3 private subnets across 3 AZs with NAT Gateways for outbound traffic. 2. Create an ECS Fargate cluster running the Node.js API container with auto-scaling (2-10 tasks) based on 70% CPU threshold. 3. Set up RDS Aurora PostgreSQL Serverless v2 with encryption, automated backups (7-day retention), and multi-AZ deployment. 4. Deploy ElastiCache Redis in cluster mode with 2 shards and 1 replica per shard for session management. 5. Configure Application Load Balancer with SSL certificate from ACM and target group health checks. 6. Store all sensitive configuration in AWS Secrets Manager with 30-day automatic rotation enabled. 7. Implement CloudWatch Log Groups with 14-day retention and custom metrics for API response times. 8. Create IAM roles with least-privilege policies for ECS tasks to access RDS, Redis, and Secrets Manager. 9. Set up CloudWatch alarms for high CPU, database connections, and Redis memory usage. 10. Enable deletion protection on production resources but allow force deletion via stack configuration. Expected output: A complete Pulumi TypeScript program that creates all infrastructure components with proper networking, security, and monitoring configurations, ready for blue-green deployments.","A rapidly growing e-commerce startup needs to deploy their Node.js product catalog API with high availability and automated scaling. The application requires PostgreSQL for product data, Redis for session management, and must handle sudden traffic spikes during flash sales. The deployment must support blue-green deployments for zero-downtime updates.","""Production-grade web application infrastructure deployed in us-west-2 across 3 availability zones. Uses ECS Fargate for containerized Node.js API, RDS Aurora PostgreSQL Serverless v2 for product database, ElastiCache Redis cluster mode for session store. Application Load Balancer with SSL termination and protection. VPC with public subnets for ALB, private subnets for compute and data layers. NAT Gateways for outbound internet access from private subnets. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, Docker for local testing, AWS CLI configured with appropriate permissions.""","[""All database credentials must be stored in AWS Secrets Manager and rotated automatically every 30 days"", ""The application must auto-scale between 2-10 instances based on CPU utilization above 70%"", ""PostgreSQL database must use encrypted storage with automated daily snapshots retained for 7 days"", ""Redis cluster must be deployed in cluster mode with 2 shards and 1 replica per shard"", ""Application logs must be centralized in CloudWatch with custom metrics for response time tracking""]"
s9q1d3,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy an event-driven payment notification processor. The configuration must: 1. Deploy a Lambda function that validates incoming payment webhooks and publishes events to EventBridge. 2. Create a DynamoDB table with partition key 'transactionId' and sort key 'timestamp' for idempotency tracking. 3. Configure EventBridge custom event bus with rules routing events based on payment provider type. 4. Implement SQS queue with visibility timeout of 6 times the Lambda timeout for reliable processing. 5. Add a second Lambda function consuming from SQS that updates transaction status in DynamoDB. 6. Configure IAM roles following least privilege with no Admin or wildcard permissions. 7. Set up CloudWatch Log Groups with 30-day retention for all Lambda functions. 8. Create DLQ for the SQS queue with redrive policy after 3 attempts. 9. Enable Lambda reserved concurrent executions of 100 for the webhook processor. 10. Add resource tags including Environment=production and CostCenter=payments. Expected output: A complete Pulumi program that provisions the serverless payment processing pipeline with proper error handling, monitoring, and idempotency guarantees.","A financial services company needs to process real-time payment notifications from multiple payment providers. The system must handle variable traffic patterns, maintain strict audit trails, and ensure no duplicate processing occurs during peak shopping seasons.","""Production-grade serverless infrastructure deployed in us-east-1 region using AWS Lambda for event processing, DynamoDB for state management, EventBridge for event routing, and SQS for message queuing. Requires Node.js 18+, Pulumi CLI 3.x, AWS CLI configured with appropriate credentials. No VPC required as all services are fully managed. Infrastructure spans single region with multi-AZ redundancy for DynamoDB. Account must have service quotas for concurrent Lambda executions increased to handle burst traffic.""","[""Lambda functions must use Node.js 20 runtime with 3GB memory allocation"", ""DynamoDB tables must use pay-per-request billing mode with point-in-time recovery enabled"", ""All Lambda functions must have X-Ray tracing active and custom segments defined"", ""Dead letter queues must retain failed messages for exactly 14 days"", ""EventBridge rules must use event pattern matching, not scheduled expressions""]"
k0z5c4,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a complete CI/CD pipeline for containerized applications. The configuration must: 1. Set up a with three stages: Source (GitHub), Build, and Deploy (ECS Fargate). 2. Configure project to run unit tests and build Docker images, pushing to ECR repository. 3. Create ECS cluster with Fargate service running the built container image. 4. Implement S3 bucket for pipeline artifacts with server-side encryption using KMS. 5. Configure SNS topic for pipeline failure notifications with email endpoint. 6. Set up CodeStar connection for GitHub integration without storing credentials. 7. Create CloudWatch Log Groups for both and ECS with 30-day retention. 8. Implement proper IAM roles for, and ECS task execution. 9. Configure ECS service with desired count of 2 and rolling update deployment. 10. Add pipeline stage transitions with manual approval between Build and Deploy stages. Expected output: A Pulumi program that creates a fully automated CI/CD pipeline capable of building and deploying containerized applications from GitHub to ECS Fargate, with proper security controls and monitoring in place.","A software development team needs to automate their deployment pipeline for a Node.js application. The pipeline should automatically build, test, and deploy code changes from GitHub to ECS Fargate, with proper notifications and artifact management.","""AWS CI/CD infrastructure deployed in us-east-1 region using for orchestration, for build/test execution, and ECS Fargate for container deployment. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate credentials. VPC with private subnets for ECS tasks, public subnet for NAT gateway. S3 bucket for artifact storage with versioning enabled. GitHub connection established via CodeStar Connections.""","[""Pipeline must trigger automatically on GitHub push events to main branch"", ""Build artifacts must be encrypted at rest using customer-managed KMS keys"", ""Pipeline execution logs must be retained for exactly 30 days"", ""All IAM roles must follow least-privilege principle with no wildcard actions"", ""Build environment must use compute type BUILD_GENERAL1_SMALL to minimize costs"", ""Pipeline must send failure notifications to SNS topic with email subscription"", ""All resources must be tagged with Environment=production and ManagedBy=pulumi""]"
k4j6z0,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement automated cross-region disaster recovery for Aurora PostgreSQL.

MANDATORY REQUIREMENTS (Must complete):
1. Deploy Aurora PostgreSQL clusters in us-east-1 (primary) and us-west-2 (standby) with Global Database (CORE: Aurora)
2. Configure Route53 health checks monitoring primary cluster endpoint on port 5432 (CORE: Route53)
3. Create Lambda function triggered by Route53 health check alarms to promote standby cluster (CORE: Lambda)
4. Implement weighted routing policy: 100% primary when healthy, automatic failover to standby
5. Set Aurora backup retention to 7 days with point-in-time recovery enabled
6. Configure CloudWatch alarms for cluster CPU > 80% and connections > 100
7. Enable deletion protection on both Aurora clusters
8. Tag all resources with Environment=production and DR-Role=primary/standby

OPTIONAL ENHANCEMENTS (If time permits):
 Add SNS notifications for failover events (OPTIONAL: SNS) - improves incident response
 Implement Step Functions for orchestrated failback (OPTIONAL: Step Functions) - adds controlled recovery
 Deploy CloudWatch dashboard for DR metrics (OPTIONAL: CloudWatch) - enhances monitoring

Expected output: Pulumi program that creates active-passive DR setup with automated failover completing within 120 seconds of primary failure detection.",A financial services company needs automated disaster recovery for their trading platform database. The system must detect Aurora cluster failures and automatically promote the standby region within 2 minutes to meet regulatory requirements for minimal downtime.,"""Multi-region AWS deployment spanning us-east-1 (primary) and us-west-2 (DR) for high availability disaster recovery setup. Uses Aurora Global Database with PostgreSQL 14.6, Route53 health checks with 30-second intervals, Lambda functions for failover automation. Requires Pulumi 3.x with TypeScript, AWS CLI configured with credentials for both regions. VPC setup with private subnets in 3 AZs per region, security groups allowing PostgreSQL traffic between regions. Minimum t3.medium instances for Aurora, Lambda with 1GB memory allocation.""","[""Aurora clusters must use encrypted storage with AWS-managed KMS keys"", ""Route53 health checks must have 3 consecutive failures before triggering failover"", ""Lambda failover function must validate standby cluster health before promotion"", ""All inter-region traffic must use VPC peering with encrypted transit"", ""Implement exponential backoff in Lambda for Aurora API calls to handle throttling""]"
n8r2f6,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a zero-trust security architecture for containerized microservices. The configuration must: 1. Create a VPC with 3 private subnets across different AZs using 10.0.0.0/16 CIDR block. 2. Deploy ECS cluster with task definitions that enforce read-only root filesystems and non-root users. 3. Configure Network Load Balancer with ACM certificates for TLS 1.3 termination and client certificate validation. 4. Implement AWS Secrets Manager with rotation functions that rotate database credentials every 30 days. 5. Create for ECR, Secrets Manager, and CloudWatch Logs to avoid internet egress. 6. Define least-privilege IAM roles with external IDs and session tags for all service interactions. 7. Configure AWS WAF with rate limiting rules attached to the load balancer. 8. Set up CloudTrail with customer-managed KMS encryption keys and bucket lifecycle policies. 9. Implement security groups that explicitly allow only required ports between specific services. 10. Enable VPC Flow Logs with CloudWatch Logs integration for network traffic analysis. Expected output: A Pulumi program that creates a fully isolated, zero-trust network environment where microservices communicate securely without internet access, all secrets rotate automatically, and comprehensive audit logging captures every API call and network flow.","A financial services company needs to implement zero-trust network architecture for their payment processing microservices. The security team requires strict network isolation, encrypted traffic between services, and comprehensive audit logging. All infrastructure must be defined as code to ensure security configurations are version-controlled and auditable.","""Zero-trust security infrastructure deployed in us-east-1 across 3 availability zones. Uses ECS for container orchestration, Network Load Balancer with TLS termination, AWS Secrets Manager for credential rotation, and AWS Certificate Manager for TLS certificates. VPC with isolated private subnets, no internet gateway, for AWS services. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI configured with security audit role. All resources tagged for compliance tracking.""","[""VPC must use non-overlapping CIDR blocks in 10.0.0.0/8 range"", ""All inter-service communication must use TLS 1.3 with mutual authentication"", ""ECS tasks must run with read-only root filesystems"", ""Secrets must rotate automatically every 30 days without downtime"", ""Network policies must block all egress except to specific AWS services"", ""IAM roles must use external ID for cross-account assumptions"", ""CloudTrail logs must be encrypted with customer-managed KMS keys"", ""Security groups must use explicit port ranges, no wildcard rules""]"
y2r4o2,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to build an automated infrastructure compliance scanning system. The configuration must: 1. Deploy AWS Config with custom rules for EC2, S3, and IAM compliance checking. 2. Create Lambda function to analyze Config findings and generate compliance scores. 3. Store compliance history in DynamoDB with partition key as ResourceId and sort key as Timestamp. 4. Generate HTML compliance reports stored in S3 with 30-day lifecycle policy. 5. Schedule hourly scans using EventBridge that trigger the analysis Lambda. 6. Send notifications for critical violations (score < 70%). 7. Create Lambda for automated remediation of S3 bucket encryption violations. 8. Export Config recorder name, DynamoDB table ARN, and S3 report bucket URL. 9. Implement dead letter queue for failed Lambda executions. 10. Add CloudWatch dashboard showing compliance trends over 7 days. Expected output: A Pulumi program that deploys a fully automated compliance scanning system capable of detecting, reporting, and remediating infrastructure violations in near real-time.",A financial services company needs automated infrastructure compliance scanning to ensure their AWS deployments meet strict regulatory requirements. The compliance team requires real-time scanning of deployed resources with detailed reporting and automated remediation for critical violations.,"""Production compliance infrastructure deployed in us-east-1 region using AWS Config for continuous monitoring, Lambda for automated scanning logic, DynamoDB for compliance history tracking, S3 for report storage, EventBridge for scheduled scans, and for alerting. Requires Pulumi 3.x with TypeScript, Node.js 18.x, AWS CLI configured with appropriate permissions. Resources deployed across single VPC with private subnets for compute resources. Integration with existing assumed.""","[""Use AWS Config for resource compliance tracking"", ""Lambda functions must have 3008MB memory allocation"", ""All Lambda functions must use Node.js 18.x runtime"", ""DynamoDB tables must use PAY_PER_REQUEST billing mode"", ""S3 buckets must have versioning enabled and lifecycle policies"", ""EventBridge rules must trigger within 2 minutes of detection"", "" topics must support both email and Lambda subscriptions"", ""All resources must have CostCenter and Compliance tags"", ""Lambda functions must have tracing enabled""]"
r2m3o8,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster for microservices. The configuration must: 1. Create an EKS cluster with managed node groups using ARM-based Graviton instances. 2. Configure the cluster with OIDC provider for IAM roles for service accounts (IRSA). 3. Deploy worker nodes exclusively in private subnets with NAT gateway egress. 4. Enable control plane logging for audit and authenticator logs to CloudWatch. 5. Configure horizontal pod autoscaler and cluster autoscaler with 2-10 node scaling limits. 6. Implement pod security standards with restricted baseline enforcement. 7. Create IAM roles for cluster autoscaler with minimal required permissions. 8. Export cluster endpoint, certificate authority data, and kubeconfig. Expected output: A Pulumi program that provisions a secure EKS cluster with auto-scaling capabilities, proper network isolation, and AWS service integrations ready for microservice deployments.","A fintech startup requires a managed Kubernetes cluster for their payment processing microservices. The infrastructure must support auto-scaling workloads, secure network isolation, and integration with AWS managed services while maintaining strict security compliance.","""Production-grade EKS cluster deployment in us-east-1 using Pulumi TypeScript. Requires Node.js 18+, Pulumi CLI 3.x, kubectl, AWS CLI configured with appropriate permissions. Infrastructure includes VPC with 3 availability zones, private subnets for EKS nodes, public subnets for load balancers, NAT gateways for outbound traffic. Integration with AWS Systems Manager for node access, CloudWatch Container Insights for monitoring. Cluster configured with managed node groups using t4g.medium instances.""","[""EKS cluster must use Kubernetes version 1.28 or higher"", ""Node groups must use Graviton-based instances (ARM architecture)"", ""Enable IRSA (IAM Roles for Service Accounts) for pod-level AWS permissions"", ""Configure OIDC provider for cluster authentication"", ""Implement pod security standards with restricted baseline"", ""Use only private subnets for worker nodes with no direct internet access"", ""Enable EKS control plane logging for audit and authenticator logs"", ""Configure cluster autoscaler with minimum 2 and maximum 10 nodes"", ""Tag all resources with Environment, Project, and CostCenter tags""]"
l1h0r4,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement a multi-region disaster recovery solution for a critical database-driven application.

MANDATORY REQUIREMENTS (Must complete):
1. Deploy RDS Aurora Global Database with a primary cluster in us-east-1 and secondary in eu-west-1 (CORE: RDS Aurora)
2. Configure Route 53 health checks and weighted routing policies for automatic failover (CORE: Route 53)
3. Create Lambda functions in both regions to monitor cluster endpoints and update Route 53 records (CORE: Lambda)
4. Implement cross-region VPC peering with proper security group rules
5. Set up CloudWatch alarms for cluster lag metrics with SNS notifications
6. Configure automated backups with 7-day retention and cross-region snapshot copying
7. Use Pulumi stack references to share outputs between regional stacks
8. Implement custom resource providers for failover orchestration

OPTIONAL ENHANCEMENTS (If time permits):
 Add DynamoDB Global Tables for session state replication (OPTIONAL: DynamoDB) - ensures session continuity during failover
 Implement Step Functions for coordinated failover workflows (OPTIONAL: Step Functions) - provides orchestrated failover procedures
 Add EventBridge rules for automated incident response (OPTIONAL: EventBridge) - enables automated runbook execution

Expected output: A Pulumi TypeScript program with separate stacks for each region that deploys a fully functional multi-region disaster recovery infrastructure. The solution should demonstrate automatic failover capabilities with Route 53 health checks updating DNS records within 60 seconds of primary region failure.",A financial services company requires their transaction processing system to remain operational even during regional AWS outages. The system must automatically failover between regions with minimal data loss and maintain strict RPO/RTO requirements for regulatory compliance.,"""Multi-region AWS deployment spanning us-east-1 (primary) and eu-west-1 (secondary) regions. Infrastructure includes RDS Aurora Global Database clusters, Route 53 hosted zones with health checks, Lambda functions for monitoring and automation. VPCs in each region with cross-region peering, private subnets for database clusters, public subnets for ALBs. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions, Node.js 18+. Each region maintains independent networking but shares global resources through Route 53 and Aurora Global Database replication.""","[""Aurora global database must use MySQL 8.0 compatible engine with db.r6g.large instances"", ""Route 53 health check intervals must be set to 30 seconds with failure threshold of 2"", ""Cross-region replication lag must not exceed 1 second under normal operations"", ""All Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""VPC peering connections must use encryption in transit and restrict traffic to port 3306 only"", ""Pulumi programs must use explicit resource naming conventions: {region}-{environment}-{resource-type}""]"
o2e1u0,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production-grade EKS cluster with microservices infrastructure. The configuration must: 1. Create an EKS cluster version 1.28+ with managed node groups across 3 availability zones. 2. Deploy RDS Aurora PostgreSQL serverless v2 cluster with encrypted storage and automated backups. 3. Set up ECR repositories with lifecycle policies to retain only the last 10 images per service. 4. Configure Kubernetes namespaces for 'payments', 'users', and 'analytics' with resource quotas. 5. Implement pod security standards with restricted baseline for all namespaces. 6. Create network policies to allow traffic only between specific service pairs. 7. Deploy Kubernetes secrets for database connections using AWS Secrets Manager integration. 8. Configure horizontal pod autoscaling for all deployments with 70% CPU threshold. 9. Set up ingress controller with AWS ALB integration for external traffic routing. 10. Implement pod disruption budgets ensuring minimum 2 replicas during updates. Expected output: A Pulumi stack that provisions a fully functional EKS cluster with isolated microservices, automated scaling, and production-ready security controls. The infrastructure should support rolling updates with zero downtime and automatic rollback on deployment failures.","A fintech startup needs to migrate their monolithic payment processing application to a microservices architecture. The application handles sensitive financial transactions and requires strict isolation between services, automated rollback capabilities, and zero-downtime deployments. They've chosen Kubernetes as their orchestration platform to manage the complex service mesh.","""Production AWS environment in us-east-1 region using EKS 1.28+, RDS Aurora PostgreSQL serverless v2, and ECR for container registry. Requires Pulumi CLI 3.x, Node.js 18+, kubectl, and AWS CLI configured with appropriate IAM permissions. VPC spans 3 availability zones with private subnets for EKS nodes and database, public subnets for ALB. Each microservice runs in isolated Kubernetes namespace with strict network policies and resource quotas. Secrets Manager integration for secure database credential rotation.""","[""EKS cluster must use only private endpoints with no public API access"", ""All container images must be scanned for vulnerabilities before deployment using ECR scanning"", ""Database connections must use IAM authentication instead of password-based auth"", ""Each Kubernetes namespace must have CPU limit of 4 cores and memory limit of 8Gi"", ""Pod-to-pod communication must be encrypted using service mesh with mTLS"", ""All Pulumi resources must have explicit deletion protection enabled for production"", ""Node groups must use Bottlerocket AMI for enhanced security and faster boot times""]"
u0w4g5,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy an advanced observability stack with custom metric aggregation and intelligent alerting.

MANDATORY REQUIREMENTS (Must complete):
1. Deploy CloudWatch custom namespace 'FinanceMetrics' with composite alarms for P99 latency > 500ms AND error rate > 5% (CORE: CloudWatch)
2. Create Lambda function that aggregates metrics from 10+ microservices every 60 seconds, calculates rolling averages, and publishes to CloudWatch (CORE: Lambda)
3. Configure SNS topic with email and SMS subscriptions for critical alerts (CORE: SNS)
4. Implement CloudWatch anomaly detector on transaction volume metrics with 2-week training period
5. Set up metric math expressions to calculate business KPIs (conversion rate = successful_transactions / total_requests)
6. Create CloudWatch dashboard with 15-minute refresh showing real-time metrics across 3 regions
7. Configure Lambda dead letter queue for failed metric processing
8. Implement metric filters on Lambda logs to extract custom error patterns
9. Set up cross-account metric sharing with IAM roles for central monitoring account
10. Configure all resources with cost allocation tags: Environment, Team, CostCenter
11. Enable CloudWatch Container Insights for ECS clusters
12. Create custom CloudWatch Logs Insights queries stored as saved searches

OPTIONAL ENHANCEMENTS (If time permits):
 Add X-Ray tracing integration for distributed request tracking (OPTIONAL: X-Ray) - provides end-to-end visibility
 Implement EventBridge rules for automated remediation workflows (OPTIONAL: EventBridge) - enables self-healing
 Add Systems Manager OpsCenter integration for incident management (OPTIONAL: Systems Manager) - centralizes operations

Expected output: Pulumi TypeScript program that creates a production-ready observability platform with intelligent alerting, custom metrics aggregation, and multi-region visibility. The stack should enable proactive monitoring of business KPIs and automated incident detection.",A financial services company needs centralized monitoring for their distributed microservices architecture. They require real-time alerting on custom business metrics and automated incident response workflows to meet strict SLA requirements.,"""Multi-region AWS deployment across us-east-1, eu-west-1, and ap-southeast-1 for global observability coverage. Core services include CloudWatch for metrics and logs, Lambda for custom metric processing, SNS for multi-channel alerting, and CloudWatch Anomaly Detector for ML-based monitoring. Requires Pulumi 3.x with TypeScript, Node.js 18+, and AWS CLI configured with cross-account permissions. Infrastructure spans multiple VPCs with VPC peering for centralized monitoring. Each region maintains local metric storage with cross-region replication to central monitoring account in us-east-1.""","[""Lambda functions must use arm64 architecture for cost optimization"", ""All CloudWatch alarms must have treat_missing_data set to 'breaching' for safety"", ""SNS topics must use server-side encryption with AWS managed keys"", ""Dashboard widgets must use metric math to show week-over-week comparisons"", ""Cross-account IAM roles must explicitly deny resource deletion permissions""]"
i8m1y7,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to deploy parallel infrastructure for migrating a payment processing system from on-premises to AWS. MANDATORY REQUIREMENTS (Must complete): 1. Create ECS Fargate service with 3 tasks minimum for payment API containers (CORE: ECS) 2. Deploy Aurora PostgreSQL cluster with 2 instances for high availability (CORE: ) 3. Configure DMS replication instance to sync data from on-premises database (CORE: DMS) 4. Set up Application Load Balancer with target groups for blue and green environments 5. Implement Route53 hosted zone with weighted routing records (100% to legacy initially) 6. Create CloudWatch dashboards comparing request rates and error rates between environments 7. Configure VPC with separate security groups for each tier (web, app, database) 8. Use Pulumi stack outputs to display migration endpoints and DMS replication status OPTIONAL ENHANCEMENTS (If time permits):  Add AWS WAF rules on ALB for payment fraud detection (OPTIONAL: WAF) - improves security posture  Implement queues for async payment notifications (OPTIONAL: ) - decouples notification system  Configure AWS Backup for automated Aurora snapshots (OPTIONAL: Backup) - ensures data recovery options Expected output: A Pulumi TypeScript program that creates complete parallel infrastructure ready for payment system migration, with separate stacks managing blue/green environments and monitoring capabilities to validate migration success.","A fintech startup needs to migrate their payment processing infrastructure from a legacy on-premises setup to AWS. The current system handles 50,000 transactions daily and requires zero downtime during migration. The team needs infrastructure that can run in parallel with the existing system before cutover.","""Multi-AZ deployment in us-east-1 for migrating payment processing infrastructure from on-premises to AWS. Core services include ECS Fargate for containerized payment APIs, Aurora PostgreSQL for transaction data, DMS for database replication, Route53 for DNS management with weighted routing policies. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate permissions. VPC setup with 3 availability zones, private subnets for compute and database tiers, public subnets for ALB, NAT gateways for outbound connectivity. Migration environment needs parallel infrastructure to minimize downtime during cutover.""","[""Use blue-green deployment pattern with separate stacks for old and new environments"", ""Implement database migration with AWS DMS from on-premises PostgreSQL to Aurora"", ""Configure weighted routing in Route53 for gradual traffic shifting between environments"", ""All sensitive data must be encrypted at rest using customer-managed keys"", ""Deploy monitoring dashboards that compare metrics between old and new environments"", ""Resource naming must include environment tags (legacy/new) for clear identification""]"
d0j0d1,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster with advanced networking configuration. MANDATORY REQUIREMENTS (Must complete): 1. Create an EKS cluster version 1.27 with managed node groups across 3 availability zones (CORE: EKS) 2. Configure a VPC with private subnets for worker nodes and public subnets for load balancers (CORE: VPC) 3. Deploy two node groups: one for general workloads (t3.medium, min 2, max 5) and one for stateful apps (m5.large, min 1, max 3) 4. Enable IRSA (IAM Roles for Service Accounts) with OIDC provider configuration 5. Install AWS Controller using Helm with proper IAM permissions 6. Configure EBS CSI driver for persistent volume support with gp3 storage class 7. Implement pod security standards at namespace level using admission controllers 8. Enable control plane logging for api, audit, authenticator, controllerManager, and scheduler 9. Configure CIDR blocks to avoid conflicts: VPC 10.0.0.0/16, pods 172.16.0.0/12, services 192.168.0.0/16 10. Tag all resources with Environment=production and ManagedBy=pulumi OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Secrets Manager integration for sensitive data (OPTIONAL: Secrets Manager) - improves security posture  Implement VPC endpoints for ECR and S3 (OPTIONAL: VPC Endpoints) - reduces data transfer costs  Configure AWS Systems Manager for node management (OPTIONAL: Systems Manager) - enables secure node access Expected output: A complete Pulumi program that provisions the EKS cluster with all networking components, node groups, and add-ons configured. The stack should export the cluster endpoint, OIDC issuer URL, and kubeconfig for cluster access.",A financial services company needs to establish a production-grade Kubernetes environment for their microservices architecture. The infrastructure must support both stateful and stateless workloads with strict network isolation and compliance requirements.,"""Production-grade EKS cluster deployment in us-east-1 across 3 availability zones. Uses EKS 1.27 with managed node groups, custom VPC with public/private subnet architecture, and NAT gateways for outbound traffic. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, AWS CLI configured with appropriate permissions. Infrastructure spans multiple subnets with dedicated CIDR ranges for pods and services. Includes AWS Controller and EBS CSI driver for full Kubernetes functionality.""","[""Node groups must use Amazon Linux 2 EKS-optimized AMIs only"", ""All inter-subnet traffic must flow through security groups with explicit rules"", ""Kubernetes API endpoint must be private with authorized IP ranges only"", ""Each availability zone must have exactly one NAT gateway for high availability"", ""Worker nodes must use IMDSv2 with hop limit of 1 for metadata service"", ""All IAM policies must follow least-privilege principle without wildcard actions"", ""Helm releases must specify exact chart versions, not latest tags"", ""Resource names must follow pattern: {project}-{environment}-{resource}-{random}"", ""Stack must complete deployment within 25 minutes on standard CI runners""]"
q3j2t1,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy and maintain consistent infrastructure across three environments (dev, staging, prod) for a transaction processing system. The configuration must: 1. Define a base component resource that encapsulates RDS Aurora cluster, Lambda functions, and S3 buckets. 2. Create environment-specific stacks that inherit from the base component. 3. Implement parameter overrides for instance sizes (t3.medium for dev, t3.large for staging, t3.xlarge for prod). 4. Configure cross-environment dependencies where staging references dev outputs. 5. Set up rules to trigger Lambda functions on S3 object creation. 6. Create DynamoDB tables with point-in-time recovery enabled. 7. Implement CloudWatch dashboards that aggregate metrics across all environments. 8. Configure RDS read replicas in different AZs for each environment. 9. Set up SNS topics with email subscriptions for critical alerts. 10. Ensure all IAM roles follow least-privilege principle with no wildcards. Expected output: A Pulumi TypeScript project with separate configuration files for each environment, base component definitions, and stack files that can be deployed independently while maintaining consistency.","A financial services company needs to maintain identical infrastructure across development, staging, and production environments for their transaction processing system. Each environment must have exact parity in resource configurations while allowing for environment-specific parameters like instance sizes and retention periods.","""Multi-environment AWS deployment across us-east-1 (production), us-west-2 (staging), and eu-west-1 (development). Each environment requires VPC with 3 availability zones, private and public subnets, NAT gateways for outbound traffic. Core services include RDS Aurora PostgreSQL clusters, Lambda functions for transaction processing, S3 buckets for audit logs. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate credentials for each environment. Each environment uses separate AWS accounts with cross-account IAM roles for deployment.""","[""Use Pulumi stack references to share outputs between environments"", ""Implement custom resource providers for environment-specific configurations"", ""All S3 buckets must have versioning enabled and lifecycle policies"", ""RDS instances must use encrypted storage with customer-managed KMS keys"", ""Lambda functions must have reserved concurrent executions set"", ""Use Pulumi configuration files for environment-specific values only"", ""Implement automated drift detection using Pulumi Automation API"", ""All resources must be tagged with Environment, Team, and CostCenter tags""]"
k3i9s9,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a secure payment processing web application infrastructure. MANDATORY REQUIREMENTS (Must complete): 1. Deploy ECS service with 2 tasks minimum for the Node.js API backend (CORE: ECS) 2. Create Aurora PostgreSQL cluster with encryption enabled (CORE: ) 3. Configure Application Load Balancer with HTTPS listener using ACM certificate 4. Set up distribution with custom error pages and geo-restrictions 5. Implement WAF web ACL with rate limiting rules attached to 6. Create private buckets for application logs with versioning enabled 7. Configure ECS task and execution roles with specific permissions only 8. Set up CloudWatch Log Groups with 30-day retention for all services 9. Enable deletion protection on cluster and set retention to 7 days 10. Output the distribution URL and ALB DNS name OPTIONAL ENHANCEMENTS (If time permits):  Add hosted zone with alias records (OPTIONAL: ) - enables custom domain management  Implement Secrets Manager for database credentials rotation (OPTIONAL: Secrets Manager) - improves security posture  Set up EventBridge rules for deployment notifications (OPTIONAL: EventBridge) - enhances deployment visibility Expected output: A complete Pulumi TypeScript program that provisions the entire infrastructure stack with proper security controls, outputs the application endpoints, and supports safe updates through Pulumi's preview functionality.",A fintech startup needs to deploy their payment processing web application with strict compliance requirements for PCI DSS. The application consists of a React frontend and Node.js API backend that must handle sensitive credit card data with end-to-end encryption and audit logging.,"""Multi-AZ deployment in us-east-1 region hosting a payment processing web application. Architecture includes ECS for containerized Node.js API and React frontend, Application Load Balancer with target group health checks, Aurora PostgreSQL Multi-AZ cluster for transaction data, distribution with WAF for global content delivery and DDoS protection. VPC spans 3 availability zones with public subnets for ALB and private subnets for ECS tasks and . Requires Node.js 18+, Pulumi CLI 3.x, Docker for container builds, and AWS CLI configured with appropriate permissions.""","[""All data must be encrypted at rest using AWS-managed KMS keys"", ""API endpoints must be accessible only through with WAF enabled"", ""Database connections must use SSL/TLS with certificate validation"", ""Application logs must be stored in separate buckets by environment with lifecycle policies"", ""All IAM roles must follow least privilege with no wildcard permissions"", ""Network traffic between services must remain within private subnets"", ""Deployment must support blue-green deployments with zero downtime"", ""All resources must be tagged with Environment, Project, and CostCenter tags""]"
e4f3i4,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a serverless cryptocurrency price alert system. The configuration must: 1. Create a DynamoDB table for user subscriptions with partition key 'userId' and sort key 'alertId', plus a GSI on 'cryptocurrency' for efficient queries. 2. Deploy a Lambda function (Node.js 18, ARM architecture) to check current prices against user thresholds, triggered by EventBridge every 30 seconds. 3. Create a second Lambda function to send SMS notifications via SNS when price conditions are met. 4. Implement a Lambda Layer containing shared cryptocurrency API client code and utility functions. 5. Configure EventBridge rule with the Lambda price checker as target, including retry policy with maximum 2 attempts. 6. Set up SNS topic for SMS delivery with appropriate permissions for Lambda execution. 7. Create CloudWatch Log groups for each Lambda with 30-day retention and metric filters for failed notifications. 8. Implement least-privilege IAM roles with no wildcard permissions for all Lambda functions. 9. Configure Lambda reserved concurrent executions (price checker: 100, notifier: 50). 10. Add resource tags including 'Environment: Production' and 'Service: CryptoAlerts' to all resources. Expected output: A complete Pulumi program that deploys the serverless alert system with all components properly configured, encrypted, and monitored. The system should handle high-frequency price checks and reliable SMS delivery while maintaining cost efficiency through ARM-based compute and optimized data access patterns.","A fintech startup needs to process cryptocurrency price alerts in real-time. Users subscribe to price thresholds for different cryptocurrencies, and the system must notify them via SMS when their conditions are met. The architecture must handle 50,000+ concurrent subscriptions with sub-second processing latency.","""Production-ready serverless infrastructure deployed in us-east-1 across 3 availability zones. Core services include Lambda functions with ARM Graviton2 architecture, DynamoDB for subscription storage with global secondary indexes, SNS for SMS notifications, and EventBridge for scheduled price checks. Requires Pulumi 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate permissions. Infrastructure includes VPC endpoints for DynamoDB to reduce data transfer costs, CloudWatch Logs with custom metric filters for monitoring alert delivery rates, and KMS keys for encryption. The system processes real-time cryptocurrency price data from external APIs with sub-second latency requirements.""","[""Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""DynamoDB tables must use point-in-time recovery and encryption at rest with customer-managed KMS keys"", ""All Lambda functions must have reserved concurrent executions set to prevent throttling"", ""Use Lambda Layers for shared dependencies across functions to reduce deployment package sizes"", ""Implement exponential backoff with jitter for all DynamoDB operations"", ""Lambda functions must use environment variables encrypted with a dedicated KMS key"", ""Enable AWS X-Ray tracing on all Lambda functions with custom subsegments for external API calls""]"
k8u6n5,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement a multi-region disaster recovery infrastructure. The configuration must: 1. Deploy RDS Aurora Global Database with a primary cluster in us-east-1 and secondary in us-west-2. 2. Configure S3 buckets in both regions with cross-region replication and versioning enabled. 3. Create Lambda functions in both regions to perform health checks every 10 seconds. 4. Implement Route 53 health checks that monitor the primary region's ALB endpoint. 5. Set up Application Load Balancers in both regions with target groups pointing to placeholder EC2 instances. 6. Configure Route 53 failover routing policy with primary and secondary record sets. 7. Establish VPC peering between regions with appropriate route table entries. 8. Create CloudWatch alarms for RDS lag metrics with SNS notifications. 9. Implement IAM roles with cross-region assume role policies for Lambda functions. 10. Configure Aurora global database with 1-second RPO target. 11. Enable automated backups with 7-day retention in both regions. 12. Set up CloudWatch Logs with cross-region log group subscriptions. Expected output: A Pulumi program that provisions a complete multi-region DR infrastructure with automated failover capabilities, monitoring, and sub-minute recovery objectives.","A financial services company requires a disaster recovery solution for their critical trading platform. The system must maintain sub-second failover capabilities and ensure zero data loss during regional outages. The architecture needs to support 100,000 concurrent connections with automatic health checks and failover orchestration.","""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary) regions. Utilizes RDS Aurora Global Database for data replication, Lambda functions for health monitoring, S3 with cross-region replication for static assets, and Route 53 for DNS failover. VPCs in both regions with private subnets across 3 availability zones each, connected via VPC peering. Requires Pulumi 3.x with TypeScript, AWS CLI v2 configured with appropriate IAM permissions. Architecture supports automatic failover with RPO of 1 second and RTO of 30 seconds.""","[""RDS Aurora Global Database must use MySQL 8.0 with backtrack enabled for point-in-time recovery"", ""Route 53 health checks must trigger failover within 30 seconds of primary region failure"", ""Lambda functions must be deployed with reserved concurrency of exactly 100 in each region"", ""S3 buckets must use cross-region replication with RTC (Replication Time Control) enabled"", ""All resources must be tagged with Environment=DR and CostCenter=FinOps for billing tracking""]"
g4f8o8,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to implement a zero-trust security infrastructure for containerized microservices. The configuration must: 1. Deploy an cluster with compute and enable Container Insights for security monitoring. 2. Create AWS Secrets Manager secrets for database credentials, API keys, and TLS certificates with -based rotation. 3. Configure ECR repositories with image scanning on push and lifecycle policies for vulnerability management. 4. Implement AWS App Mesh with mTLS enforcement between all service communications. 5. Set up fine-grained IAM policies using conditions for time-based and IP-restricted access. 6. Deploy AWS Config rules to monitor security compliance and generate alerts on violations. 7. Create KMS customer-managed keys with granular key policies for each microservice. 8. Configure for all AWS services to prevent data exfiltration. 9. Implement CloudWatch Log Insights queries for security event correlation. 10. Set up AWS GuardDuty with custom threat intelligence feeds. Expected output: A complete Pulumi TypeScript program that deploys a production-ready zero-trust architecture with automated security controls, comprehensive audit logging, and compliance monitoring capabilities.","A financial services company needs to implement a zero-trust security architecture for their microservices platform. They require automated certificate management, secrets rotation, and fine-grained access controls across their container workloads. The infrastructure must pass strict compliance audits and support forensic analysis capabilities.","""Zero-trust security infrastructure deployed in us-east-1 using for containerized microservices, AWS Secrets Manager for credentials management, AWS App Mesh for service mesh security, and ECR for container registry. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions. VPC with private subnets across 3 availability zones, no direct internet access for compute resources. All traffic flows through endpoints. Compliance requirements include SOC2 and PCI-DSS standards.""","[""All secrets must be stored in AWS Secrets Manager with automatic rotation every 30 days"", "" task definitions must use IAM roles for service accounts with session-based credentials"", ""All container images must be scanned for vulnerabilities before deployment using ECR scanning"", ""Network policies must enforce east-west traffic encryption using AWS App Mesh with mTLS"", ""CloudTrail must log all API calls with event selectors for secret access patterns"", ""Security groups must follow least-privilege with explicit deny rules for management ports"", ""KMS keys must use separate keys for each service with mandatory key rotation enabled""]"
l3w5b2,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to optimize an existing infrastructure deployment that currently costs $8,000/month. The configuration must: 1. Refactor ECS task definitions to use appropriate CPU/memory combinations (reduce from 4096MB to 2048MB for API services). 2. Create a shared security group module that consolidates 15 duplicate security groups into 3 reusable ones. 3. Implement Pulumi.Config for environment-specific values (dev/staging/prod) replacing 50+ hardcoded strings. 4. Add comprehensive tagging function that applies cost-center, environment, and owner tags to all resources. 5. Fix circular dependencies between ALB target groups and ECS services using explicit dependsOn. 6. Enable RDS automated backups with 7-day retention and point-in-time recovery. 7. Configure S3 lifecycle policies to transition objects older than 30 days to Glacier. 8. Implement resource naming convention using stack name prefix for all resources. 9. Add custom ComponentResource classes for ECS services and RDS clusters. 10. Export critical resource IDs and endpoints as stack outputs for downstream consumers. Expected output: Optimized Pulumi TypeScript program with modular components, proper configuration management, and resource optimizations that reduce monthly costs by 40% while improving deployment speed to under 10 minutes.","A financial services company discovered their Pulumi TypeScript infrastructure code has become unmaintainable after rapid growth. The existing codebase deploys ECS services with excessive resource allocation, redundant security groups, and no proper state management, resulting in $8,000/month overspend and 45-minute deployment times.","""Production environment in us-east-1 with existing Pulumi TypeScript infrastructure managing ECS Fargate services, Aurora RDS clusters, and S3 data lakes. Current setup uses Pulumi 3.x with Node.js 18.x runtime. Infrastructure spans 3 availability zones with private subnets for compute and database tiers. Application serves 50,000 daily active users with peak traffic between 9 AM-5 PM EST. Requires AWS CLI configured with appropriate permissions for ECS, RDS, S3, and IAM services.""","[""Must reduce ECS task memory allocation by at least 40% while maintaining performance"", ""Replace hardcoded values with stack-specific configuration using Pulumi.Config"", ""Consolidate duplicate security group rules into reusable components"", ""Implement proper resource tagging strategy with cost center tracking"", ""Add explicit dependency management between resources using dependsOn"", ""Enable point-in-time recovery for RDS instances without downtime"", ""Ensure all S3 buckets use intelligent tiering for cost optimization""]"
o7g4p2,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to implement infrastructure compliance validation and analysis. The configuration must: 1. Define a PolicyPack with at least 5 compliance policies for EC2, RDS, and S3 resources. 2. Validate that all EC2 instances use encrypted EBS volumes and approved AMIs. 3. Check RDS clusters have encryption enabled and automated backups configured. 4. Ensure S3 buckets have versioning enabled and block public access settings. 5. Enforce mandatory tagging standards (Environment, Owner, CostCenter). 6. Create a custom resource that runs compliance checks and stores results in DynamoDB. 7. Implement stack analytics to count resource types and calculate compliance percentages. 8. Generate a compliance report showing passed/failed policies per resource. 9. Support policy exceptions through resource-level annotations. 10. Include unit tests for at least 3 compliance policies. Expected output: A Pulumi program with PolicyPack definitions, custom compliance analyzer, and automated report generation that provides infrastructure visibility and policy enforcement.",A financial services company needs automated infrastructure compliance validation for their multi-account AWS setup. Regular audits have revealed configuration drift and non-compliant resources that must be detected programmatically. The compliance team requires continuous monitoring of infrastructure state against defined policies.,"""Multi-account AWS infrastructure deployed across us-east-1 and us-west-2 regions. Primary services include EC2 instances in Auto Scaling groups, RDS Aurora clusters, S3 buckets with versioning, and Lambda functions. Requires Pulumi 3.x with TypeScript, Node.js 16+, and AWS CLI configured with cross-account assume role permissions. Infrastructure spans development, staging, and production environments with separate VPCs per account. Compliance policies must validate encryption settings, tagging standards, and network isolation requirements.""","[""Use Pulumi Policy as Code framework for all compliance rules"", ""Implement stack validation that runs before any deployment"", ""Create reusable policy packs that can be shared across teams"", ""Generate detailed compliance reports in JSON format"", ""Support both advisory and mandatory policy enforcement levels""]"
t8j0q1,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement a multi-region disaster recovery infrastructure for a payment processing application. The configuration must: 1. Deploy an Aurora Global Database cluster with a primary cluster in us-east-1 and secondary read replica cluster in us-west-2. 2. Create S3 buckets in both regions with cross-region replication enabled for application artifacts and backups. 3. Implement Route 53 health checks and failover routing policy for automatic DNS failover. 4. Deploy Lambda functions in both regions to perform health checks on database endpoints. 5. Configure CloudWatch alarms for monitoring replication lag and triggering notifications. 6. Create SNS topics in both regions for disaster recovery event notifications. 7. Implement IAM roles with cross-region assume policies for failover automation. 8. Set up CloudWatch dashboards in both regions displaying key DR metrics. 9. Configure all resources with appropriate tags as specified in constraints. 10. Ensure all data in transit and at rest uses AWS-managed encryption. Expected output: A complete Pulumi TypeScript program that deploys a fully functional multi-region disaster recovery infrastructure with automated failover capabilities, monitoring, and alerting systems ready for production use.",A financial services company requires a disaster recovery solution for their critical payment processing application. The system must maintain near real-time data synchronization between regions and enable rapid failover with minimal data loss to meet regulatory compliance requirements.,"""Multi-region AWS deployment spanning us-east-1 (primary) and us-west-2 (secondary) for disaster recovery. Infrastructure includes Aurora Global Database with PostgreSQL 15.4, cross-region replicated S3 buckets, Route 53 failover routing, and Lambda functions for health monitoring. Requires Pulumi 3.x with TypeScript, AWS CLI v2 configured with appropriate credentials, Node.js 18+. VPCs in both regions with private subnets across 3 AZs each, VPC peering for cross-region communication, and NAT gateways for outbound connectivity.""","[""Primary region must be us-east-1 with failover to us-west-2"", ""RTO (Recovery Time Objective) must be under 5 minutes"", ""RPO (Recovery Point Objective) must be under 1 minute"", ""All resources must be tagged with Environment, Application, and DR-Role tags"", ""Route 53 health checks must monitor both regions with 30-second intervals"", ""Cross-region replication must use AWS-managed encryption keys""]"
e2t2w6,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster with automated container orchestration capabilities. MANDATORY REQUIREMENTS (Must complete): 1. Create an EKS cluster with Kubernetes 1.28+ in a custom VPC with private subnets (CORE: EKS) 2. Configure managed node groups with m7g.large instances and auto-scaling 3-10 nodes 3. Set up ECR repositories for storing application container images (CORE: ECR) 4. Implement IRSA with OIDC provider for secure pod authentication 5. Deploy AWS Load Balancer Controller as a Kubernetes deployment 6. Configure CloudWatch Container Insights for cluster monitoring 7. Create sample deployment manifests with resource limits and PDB 8. Enable image scanning on all ECR repositories OPTIONAL ENHANCEMENTS (If time permits):  Add Fargate profile for running system pods (OPTIONAL: Fargate) - reduces node management overhead  Implement Karpenter for advanced node auto-scaling (OPTIONAL: EC2 + Karpenter) - improves cost efficiency  Set up AWS App Mesh for service mesh capabilities (OPTIONAL: App Mesh) - adds traffic management features Expected output: Complete Pulumi TypeScript program that provisions a production-ready EKS cluster with secure container orchestration, automated scaling, and monitoring capabilities. Include example Kubernetes manifests demonstrating deployment patterns.",A fintech startup needs to deploy their microservices architecture on AWS EKS to handle payment processing workloads. The platform must support blue-green deployments and automatic scaling based on transaction volume. Compliance requires all traffic to remain within private networks with strict security controls.,"""Production EKS cluster deployed in us-east-2 across 3 availability zones. Infrastructure includes EKS 1.28 with managed node groups using m7g.large instances, Application Load Balancer for ingress, ECR for container registry, and VPC with private subnets. Requires Pulumi 3.x with TypeScript, kubectl, AWS CLI configured with appropriate permissions. The cluster integrates with for node management and CloudWatch Container Insights for monitoring. Network architecture includes VPC endpoints for ECR, S3, and other AWS services to keep traffic private.""","[""EKS cluster must use Kubernetes version 1.28 or higher"", ""Worker nodes must use Graviton3-based instances (m7g family) for cost optimization"", ""All pods must have resource limits and requests defined"", ""Implement pod disruption budgets for critical services"", ""Use IRSA (IAM Roles for Service Accounts) for pod-level AWS permissions"", ""Enable EKS managed node groups with auto-scaling between 3-10 nodes"", ""Configure OIDC provider for the cluster"", ""All container images must be scanned for vulnerabilities before deployment""]"
k3u0z5,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a multi-account observability platform for monitoring distributed microservices. MANDATORY REQUIREMENTS (Must complete): 1. Deploy Amazon Managed Grafana workspace with SSO authentication enabled (CORE: Amazon Managed Grafana) 2. Configure Amazon Managed Service for Prometheus with workspaces in us-east-1 and us-west-2 (CORE: Amazon Managed Service for Prometheus) 3. Create CloudWatch Log Groups with custom retention policies for application, infrastructure, and audit logs 4. Deploy Lambda functions to parse and enrich logs before CloudWatch ingestion 5. Configure topics with email subscriptions for P1 (immediate) and P2 (batched) alerts 6. Set up cross-account IAM roles for centralized monitoring from spoke accounts 7. Create CloudWatch dashboards for real-time transaction monitoring 8. Implement CloudWatch Logs Insights saved queries for common troubleshooting scenarios 9. Configure metric filters to track failed transactions and latency spikes 10. Store all sensitive configuration in Parameter Store with encryption OPTIONAL ENHANCEMENTS (If time permits):  Add service map integration (OPTIONAL: ) - provides distributed tracing visualization  Implement EventBridge rules for automated remediation (OPTIONAL: EventBridge) - enables self-healing infrastructure  Add Kinesis Data Firehose for long-term log archival to S3 (OPTIONAL: Kinesis Data Firehose) - reduces storage costs Expected output: A Pulumi TypeScript program that deploys a production-ready observability platform with centralized monitoring, alerting, and visualization capabilities across multiple AWS accounts and regions.","A fintech startup needs centralized observability for their microservices architecture. The platform must aggregate logs, metrics, and traces from multiple EKS clusters while maintaining compliance requirements for audit trails and alerting on anomalous transaction patterns.","""Multi-account observability infrastructure deployed across us-east-1 (primary) and us-west-2 (disaster recovery). Requires EKS clusters running in both regions with Prometheus exporters installed. Uses Amazon Managed Grafana for visualization, Amazon Managed Service for Prometheus for metrics collection, and CloudWatch Logs for centralized logging. VPCs in each region with private subnets and VPC peering for cross-region communication. Requires Pulumi 3.x with TypeScript, AWS CLI configured with cross-account assume role permissions. Infrastructure includes Lambda functions for log processing and for alerting.""","[""Use AWS Managed Grafana for visualization with at least 3 custom dashboards"", ""Configure Amazon Managed Service for Prometheus with remote write from EKS clusters"", ""Implement CloudWatch Logs Insights queries for transaction pattern analysis"", ""Set up topics with email subscriptions for critical alerts only"", ""Use Parameter Store for storing Grafana API keys"", ""Configure cross-account observability for at least 2 AWS accounts"", ""Implement metric retention policies with 30-day standard and 90-day archive tiers"", ""Use AWS Lambda for log transformation before ingestion into CloudWatch"", ""Deploy all resources with explicit deletion protection disabled for testing""]"
u3d4d6,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to orchestrate a phased migration of on-premises PostgreSQL databases to AWS RDS. The configuration must: 1. Define three migration phases (dev, staging, prod) as separate Pulumi stacks with shared networking. 2. Create for DMS, Secrets Manager, and to avoid internet routing. 3. Deploy RDS PostgreSQL instances with encryption at rest using KMS customer-managed keys. 4. Configure DMS replication instances with appropriate subnet groups and security rules. 5. Implement automatic secret rotation for database passwords with Lambda functions. 6. Set up CloudWatch alarms for replication lag exceeding 60 seconds. 7. Create IAM roles with cross-account assume permissions for each migration phase. 8. Export stack outputs for Direct Connect virtual interface IDs and attachment IDs. 9. Implement resource provisioning callbacks to validate network connectivity before proceeding. 10. Configure backup retention to 35 days with point-in-time recovery enabled. 11. Tag all resources with CostCenter, MigrationPhase, and ComplianceScope tags. Expected output: A Pulumi program that manages the complete migration infrastructure with proper phase isolation, automated credential management, and monitoring. The solution should support rolling back individual phases without affecting others.",A financial services company is migrating their payment processing infrastructure from an on-premises data center to AWS. The legacy system runs on VMs with PostgreSQL databases and requires strict network isolation. The migration must happen in phases to minimize downtime and maintain PCI compliance throughout the process.,"""Multi-phase migration environment spanning on-premises datacenter and AWS us-east-2 region. Requires Pulumi 3.x with TypeScript, AWS CLI v2 configured, Node.js 18+. Infrastructure includes AWS Direct Connect for hybrid connectivity, for network routing, VPC with isolated subnets per migration phase. Uses RDS PostgreSQL 14 with Multi-AZ, DMS replication instances, Secrets Manager for credential rotation. Each phase deploys in separate AWS accounts with cross-account IAM roles. Monitoring via CloudWatch and for migration progress tracking.""","[""All database migrations must use AWS DMS with CDC enabled for zero-downtime cutover"", ""Network traffic between migrated and legacy components must traverse AWS Direct Connect only"", ""Each migration phase must maintain separate state files with explicit dependencies"", ""Database credentials must rotate automatically every 30 days using Secrets Manager"", ""All resources must be tagged with migration phase, cost center, and compliance scope""]"
a1b2r1,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready cloud environment for a financial analytics platform. MANDATORY REQUIREMENTS (Must complete): 1. Create a VPC with 3 availability zones, each with public and private subnets (CORE: VPC) 2. Deploy an ECS cluster with Fargate Spot capacity providers for cost optimization 3. Set up Aurora PostgreSQL Serverless v2 cluster with automated backups (CORE: ) 4. Configure customer-managed KMS keys for encrypting backups and CloudWatch logs 5. Create buckets for raw data ingestion and processed analytics with versioning 6. Implement for , ECS, ECR, and CloudWatch to avoid costs 7. Configure security groups allowing only necessary inter-service communication 8. Set up CloudWatch log groups for all services with KMS encryption 9. Create IAM roles with least-privilege policies for ECS tasks and functions 10. Export all critical resource ARNs and endpoints as stack outputs OPTIONAL ENHANCEMENTS (If time permits):  Add Data Streams for real-time data ingestion (OPTIONAL: ) - enables streaming analytics  Implement AWS Secrets Manager for database credentials rotation (OPTIONAL: Secrets Manager) - improves security posture  Set up AWS Backup for centralized backup management (OPTIONAL: AWS Backup) - simplifies compliance reporting Expected output: A complete Pulumi TypeScript program that provisions a production-ready AWS environment with proper network isolation, encryption at rest, and cost-optimized compute resources suitable for financial data processing.","A financial services startup needs to establish their first production-ready AWS environment for a real-time trading analytics platform. The infrastructure must support high-frequency data ingestion, processing, and visualization while maintaining strict security compliance standards required for financial data handling.","""Production AWS environment in us-east-2 region for financial analytics platform. Core services include ECS Fargate for containerized microservices, Aurora PostgreSQL Serverless v2 for transactional data, for data lake storage, and Data Streams for real-time ingestion. VPC spans 3 availability zones with public and private subnets. Private subnets host all compute resources with for AWS services. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate IAM permissions for VPC, ECS, KMS, and CloudWatch services.""","[""All compute resources must run in private subnets with no direct internet access"", ""Database backups must be encrypted with customer-managed KMS keys and retained for exactly 35 days"", ""ECS tasks must use Fargate Spot instances with a minimum of 2 vCPU and 4GB memory"", ""All inter-service communication must use endpoints instead of NAT gateways"", ""CloudWatch log groups must have a retention period of exactly 30 days with KMS encryption"", "" buckets must have versioning enabled and lifecycle policies to transition objects older than 90 days to Glacier""]"
q6i5n4,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy and maintain consistent infrastructure across three AWS environments (dev, staging, prod) with automated drift detection and remediation. MANDATORY REQUIREMENTS (Must complete): 1. Create base infrastructure stack with VPC, subnets, and security groups that can be referenced by other stacks (CORE: VPC) 2. Deploy ECS Fargate clusters with auto-scaling based on CPU/memory metrics in each environment (CORE: ECS) 3. Configure RDS Aurora PostgreSQL clusters with automated backups and point-in-time recovery (CORE: RDS) 4. Implement TypeScript classes for reusable infrastructure components with strong typing 5. Create Pulumi stack outputs that can be imported by dependent stacks 6. Set up CloudWatch dashboards showing key metrics across all environments 7. Implement IAM roles with least-privilege access for each service 8. Configure automated drift detection that runs every 4 hours and reports discrepancies OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Config rules to monitor compliance across environments (OPTIONAL: Config) - ensures policy adherence  Implement Parameter Store for shared configuration (OPTIONAL: ) - centralizes config management  Add tracing across all services (OPTIONAL: ) - provides distributed tracing Expected output: A Pulumi TypeScript project with multiple stacks that maintains infrastructure consistency across environments, includes automated drift detection, and provides rollback capabilities for failed deployments.","A fintech startup needs to ensure their payment processing infrastructure remains identical across development, staging, and production environments. They've experienced configuration drift causing production incidents when features work in staging but fail in production due to environment differences.","""Multi-environment AWS deployment across three regions: us-east-1 (production), us-west-2 (staging), and eu-west-1 (development). Each environment consists of ECS Fargate clusters running containerized payment services, RDS Aurora PostgreSQL Multi-AZ for transaction data, and DynamoDB for session management. VPCs in each region with public/private subnet architecture across 3 AZs. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate IAM permissions. Cross-region VPC peering for internal service communication.""","[""Use Pulumi stack references to share outputs between environments"", ""Implement automated drift detection using Pulumi's state comparison"", ""Environment-specific configurations must use Pulumi config files"", ""All resources must be tagged with environment and cost-center"", ""Use TypeScript interfaces to enforce resource configuration schemas"", ""Implement rollback capability for failed deployments"", ""Secrets must be encrypted using Pulumi's secret management"", ""Resource naming must follow pattern: {env}-{region}-{service}-{resource}""]"
q2k1k7,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,Create a Pulumi TypeScript program to deploy a containerized payment processing web application. The configuration must: 1. Deploy ECS Fargate service with blue/green deployment strategy (CORE: ECS). 2. Configure Aurora Serverless v2 PostgreSQL cluster with encrypted storage (CORE: Aurora). 3. Set up Application Load Balancer with target group health checks every 15 seconds. 4. Implement IAM task roles with specific permissions for Secrets Manager access. 5. Configure VPC with 3 availability zones and private subnets for database tier. 6. Enable VPC flow logs and CloudWatch Container Insights for ECS cluster. 7. Create deployment pipeline using CodeDeploy for blue/green ECS deployments. Expected output: A Pulumi program that provisions the complete web application infrastructure with automated deployment capabilities and monitoring.,A fintech startup needs to deploy their payment processing web application with strict compliance requirements for data isolation and audit trails. The application requires zero-downtime deployments and must handle variable loads during market hours.,"""Production-grade infrastructure in us-east-2 region using ECS Fargate for container orchestration, Aurora Serverless v2 PostgreSQL for database tier, and Application Load Balancer for traffic distribution. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate credentials. Multi-AZ VPC setup with public subnets for ALB and private subnets for ECS tasks and Aurora. NAT gateways in each AZ for outbound internet access from private subnets. Deployment automation through AWS CodeDeploy integration.""","[""All database connections must use SSL/TLS encryption with certificate validation"", ""ECS tasks must use Parameter Store for runtime configuration"", ""Implement custom CloudWatch dashboard with ECS service metrics and Aurora performance insights"", ""Configure auto-scaling for ECS service based on ALB request count metric"", ""Use Pulumi stack references to separate network infrastructure from application deployment"", ""Enable tracing for distributed request tracking across services"", ""Implement least-privilege security groups with explicit ingress/egress rules""]"
b2k1z9,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a serverless cryptocurrency price alert system. MANDATORY REQUIREMENTS (Must complete): 1. Create an API Gateway REST API with /alerts endpoint accepting POST requests for user subscriptions (CORE: API Gateway) 2. Deploy a Lambda function (ARM architecture, 1GB memory) to process incoming alert subscriptions 3. Configure DynamoDB table 'price-alerts' with partition key 'userId' and sort key 'cryptoPair' (CORE: DynamoDB) 4. Set up SNS topic 'crypto-price-alerts' for broadcasting price updates (CORE: SNS) 5. Implement SQS queue 'alert-processor' with dead letter queue for failed messages 6. Create Lambda function to read from SQS and send personalized alerts via SNS 7. Configure IAM roles with least privilege - no Admin or wildcard permissions 8. Enable tracing on all Lambda functions with active tracing mode 9. Set up CloudWatch Log Groups with 30-day retention for all Lambda functions 10. Configure API Gateway throttling at 100 requests per second with 1000 burst limit 11. Implement API keys with usage plans limiting to 10,000 requests daily 12. Export API endpoint URL, DynamoDB table name, and SNS topic ARN as stack outputs OPTIONAL ENHANCEMENTS (If time permits):  Add EventBridge rule to trigger price checks every 5 minutes (OPTIONAL: EventBridge) - enables scheduled processing  Implement for complex alert logic (OPTIONAL: ) - adds workflow orchestration  Add Cognito User Pool for API authentication (OPTIONAL: Cognito) - improves security Expected output: A complete Pulumi TypeScript program that deploys all infrastructure components with proper IAM permissions, monitoring, and rate limiting. The system should handle high-throughput price alert processing with automatic scaling and comprehensive error handling.",A fintech startup needs to process cryptocurrency price alerts in real-time for their mobile app users. The system must handle sudden traffic spikes during market volatility while maintaining sub-second response times. Cost optimization is critical during low-activity periods.,"""Serverless infrastructure deployed in us-east-1 using Lambda functions with Graviton2 ARM processors for compute, DynamoDB for state management, and API Gateway for HTTP endpoints. SNS topics handle alert distribution with SQS queues for reliable message processing. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured. No VPC required as all services are managed. CloudWatch Logs configured with 30-day retention. enabled for distributed tracing across all Lambda functions.""","[""Lambda functions must use ARM-based Graviton2 processors for cost efficiency"", ""DynamoDB tables must use on-demand billing with point-in-time recovery enabled"", ""All Lambda functions must have tracing active for debugging production issues"", ""Dead letter queues must retain failed messages for exactly 14 days"", ""API Gateway must implement usage plans with 10,000 requests per day limit per API key""]"
g7r8j5,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a multi-stage CI/CD pipeline for containerized microservices. The configuration must: 1. Set up with source, build, test, and deploy stages connected to a GitHub repository. 2. Configure projects for Docker image builds with ARM_CONTAINER compute type and buildspec.yml support. 3. Implement for blue/green deployments to existing ECS services with traffic shifting. 4. Create S3 bucket for pipeline artifacts with versioning, server-side encryption using KMS, and 90-day lifecycle policy. 5. Generate customer-managed KMS key with key rotation enabled for artifact encryption. 6. Configure rules to capture pipeline state changes and send to SNS topic. 7. Set up CloudWatch dashboard displaying pipeline execution metrics, build duration, and failure rates. 8. Implement webhook for pull request validation builds with separate IAM permissions. 9. Create entries for Docker registry credentials and deployment configuration. 10. Configure VPC endpoints for to access ECR, S3, and CloudWatch Logs privately. 11. Enable CloudWatch Logs for all projects with log group per project. 12. Tag all resources with Environment, Team, and CostCenter tags for resource management. Expected output: A fully functional CI/CD pipeline that automatically builds Docker images from GitHub commits, runs security scans, and deploys to ECS using blue/green strategy with automated rollback capabilities.",A growing SaaS company needs to modernize their CI/CD infrastructure to support multiple development teams working on microservices. They require a GitOps-enabled pipeline that can handle both containerized and serverless workloads while maintaining strict security controls and audit trails.,"""Multi-stage CI/CD infrastructure deployed in us-east-2 using for orchestration, for build/test execution, and for blue/green deployments to ECS Fargate clusters. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate permissions. VPC endpoints for to access ECR and S3 without internet gateway. Pipeline integrates with GitHub via CodeStar connections for source control. CloudWatch Logs aggregates build logs with 30-day retention. rules trigger notifications on pipeline state changes.""","[""All projects must use ARM-based compute for cost optimization"", ""Pipeline artifacts must be encrypted with customer-managed KMS keys"", ""Each pipeline stage must publish metrics to CloudWatch with custom dimensions"", "" must use S3 versioning with lifecycle policies for artifact retention"", ""All IAM roles must follow least-privilege with no inline policies allowed""]"
c7y9k6,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to deploy a multi-region disaster recovery infrastructure for payment processing. The configuration must: 1. Set up DynamoDB global tables in us-east-1 and us-west-2 with on-demand billing and PITR enabled. 2. Deploy identical Lambda functions in both regions for payment processing (Node.js 18, 1GB memory). 3. Configure S3 buckets in both regions with cross-region replication and RTC enabled. 4. Implement Route 53 hosted zone with health checks and automatic DNS failover between regions. 5. Create CloudWatch alarms in both regions monitoring DynamoDB throttles and Lambda errors. 6. Set up SNS topics in both regions for alarm notifications with cross-region subscriptions. 7. Configure to replicate API keys automatically to the secondary region. 8. Implement CloudWatch Logs with cross-region log group data sharing. 9. Create Route 53 Application Recovery Controller for coordinated failover. 10. Deploy API Gateway REST APIs in both regions with custom domain names. Expected output: A Pulumi program that provisions a fully automated disaster recovery infrastructure with sub-5-minute RTO, health-check based failover, and synchronized data across regions. The solution should handle regional outages transparently with automatic DNS updates and maintain payment processing availability.","A financial services company needs to implement a disaster recovery solution for their critical payment processing system. The system must maintain full functionality even if an entire AWS region becomes unavailable, with automatic failover capabilities and data consistency across regions.","""Multi-region AWS deployment spanning us-east-1 (primary) and us-west-2 (secondary) for disaster recovery. Core services include DynamoDB Global Tables for transaction data, Lambda functions for payment processing, S3 with cross-region replication for document storage, and Route 53 for DNS failover. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with credentials for both regions. VPCs in each region with private subnets for Lambda functions, for AWS services, and for cross-region connectivity. CloudWatch cross-region dashboards for unified monitoring.""","[""Primary region must be us-east-1 with failover to us-west-2"", ""RTO (Recovery Time Objective) must be under 5 minutes"", ""RPO (Recovery Point Objective) must be under 1 minute"", ""Use Route 53 health checks with 30-second intervals"", ""DynamoDB global tables must have point-in-time recovery enabled"", ""Lambda functions must be deployed in both regions with identical configurations"", ""All S3 buckets must use cross-region replication with RTC (Replication Time Control)"", ""CloudWatch alarms must trigger SNS notifications for failover events"", ""Use with automatic cross-region replication"", ""Implement CloudFormation StackSets for consistent IAM roles across regions""]"
g5l7r1,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a zero-trust security infrastructure for payment processing. The configuration must: 1. Create a VPC with private subnets across 3 AZs and for , Secrets Manager, and KMS. 2. Deploy Lambda functions with VPC configuration and security groups that only allow HTTPS traffic from specific CIDR blocks. 3. Set up Aurora PostgreSQL with encryption using customer-managed KMS keys and automatic rotation every 90 days. 4. Configure AWS Secrets Manager to store database credentials with automatic rotation Lambda. 5. Implement IAM roles and policies that enforce MFA for all administrative actions and use session policies. 6. Create buckets with server-side encryption, versioning, MFA delete protection, and bucket policies that deny unencrypted uploads. 7. Set up CloudTrail with log file integrity validation and store logs in an encrypted bucket with object lock. 8. Configure AWS Config rules to monitor compliance with CIS AWS Foundations Benchmark. 9. Implement AWS GuardDuty for threat detection with findings exported to . 10. Create security groups that follow hub-spoke model with explicit ingress/egress rules and no overlapping CIDR ranges. Expected output: A Pulumi program that creates a complete zero-trust infrastructure with all security controls properly configured, outputs for connection strings that exclude sensitive data, and stack exports for security audit endpoints.","A financial services company is implementing zero-trust network architecture for their payment processing workloads. They need infrastructure that enforces strict identity verification, encrypted communications, and principle of least privilege at every layer. The architecture must support audit requirements for PCI DSS compliance.","""Zero-trust architecture deployment in us-east-1 region using AWS Lambda for compute, Aurora PostgreSQL with encryption for data storage, and AWS Secrets Manager for credentials. VPC with 3 availability zones, private subnets only, and for AWS services. Requires Pulumi 3.x with TypeScript, AWS CLI configured with MFA, Node.js 18+. Network architecture includes for hub-spoke topology, for service communication, and AWS WAF for API protection. All workloads run in isolated security contexts with session-based access controls.""","[""All traffic between services must use TLS 1.3 with mutual authentication"", ""IAM roles must follow least privilege with no wildcard permissions"", ""All data at rest must use KMS customer-managed keys with automatic rotation"", ""Network isolation must use private subnets with no direct internet access"", ""Secrets must be stored in AWS Secrets Manager with version tracking enabled"", ""All API calls must be logged to CloudTrail with integrity validation"", ""Security groups must use explicit allow rules with no 0.0.0.0/0 ingress"", "" buckets must have versioning, MFA delete, and block public access enabled"", ""Lambda functions must run in VPC with security group restrictions"", ""All resources must have mandatory tags: Environment, Owner, DataClassification, CostCenter""]"
w3m7v4,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production-grade EKS cluster with mixed architecture support.

MANDATORY REQUIREMENTS (Must complete):
1. Deploy EKS cluster version 1.28 with OIDC provider enabled (CORE: EKS)
2. Create custom VPC with 3 availability zones, private subnets for nodes (CORE: VPC)
3. Configure two managed node groups: one ARM-based (t4g.medium) and one x86-based (t3.medium)
4. Apply node taints to ARM nodes with key 'workload-type' and value 'arm-optimized'
5. Enable cluster autoscaler with IRSA (IAM Roles for Service Accounts)
6. Install AWS Load Balancer Controller using EKS add-on
7. Configure kubectl access through IAM roles, not static credentials
8. Set node groups to use launch templates with IMDSv2 required
9. Enable control plane logging for api, audit, and authenticator
10. Tag all resources with Environment=production and ManagedBy=pulumi

OPTIONAL ENHANCEMENTS (If time permits):
 Add Fargate profile for serverless workloads (OPTIONAL: Fargate) - reduces node management overhead
 Implement GuardDuty for EKS runtime monitoring (OPTIONAL: GuardDuty) - enhances security visibility
 Configure AWS Backup for EKS persistent volumes (OPTIONAL: Backup) - improves data protection

Expected output: A Pulumi TypeScript program that provisions a fully functional EKS cluster with mixed architecture node groups, proper IAM configuration, and production-ready security settings. The cluster should be immediately accessible via kubectl with appropriate RBAC configured.",A fintech startup needs to deploy their microservices architecture on AWS EKS with strict security requirements and automated node management. The platform must support both ARM and x86 workloads with separate node groups for cost optimization. Production workloads require dedicated nodes with taints while development workloads can share nodes.,"""Production EKS deployment in us-west-2 region using Pulumi with TypeScript. Infrastructure includes EKS 1.28 cluster, custom VPC across 3 AZs with private subnets, managed node groups for both ARM and x86 architectures. Requires Pulumi CLI 3.x, Node.js 18+, AWS CLI configured with appropriate permissions. VPC uses 10.0.0.0/16 CIDR with /24 subnets. Control plane endpoint is private with public access disabled. Node groups span all 3 AZs with 2-6 nodes autoscaling configuration. AWS Load Balancer Controller deployed for ingress management.""","[""Node groups must use Amazon Linux 2 EKS-optimized AMIs only"", ""All inter-node communication must use VPC CNI plugin with pod security groups enabled"", ""Cluster endpoint access must be restricted to specific CIDR ranges provided via Pulumi config"", ""Node instance profiles must follow least-privilege principle with no AdministratorAccess policies"", ""Launch templates must enforce encryption at rest using AWS-managed KMS keys"", ""Cluster DNS must use CoreDNS with at least 2 replicas for high availability"", ""All Pulumi resources must use explicit provider configuration, not ambient credentials""]"
m3i2x0,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement an active-passive disaster recovery architecture for a trading platform. MANDATORY REQUIREMENTS (Must complete): 1. Deploy Aurora Global Database cluster with one primary cluster in us-east-1 and secondary read replica cluster in us-west-2 (CORE: Aurora) 2. Configure DynamoDB global tables for session data with on-demand billing mode and point-in-time recovery (CORE: DynamoDB) 3. Implement Route 53 failover routing policy with health checks monitoring the primary region endpoints 4. Set up cross-region replication for S3 buckets storing application artifacts and logs 5. Deploy identical Lambda functions in both regions with region-specific environment variables 6. Configure CloudWatch alarms for database replication lag, health check failures, and failover events 7. Create keys in each region for encryption with automatic key rotation enabled 8. Implement IAM roles with cross-account assume permissions for DR operations OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Backup for centralized backup management (OPTIONAL: AWS Backup) - simplifies compliance reporting  Implement EventBridge rules for automated DR workflow triggers (OPTIONAL: EventBridge) - reduces manual intervention  Add for configuration management (OPTIONAL: ) - centralizes DR settings Expected output: A Pulumi TypeScript program with stack configurations for both regions that deploys a complete disaster recovery solution. The infrastructure should support automated failover with RPO < 1 minute and RTO < 5 minutes.",A financial services company requires a disaster recovery solution for their trading platform to meet regulatory requirements for 99.99% uptime. The system must maintain real-time data replication across regions and support automated failover with minimal data loss. Current manual DR processes take 4-6 hours to execute.,"""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary) regions. Utilizes Aurora Global Database for PostgreSQL 14.x with automated backups, DynamoDB global tables for session management, Lambda functions for business logic processing, and Route 53 for DNS failover. VPCs in both regions with private subnets across 3 availability zones, connected via VPC peering. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate IAM permissions for multi-region resource management. CloudWatch cross-region monitoring with centralized dashboards.""","["" instances must use Aurora Global Database with backtrack enabled"", ""Cross-region replication lag must not exceed 1 second under normal conditions"", ""Route 53 health checks must trigger failover within 30 seconds of primary region failure"", ""All data at rest must be encrypted using customer-managed keys"", ""Secondary region resources must use minimal capacity until failover occurs"", ""CloudWatch alarms must notify operations team within 60 seconds of any DR event"", ""Lambda functions must be deployed identically in both regions with environment-specific configurations"", ""DynamoDB global tables must maintain point-in-time recovery for 35 days""]"
y1p0l8,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a containerized microservices platform with advanced orchestration features. The configuration must: 1. Create a VPC with 3 public and 3 private subnets across different AZs. 2. Deploy an ECS cluster with Fargate capacity providers configured. 3. Set up App Mesh with virtual nodes for each microservice and virtual routers for traffic management. 4. Create task definitions for three services (payment, fraud-detection, notification) with 1GB memory and 0.5 vCPU each. 5. Configure ECS services with blue-green deployment using integration. 6. Deploy an ALB with target groups for each service version (blue/green). 7. Implement CloudWatch Container Insights and daemon as sidecar containers. 8. Create Aurora serverless v2 cluster with automatic scaling between 0.5 and 1 ACU. 9. Store database credentials and third-party API keys in Secrets Manager with automatic rotation. 10. Configure IAM task roles with least privilege access to required AWS services. 11. Set up CloudWatch alarms for CPU/memory thresholds triggering deployment rollbacks. 12. Output ALB DNS name and App Mesh service discovery endpoints. Expected output: A fully functional Pulumi program that creates an ECS-based microservices platform with service mesh, blue-green deployments, comprehensive monitoring, and automatic rollback capabilities. The infrastructure should handle production traffic with high availability and observability.","A fintech startup needs to deploy their microservices architecture on AWS ECS with blue-green deployment capabilities. The application consists of payment processing, fraud detection, and notification services that require strict network isolation and automated rollback mechanisms.","""Production-grade ECS cluster deployed in us-east-2 across 3 availability zones. Uses ECS Fargate for container orchestration, Application Load Balancer for traffic distribution, AWS App Mesh for service mesh capabilities, RDS Aurora PostgreSQL for database, and Secrets Manager for credential storage. Requires Pulumi CLI 3.x with TypeScript, AWS CLI configured with appropriate permissions, Node.js 18+, and Docker for local container testing. VPC with public and private subnets, NAT gateways for outbound traffic from private subnets.""","[""Use Fargate launch type exclusively with no EC2 instances"", ""Implement service mesh using AWS App Mesh for inter-service communication"", ""Configure automatic rollback if health checks fail during deployment"", ""Use AWS Secrets Manager for all database credentials and API keys"", ""Enable container insights and tracing for all services"", ""Deploy ALB with path-based routing to different service versions""]"
c7m9s2,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a complete observability stack for EKS-based microservices.

MANDATORY REQUIREMENTS (Must complete):
1. Deploy AWS Managed Service for Prometheus workspace with 90-day retention (CORE: AMP)
2. Configure AWS Managed Grafana workspace with SSO authentication (CORE: AMG)
3. Set up X-Ray daemon as DaemonSet on EKS cluster with service map enabled (CORE: X-Ray)
4. Create CloudWatch Log Groups for application logs with 30-day retention
5. Configure Container Insights for EKS cluster and node-level metrics
6. Implement 5 CloudWatch alarms for CPU, memory, error rates, latency, and availability
7. Create SNS topic with email subscriptions for critical alerts
8. Deploy CloudWatch Synthetics canary to monitor 3 critical API endpoints
9. Configure IAM roles with least privilege for all components

OPTIONAL ENHANCEMENTS (If time permits):
 Add CloudWatch Anomaly Detector for transaction volume predictions (OPTIONAL: Anomaly Detector) - enables proactive scaling
 Implement EventBridge rules for automated remediation workflows (OPTIONAL: EventBridge) - reduces MTTR
 Add Systems Manager Parameter Store for Grafana API keys (OPTIONAL: SSM) - improves security

Expected output: Complete Pulumi TypeScript code that deploys a production-ready observability platform with real-time monitoring, distributed tracing, and intelligent alerting for EKS workloads.","A financial services company needs to implement comprehensive observability for their microservices architecture running on EKS. The system processes millions of transactions daily and requires real-time monitoring, distributed tracing, and alerting capabilities to meet SLA requirements of 99.9% uptime.","""Multi-AZ EKS cluster deployment in us-east-1 with AWS X-Ray for distributed tracing, CloudWatch Container Insights for metrics, AWS Managed Prometheus and Grafana for visualization. Requires Pulumi CLI 3.x, TypeScript, Node.js 18+, kubectl configured. VPC with private subnets across 3 availability zones, NAT Gateways for outbound traffic. EKS cluster version 1.28 with managed node groups. AWS SSO authentication required for Grafana access.""","[""Use AWS X-Ray for distributed tracing with custom segments for critical business operations"", ""Implement CloudWatch Container Insights for EKS cluster monitoring"", ""Configure Prometheus metrics collection using AWS Managed Service for Prometheus"", ""Set up AWS Managed Grafana with pre-configured dashboards for service metrics"", ""Create CloudWatch Synthetics canaries for endpoint availability monitoring"", ""Implement CloudWatch Logs Insights queries for automated log analysis"", ""Configure SNS topics with email and Slack webhook integrations for alerting"", ""Use CloudWatch Anomaly Detector for predictive alerting on transaction volumes"", ""Implement cost allocation tags for all observability resources""]"
c8w6c7,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to orchestrate a phased migration from on-premises infrastructure to AWS cloud. The configuration must: 1. Create a VPC with CIDR 10.1.0.0/16 and configure VPC peering with on-premises network at 10.0.0.0/16. 2. Set up AWS Site-to-Site VPN with customer gateway and virtual private gateway using BGP for dynamic routing. 3. Deploy Aurora PostgreSQL cluster (version 15.x) with automated backups and 7-day retention. 4. Configure AWS DMS replication instance and tasks to migrate data from on-premises PostgreSQL to Aurora. 5. Create ECS cluster with launch type for containerized application workloads. 6. Implement AWS Application Load Balancer with target group supporting blue-green deployments. 7. Deploy Lambda functions for migration validation tasks using ARM64 architecture. 8. Configure AWS Config with rules for security group compliance and encryption validation. 9. Set up CloudWatch dashboards to monitor migration progress and system health. 10. Create entries for migration phase tracking. 11. Implement IAM roles with cross-account assume role capabilities for hybrid operations. 12. Enable VPC Flow Logs and store in bucket with lifecycle policies. Expected output: A complete Pulumi TypeScript program that creates a hybrid cloud environment supporting phased migration from on-premises to AWS, with automated database replication, blue-green deployment capabilities, and comprehensive monitoring of the migration process.","A fintech startup is migrating their monolithic application from on-premises infrastructure to AWS. The application consists of a Node.js API server, PostgreSQL database, and Redis cache. They need to migrate incrementally, starting with their development environment while maintaining connectivity to their on-premises systems during the transition period.","""Migration environment deployed in us-east-1 region transitioning from on-premises datacenter. Requires Pulumi CLI 3.x with TypeScript, Node.js 16+, and AWS CLI configured. Architecture includes VPC with CIDR 10.1.0.0/16, Site-to-Site VPN connection to on-premises network, Aurora PostgreSQL cluster, ECS for containerized workloads, and AWS DMS for database replication. Multi-AZ deployment across 3 availability zones with public, private application, and private data subnets. AWS Config enabled for compliance tracking during migration phases.""","[""Use VPC peering to maintain connectivity with on-premises network at 10.0.0.0/16"", ""Database must use Aurora PostgreSQL with point-in-time recovery enabled"", ""All resources must be tagged with Environment, Project, and MigrationPhase tags"", ""Use for all database credentials and API keys"", ""Implement blue-green deployment capability for zero-downtime migrations"", ""Configure VPN connection using AWS Site-to-Site VPN with BGP routing"", ""Use separate subnets for each tier (public, private app, private data)"", ""Enable AWS Config rules to track configuration compliance during migration"", ""Implement automated database migration using AWS DMS from on-premises PostgreSQL"", ""All Lambda functions must use ARM-based Graviton2 processors for cost optimization""]"
n2o9d6,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy isolated AWS environments for a payment processing platform.

MANDATORY REQUIREMENTS (Must complete):
1. Create three separate VPCs (dev: 10.0.0.0/16, staging: 10.1.0.0/16, prod: 10.2.0.0/16) with public and private subnets (CORE: VPC)
2. Deploy RDS PostgreSQL instances in private subnets with environment-specific sizing (CORE: RDS)
3. Configure Lambda functions with IAM roles for payment processing, using environment-specific memory settings
4. Create S3 buckets for transaction logs following the specified naming convention
5. Store all database passwords in Secrets Manager with unique values per environment
6. Implement proper security groups allowing Lambda to access RDS only within the same VPC
7. Configure CloudWatch Logs for all Lambda functions with 30-day retention
8. Use Pulumi stack configuration to parameterize environment-specific values
9. Export VPC IDs, RDS endpoints, Lambda ARNs, and S3 bucket names as stack outputs
10. Ensure all resources are properly tagged according to constraints

OPTIONAL ENHANCEMENTS (If time permits):
 Add API Gateway REST endpoints for Lambda invocation (OPTIONAL: API Gateway) - enables external testing
 Implement DynamoDB tables for session management (OPTIONAL: DynamoDB) - adds stateful capabilities
 Configure SNS topics for payment notifications (OPTIONAL: SNS) - improves event distribution

Expected output: A complete Pulumi TypeScript program with separate configuration files for each environment that deploys fully isolated infrastructure. The program should use Pulumi's stack mechanism to manage the three environments independently while sharing the same codebase.","A financial technology startup needs to establish separate development, staging, and production environments for their payment processing platform. Each environment must be isolated with its own VPC, database instances, and compute resources while maintaining consistent configuration patterns across all three.","""Multi-environment AWS infrastructure deployed across us-east-1 region. Each environment (dev, staging, production) contains isolated VPCs with public and private subnets, RDS PostgreSQL instances in private subnets, Lambda functions for payment processing, and S3 buckets for transaction logs. Development uses 2 AZs with minimal resources, staging mirrors production architecture with smaller instances, and production spans 3 AZs with high-availability configuration. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions, Node.js 18+. Total infrastructure includes 3 VPCs, 3 RDS instances, 3 Lambda functions, and 3 S3 buckets with environment-specific configurations.""","[""Each environment must have its own isolated VPC with non-overlapping CIDR blocks"", ""RDS instances must use different instance classes based on environment (db.t3.micro for dev, db.t3.small for staging, db.r5.large for production)"", ""All database passwords must be stored in AWS Secrets Manager with automatic rotation disabled"", ""Lambda functions must have environment-specific memory allocations (512MB for dev, 1024MB for staging, 3008MB for production)"", ""S3 buckets must follow the naming pattern: company-{env}-{purpose}-{random-suffix}"", ""Production environment must span 3 availability zones while dev/staging use only 2"", ""All resources must be tagged with Environment, Owner, and CostCenter tags""]"
o1a6n8,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a payment processing system across three environments (dev, staging, production) with consistent infrastructure patterns. The configuration must: 1. Define a reusable PaymentInfrastructure ComponentResource that encapsulates Lambda, DynamoDB, S3, and SNS resources. 2. Create three separate Pulumi stacks (dev, staging, prod) that instantiate the component with environment-specific configurations. 3. Configure Lambda functions with environment-appropriate memory settings and timeout values (30s dev, 60s staging, 120s prod). 4. Set up DynamoDB tables with GlobalSecondaryIndex on 'transactionDate' attribute and environment-specific capacity modes. 5. Create S3 buckets with intelligent tiering and 90-day lifecycle rules for dev/staging, 365-day for production. 6. Implement SNS topics with email subscriptions using environment-specific endpoints from Pulumi Config. 7. Use StackReference to import VPC IDs and subnet IDs from a shared networking stack. 8. Create CloudWatch dashboards that aggregate metrics from Lambda, DynamoDB, and S3 across all environments. 9. Export critical resource ARNs and endpoints as stack outputs for cross-stack consumption. 10. Implement a deployment validation function that checks resource consistency across environments. Expected output: A Pulumi TypeScript project with index.ts containing the PaymentInfrastructure component, separate configuration files for each environment (Pulumi.dev.yaml, Pulumi.staging.yaml, Pulumi.prod.yaml), and deployment instructions that ensure infrastructure parity while respecting environment-specific requirements.","A fintech startup needs to synchronize their payment processing infrastructure across development, staging, and production environments. Each environment must maintain identical configuration while allowing for environment-specific variations in resource sizing and endpoints.","""Multi-environment AWS deployment across three separate accounts (dev: 123456789012, staging: 234567890123, prod: 345678901234) in us-east-1 region. Each environment has its own VPC with 10.0.0.0/16 (dev), 10.1.0.0/16 (staging), and 10.2.0.0/16 (prod) CIDR blocks. Requires Pulumi 3.x with TypeScript, AWS CLI configured with profiles for each environment, Node.js 18+. Infrastructure includes Lambda functions for payment processing, DynamoDB for transaction storage, S3 for audit logs, and SNS for notifications. Each environment must be deployable independently while maintaining configuration parity.""","[""Use Pulumi Stack References to share outputs between environments"", ""Implement environment-specific configuration using Pulumi Config"", ""All S3 buckets must have versioning enabled and lifecycle policies"", ""Lambda functions must use environment-specific memory allocations (dev: 256MB, staging: 512MB, prod: 1024MB)"", ""DynamoDB tables must use on-demand billing in dev/staging but provisioned capacity in production"", ""All resources must be tagged with Environment, Team, and CostCenter tags"", ""Use Pulumi ComponentResource pattern to create reusable infrastructure modules"", ""Implement cross-stack dependencies for shared networking resources"", ""All IAM roles must follow least-privilege principle with no wildcard actions""]"
q9t8a2,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a content management web application with global CDN distribution. MANDATORY REQUIREMENTS (Must complete): 1. Deploy ECS Fargate service running containerized Node.js application on ARM architecture (CORE: ECS) 2. Create Aurora Serverless v2 PostgreSQL cluster with automated backups (CORE: Aurora) 3. Configure Application Load Balancer with health checks on /health endpoint 4. Set up S3 bucket for static assets with versioning enabled 5. Configure CloudFront distribution pointing to S3 origin with OAI 6. Implement auto-scaling policy for ECS service based on CPU metrics 7. Create CloudWatch Log Groups with 30-day retention for application logs 8. Configure security groups allowing only HTTPS traffic from ALB to ECS 9. Enable encryption at rest for Aurora using customer-managed KMS key 10. Apply required tags to all resources using Pulumi's transformation API OPTIONAL ENHANCEMENTS (If time permits):  Add rules to CloudFront distribution (OPTIONAL: ) - improves security posture  Implement Route 53 health checks with failover (OPTIONAL: Route 53) - adds DNS-level resilience  Configure for database credentials (OPTIONAL: ) - enhances credential management Expected output: A complete Pulumi TypeScript program that provisions all infrastructure components with proper networking, security configurations, and monitoring. The deployed application should be accessible via CloudFront URL with database connectivity verified.",A digital media company needs to deploy their content management platform that handles video processing and serves static assets globally. The platform requires auto-scaling capabilities for compute resources and efficient content delivery with low latency for users worldwide.,"""Production deployment in us-west-2 region using ECS Fargate for containerized Node.js application, Aurora Serverless v2 PostgreSQL for database tier, and S3 with CloudFront for static asset delivery. Infrastructure spans 3 availability zones with Application Load Balancer distributing traffic. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate credentials, Node.js 18+, and Docker for local testing. VPC configured with public subnets for ALB and private subnets for ECS tasks and Aurora cluster.""","[""All compute resources must use ARM-based instances (Graviton2) for cost optimization"", ""Database backups must be encrypted with customer-managed KMS keys"", ""Application logs must be searchable and retained for exactly 30 days"", ""Static assets must be served through CloudFront with origin access identity"", ""Auto-scaling must trigger at 70% CPU utilization with 2-minute cooldown"", ""Database connections must use SSL/TLS with certificate validation"", ""All resources must be tagged with Environment, Project, and CostCenter""]"
z7s1o7,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a serverless cryptocurrency price alert system. MANDATORY REQUIREMENTS (Must complete): 1. Create a Lambda function with ARM64 architecture to receive webhook events from exchanges (CORE: Lambda) 2. Set up DynamoDB table to store user alerts with partition key 'userId' and sort key 'alertId' (CORE: DynamoDB) 3. Configure EventBridge rule to trigger price check Lambda every 5 minutes 4. Implement Lambda function to evaluate alerts and send notifications via SNS 5. Enable tracing on all Lambda functions with custom subsegments 6. Configure point-in-time recovery on DynamoDB table with 7-day backup retention 7. Create SNS topic with SMS subscription support and AWS managed encryption 8. Set Lambda memory to 1024MB and timeout to 30 seconds for webhook processor 9. Use Pulumi stack outputs to export Lambda function ARNs and DynamoDB table name 10. Apply resource tags: Environment=production, Service=price-alerts OPTIONAL ENHANCEMENTS (If time permits):  Add API Gateway REST API for manual alert management (OPTIONAL: API Gateway) - enables user self-service  Implement SQS FIFO queue for guaranteed webhook processing order (OPTIONAL: SQS) - ensures data consistency  Add Step Functions for complex alert workflows (OPTIONAL: Step Functions) - enables advanced alert conditions Expected output: A Pulumi TypeScript program that deploys a production-ready serverless price alert system with all mandatory components properly configured, including IAM roles, environment variables, and monitoring capabilities.","A financial technology startup needs to process cryptocurrency price alerts in real-time. The system must handle webhook events from multiple exchanges, validate price thresholds, and notify users via SMS when their alert conditions are met. The architecture must be cost-effective during low-volume periods but scale instantly during market volatility.","""Serverless infrastructure deployed in us-east-1 using AWS Lambda for webhook processing and alert evaluation, DynamoDB for storing user alerts and price thresholds, SNS for SMS notifications, and EventBridge for scheduling periodic price checks. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured. All Lambda functions run on ARM64 architecture with tracing. DynamoDB tables use on-demand billing with point-in-time recovery. SNS topics encrypted with AWS managed keys for compliance.""","[""Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""DynamoDB tables must use point-in-time recovery with automated backups"", ""All Lambda functions must have tracing enabled with custom segments"", ""SNS topics must use server-side encryption with AWS managed keys"", ""Lambda environment variables containing API keys must use KMS encryption""]"
d3d1s7,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a complete CI/CD pipeline for containerized microservices. The configuration must: 1. Set up CodeCommit repository with webhook triggers for main and develop branches. 2. Create multi-stage with Source, Build, Test, and Deploy stages. 3. Configure CodeBuild project to build Docker images and push to ECR. 4. Implement automated testing stage using CodeBuild with test reports. 5. Add manual approval action before production deployment stage. 6. Deploy to ECS Fargate service with blue/green deployment configuration. 7. Create SNS topic and subscriptions for pipeline failure notifications. 8. Configure IAM roles with least privilege for each service. 9. Store build artifacts in S3 with lifecycle policies for 90-day retention. 10. Use CloudWatch Events to trigger pipeline on code commits. Expected output: Pulumi program that creates a fully automated CI/CD pipeline with proper separation of environments, security controls, and monitoring capabilities.","A software development team needs to automate their deployment pipeline for a microservices application. The pipeline must build Docker images, run automated tests, and deploy to ECS Fargate across multiple environments. Security scanning and approval workflows are critical requirements for production deployments.","""Multi-stage CI/CD infrastructure deployed in us-east-2 using , CodeBuild, and CodeDeploy for automated deployments to ECS Fargate. Requires Pulumi CLI 3.x with TypeScript, Node.js 16+, and AWS CLI configured. VPC with private subnets across 2 AZs for ECS tasks. CodeCommit repository with main, develop, and release branches. ECR repositories for container images. for database credentials and API keys. S3 bucket for build artifacts. SNS topic for pipeline notifications to development team.""","[""Use CodeCommit for source control with branch-based triggers"", ""CodeBuild projects must use compute type BUILD_GENERAL1_SMALL"", ""Store Docker images in ECR with tag immutability enabled"", ""Implement manual approval step for production deployments only"", ""Use for all sensitive configuration values"", ""Enable CodeBuild logs with 30-day retention period"", ""Deploy to ECS Fargate with rolling update deployment type"", ""Use separate IAM roles for each pipeline stage"", ""Configure SNS notifications for pipeline state changes"", ""Set all S3 buckets to use AES256 encryption""]"
w4o4z7,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement an active-passive disaster recovery architecture for a transaction processing system. The configuration must: 1. Deploy Aurora Global Database with a primary cluster in us-east-1 and secondary in us-west-2. 2. Configure Route 53 health checks monitoring the primary ALB endpoint every 30 seconds. 3. Set up failover routing policy switching to secondary region within 5 minutes of primary failure. 4. Create S3 buckets in both regions with cross-region replication and versioning enabled. 5. Deploy Lambda@Edge functions for intelligent request routing based on region health. 6. Implement EventBridge rules to replicate critical events between regions. 7. Configure CloudWatch alarms for database lag monitoring with SNS notifications. 8. Set up automated Aurora backtrack for point-in-time recovery capabilities. 9. Create IAM roles with cross-region assume permissions for failover automation. 10. Enable Aurora Performance Insights with 7-day retention in both regions. Expected output: A Pulumi TypeScript program that deploys a complete multi-region disaster recovery solution with automated failover capabilities, health monitoring, and data replication ensuring business continuity with minimal data loss.","A financial services company requires a disaster recovery solution for their critical transaction processing system. After a recent regional outage affected their primary site, they need to implement automated failover capabilities with RTO under 5 minutes and RPO under 1 minute.","""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary). Deployment includes Aurora Global Database with PostgreSQL 15.4 engine, Application Load Balancers in both regions, Lambda@Edge for intelligent routing, S3 buckets with cross-region replication, and EventBridge for event synchronization. VPCs in both regions with 3 private subnets each, VPC peering connection between regions. Requires Pulumi CLI 3.x, TypeScript 5.x, Node.js 18+, and AWS credentials with multi-region permissions. Infrastructure monitors transaction processing workloads with sub-second latency requirements.""","[""Use Route 53 health checks with failover routing policy and alarm threshold of 2 failed checks"", ""Configure Aurora Global Database with automated backtrack enabled for 72 hours"", ""Implement Lambda@Edge functions for request routing with 128MB memory allocation"", ""Set up cross-region replication for all S3 buckets with RTC (Replication Time Control) enabled"", ""Use EventBridge cross-region event replication with DLQ retry policy of 3 attempts""]"
e5e0v8,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a secure data processing pipeline with end-to-end encryption. MANDATORY REQUIREMENTS (Must complete): 1. Create a KMS key hierarchy with separate keys for , CloudWatch Logs, and Secrets Manager (CORE: KMS) 2. Deploy Lambda functions in private subnets to process encrypted data from (CORE: Lambda) 3. Configure buckets with SSE-KMS encryption, versioning, and access logging 4. Implement VPC with private subnets across 3 AZs and for and KMS 5. Create Secrets Manager secrets for API keys with automatic 30-day rotation 6. Configure CloudWatch Log groups with KMS encryption and 90-day retention 7. Implement IAM roles with explicit permissions for each service component 8. Set up bucket policies that enforce encryption in transit (aws:SecureTransport) 9. Configure Lambda environment variables to reference Secrets Manager ARNs 10. Enable CloudTrail logging for all KMS key usage to a separate encrypted bucket OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Config rules for compliance monitoring (OPTIONAL: Config) - automates compliance checks  Implement GuardDuty for threat detection (OPTIONAL: GuardDuty) - adds security monitoring  Add for non-sensitive config (OPTIONAL: ) - centralizes configuration Expected output: Complete Pulumi TypeScript program that deploys a fully encrypted data processing pipeline meeting all PCI-DSS security requirements with comprehensive IAM policies and network isolation.","A financial services company needs to implement a secure data processing pipeline that meets PCI-DSS compliance requirements. The architecture must enforce encryption at rest and in transit, implement strict access controls, and maintain audit trails for all data access operations.","""Secure multi-AZ deployment in us-east-1 region for PCI-DSS compliant data processing. Infrastructure includes Lambda functions in private subnets, buckets with KMS encryption, Secrets Manager for credential storage, and VPC with private endpoints for AWS services. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions. Network architecture spans 3 availability zones with no direct internet access for compute resources. All data encrypted at rest and in transit using customer-managed KMS keys.""","[""All buckets must use SSE-KMS encryption with customer-managed keys"", ""Lambda functions must run in private subnets with no direct internet access"", "" must be used for all AWS service communications"", ""IAM roles must follow least-privilege principle with no wildcard permissions"", ""All security group rules must explicitly define source and destination CIDR blocks"", ""CloudWatch Logs must use KMS encryption with separate keys per log group"", ""Secrets Manager must be used for all sensitive configuration with automatic rotation enabled""]"
a0q8m0,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to optimize and fix an existing fraud detection system infrastructure. The configuration must: 1. Refactor monolithic index.ts file into modular components (networking, compute, storage) using Pulumi ComponentResource pattern. 2. Fix resource naming conflicts by implementing consistent naming convention with stack-aware prefixes. 3. Optimize Lambda functions by adjusting memory allocation based on CloudWatch metrics (reduce from 3GB to appropriate levels). 4. Convert fixed-capacity DynamoDB tables to on-demand billing mode for cost optimization. 5. Add missing error handling for AWS API throttling issues during deployments. 6. Implement S3 lifecycle policies to move logs older than 90 days to Glacier storage class. 7. Create reusable Pulumi components for Lambda functions with built-in retry logic. 8. Add comprehensive resource tagging including CostCenter, Environment, Owner, and Project tags. 9. Replace all hardcoded ARNs and resource names with Pulumi config values. 10. Ensure proper IAM role boundaries and remove overly permissive policies. 11. Add stack outputs for critical resource ARNs and endpoints. Expected output: A well-structured Pulumi TypeScript project with separate component files, optimized resource configurations, proper error handling, and clear documentation of the refactoring changes made.","A financial services company has an existing Pulumi TypeScript codebase that deploys their fraud detection system. The current implementation suffers from deployment failures, resource naming conflicts, and excessive AWS costs due to inefficient resource configurations.","""Production environment in us-east-1 with existing fraud detection infrastructure consisting of Lambda functions processing transaction events, DynamoDB tables storing fraud patterns and transaction history, S3 buckets for audit logs and ML model artifacts. Current setup uses Pulumi 3.x with TypeScript, Node.js 18.x runtime. VPC with private subnets across 3 availability zones, NAT Gateway for outbound traffic. Deployment through GitHub Actions CI/CD pipeline with separate dev, staging, and prod stacks.""","[""Refactor existing Pulumi code to use proper component resources for better modularity"", ""Implement resource tagging strategy with cost center, environment, and owner tags"", ""Add retry logic with exponential backoff for Lambda function deployments"", ""Optimize DynamoDB capacity settings using on-demand billing where appropriate"", ""Ensure all S3 buckets have lifecycle policies to transition objects to Glacier after 90 days"", ""Replace hardcoded values with Pulumi config variables for all environment-specific settings"", ""Implement proper error handling and rollback mechanisms in the deployment stack""]"
o6j6f1,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to analyze and audit existing AWS infrastructure for compliance violations and security best practices. The configuration must: 1. Connect to an existing Pulumi stack using stack references to read deployed resource state. 2. Implement analyzers for EC2 instances to check for IMDSv2 enforcement and SSM agent installation. 3. Scan S3 buckets for public access blocks, encryption settings, and lifecycle policies. 4. Validate RDS instances for encryption at rest, automated backups, and multi-AZ deployments. 5. Check Lambda functions for dead letter queue configuration and reserved concurrency limits. 6. Analyze VPC security groups and NACLs for overly permissive rules. 7. Generate a detailed compliance report showing resource counts, violations by severity, and remediation steps. 8. Export findings to CloudWatch custom metrics for dashboard visualization. 9. Implement a dry-run mode that simulates analysis without accessing actual AWS APIs. Expected output: A Pulumi TypeScript program that performs comprehensive infrastructure analysis, generates multi-format compliance reports, and provides actionable insights for security improvements.","A financial services company needs to audit their existing AWS infrastructure deployed via Pulumi to ensure compliance with new security regulations. They require a TypeScript-based analysis tool that can inspect their current state, identify non-compliant resources, and generate actionable reports for remediation.","""Production AWS environment in us-east-1 region with existing Pulumi-deployed infrastructure including EC2 instances across multiple VPCs, RDS databases, S3 buckets with various encryption settings, Lambda functions, and API Gateway endpoints. Requires Pulumi 3.x with TypeScript, AWS CLI configured with read-only IAM role. Infrastructure spans 3 AWS accounts with cross-account access configured. Node.js 18+ required for TypeScript execution.""","[""Must use Pulumi's stack reference feature to analyze existing deployed infrastructure without modifying it"", ""Analysis must cover at least 15 different AWS resource types across compute, storage, and networking"", ""Generate compliance reports in both JSON and HTML formats with severity ratings"", ""Implement custom policy rules using Pulumi's Policy as Code framework"", ""Support incremental analysis to track compliance improvements over time"", ""Analysis execution must complete within 5 minutes for infrastructures with up to 500 resources"", ""Must detect and flag resources missing required tags (Environment, Owner, CostCenter)"", ""Identify security group rules that allow unrestricted inbound access (0.0.0.0/0)"", ""Create a scoring system that rates overall infrastructure compliance from 0-100""]"
x6n9n9,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production-ready Amazon EKS cluster for microservices workloads. MANDATORY REQUIREMENTS (Must complete): 1. Create an EKS cluster version 1.28+ with OIDC provider enabled (CORE: EKS) 2. Configure two managed node groups: one for general workloads (t4g.medium) and one for compute-intensive workloads (c7g.large) (CORE: EC2) 3. Deploy cluster with private endpoint access only and configure security groups to restrict API server access 4. Enable all five EKS control plane log types and stream to CloudWatch Logs with 30-day retention 5. Install and configure AWS Load Balancer Controller using IRSA for Kubernetes service integration 6. Create IAM roles and policies for cluster autoscaler with proper IRSA configuration 7. Configure EBS CSI driver with encryption enabled for persistent volume support 8. Output kubeconfig data and cluster endpoint for kubectl access 9. Implement proper resource tagging with Environment=production, Team=platform, CostCenter=engineering 10. Set up for ECR, S3, and EC2 to keep traffic private OPTIONAL ENHANCEMENTS (If time permits):  Add Fargate profile for serverless workloads (OPTIONAL: Fargate) - reduces operational overhead  Implement integration for secret rotation (OPTIONAL: ) - improves security posture  Configure Amazon for EKS threat detection (OPTIONAL: ) - enhances cluster security monitoring Expected output: A Pulumi TypeScript program that provisions a fully functional EKS cluster with managed node groups, proper networking isolation, IRSA configuration, and essential add-ons. The cluster should be ready to deploy microservices with both stateless and stateful workloads while maintaining security best practices.","A fintech company needs to modernize their monolithic payment processing system by migrating to microservices on Kubernetes. They require a production-grade EKS cluster with strict security requirements, automated node management, and integration with their existing monitoring infrastructure. The cluster must support both stateful and stateless workloads with different resource requirements.","""Production EKS infrastructure deployed in us-east-1 using Amazon EKS managed Kubernetes service with Graviton3-based node groups. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, AWS CLI v2 configured with appropriate permissions. VPC spans 3 availability zones with private subnets for worker nodes, public subnets for load balancers. NAT Gateways provide outbound internet access. Integration with for node management and AWS CloudWatch for comprehensive logging and monitoring. Cluster autoscaler dynamically adjusts node count based on workload demands.""","[""EKS cluster must use version 1.28 or higher with managed node groups"", ""Implement IRSA (IAM Roles for Service Accounts) for pod-level AWS permissions"", ""Node groups must use Graviton3 (ARM) instances for cost optimization"", ""Enable EKS add-ons: VPC CNI, CoreDNS, kube-proxy, and AWS EBS CSI Driver"", ""Configure OIDC provider for the cluster to enable IRSA functionality"", ""Implement cluster autoscaling with minimum 2 and maximum 10 nodes per group"", ""Use private subnets only for worker nodes with no direct internet access"", ""Enable EKS control plane logging for api, audit, authenticator, controllerManager, and scheduler"", ""Tag all resources with Environment, Team, and CostCenter for billing tracking"", ""Configure kubectl access through IAM roles, not static credentials""]"
s8p2v0,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement an active-passive multi-region disaster recovery solution for a PostgreSQL database system. MANDATORY REQUIREMENTS (Must complete): 1. Deploy RDS PostgreSQL instances in us-east-1 (primary) and us-west-2 (standby) with cross-region read replicas (CORE: RDS) 2. Configure Route53 health checks and failover routing between regions (CORE: Route53) 3. Create Lambda functions in both regions to monitor replication lag and trigger alerts when lag exceeds 60 seconds (CORE: Lambda) 4. Implement automated backups with point-in-time recovery enabled for 7 days 5. Configure VPC peering between regions for secure replication traffic 6. Set up CloudWatch alarms for database CPU, storage, and replication metrics 7. Create IAM roles with least-privilege access for Lambda functions 8. Enable encryption at rest using KMS keys in each region OPTIONAL ENHANCEMENTS (If time permits):  Add DynamoDB Global Tables for application state storage (OPTIONAL: DynamoDB) - provides consistent application state across regions  Implement for orchestrated failover workflow (OPTIONAL: ) - automates complex failover sequences  Configure AWS Backup for centralized backup management (OPTIONAL: AWS Backup) - simplifies backup compliance Expected output: A complete Pulumi TypeScript program that provisions primary and standby RDS instances with automated health monitoring and DNS-based failover capabilities, enabling recovery within 5 minutes of regional failure.",A financial services company requires a disaster recovery solution for their critical PostgreSQL database that processes payment transactions. The system must maintain data consistency across regions and enable rapid failover with minimal data loss to meet strict regulatory requirements.,"""Multi-region AWS deployment spanning us-east-1 (primary) and us-west-2 (DR) for PostgreSQL database disaster recovery. Requires Pulumi 3.x with TypeScript, AWS CLI configured with permissions for RDS, Route53, Lambda, VPC, CloudWatch, and KMS. Each region contains a VPC with private subnets across 3 AZs, VPC peering connection for cross-region replication. RDS PostgreSQL 14.x instances with db.r5.xlarge for production workloads, automated backups to S3.""","[""RDS instances must use PostgreSQL 14.x or higher with db.r5.xlarge instance class"", ""Cross-region read replica lag must not exceed 60 seconds under normal operations"", ""Route53 health checks must detect primary failure within 30 seconds"", ""All data must be encrypted using customer-managed KMS keys unique to each region"", ""Lambda functions must be written in TypeScript and bundled with the Pulumi program"", ""VPC peering connection must restrict traffic to PostgreSQL port 5432 only"", ""Automated backups must be retained for exactly 7 days with deletion protection disabled"", ""Total monthly infrastructure cost must not exceed $3000 across both regions""]"
f8j3i4,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a blue-green ECS Fargate cluster with automated deployment pipelines. The configuration must: 1. Create a VPC with 3 public and 3 private subnets across availability zones. 2. Deploy an Application Load Balancer with two target groups (blue and green). 3. Set up an ECS cluster with Fargate compute capacity. 4. Create an ECR repository with image scanning enabled and lifecycle policies. 5. Define ECS task definitions with 2 vCPU and 4GB memory for application containers. 6. Implement ECS services with auto-scaling (min: 2, max: 10 tasks) based on CPU utilization. 7. Configure AWS Cloud Map service discovery for service-to-service communication. 8. Set up CloudWatch Container Insights for monitoring and alerting. 9. Implement weighted target group routing for gradual traffic shifting. 10. Create CloudWatch alarms that trigger automatic rollback on high error rates. 11. Configure task execution roles with minimal required permissions. 12. Enable VPC Flow Logs and ECS task logging to CloudWatch. Expected output: A complete Pulumi program that provisions a production-ready ECS infrastructure supporting blue-green deployments with automatic rollback capabilities, service discovery, and comprehensive monitoring.",A fintech startup needs to deploy their microservices architecture on AWS ECS with blue-green deployment capabilities. The platform processes sensitive financial transactions and requires zero-downtime deployments with automatic rollback mechanisms.,"""Production-grade container orchestration infrastructure deployed in us-east-1 across 3 availability zones. Uses ECS Fargate for serverless container execution, Application Load Balancer for traffic distribution, ECR for container registry with vulnerability scanning enabled. VPC with public subnets for ALB and private subnets for ECS tasks. NAT Gateways provide outbound internet access. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 16+, and AWS CLI configured with appropriate IAM permissions for ECS, VPC, ALB, and ECR services.""","[""Use ECS Fargate launch type exclusively - no EC2 instances"", ""Implement service discovery using AWS Cloud Map for inter-service communication"", ""Configure automatic rollback if new deployment fails health checks within 5 minutes"", ""Use separate target groups for blue and green environments with weighted routing"", ""Implement container-level security scanning before deployment using ECR scan on push"", ""Deploy all resources in private subnets with no direct internet access for containers""]"
o9w5p9,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a comprehensive observability stack for ECS Fargate microservices. The configuration must: 1. Set up CloudWatch Container Insights on an existing ECS cluster named 'production-services' with enhanced monitoring enabled. 2. Create separate CloudWatch Log Groups for each of 5 services (auth-service, payment-service, notification-service, audit-service, analytics-service) with 30-day retention and encryption using AWS-managed keys. 3. Deploy a custom CloudWatch dashboard named 'microservices-health' with widgets for CPU utilization, memory usage, request count, error rate, and p99 latency across all services. 4. Configure CloudWatch alarms for each service monitoring CPU (>80%), memory (>85%), and error rate (>5%) with 2-minute evaluation periods. 5. Set up SNS topic 'critical-alerts' with email subscription to 'ops-team@company.com' and SMS to '+1234567890' for alarm notifications. 6. Create metric filters on each log group to extract custom metrics for 'failed_transactions', 'api_errors', and 'slow_queries' from structured logs. 7. Implement CloudWatch Logs Insights queries as saved searches for common troubleshooting scenarios (error spike detection, slow API analysis). 8. Enable CloudWatch Application Insights for the ECS cluster with automated anomaly detection on key performance indicators. 9. Configure cross-account observability sharing to allow account '123456789012' read-only access to all dashboards and metrics. 10. Apply consistent tagging strategy with Environment='production', Team='platform', and Service='<service-name>' on all resources. Expected output: A complete Pulumi program that provisions all observability components with proper configurations, enabling real-time monitoring, alerting, and troubleshooting capabilities for the microservices architecture while maintaining security and cost optimization best practices.","A financial services company needs to implement centralized observability for their microservices architecture running on ECS Fargate. The system must capture metrics, logs, and traces from multiple services while maintaining compliance with data retention policies. Real-time alerting is critical for detecting anomalies in transaction processing workflows.","""Production environment in us-east-1 with ECS Fargate cluster running 5 microservices across 3 availability zones. Services generate approximately 50GB of logs daily with peak traffic of 10,000 requests per minute. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions. VPC with private subnets for ECS tasks, NAT gateways for outbound connectivity. Central monitoring account (123456789012) requires cross-account access for aggregated dashboards. Services use structured JSON logging with correlation IDs for distributed tracing.""","[""Use CloudWatch Container Insights for ECS cluster monitoring with enhanced metrics enabled"", ""Implement custom CloudWatch dashboards with at least 5 widgets showing service-level metrics"", ""Configure CloudWatch Logs with separate log groups per service and 30-day retention"", ""Set up SNS topics with email and SMS subscriptions for critical alerts"", ""Create CloudWatch alarms for CPU > 80%, memory > 85%, and error rate > 5%"", ""Use CloudWatch Logs Insights queries to detect specific error patterns"", ""Enable CloudWatch Application Insights for automated anomaly detection"", ""Implement metric filters to extract custom metrics from application logs"", ""Configure cross-account observability sharing with a central monitoring account"", ""All resources must have cost allocation tags: Environment, Team, and Service""]"
m5x1p3,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to orchestrate a zero-downtime migration from on-premises infrastructure to AWS cloud. MANDATORY REQUIREMENTS (Must complete): 1. Deploy ECS Fargate service with blue and green task definitions, each with 2 desired tasks (CORE: ECS) 2. Create Aurora MySQL Serverless v2 cluster with automated backups and point-in-time recovery (CORE: Aurora) 3. Configure Application Load Balancer with weighted target groups (80% blue, 20% green initially) 4. Mount filesystem to containers at /shared/data with 1000:1000 ownership 5. Set up ECR repository with image scanning on push enabled 6. Configure ECS task IAM roles with permissions for , CloudWatch Logs, and ECR only 7. Implement CloudWatch alarms for UnHealthyHostCount > 0 for 2 consecutive periods 8. Create AWS DMS replication instance and task for MySQL to Aurora migration 9. Set up for ECR, S3, and CloudWatch Logs to reduce NAT costs 10. Output the ALB DNS name, Aurora endpoint, and DMS task ARN OPTIONAL ENHANCEMENTS (If time permits):  Add AWS WebACL to ALB for protection against common attacks (OPTIONAL: ) - improves security posture  Implement weighted routing between old and new infrastructure (OPTIONAL: ) - enables gradual DNS cutover  Create state machine to orchestrate migration phases (OPTIONAL: ) - adds migration automation Expected output: A complete Pulumi TypeScript program that creates all infrastructure components with proper dependencies, enables zero-downtime migration through weighted routing, and provides monitoring for the migration process. The program should use Pulumi's strong typing and include proper error handling for migration scenarios.",Your company is migrating a legacy monolithic application from on-premises to AWS. The application currently runs on physical servers with a MySQL database and file storage on NFS. The migration strategy involves containerizing the application while maintaining compatibility with existing deployment scripts and minimizing downtime during the cutover.,"""AWS environment deployed in us-east-1 across 3 availability zones for high availability. Infrastructure includes ECS Fargate for containerized workloads, Aurora MySQL Serverless v2 for database tier, Application Load Balancer with weighted routing, for shared file storage, ECR for container registry with image scanning enabled. VPC configuration with public subnets for ALB and private subnets for ECS tasks and Aurora. NAT Gateways in each AZ for outbound connectivity. Requires Pulumi CLI 3.x with TypeScript, AWS CLI configured with appropriate permissions, Node.js 18+ and npm installed.""","[""Use blue-green deployment pattern with weighted target groups for zero-downtime cutover"", ""Implement database migration with AWS DMS from on-premises MySQL to Aurora MySQL"", ""Container images must be scanned for vulnerabilities before deployment"", ""Preserve existing file hierarchy structure when migrating to "", ""Network latency between application and database must not exceed 5ms"", ""All resources must be tagged with Environment, MigrationPhase, and CostCenter"", ""Implement automated rollback if health checks fail for more than 2 minutes""]"
o4c8z1,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production cloud environment for fraud detection services. The configuration must: 1. Create a VPC with 10.0.0.0/16 CIDR containing 3 public and 3 private subnets across different AZs. 2. Deploy an ECS cluster with Fargate capacity providers for running containerized fraud detection services. 3. Set up RDS Aurora PostgreSQL Serverless v2 cluster with minimum 0.5 ACUs and maximum 1 ACU. 4. Configure table for session storage with point-in-time recovery enabled. 5. Implement Application Load Balancer in public subnets with target group for ECS services. 6. Create NAT Gateways in each public subnet for private subnet outbound connectivity. 7. Set up for S3, and ECR to reduce data transfer costs. 8. Configure CloudWatch Log Groups for all services with encryption. 9. Deploy topic for operational alerts with email subscription endpoint. 10. Create S3 bucket for VPC Flow Logs with lifecycle policy to transition to Glacier after 30 days. 11. Implement CloudWatch dashboard displaying ECS task count, RDS connections, and consumed capacity. 12. Export all resource ARNs and endpoints as stack outputs for integration with other systems. Expected output: A fully functional Pulumi TypeScript program that creates a complete AWS environment ready for deploying fraud detection services, with all networking, compute, storage, and monitoring components properly configured and interconnected.","A fintech startup needs to establish a production-grade cloud environment for their real-time fraud detection system. The infrastructure must support high-throughput data processing with strict latency requirements while maintaining PCI DSS compliance standards. The system will process approximately 50,000 transactions per minute during peak hours.","""Production-ready AWS environment in eu-central-1 region for fraud detection workloads. Core services include ECS Fargate for containerized microservices, RDS Aurora PostgreSQL Serverless v2 for transaction storage, and for session management. Infrastructure spans 3 availability zones with dedicated VPC containing public subnets for load balancers and private subnets for compute/data resources. Requires Pulumi CLI 3.x, Node.js 18+, TypeScript 5.x, and AWS CLI configured with appropriate credentials. Network architecture includes NAT Gateways for outbound connectivity and for AWS services.""","[""All resources must be deployed in eu-central-1 region with multi-AZ redundancy"", ""Use TypeScript with strict type checking enabled and no any types"", ""Implement resource tagging with Environment, Owner, CostCenter, and DataClassification tags"", ""Configure VPC with exactly 3 private subnets and 3 public subnets across different AZs"", ""Enable VPC Flow Logs with S3 storage and 30-day retention"", ""Deploy all compute resources in private subnets with no direct internet access"", ""Use customer-managed keys for all encryption with automatic rotation enabled"", ""Implement least-privilege IAM policies with no inline policies allowed"", ""Configure CloudWatch alarms for all critical metrics with notifications"", ""Ensure all resources support deletion protection flags set to false for testing""]"
c4e0z6,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy identical payment processing infrastructure across three environments (dev, staging, prod) with controlled variations. The configuration must: 1. Deploy ECS Fargate services running containerized payment processors in each environment (CORE: ECS). 2. Create RDS Aurora PostgreSQL clusters with environment-specific instance sizes: db.t3.medium (dev), db.r5.large (staging), db.r5.xlarge (prod) (CORE: RDS Aurora). 3. Implement Route53 private hosted zones for service discovery within each environment (CORE: Route53). 4. Use Pulumi stack configurations to manage environment-specific values without code duplication. 5. Create identical VPC structures with 3 availability zones per environment using CIDR blocks: 10.0.0.0/16 (dev), 10.1.0.0/16 (staging), 10.2.0.0/16 (prod). 6. Deploy Application Load Balancers with identical path-based routing rules across all environments. 7. Implement cross-stack references to share common resources like ECR repositories between environments. 8. Use Pulumi's component resources to create reusable infrastructure modules. 9. Configure identical auto-scaling policies but with environment-specific thresholds: 50% CPU (dev), 70% CPU (staging/prod). 10. Output a comparison report showing configuration differences between environments. Expected output: A Pulumi TypeScript project with stack configurations for dev/staging/prod that deploys identical infrastructure topologies while allowing controlled variations through configuration.","A fintech startup needs to ensure their payment processing infrastructure is identical across development, staging, and production environments. They've experienced configuration drift causing production incidents and need a solution that guarantees environment parity while allowing controlled variations like instance sizes.","""Multi-environment AWS deployment across us-east-1 (production), us-west-2 (staging), and eu-west-1 (development). Each environment requires ECS Fargate for container orchestration, RDS Aurora PostgreSQL for data persistence, and Route53 for DNS management. Infrastructure deployed in custom VPCs with public/private subnets across 3 AZs per region. Requires Pulumi CLI 3.x, Node.js 18+, TypeScript 5.x, and AWS CLI configured with appropriate credentials. Each environment maintains identical network topology and service architecture with parameterized variations for instance sizes, scaling thresholds, and CIDR blocks.""","[""All infrastructure components must be defined as reusable Pulumi ComponentResource classes"", ""Stack configuration files must not exceed 50 lines each to enforce simplicity"", ""Database passwords must be generated using Pulumi's random provider and stored in AWS Secrets Manager"", ""Each environment must have identical security group rules defined through shared modules"", ""Container images must be pulled from a single shared ECR repository across all environments"", ""All resources must include environment-prefixed names following pattern: {env}-{service}-{resource}"", ""Network ACLs must be identical across environments with no hardcoded values"", ""The program must validate CIDR blocks don't overlap before deployment"", ""Auto-scaling policies must be defined in a single location and parameterized per environment"", ""Output comparison must be generated as a structured JSON file showing all configuration differences""]"
l8o5q2,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a high-performance web application for real-time trading data visualization. MANDATORY REQUIREMENTS (Must complete): 1. Deploy ECS Fargate service running containerized Node.js application with auto-scaling (2-10 tasks) based on CPU utilization (CORE: ECS) 2. Create Aurora Serverless v2 PostgreSQL cluster with read replica for high availability (CORE: RDS Aurora) 3. Configure Application Load Balancer with target group health checks every 15 seconds 4. Set up CloudFront distribution with custom error pages and 24-hour default TTL 5. Implement VPC with public subnets for ALB, private subnets for ECS, and database subnets for Aurora 6. Create v2 web ACL with rate limiting (2000 requests per 5 minutes per IP) 7. Configure for database credentials with Lambda rotation function 8. Set up CloudWatch dashboard displaying ECS metrics, ALB response times, and Aurora connections 9. Export ALB DNS name, CloudFront distribution URL, and Aurora cluster endpoint 10. Ensure all security groups follow least-privilege principle with explicit egress rules OPTIONAL ENHANCEMENTS (If time permits):  Add ElastiCache Redis cluster for session management (OPTIONAL: ElastiCache) - improves performance for WebSocket connections  Implement health checks with failover routing (OPTIONAL: ) - adds DNS-level resilience  Set up for automated Aurora snapshots (OPTIONAL: ) - enhances disaster recovery capabilities Expected output: Complete Pulumi TypeScript program that provisions production-ready infrastructure supporting 50,000 concurrent WebSocket connections with automated scaling, security hardening, and comprehensive monitoring. The stack should be deployable with `pulumi up` and include proper error handling and resource dependencies.","A fintech startup needs to deploy their real-time trading dashboard application with strict performance and security requirements. The application processes market data streams and must handle 50,000 concurrent WebSocket connections while maintaining sub-100ms latency for price updates.","""Production-grade web application infrastructure deployed in us-east-1 using ECS Fargate for containerized Node.js application, Aurora Serverless v2 PostgreSQL for real-time data storage, CloudFront for global content delivery. Multi-AZ VPC with public, private, and database subnets across 3 AZs. ALB with v2 for application security. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, AWS CLI configured with appropriate permissions. Infrastructure supports WebSocket connections through ALB with sticky sessions.""","[""ECS tasks must use Fargate Spot instances with at least 4 vCPU and 8GB memory"", ""ALB must have v2 rules blocking SQL injection and XSS attacks"", ""RDS Aurora must use serverless v2 with minimum 0.5 ACUs and maximum 4 ACUs"", ""All data at rest must use customer-managed keys with yearly rotation"", ""VPC must span exactly 3 availability zones with dedicated subnets for each tier"", ""CloudFront distribution must use custom SSL certificate from ACM"", ""ECS service must have circuit breaker enabled with 50% rollback threshold"", ""Aurora must have point-in-time recovery enabled with 7-day retention"", ""All resources must be tagged with Environment, CostCenter, and Owner tags"", "" must store database credentials with automatic rotation every 30 days""]"
m7f3m2,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to provision AWS infrastructure that mirrors an on-premises environment for migration testing. MANDATORY REQUIREMENTS (Must complete): 1. Create a VPC with 3 public and 3 private subnets across different AZs (CORE: VPC) 2. Deploy RDS Aurora PostgreSQL cluster with Multi-AZ, encryption, and 7-day retention (CORE: RDS) 3. Configure ECS Fargate service running a containerized application with 2 tasks minimum 4. Set up Application Load Balancer in public subnets routing to ECS tasks 5. Implement secret for database credentials with automatic rotation 6. Create for S3 and ECR services 7. Configure WebACL with SQL injection rule attached to ALB 8. Enable VPC Flow Logs storing to S3 bucket 9. Create CloudWatch alarms for RDS CPU utilization and ECS task failures 10. Apply consistent tagging: Environment=production, Application=trading-app, CostCenter=finance OPTIONAL ENHANCEMENTS (If time permits):  Add plan for automated cross-region snapshots (OPTIONAL: ) - improves disaster recovery  Implement private hosted zone for service discovery (OPTIONAL: ) - simplifies internal DNS management  Add for non-sensitive configuration (OPTIONAL: ) - centralizes application Expected output: Complete Pulumi TypeScript program that creates production-ready infrastructure matching on-premises architecture with enhanced security and monitoring suitable for migration testing.",A financial services company is migrating their on-premises trading application to AWS. The legacy system runs on VMs with a PostgreSQL database and requires strict network isolation. They need to replicate their production environment in AWS before migrating workloads.,"""Production-grade infrastructure in us-east-1 for migrating on-premises trading application. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate permissions. Multi-AZ VPC with 3 public and 3 private subnets across availability zones. Uses ECS Fargate for containerized application, RDS Aurora PostgreSQL for data persistence, ALB with for ingress. for S3 and ECR to maintain private connectivity. for credential management with Lambda-based rotation. CloudWatch for monitoring with alerting.""","[""Use RDS Aurora PostgreSQL 14.x with encrypted storage and automated backups"", ""Deploy ECS Fargate tasks only in private subnets with no direct internet access"", ""Implement for all database credentials with automatic rotation"", ""Configure for S3 and ECR to avoid internet gateway traffic"", ""Use Application Load Balancer with rules for SQL injection protection"", ""Enable VPC Flow Logs with S3 storage for compliance auditing"", ""Tag all resources with Environment, Application, and CostCenter tags"", ""Implement CloudWatch alarms for database CPU > 80% and ECS task failures""]"
c4h6k0,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a serverless cryptocurrency price alert system. The configuration must: 1. Create an EventBridge custom event bus named 'crypto-events' to receive price updates from exchanges. 2. Deploy a Lambda function 'price-processor' that validates incoming price events and calculates percentage changes. 3. Create a DynamoDB table 'price-history' with partition key 'symbol' and sort key 'timestamp' to store historical prices. 4. Deploy a second Lambda function 'alert-generator' that queries price history and generates alerts when thresholds are exceeded. 5. Configure EventBridge rules to route price events to the price-processor function and schedule the alert-generator to run every 5 minutes. 6. Create an SNS topic 'price-alerts' for distributing notifications to subscribers. 7. Implement dead letter queues for both Lambda functions using SQS. 8. Configure CloudWatch Log groups with specified retention for all Lambda functions. 9. Set up IAM roles with minimal permissions for each Lambda function to access only required services. 10. Export the EventBridge bus ARN, Lambda function names, DynamoDB table name, and SNS topic ARN as stack outputs. Expected output: A complete Pulumi TypeScript program that creates all infrastructure components with proper error handling, typed configurations, and stack outputs for integration with external systems.",A financial technology startup needs to process cryptocurrency price alerts in real-time. Their current monolithic system cannot handle the increasing volume of price change events from multiple exchanges. They require a serverless event-driven architecture that can scale automatically based on demand while maintaining low latency.,"""Serverless infrastructure deployed in us-east-1 using EventBridge for event routing, Lambda functions for processing, DynamoDB for state storage, and SNS for notifications. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate credentials. Architecture includes EventBridge custom event bus, multiple Lambda functions with TypeScript runtime, DynamoDB table for price history, SNS topics for alert distribution, and CloudWatch Logs for monitoring. All resources deployed without VPC dependencies for simplified serverless operation.""","[""Lambda functions must have reserved concurrent executions set to 100"", ""DynamoDB tables must use on-demand billing mode"", ""All Lambda functions must have 512MB memory allocation"", ""Dead letter queues must be configured for all asynchronous invocations"", ""Lambda timeout must be set to 30 seconds maximum"", ""All resources must be tagged with Environment and CostCenter tags"", ""EventBridge rules must use cron expressions for scheduling"", ""Lambda functions must use ARM64 architecture for cost optimization"", ""All IAM roles must follow least privilege principle with no wildcard actions"", ""CloudWatch Log groups must have 14-day retention period""]"
q9l2b6,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to deploy a multi-region disaster recovery environment. The configuration must: 1. Deploy RDS Aurora Global Database with primary cluster in us-east-1 and secondary in eu-west-1 (CORE: RDS Aurora Global) 2. Configure Route 53 health checks with automatic DNS failover between regions (CORE: Route 53) 3. Create Lambda functions in both regions to validate database connectivity with 256MB memory 4. Implement cross-region VPC peering with for secure communication 5. Set up CloudWatch dashboards showing replication lag and health metrics 6. Configure automated backups with 7-day retention and cross-region snapshot copying 7. Implement IAM roles with cross-region assume permissions for failover operations 8. Create S3 buckets in both regions for application assets with cross-region replication 9. Set up topics for failover notifications with email subscriptions 10. Configure parameter groups optimized for high-throughput replication Expected output: Complete Pulumi program that provisions a production-ready multi-region environment with automated failover, monitoring, and disaster recovery capabilities.",A financial services company needs to establish a new cloud environment with strict regulatory requirements for data residency and disaster recovery. The system must maintain transaction data across multiple regions with automatic failover capabilities to meet their 99.99% uptime SLA.,"""Multi-region AWS deployment spanning us-east-1 (primary) and eu-west-1 (secondary) regions. Requires Pulumi 3.x with TypeScript, AWS CLI configured with credentials for both regions. Infrastructure includes RDS Aurora Global Database clusters, Route 53 hosted zones, Lambda functions, VPCs with connectivity, S3 buckets with cross-region replication, CloudWatch dashboards, and topics. Each region has isolated VPCs with 10.0.0.0/16 (us-east-1) and 10.1.0.0/16 (eu-west-1) CIDR blocks, private subnets across 3 AZs, and NAT gateways for outbound connectivity.""","[""Aurora clusters must use db.r6g.large instances with Aurora MySQL 8.0 compatibility"", ""Route 53 health checks must have 30-second intervals with 3 consecutive failures before triggering"", ""Lambda functions must use Node.js 18.x runtime with tracing enabled"", ""VPC peering must restrict traffic to port 3306 only between database subnets"", ""CloudWatch dashboard must display RPO and RTO metrics with 5-minute refresh intervals"", ""Backup window must be between 03:00-04:00 UTC with performance insights enabled"", ""IAM roles must follow naming convention: dr-{region}-{service}-role"", ""S3 bucket names must include region suffix and enable versioning with lifecycle policies"", "" topics must use AWS KMS encryption with customer-managed keys"", ""All resources must have consistent tagging: Environment=Production, DR=Enabled, Owner=DevOps""]"
c6s6r0,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a multi-stage CI/CD pipeline for containerized applications. MANDATORY REQUIREMENTS (Must complete): 1. Create with source stage connected to CodeCommit repository (CORE: ) 2. Configure project with ARM64 compute type and buildspec for Docker image builds (CORE: ) 3. Set up ECR repository with image scanning on push enabled 4. Implement build stage that creates container images and pushes to ECR 5. Add manual approval stage with SNS topic for production deployments 6. Configure deployment stages for both dev and prod ECS services 7. Create customer-managed KMS key for encrypting build logs 8. Set S3 lifecycle policy on artifact bucket for 30-day expiration 9. Implement least-privilege IAM roles for each pipeline component 10. Output pipeline execution URL and SNS topic ARN OPTIONAL ENHANCEMENTS (If time permits):  Add project for running unit tests before image build (OPTIONAL: Additional ) - improves quality gates  Implement rule to trigger pipeline on ECR scan findings (OPTIONAL: ) - automates security response  Add tracing to projects (OPTIONAL: ) - enhances build performance debugging Expected output: Complete Pulumi program that provisions a production-ready CI/CD pipeline with automated container builds, security scanning, and multi-environment deployments with proper access controls and cost optimization measures.","A software development team needs to establish a GitOps-based CI/CD pipeline for their microservices application. The pipeline must automatically build container images from source code commits, run security scans, and deploy to ECS clusters across development and production environments.","""Multi-account AWS infrastructure spanning us-east-1 region with separate development (123456789012) and production (987654321098) accounts. Uses for orchestration, for container builds, ECR for image registry with vulnerability scanning enabled, and ECS Fargate for runtime. Requires Pulumi 3.x with TypeScript, Node.js 18+, and AWS CLI configured with cross-account assume role permissions. VPC endpoints configured for ECR, S3, and CloudWatch to keep traffic private.""","[""All projects must use ARM-based compute for cost efficiency"", ""Container image scans must block deployment if critical vulnerabilities are detected"", ""Production deployments require manual approval via SNS notifications"", ""Build logs must be encrypted with customer-managed KMS keys"", ""Pipeline artifacts must expire after 30 days to minimize storage costs""]"
q3l4a0,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy and maintain consistent infrastructure across three AWS environments (dev, staging, prod) with automated drift detection and environment promotion capabilities. The configuration must: 1. Define a base infrastructure template as a ComponentResource that accepts environment-specific parameters. 2. Deploy VPCs in three different regions with consistent CIDR ranges but non-overlapping IP spaces. 3. Create RDS Aurora PostgreSQL clusters with read replicas, using different instance sizes per environment. 4. Deploy containerized Lambda functions from ECR with environment-specific memory allocations. 5. Configure S3 buckets with cross-region replication from prod to staging for data consistency. 6. Implement ALB with path-based routing to Lambda functions, with environment-specific domain names. 7. Use Pulumi Automation API to create a drift detection script that runs hourly. 8. Generate a promotion script that copies staging configuration to production with approval workflow. 9. Export critical outputs (ALB URLs, RDS endpoints, S3 bucket names) for cross-stack references. 10. Create policy pack that validates all resources follow naming pattern: {env}-{service}-{resource}. Expected output: A Pulumi TypeScript project with separate stacks for each environment, reusable components for consistent deployment, automated drift detection capabilities, and scripts for safe environment promotion with built-in validation checks.","A financial services company needs to maintain identical infrastructure across development, staging, and production environments to ensure consistent behavior during the software release cycle. They require automated environment replication with drift detection capabilities to prevent configuration divergence that could lead to production incidents.","""Multi-environment AWS infrastructure spanning us-east-1 (production), us-west-2 (staging), and eu-west-1 (development). Each environment requires isolated VPCs with 3 availability zones, private and public subnets, NAT gateways for outbound traffic. Core services include RDS Aurora PostgreSQL clusters, Lambda functions using container images, S3 buckets for data storage, and Application Load Balancers. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, Docker for building Lambda containers, and AWS CLI configured with appropriate credentials for all three regions.""","[""Use Pulumi stack references to share outputs between environments"", ""Implement custom ComponentResource classes for reusable infrastructure patterns"", ""Environment-specific configurations must use Pulumi config values, not hardcoded strings"", ""All S3 buckets must have versioning enabled and lifecycle policies defined"", ""RDS instances must use encrypted storage with customer-managed KMS keys"", ""Lambda functions must use container images stored in private ECR repositories"", ""Implement automated drift detection using Pulumi Automation API"", ""Use Pulumi policy packs to enforce naming conventions across all environments"", ""Network segmentation must prevent direct communication between environments"", ""Resource tags must include Environment, CostCenter, and DataClassification keys""]"
j8x8g6,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to implement a multi-AZ payment processing infrastructure with automated failover capabilities. The configuration must: 1. Deploy an Aurora PostgreSQL cluster with one writer and two read replicas across three AZs. 2. Create ECS Fargate services running payment API containers in private subnets of each AZ. 3. Configure Application Load Balancer with target groups spanning all three AZs. 4. Implement Route 53 health checks that monitor both ALB and Aurora endpoints. 5. Set up automated failover routing policies that redirect traffic during AZ failures. 6. Configure CloudWatch alarms for database CPU, connection count, and replication lag. 7. Create SNS topic for failure notifications with email subscriptions. 8. Implement auto-scaling for ECS services based on request count and CPU utilization. 9. Set up automated Aurora backups with 7-day retention and encryption at rest. 10. Configure for S3 and ECR to reduce data transfer costs. Expected output: A complete Pulumi TypeScript program that provisions highly available infrastructure capable of surviving single AZ failures with automatic failover for both compute and database tiers, ensuring payment processing continues uninterrupted.","A financial services company needs to ensure their payment processing API remains operational during availability zone failures. The system must automatically detect failures and reroute traffic to healthy instances without manual intervention, maintaining sub-second failover times for critical transactions.","""Multi-AZ deployment across us-east-1a, us-east-1b, and us-east-1c for high availability payment processing. Infrastructure includes Application Load Balancer with cross-zone load balancing, ECS Fargate services running containerized Node.js applications, Aurora PostgreSQL Multi-AZ cluster with automated failover, and Route 53 health checks. VPC spans three availability zones with public subnets for ALB and private subnets for ECS tasks and Aurora. Requires Pulumi CLI 3.x, TypeScript 4.5+, AWS CLI configured with appropriate permissions, and Docker for local container testing. NAT Gateways in each AZ provide outbound internet access for private resources.""","[""Health checks must validate both HTTP endpoints and database connectivity"", ""Failover must complete within 30 seconds of failure detection"", ""All resources must be tagged with Environment, Owner, and CostCenter tags"", ""Database read replicas must be promoted automatically during primary failure"", ""Use separate security groups for each tier with least-privilege rules"", ""Implement automated backups with point-in-time recovery enabled""]"
n1s1a7,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a secure payment processing web application. The configuration must: 1. Set up a VPC with public and private subnets across 3 availability zones with NAT Gateways. 2. Deploy containerized Node.js API in ECS Fargate with auto-scaling (min 2, max 10 tasks). 3. Configure Aurora PostgreSQL Serverless v2 with encryption and automated backups. 4. Implement Application Load Balancer with WAF rules for SQL injection and XSS protection. 5. Create bucket for React frontend with distribution and OAI. 6. Configure Secrets Manager to store database credentials and API keys with rotation. 7. Set up KMS keys for encrypting ECS task definitions, Aurora database, and bucket. 8. Implement CloudWatch Log Groups with 30-day retention for all services. 9. Create IAM roles following least privilege for ECS tasks and rotation functions. 10. Configure CloudWatch alarms for high error rates, database connections, and ECS task failures. 11. Enable VPC Flow Logs for security monitoring and compliance. 12. Set deletion protection to false for development but include parameter for production override. Expected output: A complete Pulumi TypeScript program with modular components that deploys all infrastructure resources, outputs the URL and ALB DNS name, and includes inline documentation for security configurations.",A fintech startup needs to deploy their payment processing web application with strict compliance requirements for PCI DSS. The application consists of a React frontend and Node.js API backend that processes credit card transactions through a third-party payment gateway.,"""Production-grade infrastructure deployed in us-east-1 across 3 availability zones. Uses ECS Fargate for containerized Node.js API and React frontend, Application Load Balancer with WAF integration, Aurora PostgreSQL Serverless v2 for database, with for static assets. VPC with public subnets for ALB, private subnets for ECS tasks and Aurora. NAT Gateways provide outbound internet access. Requires Pulumi CLI 3.x, Node.js 18+, TypeScript 5.x, Docker for local testing. AWS account with appropriate IAM permissions for ECS, VPC, WAF, Secrets Manager, and KMS services.""","[""All data at rest must be encrypted using AWS KMS customer-managed keys"", ""Network traffic must flow through AWS WAF with rate limiting enabled"", ""Database connections must use SSL/TLS with certificate validation"", ""API endpoints must implement request throttling at 100 requests per minute per IP"", ""All compute resources must run in private subnets with no direct internet access"", ""Secrets must be stored in AWS Secrets Manager and rotated every 30 days"", ""CloudWatch alarms must trigger when error rates exceed 1% over 5 minutes""]"
z7j9w8,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to implement a secure secrets vault for database credentials. MANDATORY REQUIREMENTS (Must complete): 1. Create an Aurora MySQL instance with encryption enabled (CORE: ) 2. Store database master credentials in AWS Secrets Manager (CORE: Secrets Manager) 3. Implement automatic rotation using Lambda function with 30-day schedule 4. Create customer-managed KMS key with annual rotation enabled 5. Configure for Secrets Manager and KMS services 6. Implement IAM roles with least-privilege access - no wildcards allowed 7. Enable CloudWatch Logs encryption using KMS for Lambda logs 8. Apply mandatory tags: Environment, CostCenter, Compliance 9. Export secret ARN as stack output for cross-stack reference 10. Set deletion protection on KMS key and instance OPTIONAL ENHANCEMENTS (If time permits):  Add EventBridge rule to notify on rotation events (OPTIONAL: EventBridge) - improves operational visibility  Implement topic for rotation failure alerts (OPTIONAL: ) - enables proactive incident response  Add for non-secret config (OPTIONAL: ) - centralizes configuration management Expected output: A Pulumi TypeScript program that deploys a complete secrets management solution with automated rotation, encryption at rest and in transit, comprehensive audit logging, and cross-stack integration capabilities.",A financial services company needs to implement a secure secrets management system that rotates database credentials automatically. The solution must comply with PCI DSS requirements for credential rotation and audit logging. All secrets must be encrypted at rest and in transit with strict access controls.,"""Secure secrets management infrastructure deployed in us-east-1 using AWS Secrets Manager for credential storage, Lambda for rotation logic, and KMS for encryption. Requires Pulumi CLI 3.x with TypeScript, Node.js 18.x, AWS CLI configured with appropriate permissions. for Secrets Manager and KMS to ensure traffic stays within AWS network. CloudWatch Logs with encryption enabled for audit compliance. Multi-account setup with separate security account for centralized secrets management.""","[""Use AWS Secrets Manager for storing database credentials with automatic rotation enabled"", ""Implement Lambda functions using Node.js 18.x runtime for custom rotation logic"", ""Configure rotation to occur every 30 days with immediate rotation capability"", ""Use KMS customer-managed keys (CMK) for encryption with key rotation enabled"", ""Implement least-privilege IAM policies with no wildcard permissions"", ""Enable CloudWatch Logs encryption for all Lambda function logs"", ""Tag all resources with Environment, CostCenter, and Compliance tags"", ""Use Pulumi stack references to share the secret ARN with other stacks""]"
x6k3h9,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a serverless crypto price alert system. The configuration must: 1. Create two Lambda functions - one for webhook ingestion (256MB memory) and one for rule evaluation (512MB memory), both using ARM64 architecture. 2. Set up a DynamoDB table for user alert rules with partition key 'userId' and sort key 'alertId', with point-in-time recovery enabled. 3. Create another DynamoDB table for price history with TTL enabled on 'expiryTime' attribute. 4. Configure an SNS topic for outbound notifications with server-side encryption. 5. Set up API Gateway REST API with POST endpoint '/webhook' that triggers the ingestion Lambda. 6. Implement request validation on the API Gateway endpoint for required fields: 'exchange', 'symbol', 'price'. 7. Create SQS queue between ingestion and evaluation Lambdas with visibility timeout of 5 minutes. 8. Configure dead letter queue for the main SQS queue with maximum receive count of 3. 9. Set up EventBridge rule to trigger evaluation Lambda every 5 minutes for batch processing. 10. Create custom KMS key for encrypting Lambda environment variables. 11. Enable X-Ray tracing on all Lambda functions and API Gateway. 12. Implement least-privilege IAM roles for each Lambda with no wildcard permissions. Expected output: A Pulumi program that deploys a complete serverless webhook processing system with proper error handling, encryption, and monitoring capabilities suitable for production use.","A fintech startup needs to process cryptocurrency price alerts for their mobile app users. The system must handle webhook notifications from various crypto exchanges, apply user-defined threshold rules, and send push notifications when price targets are met. The architecture must be event-driven to minimize costs during low-activity periods.","""Serverless infrastructure deployed in us-east-1 using Lambda for webhook processing and rule evaluation, DynamoDB for storing user alert configurations and price history, SNS for push notification delivery, and API Gateway for webhook endpoints. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured. No VPC required as all services are managed. Custom KMS key for encryption at rest. X-Ray distributed tracing enabled across all Lambda functions for debugging webhook processing flows.""","[""Lambda functions must use ARM64 architecture for cost optimization"", ""DynamoDB tables must use point-in-time recovery enabled"", ""All Lambda functions must have X-Ray tracing active"", ""SNS topics must have server-side encryption enabled"", ""Lambda environment variables containing API keys must be encrypted with a custom KMS key"", ""Dead letter queues must have a maximum receive count of 3"", ""API Gateway must use request validation for all endpoints""]"
j3g6z5,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a complete CI/CD pipeline infrastructure. The configuration must: MANDATORY REQUIREMENTS (Must complete): 1. Create a CodePipeline with source, build, staging deploy, manual approval, and production deploy stages (CORE: CodePipeline) 2. Configure CodeBuild project with Docker support for building container images (CORE: CodeBuild) 3. Set up ECR repository with vulnerability scanning and lifecycle policies for image retention 4. Create S3 bucket for pipeline artifacts with server-side encryption and versioning 5. Implement Lambda function to send pipeline status notifications to Slack webhook 6. Configure EventBridge rules to trigger notifications on pipeline state changes 7. Create cross-account IAM roles for deployments to staging and production accounts 8. Set up CloudWatch Log groups with 14-day retention for all services OPTIONAL ENHANCEMENTS (If time permits):  Add CodeCommit repository with branch protection rules (OPTIONAL: CodeCommit) - provides Git hosting within AWS  Implement for complex deployment workflows (OPTIONAL: ) - enables advanced orchestration  Add for secure configuration (OPTIONAL: ) - centralizes secrets management Expected output: A Pulumi TypeScript program that creates a fully functional CI/CD pipeline with automated builds, security scanning, and multi-account deployment capabilities. The stack should output the pipeline ARN, ECR repository URI, and webhook URL for notifications.","A financial services company needs to modernize their CI/CD infrastructure to support their new microservices architecture. Their development teams require automated build pipelines with security scanning, artifact management, and deployment capabilities across multiple environments. The solution must integrate with their existing GitHub repositories and support both containerized and serverless workloads.","""AWS multi-account setup in us-east-1 region for CI/CD pipeline infrastructure. Primary services include CodePipeline for orchestration, CodeBuild for compilation and testing, ECR for container registry, S3 for artifact storage, Lambda for notifications, and EventBridge for event routing. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate credentials. VPC endpoints required for private CodeBuild access to ECR and S3. Cross-account IAM roles needed for deployment to staging (account 222222222222) and production (account 333333333333) environments.""","[""All CodeBuild projects must use compute type BUILD_GENERAL1_SMALL to minimize costs"", ""CodePipeline must have manual approval stage between staging and production deployments"", ""ECR repositories must have image scanning enabled with scan on push"", ""Lambda functions for pipeline notifications must have 128MB memory allocation"", ""S3 buckets for artifacts must have versioning enabled and lifecycle rules for 30-day retention"", ""All IAM roles must follow least privilege principle with no wildcard resource permissions"", ""CodeBuild projects must have build timeout of 15 minutes maximum"", ""Pipeline must fail if any security vulnerabilities with severity HIGH or CRITICAL are found""]"
q5p5y1,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy an automated infrastructure compliance scanning system. MANDATORY REQUIREMENTS (Must complete): 1. Configure AWS Config with custom rules for tag compliance and security group validation (CORE: Config) 2. Create Lambda functions to process Config rule evaluations and generate compliance reports (CORE: Lambda) 3. Set up S3 buckets with versioning and 90-day lifecycle policies for compliance data retention 4. Implement topics for violation alerts with email subscriptions 5. Create DynamoDB table for tracking compliance state history 6. Deploy identical infrastructure to us-east-1 and eu-west-1 regions 7. Configure cross-account IAM roles for centralized compliance dashboard access 8. Enable CloudWatch Logs for all Lambda functions with 30-day retention 9. Implement error handling and retry logic in Lambda functions 10. Use Pulumi stack references for sharing outputs between regions OPTIONAL ENHANCEMENTS (If time permits):  Add EventBridge rules for scheduled compliance scans (OPTIONAL: EventBridge) - enables automated periodic scanning  Implement for complex remediation workflows (OPTIONAL: ) - adds automated remediation capabilities  Add Parameter Store for configuration management (OPTIONAL: ) - improves configuration flexibility Expected output: A Pulumi TypeScript program that deploys a multi-region compliance scanning infrastructure with automated violation detection, alerting, and audit trail storage. The solution should handle cross-account access and provide real-time compliance visibility.","A financial services company needs automated infrastructure compliance scanning to meet regulatory requirements. Their infrastructure spans multiple AWS accounts and regions, requiring continuous validation of security configurations, resource tagging, and access policies. The compliance team needs real-time alerts when violations are detected.","""Multi-region AWS deployment across us-east-1 and eu-west-1 for compliance scanning infrastructure. Uses AWS Config for continuous monitoring, Lambda for custom compliance rules, S3 for audit trail storage, for alerting, and DynamoDB for state tracking. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate IAM permissions. for secure API access, cross-account IAM roles for centralized monitoring across 3 AWS accounts (dev, staging, prod).""","[""Use AWS Config for compliance rule evaluation"", ""Store compliance results in S3 with lifecycle policies"", ""Implement Lambda functions in TypeScript for custom rule logic"", ""Use for multi-channel alerting (email and Slack)"", ""Deploy to at least 2 AWS regions simultaneously"", ""Enable cross-account access for centralized compliance monitoring"", ""Implement retry logic for transient AWS API failures"", ""Use Pulumi's automation API for programmatic deployments"", ""Enforce encryption at rest for all data storage""]"
r2y1q7,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement an active-passive disaster recovery architecture for a payment processing API. The configuration must: 1. Create DynamoDB global tables with on-demand billing in both regions for transaction storage. 2. Deploy identical Lambda functions (Node.js 18.x runtime, 1GB memory) in both regions to process payments. 3. Configure S3 buckets with cross-region replication for storing payment receipts. 4. Set up Route53 hosted zone with failover routing policy between regions. 5. Create Application Load Balancers in both regions as Lambda targets. 6. Implement Route53 health checks monitoring ALB endpoints every 30 seconds. 7. Configure CloudWatch alarms that trigger after 3 failed health checks. 8. Set up SNS topics in both regions for failover notifications. 9. Create CloudWatch dashboard showing health status of both regions. 10. Ensure all resources have deletion protection disabled for testing. Expected output: A Pulumi program that deploys a complete multi-region DR infrastructure where traffic automatically fails over to the secondary region within 60 seconds of primary region failure, with zero data loss and automated alerting.","A financial services company requires a disaster recovery solution for their critical payment processing API. The system must automatically failover to a secondary region within 60 seconds of detecting a regional outage, maintaining all transaction records and ensuring zero data loss.","""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary). Utilizes Route53 for DNS failover, DynamoDB Global Tables for data replication, Lambda for compute, and S3 with cross-region replication for static content. Requires Pulumi 3.x with TypeScript, AWS CLI configured with multi-region access. Both regions will have identical VPC configurations with 3 availability zones, private subnets for compute resources, and public subnets for ALB endpoints. CloudWatch cross-region dashboards monitor health status.""","[""Use Route53 health checks with 30-second intervals for automatic failover"", ""Implement cross-region DynamoDB global tables with point-in-time recovery enabled"", ""Deploy Lambda functions in both primary (us-east-1) and secondary (us-west-2) regions"", ""Configure CloudWatch alarms to trigger on 3 consecutive health check failures"", ""Use S3 cross-region replication with versioning for static assets"", ""Implement dead letter queues with 14-day message retention"", ""Set RTO (Recovery Time Objective) to 60 seconds maximum"", ""Ensure all IAM roles use least-privilege with no Admin or wildcard permissions"", ""Tag all resources with Environment=DR and CostCenter=Finance""]"
h2r7r5,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster with enhanced security. The configuration must: 1. Create a VPC with 3 private subnets across different AZs and NAT gateways for outbound traffic. 2. Deploy an EKS cluster version 1.28 with private API endpoint and OIDC provider enabled. 3. Create two managed node groups: one for general workloads (t3.large, min 2, max 10) and one for GPU workloads (g4dn.xlarge, min 1, max 5), both using Bottlerocket AMI. 4. Configure cluster autoscaler with appropriate IAM roles using IRSA for dynamic scaling. 5. Install and configure Calico CNI for network policy enforcement. 6. Create a KMS key with automatic rotation for encrypting EKS secrets and EBS volumes. 7. Configure pod security standards with 'restricted' as cluster-wide default and 'baseline' for system namespaces. 8. Deploy AWS Load Balancer Controller using Helm with IRSA for ingress management. 9. Create three namespaces: 'production', 'ml-workloads', and 'monitoring' with appropriate network policies. 10. Configure RBAC with separate roles for developers (read-only) and SRE team (admin access to specific namespaces). 11. Enable control plane logging for api, audit, authenticator, controllerManager, and scheduler. 12. Output the kubeconfig, cluster endpoint, and OIDC issuer URL for CI/CD integration. Expected output: A Pulumi program that creates a hardened EKS cluster with automatic scaling, network isolation, and enterprise-grade security controls ready for production workloads.","A fintech startup needs to deploy their microservices architecture on Kubernetes in AWS. They require a production-ready EKS cluster with strict security requirements, automated node scaling, and integration with their existing monitoring stack. The infrastructure must support both CPU and GPU workloads for their fraud detection ML models.","""Production EKS cluster deployed in us-east-2 across 3 availability zones. Uses EKS 1.28 with managed node groups running Bottlerocket OS. VPC with private subnets only, NAT gateways for outbound traffic. Requires Pulumi 3.x with TypeScript, kubectl 1.28+, AWS CLI v2 configured. Integration with AWS Systems Manager for node access. OIDC provider configured for IRSA. Network policies enforced via Calico CNI. KMS encryption for EBS volumes and secrets.""","[""EKS cluster must use private subnets only with no direct internet access for worker nodes"", ""Node groups must use Bottlerocket AMI for enhanced security and immutable infrastructure"", ""Implement pod security standards with restricted baseline as the default"", ""All secrets must be encrypted using AWS KMS with automatic key rotation enabled"", ""Network policies must enforce zero-trust communication between namespaces""]"
p4h0g9,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a secure secrets management infrastructure with automated rotation. MANDATORY REQUIREMENTS (Must complete): 1. Create KMS customer-managed key with automatic rotation and key policy restricting usage to specific IAM roles (CORE: KMS) 2. Deploy Secrets Manager secrets for database credentials with Lambda-based rotation every 30 days (CORE: Secrets Manager) 3. Implement Lambda rotation function in private subnet that generates cryptographically secure passwords 4. Configure VPC with 3 private subnets and for Secrets Manager, KMS, and Lambda 5. Create IAM roles with explicit permissions - no wildcards, following least-privilege principle 6. Set up CloudWatch Logs with 90-day retention and KMS encryption for all audit trails 7. Configure security groups allowing only HTTPS traffic between Lambda and 8. Apply mandatory tags (CostCenter, Environment, Compliance) to all resources 9. Enable CloudTrail logging for all KMS and Secrets Manager API calls OPTIONAL ENHANCEMENTS (If time permits):  Add topic for rotation failure alerts (OPTIONAL: ) - enables proactive incident response  Implement EventBridge rules for secret access patterns (OPTIONAL: EventBridge) - improves security monitoring  Create parameters for non-sensitive config (OPTIONAL: ) - centralizes configuration management Expected output: Pulumi TypeScript program that provisions a complete secrets management infrastructure with automatic rotation, comprehensive audit logging, and defense-in-depth security controls. The solution should output the KMS key ARN, secret ARNs, and rotation Lambda function ARN.","A financial services company needs to implement a secure secrets management system for their microservices architecture. The system must enforce strict access controls, audit all secret access attempts, and rotate credentials automatically. Compliance requires all secrets to be encrypted at rest and in transit with customer-managed keys.","""Secure multi-AZ deployment in us-east-2 region using AWS Secrets Manager for credential storage, KMS for encryption key management, Lambda for automatic rotation logic, and VPC with private subnets across 3 availability zones. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate permissions. VPC includes endpoints for Secrets Manager, KMS, and Lambda services. All traffic flows through private subnets with no internet gateway. NAT instances provide outbound connectivity for Lambda functions when needed.""","[""All KMS keys must use AES-256 encryption with automatic key rotation enabled"", ""Secrets Manager secrets must have automatic rotation configured with Lambda functions"", ""IAM roles must follow least-privilege principle with no wildcard permissions"", ""All Lambda functions must run in private subnets with for AWS services"", ""CloudWatch Logs must retain audit logs for exactly 90 days with encryption enabled"", ""Security groups must explicitly deny all traffic except required ports"", "" buckets storing rotation templates must have versioning and MFA delete enabled"", "" topics for alerts must use encrypted transport with verified subscriptions"", ""All resources must be tagged with CostCenter, Environment, and Compliance tags""]"
j1k3i2,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to optimize an existing ECS Fargate infrastructure for cost and performance. The configuration must: 1. Analyze and fix over-provisioned ECS task definitions by implementing dynamic memory/CPU allocation based on service type (API: 512MB/0.25vCPU, Workers: 1024MB/0.5vCPU, Background: 256MB/0.25vCPU). 2. Implement ECS service auto-scaling with target tracking policies for both CPU (target: 70%) and memory (target: 80%) utilization. 3. Create separate task definitions for Fargate and Fargate Spot, with automatic failover logic. 4. Configure CloudWatch Container Insights for all services with custom metrics for response time monitoring. 5. Implement a custom Pulumi component that validates container image sizes before deployment. 6. Set up scheduled scaling for predictable traffic patterns (scale up at 8 AM, scale down at 8 PM EST). 7. Configure ECS task placement strategies to maximize Spot instance usage for background workers. 8. Create CloudWatch dashboards showing cost optimization metrics (Spot vs On-Demand usage, right-sizing recommendations). 9. Implement health check optimizations with appropriate intervals (APIs: 30s, Workers: 60s). 10. Add Pulumi stack outputs showing estimated monthly cost savings. Expected output: Optimized Pulumi TypeScript code that reduces ECS costs by 30-40% through right-sizing, Spot usage, and intelligent scaling while maintaining performance SLAs. The solution should include monitoring dashboards and cost tracking capabilities.",A fintech startup's ECS-based payment processing system experiences cold starts and memory spikes during peak hours. The existing Pulumi infrastructure needs optimization to reduce costs while maintaining sub-second response times for transaction processing.,"""Production environment in us-east-1 across 3 availability zones with existing VPC (10.0.0.0/16). Current setup uses ECS Fargate with ALB, RDS Aurora PostgreSQL (db.t3.medium), and CloudWatch. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI v2 configured. Infrastructure includes private subnets (10.0.1.0/24, 10.0.2.0/24, 10.0.3.0/24) with NAT gateways. Existing ECS cluster running 15 services with mixed CPU/memory utilization patterns.""","[""ECS task memory must be right-sized based on actual usage patterns with 20% buffer"", ""Container images must use multi-stage builds to reduce size below 200MB"", ""Auto-scaling must respond within 30 seconds to CPU or memory threshold breaches"", ""All resources must use cost allocation tags for department-level billing"", ""Task definitions must use Fargate Spot for non-critical background jobs""]"
t5n5m9,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy an automated compliance scanning system. The configuration must: 1. Set up AWS Config with a configuration recorder and delivery channel using an S3 bucket with versioning enabled. 2. Create 3 Lambda functions for custom Config rules: detect unencrypted S3 buckets, identify publicly accessible RDS instances, and find EC2 instances without agent installed. 3. Configure AWS Config custom rules that trigger the Lambda functions on configuration changes. 4. Establish a DynamoDB table with on-demand billing to store compliance results with TTL of 90 days. 5. Implement an aggregator Lambda function that processes Config evaluation results and writes to DynamoDB. 6. Create an topic with email subscription for HIGH severity non-compliance alerts. 7. Set up EventBridge rule to trigger the aggregator Lambda when Config evaluations complete. 8. Apply resource tags including Environment=compliance-scanning and ManagedBy=pulumi. 9. Configure Lambda functions with VPC access for secure execution without internet connectivity. 10. Implement CloudWatch dashboard showing compliance status metrics from the last 24 hours. Expected output: A complete Pulumi TypeScript program that deploys the compliance scanning infrastructure, automatically evaluates resources against security policies, stores historical compliance data, and alerts on critical violations through notifications.",Your company's security team needs automated infrastructure compliance scanning to identify policy violations across all AWS resources. The solution must analyze deployed infrastructure against predefined security policies and generate actionable reports for remediation.,"""Infrastructure compliance scanning environment deployed in us-east-1 region using AWS Config for continuous monitoring, Lambda functions for custom compliance rules, DynamoDB for storing compliance history, and for alerting. Requires Pulumi CLI 3.x with TypeScript, Node.js 18.x, and AWS CLI configured with appropriate permissions. Resources span across multiple AWS accounts using AWS Organizations. configured for private Lambda execution without internet access.""","[""Use AWS Config with custom rules written in Lambda functions for compliance checks"", ""Implement exactly 3 custom compliance rules: unencrypted S3 buckets, public RDS instances, and EC2 instances without agent"", ""Store compliance results in DynamoDB with partition key as resource ARN and sort key as timestamp"", ""Generate notifications only for non-compliant resources with HIGH severity"", ""All Lambda functions must use Node.js 18.x runtime with 256MB memory allocation""]"
f9d8f3,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster with advanced node group configuration and security hardening.

MANDATORY REQUIREMENTS (Must complete):
1. Create an EKS cluster with private API endpoint and public access disabled (CORE: EKS)
2. Deploy two managed node groups: one on-demand (2-4 nodes) and one spot (2-6 nodes) with mixed instance types
3. Configure custom launch templates with encrypted EBS volumes using customer KMS key
4. Enable IRSA and create the OIDC provider for pod identity
5. Deploy EKS managed add-ons: CoreDNS v1.10.1, kube-proxy v1.28.1, VPC CNI v1.15.0
6. Implement cluster autoscaler with proper IAM permissions and node labels
7. Configure CloudWatch Container Insights for cluster monitoring (CORE: CloudWatch)
8. Create at least 3 IAM roles for service accounts with different permission sets
9. Enable EKS control plane logging for API, audit, authenticator, controllerManager, and scheduler
10. Tag all resources with Environment=production, ManagedBy=pulumi, CostCenter=engineering

OPTIONAL ENHANCEMENTS (If time permits):
 Add AWS Load Balancer Controller for ingress management (OPTIONAL: ELB) - enables native AWS load balancing
 Implement Secrets Manager integration for sensitive data (OPTIONAL: Secrets Manager) - improves secret management
 Add EFS storage class for persistent volumes (OPTIONAL: EFS) - provides shared storage for stateful workloads

Expected output: A complete Pulumi TypeScript program that creates an EKS cluster with hardened security, mixed node groups supporting both spot and on-demand instances, full observability through CloudWatch Container Insights, and IRSA configured for fine-grained pod permissions. The cluster should be production-ready with all specified add-ons and autoscaling capabilities.","A fintech company is migrating their microservices from on-premises Kubernetes to AWS EKS. They need a production-grade EKS cluster with strict security requirements, automated node group management, and integration with their existing monitoring stack. The infrastructure must support both spot and on-demand instances for cost optimization.","""Production EKS deployment in us-east-2 across 3 availability zones. Requires Pulumi 3.x with TypeScript, AWS CLI v2 configured with appropriate permissions, kubectl 1.28+, and Node.js 18+. VPC with 3 public subnets (10.0.1.0/24, 10.0.2.0/24, 10.0.3.0/24) and 3 private subnets (10.0.101.0/24, 10.0.102.0/24, 10.0.103.0/24). NAT gateways in each AZ for high availability. Customer-managed KMS key for EBS encryption. Existing S3 bucket for EKS audit logs.""","[""EKS cluster must use Kubernetes version 1.28 or higher with private API endpoint"", ""Node groups must use custom launch templates with encrypted EBS volumes using customer-managed KMS keys"", ""Implement IRSA (IAM Roles for Service Accounts) for pod-level AWS permissions"", ""All worker nodes must be in private subnets with no direct internet access"", ""Enable EKS managed add-ons for CoreDNS, kube-proxy, and VPC CNI with specific version requirements"", ""Implement cluster autoscaler with mixed instance types (t3.medium, t3.large) and spot instances"", ""Configure OIDC provider for the cluster to enable pod identity webhooks""]"
h2g7l0,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to deploy a multi-region disaster recovery infrastructure. The configuration must: 1. Create Aurora Global Database cluster with primary in us-east-1 and secondary in us-west-2. 2. Deploy ECS services in both regions with identical task definitions. 3. Configure Application Load Balancers in each region with target groups pointing to ECS services. 4. Set up Route 53 hosted zone with health checks and failover routing between regions. 5. Create S3 buckets with cross-region replication for application artifacts and logs. 6. Implement EventBridge rules that monitor primary region health and trigger failover Lambda. 7. Deploy Lambda function to orchestrate failover procedures including Aurora promotion. 8. Configure parameters in primary region with replication to DR region. 9. Set up AWS Backup plans for Aurora, ECS task definitions, and S3 buckets. 10. Implement CloudWatch dashboards in both regions for monitoring DR readiness. 11. Create topics for alerting on failover events and replication lag. 12. Ensure all resources are tagged with Environment=DR and CostCenter=Platform. Expected output: A Pulumi TypeScript program that creates a fully automated disaster recovery infrastructure with sub-minute RPO, automated failover capabilities, and comprehensive monitoring across both regions.",A financial services company needs to implement a disaster recovery solution for their critical trading platform. The primary region hosts a containerized application that processes real-time market data and must maintain 99.99% uptime with RPO under 1 minute.,"""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (DR). Utilizes Aurora PostgreSQL 15.x Global Database, ECS for container orchestration, Application Load Balancers in each region, S3 for artifact storage, EventBridge for automated failover orchestration, Route 53 for DNS failover, for configuration management, and AWS Backup for centralized backup operations. Requires Pulumi CLI 3.x, Node.js 18+, TypeScript 5.x, and AWS CLI configured with appropriate credentials. VPCs in both regions with private subnets across 3 AZs, VPC peering for cross-region communication.""","[""Use Aurora Global Database with automated failover capabilities"", ""Implement Route 53 health checks with failover routing policies"", ""Configure cross-region replication for all S3 buckets containing application artifacts"", ""Set up EventBridge rules to trigger DR procedures automatically"", ""Use AWS Backup for centralized backup management across both regions"", ""Implement replication for application configurations"", ""Ensure all IAM roles use condition keys restricting access to specific regions""]"
u2u0j1,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster with managed node groups and Fargate profiles. The configuration must: 1. Create a custom VPC with 6 subnets (3 public, 3 private) across 3 AZs. 2. Deploy an EKS cluster with private endpoint access only. 3. Configure two managed node groups: one for general workloads (2-10 nodes) and one for compute-intensive workloads (1-5 nodes, using m5.xlarge). 4. Create a Fargate profile for running CoreDNS and other system pods in the kube-system namespace. 5. Enable EKS add-ons: VPC CNI, CoreDNS, and kube-proxy with latest compatible versions. 6. Configure OIDC provider for the cluster to enable IRSA (IAM Roles for Service Accounts). 7. Create an IAM role for the AWS Load Balancer Controller with proper trust policy. 8. Output the cluster endpoint, certificate authority data, and kubeconfig content. 9. Tag all resources with Environment=production and ManagedBy=pulumi. 10. Enable control plane logging for api, audit, authenticator, controllerManager, and scheduler. Expected output: A complete Pulumi program that provisions an EKS cluster with proper network isolation, managed node groups with auto-scaling, Fargate profile for system workloads, and necessary IAM roles for service integration. The stack should output connection details for kubectl access.",Your company is modernizing its microservices architecture by adopting Kubernetes. The DevOps team needs a reproducible way to provision EKS clusters with proper networking isolation and automated node scaling capabilities across multiple environments.,"""Production-grade EKS infrastructure deployed in us-east-1 region using AWS EKS managed Kubernetes service with Fargate profiles for system workloads and EC2 node groups for application workloads. Requires Pulumi CLI 3.x with TypeScript, kubectl 1.28+, AWS CLI v2 configured with appropriate IAM permissions. VPC architecture includes 3 public subnets for load balancers and 3 private subnets for worker nodes across availability zones us-east-1a, us-east-1b, and us-east-1c. Node groups use m5.large instances with auto-scaling configured between 2-10 nodes per AZ.""","[""EKS cluster must use Kubernetes version 1.28 or higher"", ""Worker nodes must use Bottlerocket AMI for enhanced security"", ""All node groups must have encryption at rest enabled using AWS-managed KMS keys"", ""Cluster endpoint access must be restricted to private-only mode"", ""Node groups must span exactly 3 availability zones for high availability""]"
j7n6z7,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy an observability platform for payment processing systems. MANDATORY REQUIREMENTS (Must complete): 1. Configure CloudWatch Logs groups with metric filters to detect payment failures (CORE: CloudWatch) 2. Set up service map for tracing requests across Lambda and ECS services (CORE: ) 3. Create CloudWatch dashboard with 6 widgets showing payment metrics and error rates 4. Deploy SNS topics for critical (email) and warning (SMS) alerts 5. Configure composite CloudWatch alarms for payment processing SLA violations 6. Implement Lambda function to process custom metrics from CloudWatch Logs 7. Set up CloudWatch Insights queries as saved queries for troubleshooting 8. Enable Container Insights for ECS Fargate task monitoring 9. Configure sampling rules based on request paths 10. Create CloudWatch Events rules to trigger on specific error patterns OPTIONAL ENHANCEMENTS (If time permits):  Add CloudWatch Synthetics canary for endpoint monitoring (OPTIONAL: Synthetics) - proactive issue detection  Implement EventBridge rules for audit logging (OPTIONAL: EventBridge) - improves compliance tracking  Add Systems Manager Parameter Store for dashboard URLs (OPTIONAL: SSM) - centralizes configuration Expected output: Pulumi program that creates a complete observability stack with dashboards displaying real-time metrics, service maps showing request flows, and multi-channel alerting for critical issues.",A fintech startup needs to monitor their payment processing infrastructure with detailed metrics and logs. They require real-time alerting for transaction anomalies and performance degradation. The observability stack must support distributed tracing across microservices.,"""Production monitoring infrastructure deployed in us-east-1 using CloudWatch for metrics and logs, for distributed tracing, SNS for alerting, and Lambda for custom metric processing. Requires Pulumi CLI 3.x with TypeScript, AWS CLI configured with appropriate credentials. Infrastructure monitors ECS Fargate services, Lambda functions, and API Gateway endpoints across multiple availability zones. VPC flow logs and application logs are centralized in CloudWatch Logs with metric filters for anomaly detection.""","[""Use for distributed tracing with 30% sampling rate"", ""CloudWatch dashboards must auto-refresh every 60 seconds"", ""SNS topic names must follow pattern: alerts-{severity}-{service}"", ""All Lambda functions must have tracing enabled"", ""CloudWatch alarms must use composite alarms for critical alerts"", ""Metric filters must parse JSON-structured logs only"", ""Dashboard widgets must use metric math expressions"", ""All resources must have Cost Allocation Tags: Project and Environment"", ""CloudWatch Logs retention must be 30 days for production logs""]"
l7o6g7,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to orchestrate a phased migration from legacy EC2-based infrastructure to modern serverless architecture. MANDATORY REQUIREMENTS (Must complete): 1. Deploy Lambda functions with 3GB memory in us-west-2 for payment processing logic (CORE: Lambda) 2. Create DynamoDB tables with global tables replication between regions (CORE: DynamoDB) 3. Configure Route53 hosted zone with weighted routing (80% legacy, 20% new initially) 4. Set up VPC peering connection between us-east-1 and us-west-2 regions 5. Implement for storing RDS and DynamoDB credentials 6. Create buckets for application assets with cross-region replication 7. Configure CloudWatch dashboard showing migration metrics (request count, latency, errors) 8. Export stack outputs for DNS names, VPC peering ID, and Lambda ARNs OPTIONAL ENHANCEMENTS (If time permits):  Add API Gateway with usage plans for the Lambda functions (OPTIONAL: API Gateway) - provides rate limiting and API management  Implement queues for asynchronous payment processing (OPTIONAL: ) - decouples services and improves reliability  Set up AWS X-Ray tracing across Lambda functions (OPTIONAL: X-Ray) - enables distributed tracing for debugging Expected output: A complete Pulumi TypeScript program with multiple stacks that deploys migration infrastructure across two regions, enables controlled traffic shifting, and provides monitoring capabilities for the migration process.","A financial services company needs to migrate their monolithic application from on-premises to AWS while maintaining zero downtime. The application currently serves 50,000 daily users and processes sensitive payment data. They require a phased migration approach that allows gradual traffic shifting between environments.","""Migration infrastructure spanning two AWS regions: us-east-1 (legacy) and us-west-2 (new). Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI v2 configured with multi-region access. VPC setup includes transit gateway for cross-region connectivity, private subnets in 3 AZs per region. Legacy environment runs on EC2 with RDS MySQL, migrating to serverless architecture with Lambda, API Gateway, and DynamoDB. Route53 manages DNS failover between regions. CloudWatch cross-region monitoring enabled.""","[""Use AWS Lambda for the new microservices with 3GB memory allocation"", ""Implement blue-green deployment strategy using Route53 weighted routing"", ""Configure DynamoDB with point-in-time recovery enabled"", ""Set up VPC peering between legacy and new environments"", ""Use for all database credentials"", ""Implement CloudWatch dashboards with custom metrics for migration monitoring"", ""Ensure all buckets have versioning enabled and lifecycle policies""]"
b9m4d5,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy three isolated AWS environments for a payment processing platform. The configuration must: 1. Create three separate VPCs (dev: 10.0.0.0/16, staging: 10.1.0.0/16, prod: 10.2.0.0/16) with public/private subnets across 3 AZs. 2. Deploy RDS Aurora PostgreSQL clusters with encryption using environment-specific KMS keys and automated backups. 3. Set up ECS Fargate services running a containerized payment API with CPU-based auto-scaling (min: 2, max: 10 tasks). 4. Create Lambda functions using ARM Graviton2 architecture for webhook processing with environment variables for configuration. 5. Configure DynamoDB tables for session management with on-demand billing and point-in-time recovery. 6. Deploy S3 buckets with SSE-S3 encryption and lifecycle rules to transition objects to Glacier after 90 days. 7. Create Application Load Balancers with AWS integration and rate limiting (2000 requests per 5 minutes per IP). 8. Implement CloudWatch alarms for ECS task count, RDS CPU usage, and Lambda errors with email notifications. 9. Apply consistent tagging (Environment, Project: 'payment-platform', CostCenter: 'engineering') to all resources. 10. Use Pulumi stack configurations to parameterize differences between environments. Expected output: A modular Pulumi TypeScript program with separate stack configurations for dev, staging, and production that deploys fully isolated environments with all specified services and security configurations.","A fintech startup needs to establish isolated cloud environments for their payment processing platform across development, staging, and production. Each environment requires strict network isolation, encrypted data storage, and automated scaling capabilities while maintaining cost efficiency during non-peak hours.","""Three isolated AWS environments deployed in us-east-1 for development, staging, and production. Each environment contains ECS Fargate for containerized services, RDS Aurora PostgreSQL for transactional data, DynamoDB for session storage, and Lambda functions for event processing. Infrastructure includes Application Load Balancer with AWS , S3 for static assets, and comprehensive CloudWatch monitoring. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate credentials, Node.js 18+, and Docker for local testing. VPCs use 10.0.0.0/16, 10.1.0.0/16, and 10.2.0.0/16 CIDR blocks respectively with public and private subnets across 3 availability zones.""","[""Each environment must use separate VPCs with non-overlapping CIDR blocks"", ""RDS instances must use encrypted storage with customer-managed KMS keys"", ""ECS services must auto-scale based on CPU utilization with min 2 and max 10 tasks"", ""All Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""DynamoDB tables must have point-in-time recovery enabled"", ""S3 buckets must enforce server-side encryption with lifecycle policies for 90-day archival"", ""ALB must use AWS with rate limiting rules"", ""All resources must be tagged with Environment, Project, and CostCenter tags"", ""CloudWatch alarms must trigger notifications for critical metrics""]"
n2w6b5,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a consistent payment processing infrastructure across development, staging, and production environments. The configuration must: 1. Define a reusable ComponentResource class for the complete infrastructure stack. 2. Create separate Pulumi stacks for dev, staging, and prod environments with environment-specific configurations. 3. Deploy an ECS Fargate cluster with auto-scaling based on CPU utilization (1-3 tasks for dev, 2-10 for staging/prod). 4. Set up RDS Aurora PostgreSQL with environment-appropriate instance sizes (db.t3.medium for dev, db.r5.large for staging/prod). 5. Configure DynamoDB tables with on-demand billing for dev and provisioned capacity for staging/prod. 6. Implement least-privilege IAM roles for ECS tasks with separate roles per environment. 7. Create Application Load Balancer with HTTPS listeners using ACM certificates. 8. Set up CloudWatch Log Groups with environment-specific retention periods (3 days for dev, 7 for staging, 30 for prod). 9. Export critical resource ARNs and endpoints as stack outputs for cross-stack references. 10. Ensure all resources can be destroyed cleanly with no manual intervention required. Expected output: A Pulumi TypeScript program with a main infrastructure component that can be instantiated with different configurations per environment, along with separate stack configuration files demonstrating how to deploy to each environment consistently while maintaining environment-specific settings.","A fintech startup needs to maintain identical infrastructure across development, staging, and production environments for their payment processing platform. The infrastructure must be deployed consistently across all three environments with environment-specific configurations while maintaining strict compliance requirements.","""Multi-environment AWS infrastructure deployed in us-east-2 region across three separate AWS accounts (dev, staging, prod). Each environment uses isolated VPCs with public and private subnets across 2 AZs for dev and 3 AZs for staging/prod. Core services include ECS Fargate for containerized payment processing services, RDS Aurora PostgreSQL for transaction data, and DynamoDB for session management. Requires Pulumi 3.x with TypeScript, AWS CLI configured with assume-role permissions for each account, Node.js 18+, and Docker for local testing. Network architecture includes NAT Gateways for private subnet outbound traffic and for AWS services.""","[""All environments must use identical resource naming patterns with environment prefixes"", ""Database passwords must be stored in AWS Secrets Manager with automatic rotation enabled"", ""Each environment must have its own isolated VPC with no cross-environment connectivity"", ""Use Pulumi stack references to share outputs between stacks without hardcoding values"", ""Implement resource tagging strategy that includes Environment, CostCenter, and Compliance tags"", ""Deploy all resources in us-east-2 region with multi-AZ configuration for staging and production""]"
w4p5r0,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production-ready web application infrastructure. The configuration must: 1. Set up a VPC with public and private subnets across 3 availability zones. 2. Deploy an ECS cluster with Fargate tasks running ARM64 containers for the web application. 3. Configure an Application Load Balancer with sticky sessions enabled for WebSocket support. 4. Create an Aurora PostgreSQL Serverless v2 cluster with IAM database authentication enabled. 5. Set up CloudFront distribution with S3 origin for static assets and ALB origin for dynamic content. 6. Implement Auto Scaling for ECS services based on CPU and memory metrics. 7. Configure CloudWatch Log Groups with JSON-structured logging and 30-day retention. 8. Create IAM roles following least-privilege principles for all services. 9. Implement Route 53 health checks with automated failover between regions. 10. Set up for application configuration. 11. Enable deletion protection on production resources but allow it to be toggled via configuration. 12. Tag all resources with Environment, Project, and CostCenter tags for billing tracking. Expected output: A complete Pulumi TypeScript project that deploys a fully functional web application infrastructure ready for production workloads, with all resources properly configured for high availability, security, and cost optimization.",Your company is launching a new SaaS product that requires a highly available web application with real-time features. The architecture must support WebSocket connections for live updates and handle both static content and dynamic API requests efficiently.,"""Production-grade web application infrastructure deployed in us-west-2 across 3 availability zones. Core services include ECS Fargate for container orchestration, Application Load Balancer with WebSocket support, Aurora PostgreSQL Serverless v2 for the database, and CloudFront for global content delivery. The environment requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and Docker for local testing. VPC spans three AZs with public subnets for load balancers and private subnets for compute and database resources. NAT Gateways provide outbound internet access for private resources.""","[""All compute resources must use ARM-based instances for cost optimization"", ""WebSocket connections must maintain session affinity across all load balancer layers"", ""Static assets must be served with sub-50ms latency globally"", ""Database connections must use IAM authentication instead of passwords"", ""All logs must be centralized with structured JSON formatting"", ""Infrastructure must support blue-green deployments with zero downtime"", ""Total monthly infrastructure cost must not exceed $500 for the base configuration""]"
j2y9m3,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a serverless data processing pipeline. MANDATORY REQUIREMENTS (Must complete): 1. Create an S3 bucket with event notifications that trigger a Lambda function on object creation (CORE: S3) 2. Deploy a Lambda function (Node.js 18, ARM64) that validates uploaded CSV files and writes metadata to DynamoDB (CORE: Lambda) 3. Set up a DynamoDB table to track file processing status with partition key 'fileId' and sort key 'timestamp' (CORE: DynamoDB) 4. Configure EventBridge to trigger a secondary Lambda function every 5 minutes to check for stalled processes 5. Implement SNS topic for sending processing completion notifications 6. Create proper IAM roles with specific permissions for each Lambda function 7. Set up CloudWatch Log Groups with 14-day retention for all Lambda functions 8. Configure Lambda environment variables for DynamoDB table name and SNS topic ARN 9. Enable tracing on all Lambda functions 10. Add resource tags: Environment=Production, Project=DataPipeline OPTIONAL ENHANCEMENTS (If time permits):  Add SQS queue between S3 and Lambda for better fault tolerance (OPTIONAL: SQS) - provides retry mechanism and decoupling  Implement Step Functions for complex multi-step processing workflows (OPTIONAL: Step Functions) - adds visual workflow management  Create API Gateway endpoint for manual file upload and status checking (OPTIONAL: API Gateway) - enables direct client interaction Expected output: A complete Pulumi TypeScript program that deploys the serverless data pipeline with proper error handling, monitoring, and notification capabilities. The stack should be idempotent and include proper exports for key resource identifiers.","A financial analytics startup needs to process thousands of market data files uploaded daily by partner institutions. The system must handle variable loads, ensure data integrity, and provide near real-time processing status updates to downstream consumers.","""AWS serverless infrastructure deployed in us-east-1 region using Lambda functions for data processing, DynamoDB for state management, S3 for file storage, EventBridge for orchestration, and SNS for notifications. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate credentials. Architecture uses event-driven patterns with S3 triggers initiating Lambda processing chains. DynamoDB tracks processing status and metadata while EventBridge coordinates multi-step workflows across Lambda functions.""","[""Lambda functions must use ARM64 architecture for cost optimization"", ""DynamoDB tables must use point-in-time recovery with on-demand billing"", ""All S3 buckets must have versioning enabled and lifecycle policies for 30-day object expiration"", ""Lambda functions must have reserved concurrent executions set to prevent throttling"", ""EventBridge rules must use specific event patterns, not catch-all configurations"", ""All IAM policies must follow least privilege with no wildcard actions"", ""Lambda functions must use environment variables for configuration, not hardcoded values""]"
x9t8m4,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a self-managed CI/CD pipeline that can deploy Pulumi stacks. MANDATORY REQUIREMENTS (Must complete): 1. Create CodePipeline with 3 stages: Source (GitHub webhook), Build (Pulumi preview), Deploy (Pulumi up) (CORE: CodePipeline) 2. Configure CodeBuild project with custom Docker image from ECR containing Pulumi CLI (CORE: CodeBuild) 3. Implement separate pipelines for dev, staging, and prod environments 4. Store Pulumi state in with versioning and encryption enabled 5. Configure IAM roles with cross-account assume permissions for multi-account deployments 6. Create CloudWatch EventBridge rule to trigger pipeline on git push events 7. Implement manual approval step before production deployments 8. Configure build artifacts encryption using customer-managed KMS keys 9. Set up notifications for pipeline state changes 10. Enable deletion_protection = false for all resources OPTIONAL ENHANCEMENTS (If time permits):  Add Lambda function for custom Pulumi policy validation (OPTIONAL: Lambda) - enforces organizational policies  Implement for complex multi-region deployments (OPTIONAL: ) - enables parallel deployments  Add for secrets management (OPTIONAL: ) - centralizes configuration Expected output: A Pulumi TypeScript program that creates a fully functional CI/CD pipeline capable of deploying other Pulumi stacks with proper security controls and multi-environment support.","A fintech startup needs to implement a GitOps workflow where infrastructure changes automatically deploy through a CI/CD pipeline. The pipeline must validate, test, and deploy Pulumi stacks across multiple environments while maintaining strict compliance requirements.","""AWS multi-account setup deployed in us-east-1 with CodePipeline orchestrating Pulumi deployments across dev (123456789012), staging (234567890123), and prod (345678901234) accounts. Requires Pulumi 3.x with TypeScript, AWS CLI configured with administrative access, Docker for building custom CodeBuild images. bucket for state storage with cross-account policies. for private CodeBuild execution. EventBridge integration with GitHub webhooks.""","[""CodeBuild compute type must be BUILD_GENERAL1_LARGE for faster Pulumi operations"", ""Pipeline artifacts must use SSE-KMS encryption with rotation enabled"", ""Each environment pipeline must have unique state bucket with lifecycle policies"", ""CodeBuild service role must have sts:AssumeRole permission for target accounts"", ""GitHub webhook must validate HMAC signatures for security"", ""Build logs retention must be set to 30 days with CloudWatch Logs"", ""Pipeline must support parallel execution of non-production stages"", ""Custom CodeBuild image must be scanned for vulnerabilities before use"", ""All IAM policies must follow least-privilege principle without wildcards"", ""Pipeline must implement retry logic for transient Pulumi backend failures""]"
s3f1e4,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement a multi-region disaster recovery infrastructure with automated failover. The configuration must: 1. Deploy Aurora Global Database cluster with writer in us-east-1 and read replica cluster in us-west-2. 2. Configure Application Load Balancers in both regions with health check endpoints. 3. Implement Route 53 health checks monitoring ALB endpoints with 30-second intervals. 4. Set up primary and secondary Route 53 failover routing policies for automatic DNS switching. 5. Create Lambda functions in both regions that perform application health validation. 6. Configure CloudWatch alarms for database replication lag exceeding 1 second. 7. Implement SNS topics for failover event notifications to operations team. 8. Ensure all resources use consistent tagging with Environment=Production and DR=Enabled. 9. Export critical resource IDs and endpoints for monitoring dashboard integration. Expected output: A Pulumi TypeScript program that deploys a complete multi-region infrastructure with automated failover capabilities, ensuring high availability for the payment processing application with minimal manual intervention during regional failures.",A financial services company requires a disaster recovery solution for their critical payment processing application. The system must maintain 99.95% availability with automated failover capabilities across multiple AWS regions. Recent outages have highlighted the need for a robust multi-region architecture with health checks and automatic DNS failover.,"""Multi-region AWS deployment spanning us-east-1 (primary) and us-west-2 (secondary) regions. Infrastructure includes Aurora Global Database with PostgreSQL 14.x compatibility, Application Load Balancers with target group health checks, and Route 53 failover routing policies. Requires Pulumi 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate IAM permissions. VPCs in both regions with private subnets across 3 availability zones, VPC peering for cross-region connectivity, and NAT gateways for outbound internet access. KMS keys provisioned in both regions for encryption.""","[""Primary region resources must include automated backups with point-in-time recovery enabled"", ""Health check intervals must not exceed 30 seconds with a failure threshold of 3 consecutive checks"", ""All data must be encrypted at rest using AWS KMS with customer-managed keys"", ""Cross-region replication lag must not exceed 1 second for database writes"", ""Failover DNS switching must complete within 60 seconds of health check failure""]"
e0i5e2,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a secure data processing pipeline that meets financial compliance requirements. MANDATORY REQUIREMENTS (Must complete): 1. Create a KMS key with automatic rotation enabled and alias 'data-processing-key' (CORE: KMS) 2. Deploy S3 bucket with server-side encryption using the KMS key, versioning enabled, and lifecycle rules to transition objects older than 30 days to STANDARD_IA (CORE: S3) 3. Configure bucket policies that deny any unencrypted uploads and require TLS 1.2 minimum 4. Create Lambda function in private subnet for processing data with 1GB memory and 5-minute timeout (CORE: Lambda) 5. Implement IAM role for Lambda with explicit deny for s3:DeleteBucket and kms:ScheduleKeyDeletion actions 6. Enable VPC flow logs capturing ALL traffic and store in separate S3 bucket 7. Create Secrets Manager secret for API credentials with 7-day automatic rotation 8. Configure CloudTrail trail logging all management events to dedicated S3 bucket 9. Implement security group allowing only HTTPS (443) outbound to 10. Add required tags to all resources: Environment=production, Owner=security-team, DataClassification=sensitive OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Config rules for compliance monitoring (OPTIONAL: Config) - enables automated compliance checks  Implement GuardDuty for threat detection (OPTIONAL: GuardDuty) - adds real-time security monitoring  Create Parameter Store for non-sensitive configuration (OPTIONAL: ) - centralizes configuration management Expected output: A complete Pulumi TypeScript program that deploys a hardened data processing infrastructure with encryption everywhere, comprehensive logging, and strict access controls. The solution should pass AWS security best practices checks and be ready for compliance audits.","A financial services company requires a secure data processing environment for handling sensitive customer records. The infrastructure must meet strict compliance requirements including encryption at rest, in transit, and comprehensive audit logging. All resources must be deployed with principle of least privilege and pass automated security scans.","""Secure multi-AZ deployment in us-east-1 region using Lambda for data processing, S3 for encrypted storage, and Secrets Manager for credential management. Infrastructure includes a custom VPC with private subnets across 3 availability zones, for S3 and Secrets Manager, and KMS for encryption key management. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate permissions. The environment enforces strict security controls including no internet-facing resources, all traffic routed through , and comprehensive logging to dedicated S3 buckets.""","[""All S3 buckets must use AES-256 encryption with customer-managed KMS keys"", ""Lambda functions must run in private subnets with no direct internet access"", ""All IAM roles must use explicit deny policies for sensitive actions"", ""VPC flow logs must be enabled and stored in a separate logging bucket"", ""Security groups must explicitly define all ingress and egress rules with no 0.0.0.0/0 CIDR blocks"", ""All resources must be tagged with 'Environment', 'Owner', and 'DataClassification' tags"", ""Secrets must be stored in AWS Secrets Manager with automatic rotation enabled"", ""CloudTrail must log all API calls to a dedicated S3 bucket with MFA delete protection"", ""All inter-service communication must use instead of internet gateways""]"
o0m7r0,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to optimize an existing ECS Fargate deployment for cost and performance. The configuration must: 1. Define capacity providers mixing Fargate and Fargate Spot with 70/30 weight distribution. 2. Create optimized task definitions with memory/CPU combinations that minimize waste (512MB/256 CPU units for services under 100 RPS). 3. Implement ECS Service auto-scaling policies triggered at 60% CPU utilization. 4. Configure ALB target group health checks with 15-second intervals and 2 consecutive failures. 5. Set up CloudWatch alarms for container restarts exceeding 3 per hour. 6. Create cost allocation tags following FinOps best practices. 7. Implement graceful shutdown hooks with 25-second timeout. 8. Configure service discovery using AWS Cloud Map for internal communication. 9. Set container stop timeout to 120 seconds for stateful services. 10. Create CloudWatch dashboard showing cost per transaction metrics. Expected output: Optimized Pulumi program that reduces ECS costs by 40% through efficient resource allocation, spot instance usage, and right-sizing, while maintaining high availability and sub-200ms p99 latency.",A fintech company's containerized microservices platform is experiencing cold start latencies and inefficient resource utilization. The existing ECS Fargate deployment needs optimization to reduce costs by 40% while maintaining sub-200ms response times for their payment processing services.,"""Production environment in us-east-1 with existing ECS Fargate cluster running 15 microservices across 3 availability zones. Current setup uses Application Load Balancer with target groups, RDS Aurora PostgreSQL Multi-AZ, and ElastiCache Redis cluster. VPC configured with private subnets and NAT gateways. Requires Pulumi 3.x with TypeScript, AWS SDK v3, and access to CloudWatch metrics history for optimization analysis. Monthly AWS spend exceeds $50K with 70% attributed to compute resources.""","[""Must use ECS capacity providers with mixed EC2 and Fargate strategies"", ""Implement automatic task deregistration delay of exactly 30 seconds"", ""Configure memory-to-CPU ratios that match Fargate pricing tiers exactly"", ""Use CloudWatch Container Insights for granular cost tracking"", ""Implement circuit breaker pattern with 5% error threshold"", ""Configure ECS Service Connect for zero-downtime deployments"", ""Set up predictive auto-scaling based on custom CloudWatch metrics"", ""Use AWS Compute Savings Plans tags on all compute resources"", ""Implement task placement constraints for multi-AZ distribution"", ""Configure health check grace period to minimize unnecessary restarts""]"
b7l0g1,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy AWS Config-based compliance monitoring infrastructure.

MANDATORY REQUIREMENTS (Must complete):
1. Deploy AWS Config with configuration recorder tracking all resource types (CORE: AWS Config)
2. Create S3 bucket for Config delivery with versioning and KMS encryption
3. Implement 3 AWS Config managed rules: s3-bucket-public-read-prohibited, encrypted-volumes, iam-password-policy
4. Deploy Lambda function to process Config compliance changes and format alerts (CORE: Lambda)
5. Configure SNS topic with email subscription for compliance violations
6. Set up Config delivery channel with 24-hour snapshot delivery frequency
7. Create IAM roles with least-privilege policies for Config and Lambda
8. Implement CloudWatch Logs groups with 30-day retention for all services
9. Output Config rule ARNs, S3 bucket name, and SNS topic ARN
10. Enable Config aggregator for future multi-region compliance views

OPTIONAL ENHANCEMENTS (If time permits):
 Add EventBridge rules to trigger Lambda on specific compliance changes (OPTIONAL: EventBridge) - enables event-driven processing
 Create Config dashboard using CloudWatch custom metrics (OPTIONAL: CloudWatch) - improves visibility
 Implement SQS queue for reliable alert processing (OPTIONAL: SQS) - adds message durability

Expected output: A complete Pulumi program that provisions AWS Config compliance infrastructure with automated violation alerting. The stack should handle initial deployment and updates gracefully, with all resources properly tagged and secured.",A financial services company needs automated infrastructure compliance scanning to meet SOC 2 requirements. Their cloud security team requires continuous validation of resource configurations against CIS benchmarks and custom security policies. The solution must integrate with their existing SIEM platform for centralized alerting.,"""AWS compliance monitoring infrastructure deployed in us-east-1 region. Uses AWS Config for continuous configuration recording and compliance evaluation. Lambda functions process Config rule violations and format alerts. S3 bucket stores Config snapshots and history. SNS topic distributes compliance alerts to security team. Requires Pulumi CLI 3.x with TypeScript, AWS CLI configured with appropriate permissions. Single-region deployment with Config aggregator for future multi-region expansion.""","[""Use AWS Config rules exclusively for compliance checks - no third-party scanning tools"", ""All S3 buckets must have versioning enabled and default encryption with customer-managed KMS keys"", ""Lambda functions must use Python 3.11 runtime with 256MB memory allocation"", ""Config delivery channel must use SNS topic with email subscription to security@company.com"", ""All resources must be tagged with Environment, Owner, and ComplianceLevel tags""]"
n4q2w4,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster with GitOps capabilities. The configuration must: 1. Create an EKS cluster with managed node groups using t4g.medium instances across 3 AZs. 2. Configure cluster autoscaler with IAM roles for service accounts to scale between 2-10 nodes. 3. Deploy AWS Load Balancer Controller using Helm with proper IAM permissions. 4. Install cert-manager with ClusterIssuer for Let's Encrypt staging environment. 5. Deploy ArgoCD in 'argocd' namespace with admin password stored in AWS Secrets Manager. 6. Configure FluentBit DaemonSet to ship logs to CloudWatch with proper IAM roles. 7. Enable envelope encryption for Kubernetes secrets using customer-managed KMS key. 8. Create an example Application CRD in ArgoCD pointing to a git repository. 9. Configure Network Policies to isolate ArgoCD namespace from other workloads. 10. Output the EKS cluster endpoint, ArgoCD URL, and kubeconfig command. Expected output: A Pulumi program that provisions a fully functional EKS cluster with GitOps tooling ready for microservices deployment, including all necessary IAM roles, security configurations, and monitoring setup.","A fintech startup needs to deploy their microservices architecture on AWS EKS with GitOps workflows. They require a production-grade cluster with automated certificate management, ingress control, and GitOps tooling for continuous deployment.","""Production EKS cluster deployed in eu-west-1 across 3 availability zones using AWS EKS managed service with Graviton3 nodes. Requires Pulumi 3.x with TypeScript, kubectl 1.27+, helm 3.x, AWS CLI v2 configured. VPC with private subnets for worker nodes, public subnets for load balancers. NAT gateways in each AZ for outbound connectivity. AWS Load Balancer Controller for ingress, Cert-manager for TLS certificates, ArgoCD for GitOps deployments.""","[""EKS cluster must use Kubernetes version 1.27 or higher"", ""Node groups must use Graviton3 instances (t4g.medium) for cost optimization"", ""Cluster autoscaler must scale between 2-10 nodes based on workload"", ""OIDC provider must be configured for IAM roles for service accounts"", ""Ingress controller must use AWS Load Balancer Controller v2.6+"", ""Cert-manager must automatically provision Let's Encrypt certificates"", ""ArgoCD must be deployed in a dedicated namespace with RBAC configured"", ""All node groups must use private subnets with no public IP assignment"", ""Kubernetes secrets must be encrypted using AWS KMS with customer-managed keys"", ""FluentBit must forward all container logs to CloudWatch Logs with 30-day retention""]"
g9y0y9,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to deploy a multi-region disaster recovery infrastructure. The configuration must: 1. Set up Aurora Global Database with primary cluster in us-east-1 and secondary in us-west-2. 2. Configure DynamoDB global tables for session data with on-demand billing mode. 3. Implement Route53 failover routing with primary and secondary record sets. 4. Create S3 buckets in both regions with cross-region replication rules. 5. Deploy Lambda functions in both regions with environment-specific configurations. 6. Set up EventBridge rules to replicate critical events between regions. 7. Configure VPC peering with appropriate route tables and security groups. 8. Implement CloudWatch dashboards showing cross-region metrics. 9. Create topics in both regions for failure notifications. 10. Deploy Application Load Balancers with target groups in each region. 11. Configure AWS Backup plans with cross-region copy enabled. 12. Set up CloudWatch Synthetics canaries to monitor endpoints. Expected output: A Pulumi program that provisions complete disaster recovery infrastructure with automated failover capabilities, achieving RTO under 5 minutes and RPO under 1 hour.",A financial services company requires a disaster recovery solution for their payment processing system that can failover between AWS regions within 5 minutes. The system processes time-sensitive transactions and must maintain data consistency across regions while meeting strict RTO/RPO requirements.,"""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary) regions. Utilizes Aurora PostgreSQL 15.x Global Database for transactional data, DynamoDB global tables for session management, and S3 with cross-region replication for static assets. Each region contains a VPC with 3 availability zones, private subnets for databases, public subnets for ALBs, and for inter-region connectivity. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate IAM permissions for multi-region deployments. CloudWatch Synthetics monitors endpoint availability.""","[""Use Aurora Global Database with automated backpoint-in-time recovery"", ""Implement Route53 health checks with 10-second intervals"", ""Configure DynamoDB global tables with contributor insights enabled"", ""Set up cross-region VPC peering with attachments"", ""Use AWS Backup for centralized backup management with 1-hour RPO"", ""Implement Lambda@Edge functions for request routing logic"", ""Configure S3 cross-region replication with RTC (Replication Time Control)"", ""Use EventBridge global endpoints for event routing between regions"", ""Implement CloudWatch cross-region metric streams for unified monitoring""]"
n1i3e0,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy an EKS cluster with Fargate-only compute profiles for containerized workloads.

MANDATORY REQUIREMENTS (Must complete):
1. Create EKS cluster version 1.28 with OIDC provider enabled (CORE: EKS)
2. Deploy two Fargate profiles: one for 'development' namespace, one for 'production' namespace (CORE: Fargate)
3. Configure separate IAM roles for each Fargate profile with minimal permissions
4. Patch CoreDNS deployment to include compute-type: fargate annotation
5. Install AWS Load Balancer Controller as Helm chart with IRSA configuration
6. Create NetworkPolicy resources to block cross-namespace traffic
7. Configure pod security standards with baseline enforcement
8. Output cluster endpoint, certificate authority data, and kubeconfig command

OPTIONAL ENHANCEMENTS (If time permits):
 Add EBS CSI driver with IRSA for persistent volumes (OPTIONAL: EBS) - enables stateful workloads
 Implement Karpenter for mixed Fargate/EC2 workloads (OPTIONAL: EC2 Auto Scaling) - adds cost optimization
 Configure Container Insights for monitoring (OPTIONAL: CloudWatch) - improves observability

Expected output: Complete Pulumi program that provisions EKS cluster with Fargate profiles, proper IAM roles, network isolation, and returns connection details for kubectl access.",A fintech startup needs to deploy their microservices architecture on AWS EKS with Fargate profiles to eliminate node management overhead. The platform must support both stateless API services and stateful data processing workloads with proper isolation between development and production namespaces.,"""AWS EKS 1.28 cluster deployed in us-east-2 region using Fargate profiles for compute. VPC with 3 availability zones, private subnets for pods, public subnets for load balancers. Requires Pulumi 3.x with TypeScript, AWS CLI v2 configured, kubectl 1.28+. Architecture includes Application Load Balancer for ingress, AWS Load Balancer Controller addon, EBS CSI driver for persistent volumes. IAM OIDC provider enables pod-level AWS permissions. Fargate profiles target specific namespaces with dedicated pod execution roles.""","[""EKS cluster must use Fargate profiles exclusively - no EC2 node groups allowed"", ""Each namespace must have its own Fargate profile with specific pod execution role"", ""OIDC provider must be configured for IRSA (IAM Roles for Service Accounts)"", ""CoreDNS must be patched to run on Fargate compute"", ""Pod security standards must enforce baseline policy at namespace level"", ""Network policies must restrict cross-namespace communication except for shared services"", ""All container images must be pulled from private ECR repositories only""]"
h5e1y3,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to orchestrate a complete on-premises to AWS migration infrastructure. The configuration must: 1. Create a VPC with public and private subnets across 3 availability zones for high availability. 2. Set up Site-to-Site VPN connection with customer gateway configuration for secure on-premises connectivity. 3. Deploy Aurora MySQL cluster with one writer and two read replicas for database tier. 4. Configure AWS DMS replication instance and tasks for continuous database synchronization. 5. Implement ECS service with blue-green deployment using two target groups. 6. Set up Application Load Balancer with path-based routing and health checks. 7. Create DataSync locations and tasks for migrating file storage from on-premises NFS. 8. Configure with automatic rotation Lambda for database credentials. 9. Implement Systems Manager Session Manager for bastion-less access to private resources. 10. Set up CloudWatch dashboards monitoring migration progress and application health. 11. Create topics for migration alerts with email subscriptions. 12. Tag all resources with migration phase tracking (pre-migration, migration, post-migration). Expected output: A Pulumi TypeScript program that creates a complete migration infrastructure enabling zero-downtime transition from on-premises to AWS, with automated database replication, file synchronization, and blue-green deployment capabilities for safe cutover.",A financial services company needs to migrate their legacy monolithic application from on-premises to AWS. The application currently runs on dedicated servers with a MySQL database and file storage. They require a phased migration approach to minimize downtime and risk.,"""Multi-AZ infrastructure deployment in us-east-1 region for migrating on-premises financial application to AWS cloud. Uses ECS for containerized application hosting, Aurora MySQL for database with read replicas across 3 AZs, Application Load Balancer for traffic distribution. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate credentials. VPC with public and private subnets, Site-to-Site VPN for hybrid connectivity during migration phase. AWS DMS for database replication, DataSync for file migration, Systems Manager for secure access.""","[""Use AWS Database Migration Service (DMS) for database migration with continuous replication enabled"", ""Implement blue-green deployment strategy using Application Load Balancer target group switching"", ""Configure read replicas in at least two different availability zones"", ""Use AWS DataSync for initial bulk file transfer from on-premises storage"", ""Set up VPN connection between on-premises network and AWS VPC for secure migration"", ""Implement AWS Systems Manager Session Manager for secure instance access without SSH keys"", ""Configure rotation for all database credentials with 30-day rotation schedule"", ""Use CloudFormation custom resources for migration status tracking and rollback capabilities"", ""Implement cost allocation tags with environment, application, and migration-phase values""]"
e1e4f7,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to deploy a multi-region analytics processing environment. The configuration must: 1. Create VPCs in us-east-1 (10.1.0.0/16) and eu-west-1 (10.2.0.0/16) with 3 availability zones each. 2. Establish VPC peering between regions with appropriate route table updates. 3. Deploy Lambda functions using ARM Graviton2 architecture for processing market data streams. 4. Create DynamoDB Global Tables with on-demand capacity and point-in-time recovery enabled. 5. Configure S3 buckets in both regions with cross-region replication, versioning, and 90-day Glacier transition. 6. Implement KMS keys in each region for encrypting all data at rest. 7. Set up Route 53 hosted zone with health checks and 60-second failover between regions. 8. Create CloudWatch Log Groups with 30-day retention for all Lambda functions. 9. Implement IAM roles with explicit permissions (no wildcards) for each service. 10. Add resource tags for cost allocation: Environment=Production, Project=TradingAnalytics. Expected output: A complete Pulumi TypeScript program that provisions the multi-region infrastructure with proper error handling, type safety, and stack outputs displaying endpoint URLs, VPC IDs, and peering connection status.",A financial services company needs to establish a new cloud environment for their trading analytics platform. The system must process real-time market data and serve results to global clients with minimal latency. Regulatory requirements mandate data residency in specific regions and comprehensive audit logging.,"""Multi-region AWS deployment spanning us-east-1 (primary) and eu-west-1 (failover). Core services include Lambda functions on ARM Graviton2 for data processing, DynamoDB Global Tables for distributed storage, S3 with cross-region replication, and Route 53 for DNS failover. Each region has isolated VPCs (10.1.0.0/16 for us-east-1, 10.2.0.0/16 for eu-west-1) connected via peering. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, and AWS CLI configured with appropriate credentials. Infrastructure includes NAT Gateways in each region for outbound connectivity from private subnets.""","[""Primary region must be us-east-1 with failover to eu-west-1"", ""All data at rest must use KMS encryption with customer-managed keys"", ""VPC peering must be established between regions with non-overlapping CIDR blocks"", ""Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""DynamoDB tables must use point-in-time recovery and on-demand billing"", ""All IAM roles must follow least privilege with no wildcard actions"", ""CloudWatch Logs retention must be exactly 30 days for compliance"", ""S3 buckets must have versioning enabled and lifecycle policies for 90-day archival"", ""Route 53 health checks must trigger automatic failover within 60 seconds""]"
d8d9u8,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to manage infrastructure replication across three AWS environments (dev, staging, production) with automated consistency checks. The configuration must: 1. Define a base infrastructure component that includes API Gateway REST API, two Lambda functions (order processing and notification), and a DynamoDB table. 2. Create reusable Pulumi ComponentResource classes for each service type with environment-agnostic configurations. 3. Implement environment-specific parameter overrides for Lambda memory (dev: 256MB, staging: 512MB, prod: 1024MB) while keeping all other settings identical. 4. Use Pulumi StackReferences to share API Gateway endpoint URLs between environments for integration testing. 5. Configure DynamoDB point-in-time recovery only for production, with on-demand billing for all environments. 6. Set up S3 buckets with environment-specific naming but identical lifecycle policies (90-day transition to Glacier). 7. Create a validation function that compares resource configurations across stacks and reports any unintended differences. 8. Implement IAM roles with cross-environment trust relationships for deployment automation. 9. Configure CloudWatch alarms with environment-appropriate thresholds (lower for dev/staging). 10. Output a comparison report showing configuration differences between environments. Expected output: A Pulumi program with separate stack configurations for each environment, reusable components ensuring consistency, and automated validation that flags any configuration drift between environments.","A fintech startup needs to maintain identical infrastructure across development, staging, and production environments to ensure consistent testing and deployment. They've experienced configuration drift issues where staging doesn't match production, leading to failed deployments and rollback scenarios.","""Multi-environment AWS infrastructure spanning three accounts (dev, staging, prod) in us-east-1 region. Each environment consists of API Gateway, Lambda functions, DynamoDB tables, and S3 buckets. Requires Pulumi 3.x with TypeScript, AWS CLI configured with cross-account assume role permissions. VPCs in each environment with private subnets for Lambda functions, VPC endpoints for AWS services. Uses AWS Organizations for account management with SCPs enforcing security baselines.""","[""Use Pulumi StackReferences to share outputs between environment stacks"", ""Implement strict type definitions for all cross-environment configurations"", ""Environment-specific values must be stored in Pulumi , not hardcoded"", ""All Lambda functions must use identical runtime versions across environments"", ""Database schemas must be versioned and synchronized using AWS DMS"", ""Use resource tagging strategy that includes environment, version, and deployment timestamp"", ""Implement automated drift detection between environments using Pulumi's diff capabilities""]"
n3t8j1,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a highly available payment processing web application. The configuration must: 1. Set up a VPC with public and private subnets across 3 availability zones. 2. Deploy an ECS Fargate service running a containerized Node.js application with auto-scaling. 3. Create an RDS Aurora PostgreSQL cluster with Multi-AZ deployment and automated backups. 4. Configure an Application Load Balancer with AWS integration and health checks. 5. Implement S3 buckets for static content delivery and database storage. 6. Set up CloudWatch Log Groups with 90-day retention for application and database logs. 7. Create KMS keys for encrypting RDS, S3, and ECS task definition secrets. 8. Configure IAM roles with least-privilege access for all services. 9. Implement CloudWatch alarms for CPU, memory, and database connection metrics. 10. Apply consistent tagging strategy across all resources. 11. Set up route tables and security groups with minimal required access. 12. Configure entries for database connection strings and API keys. Expected output: A complete Pulumi TypeScript program with modular components that deploys the entire infrastructure stack, outputs the ALB URL, and exports critical resource IDs for reference.",A fintech startup needs to deploy their payment processing web application with strict compliance requirements. The application must handle sensitive financial data with proper encryption and access controls while maintaining high availability across multiple regions.,"""Production deployment in us-east-1 with disaster recovery in us-west-2. Uses Application Load Balancer with , ECS Fargate for containerized Node.js application, RDS Aurora PostgreSQL Multi-AZ cluster, S3 for static assets and backups. VPC spans 3 availability zones with public subnets for ALB and private subnets for ECS and RDS. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate IAM permissions, Docker for local testing. NAT Gateways provide outbound internet access for private resources.""","[""All data must be encrypted at rest using AWS KMS with customer-managed keys"", ""Application logs must be retained for exactly 90 days for compliance"", ""Database backups must occur daily with 30-day retention"", ""All resources must be tagged with Environment, CostCenter, and Compliance tags"", ""API endpoints must implement rate limiting of 1000 requests per minute per IP"", ""Database connections must use SSL/TLS with certificate validation"", ""Application must auto-scale between 2 and 10 instances based on CPU utilization"", ""Health checks must fail after 3 consecutive failures with 30-second intervals"", ""All S3 buckets must have versioning enabled and lifecycle policies"", ""Network traffic must flow through AWS with OWASP Top 10 protection""]"
f0z6s0,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a serverless payment webhook processor. MANDATORY REQUIREMENTS (Must complete): 1. Create three Lambda functions: webhook receiver (validates signatures), data transformer (normalizes payment data), and analytics processor (aggregates metrics) (CORE: Lambda) 2. Set up DynamoDB table 'payment-events' with partition key 'transactionId' and sort key 'timestamp', plus GSI on 'merchantId' (CORE: DynamoDB) 3. Configure state machine to orchestrate the three Lambda functions in sequence (CORE: ) 4. Implement queue 'payment-queue' with visibility timeout of 300 seconds and DLQ 'payment-dlq' 5. Create EventBridge rule to trigger execution when messages arrive in 6. Set up IAM roles with least privilege - Lambda can only read/write specific DynamoDB table 7. Configure CloudWatch Log Groups with 30-day retention for all Lambda functions 8. Implement KMS key for encrypting environment variables and DynamoDB data 9. Add CloudWatch alarms for Lambda errors exceeding 1% error rate 10. Export ARNs of all Lambda functions, DynamoDB table name, and state machine ARN OPTIONAL ENHANCEMENTS (If time permits):  Add API Gateway REST API for manual webhook testing (OPTIONAL: API Gateway) - enables direct endpoint testing  Implement topic for alerting on payment processing failures (OPTIONAL: ) - improves incident response  Add Kinesis Data Firehose for archiving processed events to S3 (OPTIONAL: Kinesis Firehose) - enables long-term analytics Expected output: Complete Pulumi TypeScript program that deploys a production-ready serverless payment processing pipeline with proper error handling, monitoring, and security configurations. The stack should be idempotent and include comprehensive resource tagging.","A fintech startup needs a serverless event processing system to handle real-time payment notifications from multiple payment providers. The system must process webhook events, validate signatures, transform data, and store results for analytics while maintaining PCI compliance standards.","""Production serverless infrastructure deployed in us-east-1 region using AWS Lambda for event processing, DynamoDB for data storage, for message queuing, and for workflow orchestration. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS credentials configured. All components deployed within a single AWS account with CloudWatch Logs for monitoring. Lambda functions execute in VPC with private subnets for enhanced security. KMS customer managed key for encryption at rest.""","[""Lambda functions must use Node.js 18.x runtime with 1GB memory allocation"", ""DynamoDB tables must use on-demand billing mode with point-in-time recovery enabled"", ""All Lambda functions must have tracing enabled for debugging"", ""Environment variables containing sensitive data must be encrypted using KMS"", ""Lambda functions must have reserved concurrent executions set to 100"", ""DynamoDB global secondary indexes must project only required attributes"", ""All resources must be tagged with Environment, Project, and CostCenter tags"", ""Lambda functions must use arm64 architecture for cost optimization"", ""Dead letter queues must have message retention period of exactly 14 days"", ""Lambda timeout must not exceed 5 minutes for any function""]"
f1r9w0,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to implement a complete CI/CD pipeline for containerized applications. The configuration must: 1. Set up with Source stage pulling from GitHub using OAuth token stored in Secrets Manager. 2. Configure Build stage using CodeBuild to build Docker images with buildspec.yml from repository root. 3. Implement Test stage running automated security scans using CodeBuild with Trivy scanner. 4. Create ECR repository with vulnerability scanning on push and retention of last 10 images. 5. Deploy ECS Fargate service with Application Load Balancer in public subnets. 6. Configure for blue-green deployments with linear 10% traffic shift every 5 minutes. 7. Set up CloudWatch Events rule to trigger pipeline on ECR image pushes. 8. Create IAM roles with least privilege for each pipeline component. 9. Configure S3 bucket for pipeline artifacts with server-side encryption. 10. Implement entries for environment-specific configurations. 11. Add CloudWatch Log Groups for CodeBuild projects with 7-day retention. 12. Export pipeline execution ARN and ECS service URL as stack outputs. Expected output: A Pulumi program that creates a fully functional CI/CD pipeline capable of building, testing, and deploying containerized applications with blue-green deployment strategy, automated security scanning, and proper monitoring across AWS services.","A software development team needs to establish a CI/CD pipeline for their containerized Node.js application. The pipeline should automatically build Docker images, run security scans, and deploy to ECS Fargate across multiple environments. The infrastructure must support blue-green deployments and integrate with their existing GitHub repository.","""AWS multi-account setup deployed in us-east-1 region. Primary services include for orchestration, CodeBuild for Docker image builds, ECR for container registry, ECS Fargate for application hosting, and for blue-green deployments. Infrastructure spans across development and production AWS accounts with cross-account IAM roles. Requires Pulumi CLI 3.x with TypeScript, Docker installed locally, Node.js 16+, and AWS CLI configured with appropriate credentials. VPC configuration includes public subnets for ALB and private subnets for ECS tasks across 2 availability zones.""","[""Use with exactly 4 stages: Source, Build, Test, and Deploy"", ""Configure CodeBuild to use compute type BUILD_GENERAL1_SMALL for cost optimization"", ""Store Docker images in ECR with image scanning enabled and lifecycle policy of 10 images"", ""Deploy to ECS Fargate with task memory of 1024 and CPU of 512"", ""Implement blue-green deployment using with 5-minute traffic shifting"", ""Use for storing deployment configuration with /pipeline/* prefix"", ""Enable CloudWatch Events to trigger pipeline on ECR image push events"", ""Configure S3 artifact store with versioning enabled and 30-day lifecycle policy""]"
q4c1o5,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement an active-passive disaster recovery architecture for a trading application. MANDATORY REQUIREMENTS (Must complete): 1. Deploy Aurora Global Database cluster with writer in us-east-1 and reader in us-west-2 (CORE: Aurora) 2. Configure DynamoDB global tables for session state across both regions (CORE: DynamoDB) 3. Create Route 53 hosted zone with primary and secondary record sets using failover routing 4. Implement health check Lambda functions in each region that verify database connectivity 5. Configure CloudWatch alarms to trigger on replication lag > 1 second 6. Set up EventBridge rules to invoke failover Lambda when primary region fails 7. Create SNS topics in both regions for alerting on failover events 8. Ensure all database resources have automated backups with 7-day retention OPTIONAL ENHANCEMENTS (If time permits):  Add Step Functions state machine for orchestrated failover (OPTIONAL: Step Functions) - provides better failover workflow visibility  Implement AWS Backup for centralized backup management (OPTIONAL: AWS Backup) - simplifies compliance reporting  Add for configuration management (OPTIONAL: ) - enables dynamic configuration updates Expected output: A complete Pulumi TypeScript program with stack exports showing primary/secondary endpoints, health check URLs, and failover status. The solution should demonstrate automatic failover when the primary region becomes unhealthy.",A financial services company needs a disaster recovery solution for their critical trading application. The system must maintain near-zero RPO and sub-minute RTO requirements while ensuring data consistency across regions during failover events.,"""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary). Deploys Aurora Global Database with automated failover, cross-region DynamoDB global tables, and Route 53 health-check based traffic routing. Each region contains private VPCs with 3 availability zones, NAT gateways for outbound traffic, and VPC peering for cross-region communication. Requires Pulumi 3.x with TypeScript, AWS CLI v2 configured with appropriate permissions, Node.js 18.x for Lambda runtime compatibility. Infrastructure includes automated failover Lambda functions triggered by CloudWatch Events monitoring database health metrics.""","[""Aurora Global Database must use PostgreSQL 14.x with backtrack enabled"", ""Route 53 health checks must monitor both database and application endpoints"", ""Lambda functions must use Node.js 18.x runtime with 3GB memory allocation"", ""DynamoDB global tables must use on-demand capacity with point-in-time recovery"", ""All resources must be tagged with Environment, Application, and CostCenter tags"", ""Cross-region replication lag must not exceed 1 second under normal conditions"", ""Failover automation must complete within 60 seconds of detection"", ""Secondary region resources must use deletion protection until manually disabled""]"
f3p8q6,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a zero-trust security framework for containerized microservices. The configuration must: 1. Create a VPC with 3 private subnets across different AZs with no internet connectivity. 2. Deploy for , DynamoDB, ECR, and Secrets Manager services. 3. Configure AWS Secrets Manager with automatic rotation for database credentials. 4. Set up KMS customer-managed keys with key policies restricting usage to specific IAM roles. 5. Create ECS task definitions with IAM roles that can only access their assigned secrets. 6. Configure with CIS benchmark and custom compliance rules. 7. Enable GuardDuty with EventBridge rules triggering remediation functions. 8. Implement CloudTrail with bucket using SSE-KMS encryption and object lock. 9. Create security groups with explicit rules allowing only necessary port/protocol combinations. 10. Deploy functions for automated security response to GuardDuty findings. Expected output: A complete Pulumi program that provisions a hardened AWS environment with defense-in-depth security controls, automated compliance monitoring, and incident response capabilities suitable for SOX-compliant financial services.","A financial services company needs to implement a zero-trust security architecture for their microservices platform. The security team requires strict network isolation, encrypted secrets management, and comprehensive audit logging to meet SOX compliance requirements.","""Zero-trust security infrastructure deployed in us-east-1 with VPC spanning 3 availability zones. Core services include ECS for container workloads, AWS Secrets Manager for credentials, KMS for encryption keys, for compliance monitoring, and GuardDuty for threat detection. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions. VPC uses private subnets only with for , DynamoDB, ECR, and Secrets Manager. No internet gateway or allowed for maximum isolation.""","[""All secrets must be stored in AWS Secrets Manager with automatic rotation enabled"", "" must be used for all AWS service communications to avoid internet exposure"", ""Each microservice must run in its own security group with explicit ingress/egress rules"", ""IAM roles must follow least-privilege principle with no inline policies"", ""All data at rest must use KMS encryption with customer-managed keys"", ""Network traffic between services must use mTLS with certificate validation"", ""CloudTrail must log all API calls with 90-day retention for compliance"", "" must be enabled with CIS AWS Foundations Benchmark v1.4.0"", ""GuardDuty findings must trigger automated remediation via functions""]"
m8q8s6,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to optimize an existing ECS Fargate infrastructure for a payment processing application. The configuration must: 1. Refactor ECS task definitions to use 0.5 vCPU and 1GB memory instead of current 2 vCPU and 4GB settings. 2. Implement target tracking autoscaling based on ALB request count with 60-second scale-out cooldown. 3. Configure RDS Aurora Serverless v2 with minimum 0.5 ACUs and maximum 1 ACU for development workloads. 4. Enable ECS Service Connect for service-to-service communication without ALB overhead. 5. Implement CloudWatch Container Insights with custom metrics for memory utilization. 6. Create separate task definitions for CPU-bound and I/O-bound workloads with optimized resource allocation. 7. Configure spot capacity providers with 70% spot and 30% on-demand split for non-critical services. 8. Set up CloudWatch alarms for container restart counts exceeding 3 within 5 minutes. Expected output: Optimized Pulumi TypeScript code that reduces infrastructure costs while maintaining performance SLAs. The program should demonstrate proper resource tagging, use of Pulumi components for reusability, and include stack configuration for dev/staging/prod environments.","A fintech startup's ECS Fargate deployment suffers from slow cold starts and inefficient resource allocation, causing customer complaints during peak trading hours. The existing Pulumi code creates oversized task definitions and lacks proper autoscaling policies. Management demands immediate optimization to reduce costs by 40% while improving response times.","""Multi-AZ deployment in us-east-1 optimizing existing ECS Fargate cluster with 15 microservices, RDS Aurora PostgreSQL 15.4, and Application Load Balancer. Current monthly spend $8,000, target $4,800. Requires Pulumi 3.x, TypeScript 4.9+, AWS CLI 2.x configured. VPC with 3 private subnets across availability zones. ECS cluster currently runs 50 tasks during peak hours with significant overprovisioning. Development team uses macOS/Linux environments with Docker Desktop installed.""","[""Task definitions must use Fargate platform version 1.4.0 or later for ECS Service Connect support"", ""All CloudWatch log groups must have KMS encryption enabled using customer-managed keys"", ""Aurora Serverless v2 must have deletion protection disabled for development environments only"", ""ECS services must maintain at least 2 running tasks for high availability"", ""Spot instances must not be used for payment processing services marked with 'critical=true' tag"", ""Container health checks must use TCP checks on port 8080 with 30-second intervals"", ""All resources must include cost allocation tags: environment, service, team, and cost-center"", ""Pulumi stack exports must include cluster ARN, service URLs, and monthly cost estimate""]"
m0q0b8,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to implement an automated infrastructure compliance and drift detection system. The configuration must: 1. Deploy AWS Config rules to monitor EC2 instances for required tags (Environment, Owner, CostCenter) across 3 AWS accounts 2. Create Lambda functions to analyze Config compliance data and detect configuration drift from approved baselines stored in DynamoDB 3. Build a DynamoDB table with composite keys (AccountId, ResourceId) to store approved resource configurations and drift history 4. Implement cross-account IAM roles using AssumeRole patterns for Config aggregator access 5. Configure topics with email subscriptions for critical compliance violations (severity >= 8) 6. Generate CloudWatch dashboards showing compliance percentages per account and resource type 7. Create EventBridge rules to trigger drift analysis every 6 hours and on Config rule evaluations 8. Export compliance reports to S3 buckets with lifecycle policies for 90-day retention. Expected output: A Pulumi program that deploys a multi-account compliance monitoring system with automated drift detection, real-time alerting, and comprehensive reporting capabilities for infrastructure governance teams.",A financial services company needs automated infrastructure compliance monitoring across multiple AWS accounts. Their audit team requires real-time detection of configuration drift and policy violations with detailed reporting capabilities.,"""Multi-account AWS infrastructure deployed across us-east-1 (primary) and us-west-2 (secondary) regions. Requires AWS Config enabled in all target accounts, Pulumi 3.x with TypeScript, Node.js 18+, and AWS CLI configured with organization management permissions. Architecture includes cross-account IAM roles, Config aggregator in management account, Lambda functions for analysis, DynamoDB for state storage, and EventBridge for orchestration. required for private Lambda execution.""","[""AWS Config rules must evaluate resources within 15 minutes of configuration changes"", ""Lambda functions must complete drift analysis within 5-minute timeout limits"", ""DynamoDB table must use on-demand billing with point-in-time recovery enabled"", ""Cross-account roles must follow least-privilege principles with explicit resource ARNs"", "" topics must implement message filtering based on compliance severity levels"", ""CloudWatch dashboards must auto-refresh every 5 minutes with custom metrics"", ""EventBridge rules must include error handling with exponential backoff retry logic"", ""S3 buckets must enable versioning and server-side encryption with AWS managed keys""]"
q6m8k8,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production-ready Amazon EKS cluster with enhanced security configurations.

MANDATORY REQUIREMENTS (Must complete):
1. Create an EKS cluster with Kubernetes 1.28+ and enable all control plane logging types (CORE: EKS)
2. Deploy 2 managed node groups: 'critical' (2-4 nodes) and 'general' (1-3 nodes) using Bottlerocket AMI
3. Configure Application Load Balancer with target group health checks (CORE: ALB)
4. Set up OIDC provider and create 3 IRSA roles: cluster-autoscaler, alb-controller, cloudwatch-agent
5. Install AWS Load Balancer Controller using Helm with service account annotation
6. Configure VPC CNI addon with version 1.15.0 and enable pod security groups
7. Implement node group taints: critical nodes with 'workload=critical:NoSchedule'
8. Create CloudWatch log groups with 30-day retention for all enabled log types
9. Output cluster endpoint, OIDC issuer URL, and kubectl config command
10. Apply mandatory tags: Environment=Production, ManagedBy=Pulumi, CostCenter=Engineering

OPTIONAL ENHANCEMENTS (If time permits):
 Add AWS Systems Manager for node management (OPTIONAL: SSM) - enables secure node access without SSH
 Implement Cluster Autoscaler with IRSA (OPTIONAL: Auto Scaling) - provides automatic node scaling
 Deploy Amazon EBS CSI driver (OPTIONAL: EBS) - enables persistent volume support

Expected output: Complete Pulumi TypeScript program that provisions a secure EKS cluster with managed node groups, ALB ingress controller, and comprehensive logging. The cluster should be production-ready with proper RBAC, IRSA roles, and network isolation.",A fintech company needs to deploy their microservices architecture on Amazon EKS to meet regulatory requirements for workload isolation and audit logging. The platform must support automatic scaling based on CPU metrics and provide secure ingress for external API consumers while maintaining strict network segmentation between development and production workloads.,"""Production-grade EKS cluster deployed in us-east-2 region across 3 availability zones. Infrastructure includes EKS control plane with managed node groups using Bottlerocket OS, Application Load Balancer for ingress, and comprehensive CloudWatch logging. Requires Pulumi CLI 3.x with TypeScript, AWS CLI configured with appropriate permissions, kubectl 1.28+, and helm 3.x. VPC architecture includes public subnets for ALB and private subnets for worker nodes with NAT gateways for outbound connectivity.""","[""EKS cluster must use Kubernetes version 1.28 or higher"", ""Worker nodes must use Bottlerocket AMI for enhanced security"", ""Each node group must span exactly 3 availability zones"", ""OIDC provider must be configured for IRSA (IAM Roles for Service Accounts)"", ""Control plane logging must include api, audit, authenticator, controllerManager, and scheduler"", ""Node groups must use only t3.medium instances with spot capacity"", ""VPC CNI addon version must be explicitly pinned to v1.15.0""]"
j0o0x9,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement an active-passive disaster recovery architecture across two AWS regions. The configuration must: 1. Deploy Aurora PostgreSQL global database with automated backups every 6 hours. 2. Create ECS Fargate services in both regions with identical task definitions. 3. Configure S3 buckets with cross-region replication and versioning enabled. 4. Implement Route53 failover routing with health checks on primary region endpoints. 5. Deploy DynamoDB global tables for session management with point-in-time recovery. 6. Set up distribution with origin groups for automatic failover. 7. Create Lambda@Edge functions to modify headers during regional failover. 8. Configure EventBridge rules to trigger ECS service updates during failover events. 9. Implement CloudWatch alarms for RTO monitoring with notifications. 10. Use to manage database connection strings across regions. Expected output: A Pulumi program that provisions a complete multi-region DR infrastructure with automated failover capabilities, maintaining data consistency and achieving sub-5-minute RTO.",A financial services company requires a disaster recovery solution that can automatically failover their critical transaction processing system between AWS regions. The system must maintain data consistency across regions and minimize Recovery Time Objective (RTO) to under 5 minutes.,"""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and eu-west-1 (secondary). Deploys Aurora Global Database clusters, ECS Fargate services, DynamoDB global tables, S3 with cross-region replication, distribution with origin failover, Route53 hosted zones with health checks. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions. VPCs in both regions with private subnets for databases and application tiers. NAT Gateways for outbound connectivity. for cross-region VPC peering.""","[""Use AWS Route53 health checks with automatic DNS failover between regions"", ""Deploy Aurora Global Database with exactly one primary and one secondary region"", ""Configure S3 bucket replication with RTC (Replication Time Control) enabled"", ""Implement Lambda@Edge functions for request routing during failover"", ""Use DynamoDB global tables for session state management"", ""Configure with multiple origin failover groups"", ""Set Aurora read replica promotion time to under 60 seconds"", ""Implement automated ECS task migration between regions using EventBridge"", ""Use for cross-region configuration synchronization""]"
g1f6g6,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a containerized payment processing system with microservices architecture. The configuration must: 1. Create a VPC with 3 public and 3 private subnets across different AZs. 2. Deploy an ECS cluster using Fargate with 3 microservices: payment-api, transaction-processor, and notification-service. 3. Configure Application Load Balancer with path-based routing to different services. 4. Implement auto-scaling for each service based on SQS queue depth metrics. 5. Set up AWS App Mesh with virtual services and circuit breaker configuration. 6. Configure service discovery for inter-service communication. 7. Create Aurora Serverless v2 cluster with encryption at rest. 8. Implement blue-green deployment using ECS deployment circuit breaker. 9. Configure ECR repositories with image scanning on push. 10. Set up CloudWatch Container Insights with custom dashboards. 11. Create SQS queues with DLQ for failed transactions. 12. Implement proper IAM roles with task execution and task roles separation. Expected output: A complete Pulumi program that deploys the containerized infrastructure with automated scaling, service mesh integration, and zero-downtime deployment capabilities. The system should handle 10,000 transactions per minute with sub-second response times.","A fintech startup needs to migrate their monolithic payment processing application to a microservices architecture on AWS ECS. The system handles sensitive financial transactions requiring strict network isolation, automated scaling based on queue depth, and zero-downtime deployments with automated rollback capabilities.","""Production payment processing infrastructure deployed in us-east-1 across 3 availability zones using AWS ECS Fargate for container orchestration, Aurora PostgreSQL Serverless v2 for transaction storage, and SQS for asynchronous message processing. VPC configured with private subnets for ECS tasks, public subnets for ALB, and for cross-account connectivity. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, Docker Desktop for local testing. Infrastructure includes AWS App Mesh for service mesh capabilities, for service discovery, and ECR for container registry with vulnerability scanning enabled.""","[""ECS tasks must run in private subnets with no direct internet access"", ""Container images must be scanned for vulnerabilities before deployment"", ""Each microservice must have dedicated auto-scaling policies based on SQS queue metrics"", ""Implement blue-green deployment pattern with automated health checks"", ""Use AWS Secrets Manager for all database credentials and API keys"", ""Container logs must be encrypted and retained for exactly 30 days"", ""Network traffic between services must use service discovery via "", ""Implement circuit breaker pattern using AWS App Mesh for service-to-service communication"", ""Each container must have CPU and memory limits enforced"", ""Deploy containers across 3 availability zones with anti-affinity rules""]"
d8k0f3,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy an observability platform for distributed applications. The configuration must: 1. Create CloudWatch Log Groups with KMS encryption for application, system, and audit logs with appropriate retention periods. 2. Set up Kinesis Data Firehose delivery stream to archive logs to S3 with GZIP compression and 5-minute buffer intervals. 3. Configure CloudWatch Metrics Filters to extract error rates, response times, and custom business metrics from JSON-formatted logs. 4. Deploy topics for critical, warning, and info alert levels with email subscription endpoints. 5. Create CloudWatch Composite Alarms combining CPU, memory, and custom metrics with 5-minute evaluation periods. 6. Build CloudWatch Dashboard with line charts for metrics, log insights widgets, and alarm status panels. 7. Enable tracing with custom sampling rules and service map generation. 8. Configure EventBridge rules to trigger Lambda functions for automated remediation of common issues. 9. Implement CloudWatch Logs Insights queries as saved searches for troubleshooting patterns. 10. Set up cross-region log replication for disaster recovery compliance. Expected output: Complete Pulumi TypeScript program that creates a production-ready observability stack with centralized logging, metrics collection, alerting, and distributed tracing capabilities.","A financial services company needs centralized observability for their microservices architecture. Their applications generate high-volume logs and metrics that must be collected, processed, and visualized in near real-time for compliance auditing and performance monitoring.","""Production observability infrastructure deployed in eu-central-1 using CloudWatch Logs, Kinesis Data Firehose, and . Requires Pulumi 3.x with TypeScript, Node.js 18+, and AWS CLI configured. VPC with private subnets for compute resources, S3 bucket for Firehose destination. Multi-account setup with centralized logging account. CloudWatch agent deployed on EC2 instances running Amazon Linux 2. Expects high-volume log ingestion (>1TB/day) with real-time processing requirements.""","[""All logs must be encrypted at rest using AWS KMS customer-managed keys"", ""Implement log retention policies with 30-day standard retention and 365-day archive for audit logs"", ""Use CloudWatch Metrics Filters to extract custom metrics from application logs"", ""Deploy Kinesis Data Firehose for log streaming with compression enabled"", ""Configure topics with email subscriptions for critical alerts"", ""Implement CloudWatch Dashboards with at least 5 custom widgets"", ""Use CloudWatch Composite Alarms for multi-metric threshold monitoring"", ""Enable tracing with sampling rate of 10% for cost optimization"", ""All IAM roles must follow least-privilege principle with no wildcard actions""]"
i2m0i7,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to orchestrate a phased migration of a three-tier application from on-premises to AWS cloud. The configuration must: 1. Set up a VPC with public and private subnets across 3 availability zones for the migrated environment. 2. Deploy an Application Load Balancer with two target groups - one pointing to on-premises (via VPN) and one to (CORE: ). 3. Configure weighted routing on ALB to gradually shift traffic from legacy to new environment (start at 90/10 split). 4. Create an Aurora MySQL cluster with encryption at rest and automated backups (CORE: ). 5. Set up DMS replication instance and tasks to continuously sync data from on-premises database to Aurora (CORE: DMS). 6. Deploy containerized API and web tiers on with auto-scaling based on CPU utilization. 7. Configure Route 53 health checks and failover routing for automatic rollback to legacy if new environment fails. 8. Store migration progress metrics in CloudWatch custom metrics with alarms for replication lag. 9. Use Parameter Store to track migration phase (pre-migration, replication, cutover, post-migration). 10. Implement Lambda function to monitor DMS replication lag and automatically pause traffic shift if lag exceeds 60 seconds. Expected output: A complete Pulumi TypeScript program that creates parallel environments enabling gradual migration with automated health monitoring and rollback capabilities. The solution should output ALB DNS name, endpoint, DMS replication status dashboard URL, and current traffic distribution percentages.","A financial services company needs to migrate their legacy monolithic application from on-premises to AWS cloud. The application consists of a web tier, API tier, and database tier that must be migrated in phases while maintaining data consistency and minimal downtime. The migration strategy requires parallel environments during transition to enable gradual cutover.","""Multi-AZ infrastructure deployment in us-east-1 for migrating on-premises workloads to AWS. Core services include Application Load Balancer with weighted target groups, for containerized applications, Aurora MySQL with DMS replication, and Route 53 for DNS failover. VPC spans 3 availability zones with public subnets for ALB and private subnets for tasks and . Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate IAM permissions. Migration involves parallel running of legacy and new environments with gradual traffic shifting.""","[""Use AWS Database Migration Service (DMS) for database replication from on-premises to "", ""Implement blue-green deployment pattern with weighted routing for zero-downtime migration"", ""All data in transit must use TLS 1.2 or higher encryption"", ""Resource naming must include environment suffix (-legacy, -migrated) for clear identification"", ""Implement automated rollback mechanism if health checks fail during migration"", ""Use Parameter Store for storing migration state and configuration""]"
s5z3x6,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to provision a multi-region VPC infrastructure with cross-region peering. The configuration must: 1. Create a VPC in us-east-1 with CIDR 10.0.0.0/16 and a VPC in eu-west-1 with CIDR 10.1.0.0/16. 2. Deploy 3 public subnets and 3 private subnets in each VPC across different AZs. 3. Provision Internet Gateways and attach them to each VPC. 4. Create NAT Gateways in each public subnet with Elastic IPs. 5. Establish a VPC peering connection between the two VPCs. 6. Configure route tables to enable communication between private subnets across regions. 7. Create security groups that allow HTTPS (443) and PostgreSQL (5432) traffic between peered VPCs. 8. Implement proper resource naming conventions using the pattern {region}-{resource-type}-{purpose}. 9. Export VPC IDs, peering connection ID, and subnet IDs for use by other stacks. 10. Ensure all resources are properly tagged and support stack deletion without manual intervention. Expected output: A Pulumi TypeScript program that creates a production-ready multi-region network infrastructure with secure VPC peering, proper routing, and security controls that can be deployed and destroyed cleanly.",A financial services company needs to establish secure network connectivity between their production environment in us-east-1 and a newly acquired subsidiary's infrastructure in eu-west-1. The architecture must support cross-region database replication and shared microservices while maintaining strict network isolation and compliance requirements.,"""Multi-region AWS deployment spanning us-east-1 and eu-west-1 regions. Each region contains a VPC with 3 availability zones, public and private subnets, NAT Gateways for outbound traffic, and Internet Gateways for public access. VPC peering connection enables secure cross-region communication. Requires Pulumi CLI 3.x with TypeScript, Node.js 16+, and AWS credentials configured for both regions. Infrastructure includes EC2 instances in private subnets, RDS Aurora Global Database clusters, and Application Load Balancers in public subnets.""","[""VPC CIDR blocks must not overlap and should use RFC1918 private address ranges"", ""All inter-region traffic must be encrypted in transit using VPC peering"", ""Each VPC must have exactly 3 availability zones with public and private subnets"", ""NAT Gateways must be deployed in each AZ for high availability"", ""Route tables must be configured to allow only specific subnet-to-subnet communication"", ""Security groups must explicitly define allowed ports and protocols between regions"", ""All resources must be tagged with Environment, Region, and CostCenter tags""]"
b1x5k0,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy and maintain consistent infrastructure across three environments (dev, staging, prod) for a trading platform. The configuration must: 1. Define a base infrastructure template using Pulumi ComponentResource classes for VPC, ECS cluster, and RDS database. 2. Implement environment-specific configuration using Pulumi config files with TypeScript interfaces for type safety. 3. Create VPCs in different regions with identical CIDR schemes but environment-specific tags. 4. Deploy ECS Fargate services with environment-specific container counts (dev: 1, staging: 2, prod: 4). 5. Set up RDS Aurora clusters with environment-specific instance classes (dev: db.t3.medium, staging: db.r5.large, prod: db.r5.xlarge). 6. Configure Application Load Balancers with environment-specific SSL certificates from ACM. 7. Implement cross-stack references to share VPC and security group IDs between stacks. 8. Create S3 buckets with environment-specific lifecycle policies and encryption settings. 9. Set up CloudWatch dashboards that aggregate metrics across all environments. 10. Implement a drift detection mechanism that compares actual vs desired state across environments. 11. Configure IAM roles with environment-specific trust policies and permission boundaries. 12. Output a comparison report showing configuration differences between environments. Expected output: A Pulumi TypeScript project with separate stacks for each environment, reusable components ensuring consistency, automated drift detection, and a configuration comparison tool that validates all environments maintain the required baseline while allowing approved variations.","A financial services company needs to maintain identical infrastructure across development, staging, and production environments for their trading platform. They require automated environment replication with drift detection to ensure compliance and reduce configuration errors between environments.","""Multi-environment AWS infrastructure spanning three regions: us-east-1 (production), us-west-2 (staging), and eu-west-1 (development). Each environment requires VPC with 3 availability zones, private and public subnets, NAT gateways for outbound traffic. Core services include ECS Fargate for containerized applications, RDS Aurora PostgreSQL Multi-AZ clusters, Application Load Balancers, and S3 buckets for static assets. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, and AWS CLI configured with appropriate IAM roles for cross-account deployment.""","[""Use Pulumi stack references to share outputs between environments"", ""Implement custom resource options for environment-specific tagging"", ""Use TypeScript interfaces to enforce consistent resource configurations"", ""Create reusable component resources for each service tier"", ""Implement automated drift detection using Pulumi's state comparison"", ""Use configuration files with schema validation for environment variables""]"
y5a1g9,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a highly secure payment processing web application. The configuration must: 1. Create a VPC with public and private subnets across 3 AZs with NAT gateways for outbound traffic. 2. Deploy an cluster with two separate services - one for the React frontend (port 3000) and one for the Node.js API (port 8080). 3. Configure an Aurora PostgreSQL Serverless v2 cluster with IAM database authentication enabled and encrypted storage. 4. Set up an Application Load Balancer with path-based routing (/api/* to backend, /* to frontend) and HTTPS listener using ACM certificate. 5. Create a distribution with custom origin headers that the ALB validates to prevent direct access. 6. Implement ECR repositories with image scanning on push and block deployment of images with HIGH or CRITICAL vulnerabilities. 7. Configure WAF web ACL with rate limiting rule (1000 requests/minute per IP) attached to . 8. Create KMS keys for encryption and task encryption with automatic annual rotation. 9. Set up least-privilege IAM roles for tasks with specific permissions for IAM auth and Secrets Manager access. 10. Store database connection details in Secrets Manager and application configuration in . 11. Enable VPC Flow Logs, task logging to CloudWatch, and performance insights. 12. Output the distribution URL and cluster endpoint for application configuration. Expected output: A complete Pulumi TypeScript program that provisions all infrastructure with proper security controls, outputs the URL for accessing the application, and ensures all payment data is encrypted and isolated according to PCI DSS requirements.",A fintech startup needs to deploy their payment processing web application with strict compliance requirements for PCI DSS. The application consists of a React frontend and Node.js API backend that must be isolated in separate security zones with encrypted data in transit and at rest.,"""Production-grade infrastructure in us-east-1 for payment processing web application using , Aurora PostgreSQL Serverless v2, Application Load Balancer, CDN, and ECR with image scanning. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate permissions. VPC spans 3 availability zones with public subnets for ALB, private subnets for tasks and cluster. Implements defense-in-depth security with WAF, and GuardDuty monitoring. Uses AWS Secrets Manager for sensitive configuration and for application settings.""","[""Frontend and backend must run in separate tasks with distinct security groups"", ""All data must be encrypted using customer-managed KMS keys with automatic rotation enabled"", "" database must use IAM authentication instead of password-based access"", ""Application Load Balancer must terminate SSL with ACM certificate for the domain"", ""Backend API must only be accessible through the ALB, not directly from the internet"", "" distribution must use custom headers to prevent direct ALB access"", ""All container images must be scanned for vulnerabilities before deployment"", ""Implement request throttling at 1000 requests per minute per IP address""]"
m5q4o6,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a serverless payment webhook processor. MANDATORY REQUIREMENTS (Must complete): 1. Create a Lambda function with ARM-based Graviton2 architecture to process incoming webhooks (CORE: Lambda) 2. Configure DynamoDB table with on-demand billing to store transaction records (CORE: DynamoDB) 3. Set up SNS topic to publish validated payment events to downstream services 4. Implement Secrets Manager secret with automatic 30-day rotation for webhook signature keys 5. Create customer-managed KMS key for encrypting DynamoDB data and Lambda environment variables 6. Configure Lambda with 512MB memory and 60-second timeout with reserved concurrent executions of 10 7. Enable point-in-time recovery on DynamoDB table with TTL on 'expires_at' attribute 8. Set up CloudWatch Log group with 30-day retention and metric filters for error tracking 9. Create least-privilege IAM execution role allowing only specific resource access 10. Output Lambda function ARN, DynamoDB table name, and SNS topic ARN for integration OPTIONAL ENHANCEMENTS (If time permits):  Add API Gateway REST endpoint for manual webhook testing (OPTIONAL: API Gateway) - simplifies integration testing  Implement SQS queue between Lambda and SNS for decoupling (OPTIONAL: SQS) - adds resilience and retry capability  Deploy tracing for end-to-end request tracking (OPTIONAL: ) - improves debugging and performance analysis Expected output: A complete Pulumi TypeScript program that deploys a production-ready serverless webhook processor with all security controls, monitoring, and high availability features configured according to the requirements.","A fintech startup needs a serverless event processing system to handle real-time payment notifications from multiple payment providers. The system must process webhook events, validate signatures, store transaction data, and trigger downstream workflows while maintaining strict security and compliance requirements.","""Multi-AZ serverless infrastructure deployed in eu-central-1 using Lambda for event processing, DynamoDB for transaction storage, and SNS for event distribution. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate credentials. Infrastructure spans 3 availability zones with for private communication between Lambda and DynamoDB. KMS customer-managed keys for encryption, Secrets Manager for API keys, and CloudWatch for monitoring. Development account with cost allocation tags required.""","[""Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""All secrets must be stored in AWS Secrets Manager with automatic rotation enabled"", ""DynamoDB tables must use point-in-time recovery and encryption at rest with customer-managed KMS keys"", ""Lambda functions must have reserved concurrent executions set to prevent throttling"", ""All IAM policies must follow least-privilege principles with no wildcard resource permissions"", ""CloudWatch Logs must have a retention period of exactly 30 days for compliance""]"
s2z9b2,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to deploy a multi-region disaster recovery infrastructure for a payment processing API. The configuration must: 1. Set up Route53 hosted zone with health checks monitoring primary region endpoints every 30 seconds. 2. Configure failover routing policy that switches to secondary region when primary fails 3 consecutive health checks. 3. Create DynamoDB global tables with 'transactions' table replicated between us-east-1 and us-west-2. 4. Deploy Lambda function in each region to process payment webhooks with environment-specific memory settings. 5. Configure Secrets Manager secrets in primary region with automatic replication to secondary. 6. Set up CloudWatch alarms monitoring Lambda errors, duration, and throttles with cross-region SNS topic. 7. Implement least-privilege IAM roles for Lambda functions with access only to specific DynamoDB table and Secrets. 8. Enable point-in-time recovery on DynamoDB tables with 7-day retention. 9. Configure Lambda reserved concurrent executions (10 for primary, 5 for secondary). 10. Create CloudWatch dashboards in both regions showing key metrics. Expected output: A Pulumi TypeScript program that deploys complete multi-region infrastructure with automatic failover capabilities, ensuring payment processing continues during regional outages with minimal data loss.","A fintech startup processes critical payment transactions through their API and needs zero-downtime during regional AWS outages. They require an active-passive multi-region architecture where traffic automatically fails over to a secondary region if the primary region experiences issues, with data consistency maintained across regions.","""Multi-region active-passive architecture deployed across us-east-1 (primary) and us-west-2 (secondary) for payment processing system. Uses Route53 for DNS failover, DynamoDB global tables for data replication, Lambda functions for API logic, and Secrets Manager for credential management. Requires Pulumi CLI 3.x with TypeScript, AWS CLI configured with appropriate permissions. Each region has isolated VPCs with private subnets for Lambda functions. CloudWatch cross-region monitoring with SNS alerting for incident response.""","[""Use Route53 health checks with failover routing policy for automatic region switching"", ""Implement DynamoDB global tables for cross-region data replication with point-in-time recovery enabled"", ""Deploy Lambda functions in both regions with identical configurations but different memory allocations (512MB primary, 256MB secondary)"", ""Use AWS Secrets Manager with automatic cross-region replication for API keys and database credentials"", ""Configure CloudWatch cross-region alarms that trigger SNS notifications when primary region degrades"", ""Set RTO (Recovery Time Objective) of 60 seconds and RPO (Recovery Point Objective) of 5 seconds"", ""Tag all resources with Environment=Production and CostCenter tags for billing tracking""]"
k0v7x5,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a zero-trust security infrastructure for payment processing. The configuration must: 1. Create a KMS key with automatic rotation and strict key policies allowing only specific IAM principals. 2. Deploy AWS Network Firewall with stateful rules blocking all outbound traffic except HTTPS to specific payment gateway domains. 3. Configure Parameter Store with SecureString parameters for API credentials using the KMS key. 4. Implement IAM roles with session policies enforcing MFA and IP restrictions for administrative access. 5. Create CloudWatch Log Groups with KMS encryption and integrity validation for audit trails. 6. Deploy AWS Config rules to monitor encryption status and network isolation compliance. 7. Set up EventBridge rules to alert on security policy violations via . 8. Configure resource tags with DataClassification='Sensitive' and ComplianceScope='PCI-DSS'. 9. Enable CloudTrail event logging for all KMS and IAM operations with log file validation. 10. Implement SCPs (Service Control Policies) preventing deletion of security resources. Expected output: A Pulumi program that creates a complete zero-trust security layer with encryption, network isolation, and audit logging that passes PCI-DSS compliance checks.","FinanceGuard Inc needs to implement a security-first infrastructure for their payment processing system to meet PCI-DSS compliance requirements. The system must enforce strict network isolation, encryption at rest and in transit, and comprehensive audit logging while maintaining zero-trust principles.","""Production security infrastructure deployed in eu-west-1 for PCI-DSS compliance. Uses AWS KMS for encryption key management, Network Firewall for traffic inspection, for secure parameter storage, and CloudWatch for audit logging. Requires Pulumi 3.x with TypeScript, AWS SDK v3, Node.js 18+. Deploys into isolated VPC with private subnets across 3 AZs, no direct internet access, all traffic routed through Network Firewall endpoints.""","[""All data must be encrypted using customer-managed KMS keys with automatic rotation enabled"", ""Network traffic must flow through AWS Network Firewall with strict egress rules"", ""IAM roles must use session policies and temporary credentials with maximum 1-hour duration"", ""All API calls must be logged to CloudWatch with integrity validation enabled"", ""Resources must be tagged with compliance metadata including DataClassification and ComplianceScope""]"
e2w5r8,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy an automated infrastructure compliance scanning system. The configuration must: 1. Set up AWS Config with a configuration recorder and delivery channel to S3. 2. Create an S3 bucket with versioning enabled for storing Config snapshots and compliance reports. 3. Implement three Lambda functions for custom compliance rules: checking for unencrypted EBS volumes, public S3 buckets, and EC2 instances without required tags. 4. Deploy Config rules that use the Lambda functions as evaluation sources. 5. Configure EventBridge rules to trigger compliance scans when resources are created or modified. 6. Create a Lambda function to aggregate compliance results and generate JSON reports. 7. Set up SNS topic for compliance violation notifications. 8. Implement proper IAM roles and policies for all components. 9. Configure CloudWatch Logs with 14-day retention for all Lambda functions. 10. Export Config rule names and S3 bucket name as stack outputs. Expected output: A Pulumi program that deploys a fully automated compliance scanning infrastructure that continuously monitors AWS resources, evaluates them against custom policies, and generates compliance reports stored in S3.",A financial services company needs automated infrastructure compliance scanning to ensure their AWS resources meet regulatory requirements. The solution must analyze deployed resources against security policies and generate actionable compliance reports for audit purposes.,"""AWS infrastructure compliance scanning deployment in us-east-1 region using AWS Config for continuous resource monitoring, Lambda functions for custom compliance rules, S3 for storing configuration snapshots and compliance reports. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate permissions. VPC not required as Config operates at the account level. Lambda functions execute in AWS-managed VPC. S3 bucket configured with versioning and lifecycle policies for compliance data retention.""","[""Use AWS Config for resource inventory and compliance tracking"", ""Implement custom Config rules using Lambda functions for policy checks"", ""Store compliance results in S3 with server-side encryption"", ""Generate compliance reports in JSON format with violation details"", ""Use EventBridge to trigger scans on resource changes"", ""Implement least-privilege IAM roles with no wildcard permissions"", ""Tag all resources with Environment and ComplianceLevel tags"", ""Use Pulumi stack outputs to export Config rule ARNs"", ""Set Lambda memory to exactly 256MB for cost optimization"", ""Configure 30-day retention for Config snapshots""]"
g2h0l6,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready Amazon EKS cluster with advanced security and operational features.

MANDATORY REQUIREMENTS (Must complete):
1. Create an EKS cluster with Kubernetes 1.28+ and enable all control plane log types (CORE: EKS)
2. Deploy managed node groups using Bottlerocket AMI across 3 AZs with spot instances (CORE: EC2)
3. Configure OIDC provider and implement IRSA for service account authentication
4. Install AWS Load Balancer Controller using Helm with proper IAM permissions
5. Deploy EBS CSI driver for persistent volume support with encryption enabled
6. Configure cluster autoscaler with IAM role and proper resource tags
7. Implement pod security standards with restricted baseline enforcement
8. Create sample namespace with IRSA-enabled service account
9. Enable VPC CNI plugin with custom networking configuration
10. Output cluster endpoint, OIDC issuer URL, and kubeconfig

OPTIONAL ENHANCEMENTS (If time permits):
 Add Karpenter for advanced node provisioning (OPTIONAL: Karpenter) - provides more flexible scaling
 Implement AWS GuardDuty EKS Protection (OPTIONAL: GuardDuty) - adds runtime threat detection
 Deploy Fluent Bit for log aggregation (OPTIONAL: CloudWatch) - centralizes application logs

Expected output: A Pulumi program that creates a production-ready EKS cluster with managed node groups, proper IAM configuration, essential add-ons installed via Helm, and security best practices implemented. The stack should output connection details and be immediately usable for workload deployment.","A financial services company needs to deploy a managed Kubernetes environment for their microservices architecture. The platform must support automated node scaling, secure secrets management, and integration with existing CI/CD pipelines while maintaining strict security compliance.","""Production-grade EKS infrastructure deployed in us-east-1 using Amazon EKS with managed node groups, AWS Load Balancer Controller, and EBS CSI driver. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, kubectl installed, and AWS CLI configured with appropriate permissions. VPC spans 3 AZs with public and private subnets, NAT gateways for outbound traffic. Integration with existing Route53 hosted zone for DNS management.""","[""EKS cluster must use Kubernetes version 1.28 or higher"", ""Worker nodes must use Bottlerocket AMI for enhanced security"", ""Implement IRSA (IAM Roles for Service Accounts) for pod-level permissions"", ""Enable EKS control plane logging for all log types"", ""Configure OIDC provider for cluster authentication"", ""Node groups must span exactly 3 availability zones"", ""Implement pod security standards with restricted baseline"", ""Use managed node groups with spot instances for cost optimization"", ""Enable cluster autoscaler with proper IAM permissions"", ""All resources must be tagged with Environment, Team, and CostCenter""]"
z4e6k8,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement a multi-region disaster recovery solution for a transaction processing system. MANDATORY REQUIREMENTS (Must complete): 1. Deploy Aurora Global Database cluster with PostgreSQL 15.x in us-east-1 as primary and us-west-2 as secondary read replica (CORE: Aurora) 2. Configure S3 buckets in both regions with cross-region replication for application artifacts and backups (CORE: S3) 3. Implement Route 53 health checks that monitor Aurora endpoints and perform automatic DNS failover 4. Create customer-managed KMS keys in each region for database and S3 encryption 5. Set up AWS Backup plans in both regions with 30-day retention and daily snapshots 6. Configure VPC peering between regions with appropriate security groups and NACLs 7. Implement CloudWatch alarms for replication lag monitoring (threshold: 5 minutes) 8. Create Lambda function to test failover readiness and generate compliance reports 9. Output primary and secondary database endpoints, S3 bucket ARNs, and failover status dashboard URL OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Backup Vault Lock for immutable backups (OPTIONAL: AWS Backup) - ensures compliance with data retention policies  Implement EventBridge rules for automated failover orchestration (OPTIONAL: EventBridge) - reduces manual intervention during disasters  Add for cross-region configuration management (OPTIONAL: ) - simplifies disaster recovery runbooks Expected output: A fully functional Pulumi program that deploys a production-ready multi-region disaster recovery infrastructure with automated failover capabilities, meeting all RPO/RTO requirements.",A financial services company needs to implement a disaster recovery solution for their critical transaction processing system. The system must maintain near-real-time data replication across regions and support automatic failover with minimal data loss. Compliance requires all data to be encrypted at rest and in transit.,"""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary). Core services include Aurora Global Database for PostgreSQL, S3 with cross-region replication, Route 53 health checks, and AWS Backup for automated snapshots. Each region has its own VPC with 3 availability zones, private subnets for database and compute resources, and VPC peering for secure cross-region communication. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, and AWS credentials with permissions for multi-region resource deployment. KMS keys in each region for encryption at rest.""","[""Primary region must be us-east-1 with failover to us-west-2"", ""RPO (Recovery Point Objective) must be under 5 minutes"", ""RTO (Recovery Time Objective) must be under 15 minutes"", ""All data must be encrypted using customer-managed KMS keys"", ""Cross-region replication must use AWS PrivateLink endpoints"", ""Health checks must trigger automatic DNS failover within 60 seconds"", ""Backup retention must be 30 days with point-in-time recovery capability""]"
d7u2m0,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster running exclusively on Fargate for containerized microservices. The configuration must: 1. Create an EKS cluster version 1.28+ with only Fargate profiles (no EC2 nodes) 2. Define three Fargate profiles for payment, fraud, and notification namespaces 3. Deploy AWS Load Balancer Controller using Helm with proper IRSA configuration 4. Create Kubernetes namespaces with pod security standards set to 'restricted' 5. Implement NetworkPolicies allowing payment->fraud, fraud->notification communication only 6. Configure HorizontalPodAutoscaler for each service targeting 70% CPU utilization 7. Create pod disruption budgets ensuring minimum 2 replicas available during updates 8. Set up Fluent Bit DaemonSet with CloudWatch Logs integration for all pods 9. Configure IRSA roles with least-privilege policies for each service's AWS access 10. Deploy Kubernetes services and ingress resources with SSL/TLS termination. Expected output: Complete Pulumi program that provisions EKS cluster with Fargate-only compute, deploys three microservices with proper isolation and scaling, configures observability through CloudWatch, and exposes services securely through ALB ingress.","A fintech startup needs to migrate their microservices from Docker Swarm to AWS EKS with Fargate profiles. The application consists of payment processing, fraud detection, and notification services that require strict network isolation and automated scaling based on transaction volume.","""Production-grade EKS cluster deployed in us-east-1 using Fargate compute profiles exclusively. Infrastructure includes VPC with private subnets across 3 AZs, NAT gateways for outbound traffic, and Application Load Balancer for ingress. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions, kubectl 1.28+, and helm 3.x. The cluster runs three microservices: payment-api, fraud-detector, and notification-service, each requiring specific IAM permissions for DynamoDB, and SNS respectively. Network policies enforce strict east-west traffic control between services.""","[""EKS cluster must use only Fargate profiles - no EC2 node groups allowed"", ""Each microservice must run in its own namespace with pod security standards enforced"", ""Implement network policies to allow only specific inter-service communication"", ""Use AWS Load Balancer Controller for ingress with SSL termination"", ""Configure Horizontal Pod Autoscaling with custom metrics from CloudWatch"", ""All container images must be stored in private ECR repositories"", ""Implement pod disruption budgets for each service to ensure availability"", ""Use IRSA (IAM Roles for Service Accounts) for AWS service access"", ""Configure Fluent Bit as a DaemonSet for centralized logging to CloudWatch"", ""Deploy all resources across 3 availability zones for high availability""]"
f2q8r3,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a comprehensive observability stack for ECS-based microservices. MANDATORY REQUIREMENTS (Must complete): 1. Deploy ECS Fargate cluster with CloudWatch Container Insights enabled (CORE: ECS) 2. Configure CloudWatch Logs groups with metric filters for each service (CORE: CloudWatch) 3. Create custom CloudWatch dashboards displaying service health metrics 4. Set up CloudWatch Synthetics canaries to monitor critical endpoints 5. Implement CloudWatch alarms with SNS notifications for threshold breaches 6. Configure ADOT sidecar containers for trace collection 7. Create S3 bucket with lifecycle rules for long-term log storage 8. Deploy Lambda function to process and enrich logs before S3 archival 9. Set up EventBridge rules to trigger incident response workflows 10. Configure for dynamic threshold management OPTIONAL ENHANCEMENTS (If time permits):  Add Kinesis Data Firehose for real-time log streaming (OPTIONAL: Kinesis Firehose) - enables real-time analytics  Implement for automated remediation workflows (OPTIONAL: ) - provides self-healing capabilities  Add QuickSight dashboards for business metrics (OPTIONAL: QuickSight) - improves visibility for non-technical stakeholders Expected output: Complete Pulumi TypeScript program that provisions a production-ready observability infrastructure with centralized monitoring, alerting, and log management capabilities.","A fintech startup needs centralized observability for their microservices architecture running on ECS. They require real-time monitoring, distributed tracing, and custom metrics collection to meet compliance requirements and maintain sub-second response times.","""Production infrastructure deployed in us-east-1 across 3 availability zones. Uses ECS Fargate for container orchestration, Application Load Balancer for traffic distribution, CloudWatch for monitoring and logging. VPC with private subnets for ECS tasks, public subnets for ALB. NAT Gateways for outbound connectivity. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate permissions. Infrastructure supports 10+ microservices with expected 100k requests/day.""","[""Use AWS Distro for OpenTelemetry (ADOT) for trace collection instead of SDK"", ""Configure CloudWatch Container Insights with enhanced monitoring enabled"", ""Set up custom CloudWatch dashboards with at least 5 widgets per service"", ""Implement metric filters on CloudWatch Logs to track error rates and latencies"", ""Use CloudWatch Synthetics for endpoint monitoring with 5-minute intervals"", ""Configure SNS topics with email subscriptions for critical alerts"", ""Set CloudWatch alarms for CPU > 80%, Memory > 75%, and 5XX errors > 1%"", ""Enable ECS task metadata endpoint v4 for all services"", ""Store all logs in S3 with lifecycle policies after 30 days"", ""Use for storing alert thresholds""]"
d7o0g9,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to migrate a payment processing system from single-AZ to multi-AZ architecture while maintaining zero downtime. The configuration must: 1. Import existing VPC, subnets, and security groups from the legacy stack without modification. 2. Create new ECS cluster with services distributed across three AZs. 3. Set up Aurora PostgreSQL cluster with automated failover and point-in-time recovery. 4. Implement Application Load Balancer with weighted target groups for gradual traffic migration. 5. Configure functions with reserved concurrent executions and dead letter queues. 6. Create buckets with versioning, lifecycle policies, and cross-region replication. 7. Set up CloudWatch dashboards and alarms for all critical metrics. 8. Implement AWS Secrets Manager rotation for database credentials. 9. Configure AWS WAF rules on the ALB for PCI compliance. 10. Create EventBridge rules to trigger rollback on migration failures. 11. Set up plans for all stateful resources. 12. Implement cost allocation tags following FinOps best practices. Expected output: A complete Pulumi program that orchestrates the migration with automated health checks, gradual traffic shifting from 0% to 100% over 2 hours, and automatic rollback capabilities if error rates exceed 1%.",A financial services company needs to migrate their payment processing infrastructure from a legacy single-AZ deployment to a new multi-AZ architecture. The existing setup has hardcoded resource names and manually configured security groups that must be preserved during migration to avoid breaking integrated systems.,"""Multi-AZ infrastructure spanning us-east-1 region across three availability zones. Includes VPC with public and private subnets, Application Load Balancer, ECS cluster running containerized payment services, Aurora PostgreSQL Multi-AZ cluster, buckets for transaction logs, and functions for asynchronous processing. Requires Pulumi 3.x with TypeScript, AWS CLI v2 configured with appropriate credentials, Node.js 18+, and Docker for local testing. The legacy environment runs in a single AZ with EC2 instances and must remain operational during migration.""","[""Use Pulumi's stack references to import state from the legacy stack named 'prod-legacy'"", ""Preserve all existing security group IDs and rules during migration"", ""Implement a blue-green deployment strategy with automated traffic shifting"", ""Use for all configuration values"", ""Enable cross-region replication for all buckets"", ""Implement custom resource providers for legacy database migration"", ""Use Pulumi's transformation feature to enforce tagging standards"", ""Configure AWS Config rules to validate compliance post-migration"", ""Implement rollback triggers based on CloudWatch alarms"", ""Use Pulumi's policy as code to prevent accidental resource deletion""]"
x2a2c1,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to establish a foundational cloud environment for a financial services platform. MANDATORY REQUIREMENTS (Must complete): 1. Create a VPC with CIDR 10.0.0.0/16 across 3 availability zones (CORE: VPC) 2. Deploy public and private subnets in each AZ with proper route tables 3. Configure NAT gateways in public subnets for outbound internet access 4. Set up an ECS Fargate cluster with capacity providers (CORE: ECS) 5. Create an RDS Aurora PostgreSQL cluster with encryption enabled (CORE: RDS) 6. Establish ECR repositories with image scanning on push 7. Configure for S3 and ECR to reduce data transfer costs 8. Implement CloudWatch log groups with 30-day retention for all services 9. Create KMS keys for RDS encryption with automatic rotation 10. Apply mandatory tags (Environment, Project, CostCenter) to all resources OPTIONAL ENHANCEMENTS (If time permits):  Add for automated RDS snapshots (OPTIONAL: ) - ensures compliance with policies  Implement VPC Flow Logs with S3 storage (OPTIONAL: VPC Flow Logs) - enhances security monitoring  Create Transit Gateway for future multi-account connectivity (OPTIONAL: Transit Gateway) - enables enterprise scaling Expected output: A Pulumi TypeScript program that provisions a complete, production-ready AWS environment with proper network isolation, encryption, and foundational services ready for application deployment.","A financial services startup needs to establish their initial cloud infrastructure for a new digital banking platform. They require a secure, compliant environment with proper network isolation and data encryption to meet regulatory requirements. The infrastructure must support both containerized microservices and managed database workloads.","""New AWS account in eu-central-1 region requiring foundational cloud setup. Infrastructure includes VPC with 3 availability zones, private and public subnets, NAT gateways for outbound traffic. Core services: ECS Fargate for containerized workloads, RDS Aurora PostgreSQL for transactional data, ECR for container registry. Requires Pulumi CLI 3.x, TypeScript, Node.js 18+, AWS CLI configured with appropriate credentials. Network architecture includes transit gateway for future multi-account connectivity.""","[""All data at rest must be encrypted using AWS KMS customer-managed keys"", ""Network traffic between services must traverse private subnets only"", ""Database backups must have a retention period of exactly 30 days"", ""Container images must be scanned for vulnerabilities before deployment"", ""All resources must be tagged with Environment, Project, and CostCenter tags""]"
f4s2b6,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy consistent infrastructure across development, staging, and production environments. MANDATORY REQUIREMENTS (Must complete): 1. Design a reusable Pulumi ComponentResource class that encapsulates the entire environment stack (CORE: Pulumi Components) 2. Deploy Aurora PostgreSQL clusters with environment-specific instance sizes and backup retention (CORE: RDS Aurora) 3. Create ECS Fargate services with ALB integration and auto-scaling policies (CORE: ECS) 4. Implement a configuration system using YAML files for each environment (dev.yaml, staging.yaml, prod.yaml) 5. Generate non-overlapping VPC CIDR blocks programmatically based on environment index 6. Apply consistent tagging strategy with Environment, CostCenter, and ManagedBy tags 7. Create S3 buckets with environment-specific lifecycle rules (7 days for dev, 30 for staging, 90 for prod) 8. Implement stack outputs that can be consumed by other stacks without cross-reference 9. Use Pulumi secrets for all sensitive values like database passwords 10. Ensure all IAM roles follow least-privilege with no wildcard actions OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Config rules for compliance checking (OPTIONAL: Config) - ensures policy adherence  Implement CloudFront distributions for static assets (OPTIONAL: CloudFront) - improves global performance  Add rules for cross-environment event routing (OPTIONAL: ) - enables event-driven architecture Expected output: A Pulumi TypeScript program with a main stack file, environment-specific YAML configurations, and a reusable ComponentResource that can deploy identical infrastructure patterns across all three environments with single-command deployment capability.","A financial services company needs to maintain identical infrastructure across development, staging, and production environments for their transaction processing system. They require strict consistency in resource naming, configurations, and security policies across all environments while allowing for environment-specific scaling parameters.","""Multi-environment AWS infrastructure spanning three regions (us-east-1 for prod, us-west-2 for staging, eu-west-1 for dev). Each environment consists of VPC with calculated CIDR blocks, ECS Fargate clusters running containerized services, Aurora PostgreSQL Multi-AZ databases, and S3 buckets for data storage. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS credentials configured for multiple accounts. Infrastructure must support blue-green deployments with Application Load Balancers distributing traffic across availability zones.""","[""All resource names must follow the pattern {env}-{region}-{service}-{resource} with no exceptions"", ""Environment-specific configurations must be loaded from separate YAML files without hardcoding any values"", ""Cross-environment state references are prohibited - each environment must be fully independent"", ""All S3 buckets must have versioning enabled and lifecycle policies that differ by environment tier"", ""Network CIDR blocks must not overlap between environments and must be calculated programmatically""]"
c3j5l8,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a PCI-compliant payment processing web application. The configuration must: 1. Create a VPC with 3 public and 3 private subnets across 3 AZs with NAT gateways. 2. Deploy an ECS Fargate service running the payment app container with 2GB RAM and 1 vCPU. 3. Configure an Aurora PostgreSQL cluster with 2 reader instances and automated backups. 4. Set up an Application Load Balancer with AWS WAF rules blocking common SQL injection patterns. 5. Create buckets for static assets and audit logs with server-side encryption. 6. Implement IAM roles following least privilege with no wildcard permissions. 7. Configure CloudWatch Log Groups for ECS tasks and slow query logs. 8. Set up for , ECR, and CloudWatch Logs services. 9. Create KMS keys with automatic rotation enabled for all encryption needs. 10. Deploy plans for with 30-day retention and cross-region copies. Expected output: A complete Pulumi TypeScript program that provisions all infrastructure components with proper security configurations, outputs the ALB DNS name, cluster endpoint, and bucket names for application configuration.","A fintech startup needs to deploy their payment processing web application with strict compliance requirements for PCI DSS. The application processes sensitive cardholder data and requires end-to-end encryption, audit logging, and automated capabilities.","""Production deployment in us-east-1 with disaster recovery in us-west-2. Infrastructure includes ECS Fargate for containerized web application, Aurora PostgreSQL Multi-AZ cluster, Application Load Balancer with WAF integration, and for static assets. VPC spans 3 availability zones with private subnets for compute and database tiers, public subnets for ALB. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate credentials. Network architecture includes for , ECR, and CloudWatch to avoid internet egress charges.""","[""All data must be encrypted at rest using AWS KMS customer-managed keys"", "" instances must use encrypted snapshots with cross-region replication"", ""Application containers must run with read-only root filesystems"", ""All buckets must have versioning enabled and lifecycle policies configured"", "" must be used for all AWS service communications"", ""CloudWatch Logs must retain audit logs for exactly 365 days""]"
n7k7s8,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a serverless payment webhook processor. The configuration must: 1. Create a DynamoDB table named 'payment-transactions' with partition key 'transactionId' (string) and sort key 'timestamp' (number), with on-demand billing and point-in-time recovery. 2. Deploy a Lambda function 'webhook-processor' that receives payment data and stores it in DynamoDB, using Node.js 18.x runtime with 1GB memory. 3. Set up an SQS queue 'payment-queue' with visibility timeout of 300 seconds and a dead letter queue that retries exactly 3 times. 4. Create an API Gateway REST API with POST endpoint '/webhook' that triggers the Lambda function, including request validation and 1000 req/min throttling. 5. Configure X-Ray tracing for Lambda functions and API Gateway for distributed tracing. 6. Set up CloudWatch Log Groups for all services with 30-day retention period. 7. Implement least-privilege IAM roles for each service with no wildcard permissions. 8. Add CloudWatch alarms for DynamoDB throttling and Lambda errors exceeding 1% error rate. 9. Enable deletion protection on the DynamoDB table to prevent accidental data loss. 10. Output the API Gateway endpoint URL and DynamoDB table ARN. Expected output: A complete Pulumi TypeScript program that deploys all infrastructure components with proper configuration, monitoring, and security settings. The webhook processor should be production-ready with fault tolerance and observability built in.","A financial services company needs to process payment webhooks from multiple providers in real-time. They require a serverless architecture that can handle variable loads, ensure transaction integrity, and provide detailed monitoring for compliance audits.","""Serverless infrastructure deployed in us-east-1 using AWS Lambda for webhook processing, DynamoDB for transaction storage, and SQS for message queuing. API Gateway provides REST endpoints with request validation. X-Ray tracing enabled across all services for debugging. Requires Pulumi 3.x with TypeScript, Node.js 18+, and AWS CLI configured. No VPC required - all services communicate via AWS service endpoints. CloudWatch Logs aggregates all service logs with 30-day retention for compliance.""","[""Lambda functions must use Node.js 18.x runtime with 1GB memory allocation"", ""DynamoDB tables must use on-demand billing with point-in-time recovery enabled"", ""All Lambda functions must have X-Ray tracing active for debugging"", ""CloudWatch Log Groups must have 30-day retention for compliance"", ""Dead letter queues must retry failed messages exactly 3 times before archiving"", ""API Gateway must use request validation and throttling limits of 1000 req/min""]"
w8q2o1,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a complete CI/CD pipeline for containerized applications. The configuration must: 1. Create a CodeCommit repository with main, develop, and feature branch structure. 2. Set up CodePipeline with 5 stages (Source, Build, Test, Security, Deploy) triggered on commits to main branch. 3. Configure CodeBuild projects for build, unit testing, and security scanning stages. 4. Implement S3 bucket for pipeline artifacts with versioning and lifecycle policies. 5. Create ECS cluster with Fargate compute and Application Load Balancer. 6. Define ECS task definitions and services for blue-green deployments. 7. Set up rules to capture pipeline state changes and route to SNS topic. 8. Implement manual approval action between Security and Deploy stages. 9. Configure CloudWatch log groups for all services with 30-day retention. 10. Create parameters for deployment configuration values. Expected output: A fully functional Pulumi program that provisions an end-to-end CI/CD pipeline capable of building, testing, and deploying containerized applications with automated security scanning and manual approval gates.","A DevOps team needs to establish a multi-stage CI/CD pipeline for their microservices architecture. The pipeline must support automated testing, security scanning, and blue-green deployments across development, staging, and production environments.","""Multi-stage CI/CD infrastructure deployed in us-east-1 using AWS CodePipeline for orchestration, CodeBuild for compilation and testing, ECS Fargate for container hosting. VPC spans 3 availability zones with private subnets for ECS tasks and public subnets for ALB. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 16+, and AWS CLI configured with appropriate credentials. Pipeline integrates with CodeCommit for source control and deploys containerized applications using blue-green strategy.""","[""CodePipeline must have exactly 5 stages: Source, Build, Test, Security, and Deploy"", ""Use CodeCommit as the source repository with branch-based triggers"", ""CodeBuild projects must use Amazon Linux 2 runtime with buildspec.yml"", ""Implement manual approval action before production deployment"", ""Store all build artifacts in S3 with AES256 encryption"", ""Use to send pipeline status notifications to SNS"", ""Deploy to ECS Fargate using blue-green deployment strategy"", ""All IAM roles must follow least-privilege principle with no wildcard actions"", ""Pipeline execution logs must be retained in CloudWatch for 30 days"", ""Use for storing deployment configuration""]"
g7w0q7,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to deploy a multi-region disaster recovery infrastructure. The configuration must: 1. Create Aurora PostgreSQL Global Database with primary cluster in us-east-1 and secondary in us-west-2. 2. Deploy ECS Fargate services running a containerized application in both regions with active-passive configuration. 3. Set up Application Load Balancers in each region with proper target groups pointing to ECS services. 4. Configure Route 53 hosted zone with health check-based failover routing between regions. 5. Create DynamoDB global tables for session state that replicate between both regions. 6. Implement VPCs in each region with cross-region peering and appropriate security groups. 7. Set up CloudWatch Synthetics canaries to monitor application endpoints every 5 minutes. 8. Configure SNS topics in both regions with cross-region subscriptions for alerting. 9. Create AWS Backup plans for Aurora clusters with daily snapshots and 30-day retention. 10. Use Parameter Store to store configuration values replicated across regions. Expected output: A Pulumi TypeScript program that creates a fully automated disaster recovery infrastructure with RTO of 5 minutes, supporting automatic failover for all components and maintaining data consistency across regions.",A financial services company needs to implement a disaster recovery solution for their critical transaction processing system. The system must maintain 99.99% availability and automatically recover from both regional failures and service-level disruptions within 5 minutes.,"""Multi-region disaster recovery infrastructure deployed across us-east-1 (primary) and us-west-2 (secondary) regions. Uses Aurora PostgreSQL Global Database for data tier, ECS Fargate for compute, DynamoDB global tables for session management, and Route 53 for DNS failover. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions. Each region has its own VPC with 3 availability zones, private subnets for compute/data tiers, public subnets for ALBs, and NAT gateways for outbound traffic. Cross-region VPC peering enables secure communication between regions.""","[""Use Aurora PostgreSQL Global Database for cross-region replication with automated failover"", ""Deploy ECS Fargate services in active-passive configuration across two regions"", ""Implement Route 53 health checks with automated failover between regions"", ""Configure DynamoDB global tables for session state replication"", ""Set up cross-region VPC peering with proper routing tables"", ""Use AWS Backup for automated daily snapshots with 30-day retention"", ""Implement CloudWatch Synthetics canaries for endpoint monitoring"", ""Configure SNS topics with cross-region replication for alerts"", ""Use Parameter Store with cross-region replication for configuration""]"
s8m7o5,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a secure data processing pipeline with end-to-end encryption and audit logging. MANDATORY REQUIREMENTS (Must complete): 1. Create a VPC with 3 private subnets across different AZs, no public subnets (CORE: VPC) 2. Deploy Lambda function for data processing with 1GB memory, 5-minute timeout (CORE: Lambda) 3. Create bucket for data storage with customer-managed KMS encryption (CORE: ) 4. Configure Secrets Manager secret for storing API credentials (CORE: Secrets Manager) 5. Implement for , Secrets Manager, and KMS services 6. Create separate KMS keys for encryption and CloudWatch Logs encryption 7. Enable VPC flow logs with 90-day retention in encrypted bucket 8. Configure Lambda execution role with minimal permissions to read from Secrets Manager and write to 9. Implement CloudWatch Log Group for Lambda with KMS encryption 10. Apply mandatory tags (CostCenter, Environment, DataClassification) to all resources 11. Enable KMS key rotation for all customer-managed keys 12. Output the Lambda function ARN and bucket name OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Config rules for compliance monitoring (OPTIONAL: Config) - ensures ongoing compliance  Implement GuardDuty for threat detection (OPTIONAL: GuardDuty) - adds security monitoring  Create CloudTrail with event filtering (OPTIONAL: CloudTrail alternative) - provides audit trail Expected output: A Pulumi TypeScript program that deploys a fully secure data processing infrastructure with encryption at rest and in transit, network isolation, and comprehensive audit logging. The stack should demonstrate security best practices with all controls defined as code.","A financial services company requires a secure data processing pipeline that encrypts sensitive customer data at every stage. The pipeline must meet strict compliance requirements including audit logging, encryption key rotation, and network isolation. All security controls must be defined as code to ensure consistency across deployments.","""Secure multi-AZ deployment in us-east-2 with Lambda for data processing, for encrypted storage, and Secrets Manager for API credentials. VPC spans 3 AZs with private subnets only, no . for , Secrets Manager, and KMS services. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions. Customer-managed KMS keys for all encryption. CloudWatch Logs encrypted with separate KMS key. VPC flow logs to dedicated bucket.""","[""All buckets must use customer-managed KMS keys with automatic rotation enabled"", ""Lambda functions must run in private subnets with no direct internet access"", ""All IAM roles must follow least-privilege principle with no wildcard actions"", ""CloudWatch Logs must be encrypted with a separate KMS key"", ""VPC flow logs must be enabled and stored encrypted for 90 days"", ""All resources must be tagged with CostCenter, Environment, and DataClassification tags""]"
h5v4a7,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy an automated infrastructure compliance analyzer. MANDATORY REQUIREMENTS (Must complete): 1. Deploy AWS Config with recording enabled for EC2, RDS, and Lambda resources (CORE: AWS Config) 2. Create Lambda function to analyze Config snapshots and detect non-compliant resources (CORE: Lambda) 3. Store compliance findings in DynamoDB with partition key 'resourceId' and sort key 'timestamp' 4. Configure EventBridge scheduled rule to trigger compliance scans every 4 hours 5. Create bucket with intelligent tiering for storing compliance reports 6. Implement CloudWatch Logs with 30-day retention for all Lambda functions 7. Export Config recorder name, Lambda ARN, and DynamoDB table name as stack outputs 8. All IAM roles must have explicit deny for resource deletion actions 9. Tag all resources with Environment='compliance' and ManagedBy='pulumi' OPTIONAL ENHANCEMENTS (If time permits):  Add topic for critical compliance violations (OPTIONAL: ) - enables real-time alerting  Implement for multi-stage analysis workflow (OPTIONAL: ) - adds orchestration capabilities  Deploy QuickSight dashboard for compliance visualization (OPTIONAL: QuickSight) - improves reporting Expected output: A Pulumi TypeScript program that deploys a fully functional compliance analysis system capable of scanning AWS resources, detecting violations, and generating reports with proper scheduling and storage.","A financial services company needs automated infrastructure compliance scanning to meet SOC 2 requirements. Their AWS environments must be continuously analyzed for configuration drift, security violations, and cost optimization opportunities across multiple accounts.","""Multi-account AWS infrastructure deployed in us-east-1 region for compliance scanning and analysis. Uses AWS Config for resource tracking, Lambda functions for analysis logic, DynamoDB for storing compliance results, for reports storage, and EventBridge for scheduling. Requires Pulumi 3.x with TypeScript, Node.js 18+, and AWS CLI configured with cross-account assume role permissions. for and DynamoDB to reduce data transfer costs. CloudWatch Logs for Lambda execution tracking.""","[""Use AWS Config for resource inventory and compliance tracking"", ""Lambda functions must have 256MB memory and 5-minute timeout"", ""All buckets must have versioning enabled and lifecycle policies"", ""DynamoDB tables must use on-demand billing mode"", ""EventBridge rules must trigger scans every 4 hours"", ""IAM roles must follow least-privilege with no inline policies"", ""All Lambda functions must use ARM-based Graviton2 processors"", ""CloudWatch Logs retention must be set to 30 days"", ""Use Pulumi stack outputs to export critical resource ARNs""]"
r8k0s5,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready Amazon EKS cluster with enhanced security and cost optimization features. MANDATORY REQUIREMENTS (Must complete): 1. Create an EKS cluster version 1.28 with private API endpoint access (CORE: EKS) 2. Configure two managed node groups: one on-demand (t3.medium, min 2, max 4) and one spot (t3.medium/t3.large, min 1, max 6) (CORE: EC2) 3. Enable IRSA and create service account for cluster autoscaler with appropriate IAM policy 4. Configure OIDC provider and associate with the cluster for workload identity 5. Enable control plane logging for api, audit, authenticator, and controllerManager logs 6. Install cluster autoscaler using Helm with 5-minute scale-down delay 7. Configure pod security standards with 'restricted' as cluster-wide default 8. Enable AWS VPC CNI with security groups for pods feature 9. Create GP3 EBS storage class with encryption as default 10. Output cluster endpoint, OIDC issuer URL, and kubeconfig OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Load Balancer Controller for ingress management (OPTIONAL: Elastic Load Balancing) - enables advanced load balancing features  Implement Karpenter for more efficient node provisioning (OPTIONAL: Karpenter) - improves scaling performance and cost  Add Amazon GuardDuty EKS Protection (OPTIONAL: GuardDuty) - enhances security monitoring Expected output: Complete Pulumi program that provisions a production-ready EKS cluster with managed node groups, IRSA enabled, cluster autoscaler deployed, and security best practices implemented. The program should be idempotent and include proper error handling.",A financial services company needs to deploy their microservices architecture on Kubernetes to meet regulatory compliance requirements for container isolation and audit logging. They require a production-grade EKS cluster with strict security controls and automated node management to support their 24/7 trading platform.,"""Production-grade EKS infrastructure deployed in us-east-2 across 3 availability zones. Uses EKS 1.28 with managed node groups mixing on-demand and spot instances. Requires Pulumi 3.x with TypeScript, kubectl 1.28+, AWS CLI v2 configured with appropriate permissions. VPC with private subnets for worker nodes, public subnets for load balancers. Integration with for node access. Container insights enabled for monitoring with CloudWatch Container Insights. Network architecture includes for ECR, S3, and EC2 to reduce data transfer costs.""","[""Use managed node groups with Spot instances for cost optimization (at least 30% spot capacity)"", ""Enable IRSA (IAM Roles for Service Accounts) for all workload pods"", ""Implement pod security standards with restricted baseline as default"", ""Configure OIDC provider for external authentication integration"", ""Use GP3 EBS volumes with encryption for all persistent storage"", ""Deploy cluster autoscaler with scale-down delay of 5 minutes"", ""Enable control plane logging for api, audit, authenticator, controllerManager"", ""Implement network policies using AWS VPC CNI with security groups for pods""]"
t9e8a0,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to deploy a multi-region disaster recovery infrastructure for a payment processing system. MANDATORY REQUIREMENTS (Must complete): 1. Set up Aurora Global Database cluster with one writer in us-east-1 and reader in us-west-2 (CORE: Aurora) 2. Create DynamoDB global tables spanning both regions with on-demand billing (CORE: DynamoDB) 3. Deploy identical Lambda functions in both regions for payment processing with 3GB memory 4. Configure Route 53 hosted zone with health check-based failover routing between regions 5. Implement EventBridge rules in both regions to replicate critical events 6. Create VPCs in both regions with 3 private subnets each and establish VPC peering 7. Set up CloudWatch alarms for RTO/RPO monitoring with notifications 8. Configure KMS keys in both regions for encryption with automatic key rotation OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Backup for automated cross-region backup copies (OPTIONAL: AWS Backup) - provides additional data protection  Implement for orchestrated failover workflows (OPTIONAL: ) - enables automated recovery procedures  Configure rules for compliance monitoring (OPTIONAL: ) - ensures DR configuration compliance Expected output: A Pulumi TypeScript program that deploys a complete multi-region disaster recovery infrastructure with automated failover capabilities, achieving RTO under 5 minutes and RPO under 30 seconds.",A financial services company requires a disaster recovery solution that can automatically failover their critical transaction processing system between AWS regions within 5 minutes. The system processes real-time payment data and must maintain data consistency across regions while minimizing recovery point objective (RPO) to under 30 seconds.,"""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary). Utilizes Aurora Global Database for PostgreSQL 14.x with automated backups, DynamoDB global tables for session management, Lambda functions for transaction processing, and EventBridge for event-driven workflows. VPCs in both regions with 3 private subnets each, connected via VPC peering. Requires Pulumi 3.x with TypeScript, AWS CLI v2, Node.js 18+. Route 53 hosted zone for health-check based DNS failover. KMS keys in both regions for encryption at rest.""","[""Use Route 53 health checks with failover routing policy for automatic DNS failover"", ""Implement DynamoDB global tables with point-in-time recovery enabled"", ""Configure Aurora Global Database with backtrack window of 72 hours"", ""Deploy Lambda functions in both regions with identical environment variables and IAM roles"", ""Set up EventBridge global endpoints for cross-region event replication"", ""All resources must be tagged with DisasterRecovery: 'primary' or 'secondary' based on region""]"
h2l6o5,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster with advanced security and autoscaling capabilities. The configuration must: 1. Create a VPC with 3 public and 3 private subnets across us-east-1a, us-east-1b, and us-east-1c. 2. Deploy an EKS cluster version 1.28+ with API endpoint access restricted to private only. 3. Configure 2 managed node groups: 'system' (2-4 nodes) for cluster components and 'application' (3-9 nodes) for workloads. 4. Enable OIDC provider and create IRSA mappings for aws-load-balancer-controller and cluster-autoscaler. 5. Install AWS Load Balancer Controller using Helm with proper IRSA configuration. 6. Deploy Calico network plugin for NetworkPolicy support with pod-to-pod encryption. 7. Create CloudWatch Log Groups with 30-day retention for all EKS control plane logs. 8. Configure Horizontal Pod Autoscaler to scale based on custom CloudWatch metrics. 9. Set up AWS Systems Manager Parameter Store integration for secret management. 10. Implement pod security standards with 'restricted' enforcement for application namespace. 11. Create ingress rules allowing HTTPS traffic only with ACM certificate integration. 12. Tag all resources with Environment=production, ManagedBy=pulumi, and CostCenter=engineering. Expected output: A fully functional EKS cluster accessible via kubectl with all security controls in place, autoscaling configured, and ingress controller ready to serve HTTPS traffic. The stack should export the cluster endpoint, OIDC issuer URL, node group ARNs, and kubeconfig for downstream automation.",A financial services company needs to modernize their monolithic trading platform by breaking it into containerized microservices. They require a production-grade Kubernetes environment on AWS with strict security requirements for PCI compliance and zero-downtime deployment capabilities.,"""Production-grade EKS cluster deployed in us-east-1 across 3 availability zones for high availability. Infrastructure includes VPC with private subnets for worker nodes, NAT Gateways for outbound traffic, and public subnets for load balancers. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions, kubectl 1.28+, and helm 3.x. The cluster uses managed node groups with t3.large instances, AWS VPC CNI for networking, and integrates with AWS services including CloudWatch, IAM, and Application Load Balancer. Security hardening includes pod security standards, IRSA for fine-grained permissions, and network policies for traffic control.""","[""Use AWS EKS version 1.28 or higher with managed node groups"", ""Deploy across exactly 3 availability zones in us-east-1"", ""Implement pod-to-pod encryption using AWS VPC CNI with SecurityGroupPolicy"", ""Configure IRSA (IAM Roles for Service Accounts) for all workload pods"", ""Enable EKS control plane logging for audit, authenticator, and controllerManager"", ""Use only private subnets for worker nodes with NAT Gateway egress"", ""Configure Horizontal Pod Autoscaler with custom metrics from CloudWatch"", ""Implement Network Policies for namespace isolation using Calico CNI"", ""Set up AWS Load Balancer Controller for ingress with SSL termination"", ""Export all resource ARNs and endpoints as stack outputs for CI/CD integration""]"
c7e9a1,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy an observability stack for microservices monitoring. The configuration must: 1. Set up CloudWatch Log Groups with KMS encryption for three microservices: payment-api, fraud-detector, and notification-service (CORE: CloudWatch). 2. Configure X-Ray tracing with environment-specific sampling rates and encryption (CORE: X-Ray). 3. Create custom CloudWatch metrics using Lambda to process payment transaction logs and emit latency metrics. 4. Deploy SNS FIFO topics for critical alerts (payment-failures.fifo, fraud-detected.fifo) with dead-letter queues. 5. Implement CloudWatch alarms for Lambda errors exceeding 1% over 5 minutes. 6. Configure CloudWatch dashboards displaying service health, transaction volume, and error rates. 7. Set up metric filters to detect specific error patterns in logs. 8. Create IAM roles with granular permissions for each service component. 9. Enable CloudWatch Container Insights for ECS services monitoring. 10. Configure log aggregation from multiple sources into centralized log groups. Expected output: A complete Pulumi TypeScript program that provisions a production-ready observability infrastructure with proper encryption, retention policies, and alerting mechanisms that can be deployed using pulumi up.","A fintech startup requires comprehensive monitoring for their payment processing platform to meet regulatory compliance requirements. The system must capture detailed metrics, logs, and traces from their microservices architecture while maintaining PCI DSS compliance standards.","""Production monitoring infrastructure deployed in us-east-1 across 3 availability zones. Core services include CloudWatch for metrics and logs, X-Ray for distributed tracing, SNS for alerting, and Lambda for custom metric processing. Requires Pulumi 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate credentials. VPC endpoints for CloudWatch and X-Ray to keep traffic within AWS network. KMS encryption keys managed in separate security account with cross-account access.""","[""All logs must be encrypted at rest and in transit using AWS KMS customer-managed keys"", ""CloudWatch Logs retention must be set to exactly 365 days for compliance"", ""X-Ray tracing sampling rate must be configurable per environment (dev: 100%, prod: 5%)"", ""Lambda functions must have custom metrics for transaction processing latency"", ""SNS topics for alerts must use FIFO configuration with content-based deduplication"", ""All IAM roles must follow least-privilege principle with no wildcard resource permissions""]"
d5d8d0,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to orchestrate a phased migration of a three-tier application from on-premises to AWS. The configuration must: 1. Set up Site-to-Site VPN connection with customer gateway configuration for on-premises connectivity. 2. Create Aurora MySQL cluster with read endpoint exposed to both environments (CORE: Aurora). 3. Configure DMS replication instance and tasks for continuous database sync (CORE: Database Migration Service). 4. Deploy application tier on with environment-aware connection strings. 5. Implement health check Lambda function to validate migration readiness. 6. Create parameters for connection strings and feature flags. 7. Set up CloudWatch dashboard to monitor replication lag and connection metrics. 8. Configure ALB with weighted target groups for blue-green traffic shifting. 9. Implement automated rollback mechanism using Step Functions for failed migrations. OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Transit Gateway for simplified multi-VPC connectivity (OPTIONAL: Transit Gateway) - simplifies network architecture  Implement AWS AppSync for real-time data sync monitoring (OPTIONAL: AppSync) - provides GraphQL API for migration status  Add AWS Backup for point-in-time recovery during migration (OPTIONAL: Backup) - enhances data protection Expected output: Complete Pulumi TypeScript program that creates hybrid infrastructure supporting phased migration with automated health checks, monitoring, and rollback capabilities. The solution should output VPN connection details, DMS endpoint URLs, and migration dashboard URL.","Your company is migrating a legacy three-tier application from an on-premises data center to AWS. The application consists of a web tier, application tier, and database tier that must be migrated in phases while maintaining connectivity to on-premises systems during the transition period.","""Hybrid AWS environment in us-east-2 region connected to on-premises data center via Site-to-Site VPN. Requires Pulumi CLI 3.x with TypeScript, Node.js 16+, AWS CLI configured with appropriate credentials. VPC with 10.0.0.0/16 CIDR spanning 3 availability zones, with dedicated subnets for VPN gateway, application tiers, and database migration. Uses Aurora MySQL 8.0 for database tier, for application tier, and ALB for web tier. AWS DMS for continuous database replication from on-premises MySQL 5.7. for configuration management across environments.""","[""Use AWS Site-to-Site VPN for hybrid connectivity during migration"", ""Implement blue-green deployment strategy for zero-downtime cutover"", ""Database must support read replicas in both environments during transition"", ""Application tier must handle both on-premises and cloud database connections"", ""Use AWS Database Migration Service for continuous replication"", ""Implement circuit breaker pattern for failover between environments"", ""All resources must be tagged with 'MigrationPhase' and 'OriginalEnvironment'"", ""Use for environment-specific configurations"", ""Implement automated rollback capability if migration health checks fail""]"
u6k1o0,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a multi-environment infrastructure setup that supports development, staging, and production workloads.

MANDATORY REQUIREMENTS (Must complete):
1. Create a reusable Pulumi ComponentResource class that accepts environment type as parameter (CORE: Lambda)
2. Deploy RDS PostgreSQL instances with environment-specific instance sizes (dev: db.t3.micro, staging: db.t3.small, prod: db.t3.medium) (CORE: RDS)
3. Configure Lambda functions with environment-appropriate memory settings and timeout values
4. Set up S3 buckets with intelligent tiering and 90-day object expiration lifecycle rules
5. Implement VPC with 2 public and 2 private subnets using environment-specific CIDR ranges
6. Create security groups allowing only necessary inter-service communication
7. Configure CloudWatch Log Groups with environment-based retention periods (dev: 3 days, staging: 7 days, prod: 30 days)
8. Use Pulumi stack configurations to manage environment-specific variables
9. Ensure all resources follow naming convention: {project}-{environment}-{resource-type}
10. Export critical resource IDs and endpoints as stack outputs

OPTIONAL ENHANCEMENTS (If time permits):
 Add API Gateway REST APIs for Lambda function invocation (OPTIONAL: API Gateway) - provides HTTP endpoints for testing
 Implement SNS topics for environment-specific alerts (OPTIONAL: SNS) - enables notification routing
 Add Secrets Manager for database credentials rotation (OPTIONAL: Secrets Manager) - improves security posture

Expected output: A Pulumi TypeScript project with reusable components that can deploy complete infrastructure stacks for dev, staging, and production environments with a single pulumi up command per stack.","A fintech startup needs to establish isolated AWS environments for development, staging, and production workloads. Each environment requires identical infrastructure components but with different sizing and security configurations based on the environment type.","""Multi-environment AWS infrastructure deployed across three separate AWS accounts in us-east-1 region. Each environment consists of VPC with public/private subnets across 2 AZs, RDS PostgreSQL instances, Lambda functions for data processing, and S3 buckets for storage. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate credentials for each account. Network architecture includes NAT Gateways for private subnet outbound connectivity and VPC peering between environments for secure cross-environment communication.""","[""All resources must be tagged with Environment, Project, and Owner tags"", ""Production RDS instances must have automated backups with 7-day retention"", ""Lambda functions must use environment-specific memory allocations (dev: 256MB, staging: 512MB, prod: 1024MB)"", ""VPC CIDR blocks must not overlap between environments (10.0.0.0/16, 10.1.0.0/16, 10.2.0.0/16)"", ""All S3 buckets must have versioning enabled and lifecycle policies for object expiration""]"
x3q5n3,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy consistent infrastructure across three environments (dev, staging, prod) with automated drift detection. The configuration must: 1. Deploy ECS Fargate services with environment-specific CPU/memory allocations (dev: 0.5vCPU/1GB, staging: 1vCPU/2GB, prod: 2vCPU/4GB) while maintaining identical container definitions. 2. Create Aurora PostgreSQL clusters with consistent engine versions (15.4) but environment-specific instance classes (dev: db.t4g.medium, staging/prod: db.r6g.large). 3. Implement Secrets Manager rotation for database credentials with identical rotation schedules across all environments (30-day rotation). 4. Define reusable component resources that enforce configuration consistency through TypeScript interfaces. 5. Generate a JSON manifest file comparing critical settings across environments for CI/CD validation. 6. Use Pulumi stack references to share VPC and subnet IDs from a separate networking stack. 7. Implement custom validation functions that fail deployment if environment configurations drift beyond allowed parameters. 8. Configure CloudWatch alarms with environment-specific thresholds but identical metric definitions. Expected output: A Pulumi TypeScript program with stack-specific configurations that deploys three environments with verifiable consistency, outputs a comparison manifest, and prevents configuration drift through compile-time type checking and runtime validation.","A fintech startup needs to maintain identical infrastructure across development, staging, and production environments for their payment processing platform. They require strict consistency in service configurations while allowing environment-specific parameters like database sizing and auto-scaling limits. The team uses GitOps workflows and needs programmatic validation of environment parity.","""Multi-environment AWS deployment across us-east-1 (production), us-west-2 (staging), and us-east-2 (development). Each environment requires separate AWS accounts with cross-account IAM roles for deployment. Infrastructure includes ECS Fargate for containerized services, Aurora PostgreSQL 15.4 for data persistence, and Secrets Manager for credential rotation. Requires Pulumi CLI 3.x, TypeScript 5.x, Node.js 18+, and AWS CLI configured with profiles for each environment. VPCs are pre-provisioned in separate stacks with 3 availability zones per region.""","[""All environments must use identical Docker image tags pulled from a shared ECR repository"", ""Database encryption keys must be customer-managed KMS keys with identical key policies"", ""ECS task definitions must have identical environment variables except for ENVIRONMENT_NAME"", ""Aurora clusters must have automated backups with 7-day retention in all environments"", ""Secrets Manager secrets must use identical naming conventions with environment prefixes"", ""CloudWatch log groups must have consistent retention periods of 30 days across environments"", ""All IAM roles must follow principle of least privilege with no use of managed policies"", ""Stack outputs must include a SHA-256 hash of the configuration for drift detection""]"
y5w6v3,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a high-performance web application infrastructure. The configuration must: 1. Set up VPC with 3 public and 3 private subnets across different AZs with NAT Gateways. 2. Deploy ECS Fargate service running containerized Node.js API with auto-scaling (2-10 tasks). 3. Configure Application Load Balancer with path-based routing and sticky sessions. 4. Create distribution with S3 origin for static assets and ALB origin for API. 5. Provision Aurora PostgreSQL Serverless v2 with one writer and two reader instances. 6. Implement Lambda@Edge function for request authentication and header manipulation. 7. Set up CloudWatch dashboards with custom metrics for API latency and error rates. 8. Configure Auto Scaling policies based on ECS CPU utilization (scale at 70%). 9. Create S3 buckets for static assets, logs, and application artifacts with encryption. 10. Implement CloudWatch alarms for ALB target health, ECS task count, and RDS connections. Expected output: Complete Pulumi TypeScript program that provisions production-ready web application infrastructure with automated scaling, monitoring, and high availability across multiple availability zones.","A growing e-commerce platform needs to deploy their Node.js product catalog API with real-time inventory updates. The system must handle 50,000 concurrent users during flash sales and integrate with their existing PostgreSQL database while maintaining sub-200ms response times.","""Production deployment in us-east-1 region across 3 availability zones. Infrastructure includes Application Load Balancer distributing traffic to ECS Fargate containers running Node.js API, Aurora PostgreSQL Serverless v2 cluster with read replicas, CDN for static assets stored in S3. VPC configured with public subnets for ALB, private subnets for ECS tasks and RDS. NAT Gateways in each AZ for outbound connectivity. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI v2 configured with appropriate IAM permissions.""","[""All Lambda functions must have 3GB memory allocation for optimal cold start performance"", "" tables must use pay-per-request billing mode to handle traffic spikes"", "" must implement usage plans with 10,000 requests per second throttling"", ""RDS proxy connections must be limited to 100 concurrent database connections"", "" distributions must use custom error pages for 4xx and 5xx responses"", ""All S3 buckets must have versioning enabled and lifecycle policies for 30-day retention"", ""Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""Application Load Balancer must perform health checks every 5 seconds with 2-second timeout""]"
x2b4k4,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a serverless payment webhook processing system. The configuration must: 1. Create an API Gateway REST API with a /webhooks endpoint that accepts POST requests. 2. Deploy a Lambda function (Node.js 18, ARM architecture) to validate webhook signatures and store events in DynamoDB. 3. Create a DynamoDB table with partition key 'paymentId' and sort key 'timestamp' with point-in-time recovery enabled. 4. Implement a Step Functions state machine that processes payment events with retry logic for failed payments. 5. Deploy a Lambda function to execute payment processing logic within the state machine. 6. Create an EventBridge rule that triggers the Step Functions execution when new items are added to DynamoDB Streams. 7. Configure X-Ray tracing across all Lambda functions and API Gateway. 8. Create a customer-managed KMS key for encrypting Lambda environment variables. 9. Output the API Gateway endpoint URL and Step Functions state machine ARN. Expected output: A complete Pulumi TypeScript program that deploys the entire serverless payment processing pipeline with proper error handling, encryption, and observability features.","A financial technology startup needs a serverless event processing system to handle real-time payment notifications from multiple payment providers. The system must process webhook events asynchronously, validate payment signatures, and trigger downstream workflows based on payment status.","""Serverless infrastructure deployed in us-east-1 using API Gateway for webhook ingestion, Lambda functions for event processing, DynamoDB for event storage, Step Functions for orchestration, and EventBridge for event routing. Requires Node.js 18+, Pulumi 3.x with TypeScript, AWS CLI configured with appropriate credentials. No VPC required as all services are serverless. KMS customer-managed keys for encryption, X-Ray for distributed tracing across all components.""","[""Lambda functions must use ARM-based Graviton2 processors for cost optimization"", ""DynamoDB tables must use point-in-time recovery and encryption at rest"", ""All Lambda functions must have reserved concurrent executions configured"", ""Use Pulumi's native AWS provider without AWS SDK imports"", ""Lambda environment variables must be encrypted with a customer-managed KMS key"", ""Enable X-Ray tracing on all Lambda functions and API Gateway stages"", ""Implement exponential backoff retry logic in Step Functions state machine"", ""All IAM policies must follow least privilege principle with no wildcard actions""]"
x8e2i6,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement a multi-region disaster recovery architecture for a transaction processing system. The configuration must: 1. Deploy RDS Aurora Global Database (PostgreSQL 14.x) with a primary cluster in us-east-1 and secondary in us-west-2. 2. Configure DynamoDB global tables for session data with on-demand billing in both regions. 3. Create Lambda functions (Node.js 18.x) in both regions with 512MB memory for transaction processing. 4. Set up Application Load Balancers in each region with target groups pointing to Lambda functions. 5. Implement Route 53 failover routing policy with health checks on primary ALB endpoints. 6. Configure S3 buckets in both regions with cross-region replication for transaction logs. 7. Create CloudWatch dashboards showing RDS replication lag, Lambda errors, and ALB response times. 8. Set up SNS topics in both regions for automated failover notifications. 9. Implement CloudWatch alarms for RDS replication lag > 5 seconds and Lambda error rate > 1%. 10. Configure VPC peering between regions with appropriate route tables for database replication. Expected output: A complete Pulumi TypeScript program that provisions all resources with proper dependencies, exports primary and secondary endpoint URLs, and includes stack outputs for monitoring dashboard URLs and failover status.","A financial services company requires a disaster recovery solution for their critical transaction processing system. The primary region experienced a 4-hour outage last quarter, resulting in significant revenue loss. They need automated failover capabilities with data replication across regions to meet their 99.99% uptime SLA.","""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary). Deploys RDS Aurora Global Database with PostgreSQL 14.x for transactional data, DynamoDB global tables for session management, and Lambda functions for business logic. Each region has its own VPC with 3 availability zones, private subnets for databases, and public subnets for ALBs. Route 53 manages DNS failover between regions. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions, Node.js 16+. Infrastructure includes automated health checks, CloudWatch monitoring, and SNS notifications for failover events.""","[""RDS Aurora Global Database must use PostgreSQL 14.x with automated backups"", ""Lambda functions must have reserved concurrency of 100 per region"", ""Route 53 health checks must trigger failover within 60 seconds"", ""DynamoDB global tables must have point-in-time recovery enabled"", ""All data must be encrypted at rest using customer-managed KMS keys"", ""VPC peering connections must use non-overlapping CIDR blocks"", ""CloudWatch alarms must send notifications to both SNS and PagerDuty"", ""S3 buckets must use cross-region replication with RTC (Replication Time Control)"", ""Application Load Balancers must have connection draining set to 30 seconds"", ""Cost allocation tags must include 'Environment', 'CostCenter', and 'Owner'""]"
u4m9o0,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a secure secrets management infrastructure with automated rotation. MANDATORY REQUIREMENTS (Must complete): 1. Create AWS Secrets Manager secrets for RDS credentials with automatic rotation every 30 days (CORE: Secrets Manager) 2. Deploy Lambda function in VPC private subnet to handle secret rotation logic (CORE: Lambda) 3. Configure customer-managed KMS key with key policy allowing only specific IAM roles 4. Create VPC with 3 private subnets across availability zones (no public subnets) 5. Set up VPC endpoint for Secrets Manager to ensure private connectivity 6. Implement IAM roles with explicit deny for any action outside the VPC 7. Configure CloudWatch log group with 365-day retention for audit logs 8. Apply mandatory tags: Environment, CostCenter, Compliance, Owner to all resources OPTIONAL ENHANCEMENTS (If time permits):  Add EventBridge rule to notify on rotation failures (OPTIONAL: EventBridge) - improves incident response  Implement for complex multi-step rotations (OPTIONAL: ) - handles dependencies  Add AWS Config rules for compliance validation (OPTIONAL: Config) - automates compliance checks Expected output: Complete Pulumi TypeScript program that deploys a production-ready secrets management system with automated rotation, strict security controls, and full audit capabilities. The infrastructure should pass security compliance checks with no public exposure of secrets.","A financial services company needs to implement a secure secrets management system for their microservices architecture. The system must enforce strict access controls, audit trails, and encryption standards to meet PCI-DSS compliance requirements. All secrets must be centrally managed with automated rotation capabilities.","""Highly secure multi-AZ deployment in us-east-1 for secrets management infrastructure. Uses AWS Secrets Manager for storing database credentials, API keys, and certificates. KMS with customer-managed keys for encryption. Lambda functions in isolated VPC for secret rotation. for private connectivity. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate permissions. Private subnets across 3 availability zones with no internet gateway. CloudWatch Logs for audit trails with 365-day retention.""","[""All secrets must be stored in AWS Secrets Manager with automatic rotation enabled"", ""KMS keys must use customer-managed keys (CMK) with strict key policies"", ""Lambda functions for rotation must run in isolated VPC with no internet access"", ""IAM roles must follow least privilege with no inline policies allowed"", ""All API calls to Secrets Manager must be logged to CloudWatch"", ""Secret access must be restricted to specific only"", ""Rotation functions must complete within 60 seconds timeout"", ""All resources must have cost allocation tags for compliance tracking""]"
o4h4m9,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,Create a Pulumi TypeScript program to optimize an existing ECS Fargate infrastructure. The configuration must: 1. Refactor ECS task definitions to use spot capacity for non-critical services. 2. Implement auto-scaling policies based on custom CloudWatch metrics. 3. Optimize RDS Aurora cluster with read replicas and connection pooling. 4. Configure ECS service discovery for internal service communication. 5. Implement resource tagging strategy for cost allocation. 6. Add Circuit Breaker pattern to ECS services. 7. Configure VPC endpoints to reduce NAT Gateway costs. 8. Implement blue-green deployment strategy for zero-downtime updates. Expected output: Optimized Pulumi TypeScript code that reduces infrastructure costs by 40% while improving performance and reliability.,A fintech company's ECS-based payment processing system is experiencing performance bottlenecks and high costs. The existing Pulumi infrastructure was hastily deployed and needs optimization for production workloads handling 50K+ transactions per hour.,"""Production environment in us-east-1 with ECS Fargate running containerized microservices, Aurora PostgreSQL cluster, and Application Load Balancer. Requires Pulumi 3.x with TypeScript, AWS CLI configured with production credentials. VPC spans 3 AZs with public/private subnets. Current monthly cost exceeds $15K with poor resource utilization. System processes financial transactions requiring 99.9% uptime.""","[""Maintain zero-downtime during optimization rollout"", ""ECS tasks must use IAM roles for service accounts"", ""Database connections must use SSL/TLS encryption"", ""Implement cost allocation tags following FinOps standards"", ""Aurora cluster must support point-in-time recovery"", ""ECS services must have health checks with proper thresholds"", ""VPC endpoints must not break existing service connectivity"", ""Resource names must follow company naming convention""]"
o7x7d5,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to analyze and validate infrastructure compliance across multiple AWS accounts. The configuration must: 1. Read a configuration file listing Pulumi stack names and their associated AWS account IDs. 2. Use Pulumi Automation API to retrieve stack outputs and resource information. 3. Implement validators for S3 bucket compliance (versioning, encryption, public access blocking). 4. Implement validators for RDS compliance (encryption, backup retention, multi-AZ deployment). 5. Check IAM roles for overly permissive policies (no wildcard actions on sensitive resources). 6. Validate that all Lambda functions have reserved concurrent executions configured. 7. Generate structured JSON reports with pass/fail status and remediation suggestions. 8. Support dry-run mode that simulates analysis without making API calls. 9. Include error handling for stacks that are unreachable or have invalid credentials. 10. Provide summary statistics showing compliance percentage per service type. Expected output: A TypeScript program using Pulumi that analyzes existing infrastructure stacks, validates them against security policies, and produces detailed compliance reports in JSON format suitable for integration with monitoring dashboards.",Your company runs a multi-tenant SaaS application on AWS. The operations team needs automated infrastructure validation to ensure all customer environments comply with security policies and resource limits. They require a Pulumi-based solution that can analyze existing infrastructure and generate compliance reports.,"""Multi-tenant AWS infrastructure deployed across us-east-1 and eu-west-1 regions. Each tenant has isolated resources including VPC with private subnets, RDS PostgreSQL instances, S3 buckets for storage, and Lambda functions for business logic. Infrastructure managed through Pulumi stacks with TypeScript. Requires Pulumi CLI 3.x, Node.js 18+, AWS SDK v3. Analysis tool needs read-only IAM role with permissions to describe resources across all tenant accounts. Tool runs from central monitoring account with cross-account assume role capabilities.""","[""Use Pulumi's automation API to programmatically inspect stack outputs"", ""Implement custom policy checks using TypeScript classes and interfaces"", ""Generate JSON reports with findings categorized by severity (critical, high, medium, low)"", ""Support analyzing multiple stacks in parallel with configurable concurrency"", ""Include timing metrics for each validation check in the report"", ""Cache AWS API responses to avoid rate limiting during large-scale analysis"", ""Validate that all S3 buckets have versioning enabled and encryption at rest"", ""Ensure all RDS instances use encrypted storage and have automated backups configured""]"
a0r4k3,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster with advanced security configurations. The configuration must: 1. Create an EKS cluster version 1.28 with private endpoint access only and encrypted secrets using KMS. 2. Configure managed node groups with mixed instance types (t3.medium as primary, t3.large as secondary) using Spot capacity with on-demand fallback. 3. Enable all EKS control plane log types (api, audit, authenticator, controllerManager, scheduler) with CloudWatch Logs retention of 30 days. 4. Implement IRSA with OIDC provider and create at least two service accounts with different IAM role bindings. 5. Deploy CoreDNS v1.10.1, kube-proxy v1.28.1, and vpc-cni v1.14.1 as EKS managed add-ons. 6. Configure pod security standards enforcement with 'restricted' baseline for default namespace. 7. Create KMS key with automatic rotation enabled for EKS secrets encryption. 8. Set up CloudWatch Container Insights with enhanced monitoring metrics. 9. Configure AWS Systems Manager Session Manager for secure node access without SSH. 10. Implement cluster autoscaler with proper IRSA permissions and Spot instance awareness. 11. Export cluster endpoint, OIDC issuer URL, and kubeconfig for downstream consumption. 12. Tag all resources with Environment=production, ManagedBy=pulumi, and CostCenter=engineering. Expected output: A fully functional EKS cluster with production-grade security, monitoring, and node management capabilities that can be accessed via kubectl using the exported kubeconfig.","A fintech company needs to modernize their monolithic application by deploying microservices on Kubernetes. They require a production-grade EKS cluster with strict security controls, automated node management, and integrated monitoring capabilities to meet regulatory compliance requirements.","""Production EKS cluster deployed in us-east-1 across 3 availability zones with managed node groups using t3.medium Spot instances. VPC with public and private subnets, NAT gateways for outbound traffic. Requires Pulumi 3.x with TypeScript, AWS CLI configured, kubectl installed. Integration with AWS Systems Manager for node access, CloudWatch Container Insights for monitoring. KMS encryption enabled for EKS secrets and EBS volumes. OIDC provider configured for federated access.""","[""EKS cluster must use managed node groups with Spot instances for cost optimization"", ""Implement IRSA (IAM Roles for Service Accounts) for pod-level AWS permissions"", ""Enable EKS control plane logging for all log types to CloudWatch"", ""Use AWS KMS customer-managed keys for EKS secrets encryption"", ""Configure OIDC identity provider for external authentication integration"", ""Deploy cluster in private subnets with no direct internet access"", ""Implement pod security standards with restricted baseline policy"", ""Enable EKS add-ons for CoreDNS, kube-proxy, and VPC CNI with specific versions""]"
q4o4i3,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement multi-region database disaster recovery with automated failover. The configuration must: MANDATORY REQUIREMENTS (Must complete): 1. Deploy RDS Aurora Global Database with primary cluster in us-east-1 and secondary in eu-west-1 (CORE: RDS) 2. Configure Route53 health checks with failover routing policy for automatic DNS switching (CORE: Route53) 3. Create Lambda functions in both regions to monitor replication lag and trigger alerts (CORE: Lambda) 4. Set up cross-region VPC peering for secure replication traffic 5. Implement automated backup retention of 7 days in both regions 6. Configure parameter groups with max_connections=1000 and log_min_duration_statement=1000 7. Enable encryption at rest using AWS-managed KMS keys in each region 8. Create CloudWatch dashboards showing replication metrics and health status 9. Export stack outputs for primary/secondary endpoints and health check URLs OPTIONAL ENHANCEMENTS (If time permits):  Add DynamoDB Global Tables for application state storage (OPTIONAL: DynamoDB) - enables stateful failover  Implement for orchestrated failover workflow (OPTIONAL: ) - provides controlled failover process  Add EventBridge rules for automated incident response (OPTIONAL: EventBridge) - improves recovery automation Expected output: Complete Pulumi TypeScript program that deploys a production-ready multi-region database infrastructure with automated failover capabilities, monitoring, and recovery mechanisms that meet RTO objectives of under 5 minutes.",A financial services company requires their critical transaction database to remain operational during regional AWS outages. They need an automated failover mechanism that can detect primary region failures and redirect traffic to a standby region within minutes while maintaining data consistency.,"""Multi-region AWS deployment spanning us-east-1 (primary) and eu-west-1 (secondary) for disaster recovery. Uses RDS Aurora Global Database for data replication, Route53 for DNS failover, Lambda for health monitoring. Requires Pulumi CLI 3.x, Node.js 16+, TypeScript 4.x, AWS CLI configured with credentials. VPCs in both regions with private subnets across 3 AZs each, VPC peering for cross-region communication. CloudWatch dashboards for monitoring replication lag and system health.""","[""Aurora cluster must use db.r6g.large instances with at least 2 reader nodes per region"", ""Route53 health checks must probe database endpoints every 30 seconds with 2 consecutive failures triggering failover"", ""Lambda functions must be written in TypeScript and use AWS SDK v3 for all API calls"", ""All resources must be tagged with Environment=Production and DisasterRecovery=Enabled"", ""VPC peering connection must restrict traffic to port 3306 only using security group rules"", ""Database passwords must be stored in and rotated every 90 days"", ""CloudWatch alarms must trigger notifications when replication lag exceeds 60 seconds"", ""Stack must use Pulumi for all region-specific settings and connection strings"", ""Deletion protection must be enabled on production database clusters only""]"
s0m4y2,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a containerized microservices platform on ECS Fargate. MANDATORY REQUIREMENTS (Must complete): 1. Create an ECS cluster with capacity providers for both Fargate and Fargate Spot (CORE: ECS) 2. Deploy 3 microservices: order-service, pricing-service, and notification-service, each with its own task definition 3. Configure Application Load Balancer with target groups for each service (CORE: ALB) 4. Set up namespace and register all services for internal DNS resolution 5. Implement auto-scaling policies: scale pricing-service based on CPU (target 50%), order-service based on memory (target 60%) 6. Configure task definitions with 1GB memory and 0.5 vCPU for each service 7. Use Fargate Spot for notification-service, regular Fargate for order and pricing services 8. Set up CloudWatch Log Groups with KMS encryption for each service 9. Configure health checks with 30-second intervals and 3 consecutive failures threshold 10. Ensure all services run in private subnets with no direct internet access OPTIONAL ENHANCEMENTS (If time permits):  Add sidecar containers for distributed tracing (OPTIONAL: ) - improves debugging across microservices  Implement Circuit Breaker pattern using AWS App Mesh (OPTIONAL: App Mesh) - adds resilience to service failures  Set up Container Insights for advanced metrics (OPTIONAL: Container Insights) - provides deeper performance visibility Expected output: A complete Pulumi TypeScript program that provisions the ECS cluster, deploys three containerized services with proper networking and service discovery, configures load balancing with health checks, and implements auto-scaling. The infrastructure should support zero-downtime deployments and provide internal DNS resolution for service-to-service communication.","A fintech startup needs to deploy their microservices architecture for a real-time trading platform. The system requires strict isolation between services, automatic scaling based on market activity, and zero-downtime deployments during trading hours.","""Production trading platform infrastructure deployed in us-east-1 across 3 availability zones. Uses ECS Fargate for container orchestration, Application Load Balancer for traffic distribution, and ECR for container registry. VPC configured with private subnets only, no internet gateway. All outbound traffic routes through VPC endpoints. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate IAM permissions. Services communicate exclusively through service discovery.""","[""Each service must run in its own ECS service with dedicated task definitions"", ""Container images must be pulled from private ECR repositories only"", ""All inter-service communication must use service discovery via "", ""Task definitions must use Fargate Spot for non-critical services to reduce costs"", ""Container logs must be encrypted at rest using customer-managed KMS keys"", ""Network traffic between services must traverse through AWS PrivateLink endpoints""]"
m0l2d5,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production-grade observability platform for monitoring distributed microservices. The configuration must: 1. Set up CloudWatch Log Groups with KMS encryption for ECS tasks and Lambda functions with 30-day retention. 2. Configure tracing for both ECS services and Lambda functions with 10% sampling rate. 3. Create custom CloudWatch metrics namespace 'FinanceApp' for business KPIs. 4. Implement CloudWatch Container Insights for ECS cluster monitoring. 5. Deploy CloudWatch Synthetics canary for monitoring critical API endpoints every 5 minutes. 6. Create metric filters on log groups to track error rates and response times. 7. Set up SNS topic with email and Slack subscriptions for alerting. 8. Configure CloudWatch alarms for CPU utilization (>80%), error rate (>5%), and API latency (>1000ms). 9. Build a CloudWatch dashboard displaying service health, performance metrics, and error trends with auto-refresh. 10. Implement composite alarms combining multiple metrics for intelligent alerting. Expected output: A complete Pulumi program that deploys all monitoring components with proper IAM roles, creates a unified observability platform accessible through CloudWatch dashboards, and ensures all metrics and logs are properly collected, encrypted, and retained according to compliance requirements.","A financial services company needs centralized observability for their distributed microservices architecture. They require real-time metrics collection, distributed tracing, and alerting across multiple services to ensure compliance with SLAs and rapid incident response.","""Production observability infrastructure deployed in us-east-1 for monitoring ECS Fargate services, Lambda functions, and RDS Aurora clusters. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions. VPC with private subnets across 3 availability zones for high availability. CloudWatch, and SNS services form the core monitoring stack. Integration with existing Slack workspace required for alerting.""","[""All metrics must be retained for at least 30 days for compliance"", ""Alerts must be sent to both email and Slack channels"", ""Use CloudWatch Container Insights for ECS metrics collection"", ""Implement custom metrics namespace for business KPIs"", ""Configure sampling at 10% for cost optimization"", ""All logs must be encrypted at rest using KMS"", ""Implement metric filters for error rate monitoring"", ""Use CloudWatch Synthetics for endpoint monitoring"", ""Configure dashboard auto-refresh at 1-minute intervals""]"
u5v8c3,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to migrate a payment processing system from on-premises to AWS with zero downtime. The configuration must: 1. Deploy Aurora PostgreSQL clusters in both regions with automated backups and point-in-time recovery enabled. 2. Set up DMS replication instances with source endpoints pointing to on-premises database (10.0.1.50:5432). 3. Configure ECS services running payment-api:v2.3 container with 2GB memory and 1 vCPU. 4. Create Application Load Balancers with weighted target groups (initial weights: legacy=100, new=0). 5. Implement Route 53 health checks with 30-second intervals and automatic failover between regions. 6. Deploy AWS rules blocking SQL injection patterns and rate limiting to 1000 requests/minute per IP. 7. Configure CloudWatch alarms for DMS lag > 60 seconds, ALB 5xx errors > 1%, and Aurora CPU > 80%. 8. Set up Lambda function triggered by CloudWatch Events to update Route 53 weights based on migration phase. 9. Create Systems Manager parameters storing migration cutover timestamps and rollback procedures. 10. Implement CloudWatch Logs Insights queries for transaction reconciliation between old and new systems. Expected output: A complete Pulumi TypeScript program that provisions the entire migration infrastructure with automated health monitoring and traffic shifting capabilities. The solution should allow operators to gradually shift traffic from on-premises to AWS by adjusting ALB target group weights, monitor replication lag, and quickly rollback if metrics indicate issues.","A financial services company needs to migrate their payment processing infrastructure from a legacy on-premises setup to AWS. The current system handles 50,000 transactions daily with strict compliance requirements for data residency and audit trails. The migration must be performed with zero downtime and the ability to quickly rollback if issues arise.","""Multi-region AWS deployment spanning us-east-1 (primary) and us-west-2 (disaster recovery) for payment processing migration. Infrastructure includes ECS for containerized microservices, Aurora PostgreSQL Multi-AZ clusters, and Application Load Balancers. VPC setup with private subnets across 3 availability zones per region, VPC peering for cross-region communication, and Transit Gateway for network routing. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, AWS CLI configured with appropriate IAM permissions. Environment supports gradual migration from on-premises PostgreSQL 12 database to Aurora using DMS continuous replication.""","[""All database migrations must use AWS DMS with CDC enabled for real-time synchronization"", ""Implement blue-green deployment strategy with weighted routing for gradual traffic shifting"", ""Use rotation for all database credentials with 30-day automatic rotation"", ""Deploy identical infrastructure in both us-east-1 and us-west-2 with active-passive failover"", ""Enforce encryption in transit using TLS 1.2+ for all inter-service communication"", ""Tag all resources with Environment, CostCenter, and MigrationPhase for tracking purposes""]"
o2t8w7,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to deploy a multi-account transit gateway network architecture. The configuration must: 1. Create a Transit Gateway in us-east-1 with DNS and ECMP support enabled. 2. Deploy three VPCs: Production (10.1.0.0/16), Staging (10.2.0.0/16), and Shared Services (10.0.0.0/16). 3. Attach each VPC to the Transit Gateway with appropriate subnet associations. 4. Configure Transit Gateway route tables with production isolation (prod can't reach staging). 5. Set up Route 53 Resolver endpoints in the Shared Services VPC for DNS forwarding. 6. Enable VPC Flow Logs for all VPCs storing to a centralized S3 bucket. 7. Create Systems Manager VPC endpoints in each VPC for secure instance access. 8. Implement Network ACLs allowing only necessary inter-VPC communication. 9. Configure security groups for Resolver endpoints allowing DNS queries from attached VPCs. 10. Export Transit Gateway ID and route table IDs as stack outputs. Expected output: A Pulumi program that provisions a complete hub-spoke network with Transit Gateway, enabling secure inter-VPC communication while maintaining environment isolation. The infrastructure should support DNS resolution across VPCs and provide secure administrative access without bastion hosts.",A global financial services company needs to establish secure connectivity between multiple AWS accounts across different regions. Each business unit operates in isolation but requires controlled access to shared services like authentication systems and data warehouses. The architecture must support future expansion to additional regions while maintaining strict network segmentation.,"""Multi-account AWS environment spanning us-east-1 and eu-west-1 regions. Core infrastructure includes Transit Gateway for hub-spoke network topology, multiple VPCs per account (production, staging, shared services), Route 53 Resolver for DNS, VPC Flow Logs to S3, Systems Manager for secure access. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, AWS CLI configured with cross-account assume role permissions. Each VPC uses /16 CIDR blocks with non-overlapping IP ranges. Transit Gateway attachments connect spoke VPCs to central hub.""","[""Use AWS Transit Gateway for inter-VPC connectivity instead of VPC peering"", ""Implement separate route tables for production and non-production traffic"", ""Enable VPC Flow Logs for all VPCs with S3 storage and 30-day retention"", ""Configure DNS resolution between VPCs using Route 53 Resolver endpoints"", ""Enforce encryption in transit using AWS-managed VPN connections"", ""Implement network ACLs restricting traffic to specific CIDR blocks only"", ""Use Systems Manager Session Manager for bastion-less access"", ""Deploy all resources using Pulumi's native AWS provider without custom resources""]"
r9i4s1,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy consistent payment processing infrastructure across three environments. The configuration must: 1. Define reusable ComponentResource classes for ECS service, RDS cluster, and DynamoDB table that accept environment-specific parameters. 2. Create three separate stacks (dev, staging, prod) that share the same infrastructure code but use different configurations. 3. Implement cross-stack references where prod RDS endpoint is used as read replica source for staging. 4. Configure ECS Fargate services with environment-specific CPU/memory (dev: 256/512, staging: 512/1024, prod: 1024/2048). 5. Deploy RDS Aurora clusters with consistent engine version 15.4 but different instance classes per environment. 6. Create DynamoDB tables with identical schema and GSIs but environment-appropriate billing modes. 7. Set up S3 buckets with consistent lifecycle policies but environment-specific retention periods. 8. Configure CloudFront distributions where only prod uses custom domain with ACM certificate. 9. Implement Lambda functions for payment webhooks with identical code but environment-specific concurrency limits. 10. Create a drift detection function that compares critical outputs across all stacks and alerts on inconsistencies. Expected output: A Pulumi TypeScript project with index.ts containing reusable components, separate configuration files for each environment, and a drift detection utility that validates infrastructure consistency across all three environments.","A financial services company needs to ensure their payment processing infrastructure is identically replicated across development, staging, and production environments. They've experienced configuration drift issues where manual changes in production weren't reflected in lower environments, causing deployment failures and compliance violations.","""Multi-environment AWS infrastructure spanning three accounts in us-east-1 region. Core services include ECS Fargate for containerized payment processor, RDS Aurora PostgreSQL 15.4 for transaction storage, DynamoDB for session management, Lambda for async processing, S3 for audit logs, CloudFront for static assets. Each environment has dedicated VPC with 3 AZs, private subnets for compute/data layers, public subnets for ALB. Requires Pulumi 3.x with TypeScript, AWS CLI profiles for each environment, Node.js 18+. Production uses m5.large instances, staging uses t3.medium, dev uses t3.small.""","[""Use Pulumi stacks to manage exactly 3 environments (dev, staging, prod) from a single codebase"", ""Implement stack references to share outputs between environments without hardcoding values"", ""Environment-specific configurations must be loaded from Pulumi files, not inline code"", ""All Lambda functions must use identical runtime versions and memory allocations across environments"", ""DynamoDB tables must have consistent GSI configurations but environment-appropriate capacity settings"", ""S3 buckets must follow naming convention: {company}-{service}-{environment}-{purpose}"", ""CloudFront distributions in prod must use custom domain with ACM certificate, dev/staging use default domains"", ""RDS instances must use identical engine versions but environment-specific instance classes"", ""All IAM roles must be prefixed with environment name and follow principle of least privilege"", ""Implement automated drift detection that compares stack outputs across all three environments""]"
z7w3l1,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production-ready web application with blue-green deployment capability. The configuration must: 1. Set up ALB with target groups for blue and green environments, using weighted routing (CORE: ALB). 2. Deploy containerized application on ECS Fargate with separate task definitions for blue/green (CORE: ECS). 3. Configure Aurora PostgreSQL Serverless v2 with automated backups every 6 hours. 4. Implement auto-scaling policies that maintain 3-10 instances based on CPU/memory metrics. 5. Create S3 buckets for static assets with distribution and versioning enabled. 6. Configure CloudWatch dashboards showing response times, error rates, and active connections. 7. Set up health checks that monitor both /health endpoints every 30 seconds. 8. Implement automated rollback mechanism using CloudWatch alarms on 5XX error rates. 9. Ensure all resources are tagged with Environment, Application, and CostCenter tags. 10. Create IAM roles with least-privilege policies for ECS tasks and Lambda functions. 11. Configure for S3 and ECR to reduce costs. 12. Output the ALB DNS name, distribution URL, and database connection string. Expected output: A complete Pulumi TypeScript program that creates a fault-tolerant web application infrastructure with automated blue-green deployments, meeting all compliance and operational requirements.",A fintech startup needs to deploy their real-time payment processing web application with strict compliance requirements for PCI-DSS. The application consists of a React frontend and Node.js API backend that processes sensitive financial transactions. They require automated blue-green deployments with zero downtime during updates.,"""Production deployment in us-east-1 with multi-AZ configuration across 3 availability zones. Uses Application Load Balancer for traffic distribution, ECS Fargate for containerized workloads running Node.js API and React frontend, Aurora PostgreSQL Serverless v2 for database with read replicas. VPC with public subnets for ALB and private subnets for ECS tasks and Aurora. NAT Gateways in each AZ for outbound connectivity. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, AWS CLI configured with appropriate IAM permissions. CloudWatch for monitoring and alerting.""","[""All data must be encrypted at rest using customer-managed keys"", ""Application logs must be retained for exactly 90 days for compliance auditing"", ""Database connections must use SSL/TLS with certificate verification enabled"", ""Auto-scaling must maintain minimum 3 instances during business hours (8 AM - 6 PM EST)"", ""Health checks must validate both frontend and backend endpoints before marking instances healthy"", ""Deployment must support immediate rollback within 30 seconds if health checks fail"", ""All S3 buckets must have versioning enabled and block public access"", ""Load balancer must terminate SSL with AWS Certificate Manager certificates only"", ""Database backups must occur every 6 hours with point-in-time recovery enabled""]"
q7u6z4,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a serverless cryptocurrency price alert system. The configuration must: 1. Create a DynamoDB table named 'price-alerts' with partition key 'alertId' (string) and sort key 'userId' (string), with point-in-time recovery and encryption enabled. 2. Deploy a Lambda function 'processAlert' using ARM64 architecture with 1GB memory that reads from DynamoDB and publishes to SNS, with 50 reserved concurrent executions. 3. Create an SNS topic 'alert-notifications' with a Lambda subscription for email processing. 4. Configure a REST API Gateway with request validation that accepts POST requests to '/alerts' endpoint, integrating with the Lambda function. 5. Implement a DLQ (SQS) for the Lambda function with maximum receive count of 3 and 14-day message retention. 6. Set up CloudWatch alarms for Lambda errors (threshold: 5 errors) and duration (threshold: 3000ms), both requiring 2 consecutive breaches. 7. Create a Lambda function 'emailProcessor' (512MB, ARM64) subscribed to SNS that processes notification delivery. 8. Configure CloudWatch Logs with 7-day retention for all Lambda functions. 9. Implement least-privilege IAM roles for each Lambda with no wildcard permissions. 10. Add Lambda environment variables for DynamoDB table name and SNS topic ARN (no hardcoded values). 11. Enable X-Ray tracing on all Lambda functions and API Gateway. 12. Output the API Gateway invoke URL and DynamoDB table ARN. Expected output: A complete Pulumi TypeScript program that deploys all serverless components with proper error handling, monitoring, and security configurations. The system should process price alerts reliably with automatic retry logic and dead letter queue handling.","A fintech startup needs to process cryptocurrency price alerts in real-time, triggering notifications when specific thresholds are met. The system must handle thousands of concurrent price checks while maintaining sub-second response times and ensuring exactly-once processing for critical alerts.","""Production-grade serverless infrastructure deployed in us-east-1 for cryptocurrency price alert processing. Core services include Lambda functions with ARM64 architecture, DynamoDB for state management, SNS for notifications, and API Gateway REST endpoints. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS credentials configured. No VPC required as all services are managed. Infrastructure must support 10,000+ concurrent price checks with sub-second latency requirements. CloudWatch monitoring with custom metrics for alert processing performance.""","[""Lambda functions must use ARM64 architecture for cost optimization"", ""DynamoDB tables must use point-in-time recovery and encryption at rest"", ""All Lambda functions must have reserved concurrent executions set"", ""Dead letter queues must have a maximum receive count of exactly 3"", ""API Gateway must use REST API (not HTTP API) with request validation"", ""CloudWatch alarms must trigger on 2 consecutive periods of threshold breach"", ""Lambda environment variables must not contain any hardcoded credentials""]"
v7q6m1,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a complete CI/CD pipeline infrastructure for containerized microservices. MANDATORY REQUIREMENTS (Must complete): 1. Create a with Source, Build, Test, and Deploy stages (CORE: ) 2. Configure projects for building Docker images and running tests (CORE: ) 3. Set up ECS Fargate service for deploying containerized applications 4. Create ECR repositories with image scanning enabled on push 5. Implement S3 bucket for pipeline artifacts with server-side encryption 6. Configure rule to trigger pipeline on ECR image pushes 7. Set up SNS topic for pipeline state change notifications 8. Create all necessary IAM roles and policies with least-privilege access 9. Implement CloudWatch Logs groups with 30-day retention for all services 10. Add resource tags for cost allocation and environment identification OPTIONAL ENHANCEMENTS (If time permits):  Add for blue/green deployments (OPTIONAL: ) - enables zero-downtime deployments  Implement for complex deployment workflows (OPTIONAL: ) - adds orchestration capabilities  Configure tracing across the pipeline (OPTIONAL: ) - improves pipeline debugging Expected output: A Pulumi TypeScript program that provisions a fully automated CI/CD pipeline with container build, test, and deployment capabilities, including proper IAM permissions, encryption, and monitoring configurations.",A software development team needs to establish a GitOps-based CI/CD pipeline for their microservices architecture. They require automated infrastructure provisioning and application deployment workflows that integrate with their existing GitHub repositories and Docker container registry.,"""Multi-stage CI/CD infrastructure deployed in us-east-1 using for orchestration, for build and test stages, and ECS Fargate for container deployments. Requires Pulumi CLI 3.x with TypeScript, Node.js 18.x, and AWS CLI configured with appropriate permissions. VPC setup includes private subnets for ECS tasks with NAT Gateway for outbound connectivity. ECR repositories for container image storage with lifecycle policies. S3 bucket for pipeline artifacts with versioning enabled.""","[""All projects must use ARM-based compute for cost optimization"", ""Pipeline artifacts must be encrypted with customer-managed KMS keys"", ""Each microservice deployment must trigger automated integration tests"", ""Pipeline execution logs must be retained for exactly 30 days"", ""IAM roles must follow least-privilege with no Admin or PowerUser policies"", ""All container images must pass vulnerability scanning before deployment"", ""Pipeline notifications must integrate with existing Slack webhooks"", ""Build environments must use specific Node.js 18.x runtime version"", ""Deploy stage must support manual approval for production environments""]"
u3u0y2,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement a multi-region active-active architecture for high availability. The configuration must: 1. Deploy Aurora Global Database cluster spanning us-east-1 and us-west-2 with automated promotion capabilities. 2. Create Lambda functions in both regions behind API Gateway with identical business logic. 3. Configure DynamoDB global tables for session management with point-in-time recovery. 4. Implement Route 53 hosted zone with health checks and failover routing between regions. 5. Set up S3 buckets with cross-region replication and versioning enabled. 6. Configure CloudWatch dashboards aggregating metrics from both regions. 7. Create SNS topics in each region for failure notifications. 8. Implement least-privilege IAM roles with cross-region assume capabilities. Expected output: A complete Pulumi TypeScript program that deploys resilient multi-region infrastructure with automated failover capabilities, achieving sub-60 second RTO and maintaining data consistency across regions during failures.",A healthcare SaaS provider needs to implement a multi-region active-active architecture for their patient records API to meet 99.99% availability SLA requirements. The system must automatically failover between regions during outages while maintaining data consistency and compliance with HIPAA regulations.,"""Multi-region deployment across us-east-1 (primary) and us-west-2 (secondary) for active-active high availability architecture. Uses Aurora Global Database for PostgreSQL 14.x with automated failover, Lambda functions for API logic, DynamoDB global tables for session state, and S3 with cross-region replication for static assets. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions, Node.js 18+. VPCs in each region with private subnets across 3 AZs, VPC peering for cross-region communication. Route 53 for global DNS failover with health checks monitoring both regions.""","[""Use Aurora Global Database with at least 2 read replicas per region"", ""Implement Route 53 health checks with failover routing policy"", ""Configure DynamoDB global tables for session management"", ""Deploy Lambda functions in both regions with identical configurations"", ""Set RTO (Recovery Time Objective) to less than 60 seconds"", ""Use for cross-region secret replication"", ""Implement CloudWatch cross-region metric aggregation"", ""Configure S3 cross-region replication with RTC (Replication Time Control)""]"
s7f4e6,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a security-hardened payment processing infrastructure. MANDATORY REQUIREMENTS (Must complete): 1. Create a VPC with private subnets across 3 AZs and for , DynamoDB, and Lambda (CORE: VPC) 2. Deploy Aurora MySQL cluster with encryption, automated backups, and IAM authentication (CORE: ) 3. Configure Lambda function with 1GB memory in VPC for payment validation with KMS encryption 4. Create bucket for audit logs with versioning, encryption, and lifecycle policies 5. Implement KMS keys with key rotation enabled and restrictive key policies 6. Configure security groups allowing only port 3306 between Lambda and 7. Create IAM roles with minimal permissions using managed policies where possible 8. Enable VPC Flow Logs to with custom format for security analysis 9. Apply mandatory tags to all resources: CostCenter, Environment, DataClassification OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Config rules for compliance monitoring (OPTIONAL: Config) - automates compliance checks  Implement GuardDuty for threat detection (OPTIONAL: GuardDuty) - adds security monitoring  Create for secure config (OPTIONAL: ) - centralizes secrets management Expected output: Complete Pulumi TypeScript program that provisions a PCI-compliant infrastructure with all security controls implemented as code, ready for payment processing workloads.","A financial services company requires a hardened AWS environment for their payment processing workload with strict compliance requirements. The infrastructure must meet PCI-DSS standards with encryption at rest and in transit, network isolation, and comprehensive audit logging. All security configurations must be codified to ensure reproducibility across environments.","""Secure multi-AZ deployment in us-east-1 for payment processing infrastructure using Lambda for compute, Aurora MySQL for transaction storage, for audit logs, and for AWS service access. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate credentials. VPC spans 3 availability zones with private subnets only, using for , DynamoDB, and Lambda service access. No NAT gateways or internet gateways permitted due to security requirements.""","[""All data must be encrypted using AWS KMS with customer-managed keys"", ""Network traffic must be isolated using private subnets with no direct internet access"", ""IAM roles must follow least-privilege principles with no wildcard permissions"", ""All API calls must be logged to CloudWatch with 90-day retention"", ""Security groups must explicitly deny all traffic except required ports"", "" buckets must have versioning enabled and block all public access"", "" instances must have automated backups with 7-day retention"", ""Lambda functions must run in VPC with security group restrictions"", ""All resources must be tagged with CostCenter, Environment, and DataClassification""]"
h7m0s6,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to optimize an existing ECS-based infrastructure for cost and performance. The configuration must: 1. Refactor ECS service definitions to use Fargate Spot capacity providers with 70/30 split between spot and on-demand. 2. Implement ECR lifecycle policies to retain only the last 10 images per repository. 3. Configure ALB target groups with optimized deregistration delay of 15 seconds. 4. Create time-based auto-scaling schedules that reduce capacity to 20% during off-hours. 5. Migrate all Lambda functions to ARM architecture with 512MB memory allocation. 6. Apply CloudWatch Logs retention policies programmatically based on log group naming patterns. 7. Implement a tagging strategy using Pulumi's auto-tagging feature for all resources. 8. Add cost allocation tags for financial tracking and reporting. 9. Configure S3 lifecycle policies to transition objects to Glacier after 90 days. 10. Implement CloudWatch dashboards to monitor cost optimization metrics. Expected output: A refactored Pulumi TypeScript program that reduces infrastructure costs by at least 40% while maintaining application performance, with clear before/after cost projections in comments.",A financial services company is experiencing performance bottlenecks and high costs in their containerized trading analytics platform. The existing Pulumi infrastructure was hastily built and needs optimization to reduce costs by 40% while maintaining performance.,"""Production environment in us-east-1 with existing ECS Fargate cluster running 50+ containerized microservices behind Application Load Balancer. Current setup uses Lambda for async processing, RDS Aurora for persistence, and S3 for static assets. Infrastructure deployed via Pulumi TypeScript with Node.js 18.x runtime. VPC spans 3 availability zones with private subnets for compute resources. Monthly AWS bill exceeds $30K with significant waste in compute and storage resources. Requires Pulumi 3.x CLI and AWS SDK v3.""","[""ECS tasks must use Fargate Spot instances where possible for cost reduction"", ""Container images must be cached using ECR lifecycle policies to prevent storage bloat"", ""ALB target group health checks must use HTTP 200-299 status codes only"", ""Auto-scaling policies must scale down aggressively during off-hours (6PM-6AM EST)"", ""All Lambda functions must use ARM-based Graviton2 processors for cost efficiency"", ""CloudWatch Logs retention must be set to 3 days for non-critical services"", ""Resource tagging must follow pattern: Environment, CostCenter, Application, Owner""]"
l3t2u8,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to analyze and audit existing AWS infrastructure while preparing for migration. The configuration must: 1. Import existing CloudFormation stack resources using Pulumi's import functionality. 2. Scan all EC2 instances and validate they use approved AMI IDs (ami-0a1b2c3d4e5f60123, ami-0z9y8x7w6v5u4t32). 3. Analyze S3 bucket configurations and report any without versioning or SSE-S3 encryption. 4. Check RDS instances for backup retention periods less than 7 days and flag non-compliant databases. 5. Calculate monthly costs per service using AWS Cost Explorer and generate CSV report. 6. Apply mandatory tags (Owner, Environment, CostCenter, CreatedDate) to all resources. 7. Create CloudWatch custom metrics for resource compliance percentage. 8. Generate JSON compliance report showing pass/fail status for each validation rule. 9. Identify unused Elastic IPs and unattached EBS volumes for cost optimization. 10. Validate all security groups follow least-privilege principle (no 0.0.0.0/0 ingress). Expected output: A Pulumi program that imports existing infrastructure, performs comprehensive analysis, applies required tags, and generates both CSV cost reports and JSON compliance reports while publishing metrics to CloudWatch.","A DevOps team has inherited a production AWS infrastructure deployed via CloudFormation that lacks proper documentation and tagging. They need to migrate to Pulumi while simultaneously auditing resource usage, identifying cost optimization opportunities, and implementing compliance checks. The analysis must generate actionable reports for management review.","""Production AWS infrastructure in us-east-1 with mixed workloads including EC2 instances across multiple AZs, RDS MySQL and PostgreSQL databases, S3 buckets for application data and backups, Application Load Balancers, and Auto Scaling Groups. Existing infrastructure uses default VPC with various security groups. Requires Pulumi 3.x with TypeScript, AWS SDK v3, Node.js 18+, and read-only access to AWS Cost Explorer API. Infrastructure audit scope covers approximately 50-100 resources with monthly spend around $15,000.""","[""Use Pulumi's AWS Native provider exclusively (not Classic)"", ""Implement custom resource validation using Pulumi's policy framework"", ""Generate cost analysis reports in CSV format using AWS Cost Explorer API"", ""Tag all resources with Owner, Environment, CostCenter, and CreatedDate"", ""Validate that all S3 buckets have versioning and encryption enabled"", ""Check that all RDS instances have automated backups with 7+ day retention"", ""Ensure all EC2 instances use approved AMI IDs from a predefined list"", ""Export infrastructure metrics to CloudWatch custom metrics namespace"", ""Create automated compliance report as JSON with pass/fail status for each check""]"
k8f4t1,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster for microservices workloads.

MANDATORY REQUIREMENTS (Must complete):
1. Create an EKS cluster version 1.28 with private API endpoint access (CORE: EKS)
2. Configure two managed node groups: one with t3.medium On-Demand instances (min 2, max 4) and one with t3.large Spot instances (min 1, max 3) (CORE: EC2)
3. Enable IRSA by creating an OIDC provider for the cluster
4. Configure EKS control plane logging for audit, authenticator, and api log types
5. Create custom launch templates for node groups with 50GB encrypted gp3 EBS volumes
6. Deploy the AWS Load Balancer Controller using Helm with proper IRSA configuration
7. Install the EBS CSI driver addon with encryption enabled
8. Output the cluster endpoint, OIDC issuer URL, and kubeconfig command

OPTIONAL ENHANCEMENTS (If time permits):
 Add Karpenter for advanced node autoscaling (OPTIONAL: Karpenter) - improves cost efficiency and scaling speed
 Implement AWS Systems Manager Session Manager on nodes (OPTIONAL: SSM) - enables secure shell access without SSH keys
 Configure Amazon GuardDuty for EKS threat detection (OPTIONAL: GuardDuty) - adds runtime security monitoring

Expected output: A Pulumi TypeScript program that creates a fully functional EKS cluster with managed node groups, IRSA enabled, logging configured, and core addons installed. The cluster should be ready to deploy containerized applications with proper AWS service integrations.",A fintech startup needs to deploy their microservices architecture on AWS EKS to handle payment processing workloads. They require a production-grade Kubernetes cluster with strict security controls and automated node management to meet PCI compliance requirements.,"""Production EKS cluster deployed in us-east-1 across 3 availability zones. Uses EKS 1.28 with managed node groups combining On-Demand and Spot instances. VPC with private subnets for worker nodes and public subnets for load balancers. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, kubectl, and AWS CLI configured. Integrates with AWS Load Balancer Controller and EBS CSI driver for persistent volumes. OIDC provider enabled for pod identity.""","[""EKS cluster must use managed node groups with Spot instances for cost optimization"", ""Implement IRSA (IAM Roles for Service Accounts) for pod-level AWS permissions"", ""Enable EKS control plane logging for audit, authenticator, and api categories"", ""Configure OIDC provider for the cluster to support external identity providers"", ""All node groups must use custom launch templates with encrypted EBS volumes""]"
t6w8q6,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to deploy a multi-region disaster recovery infrastructure for a transaction processing system. The configuration must: 1. Set up RDS Aurora Global Database with one writer cluster in us-east-1 and reader cluster in us-west-2. 2. Deploy identical Lambda functions in both regions for transaction processing with 3GB memory and 5-minute timeout. 3. Configure DynamoDB global tables for session management with on-demand billing mode. 4. Implement Route 53 hosted zone with weighted routing policy (100% to primary, 0% to DR initially). 5. Create health check endpoints using Lambda functions that verify database connectivity. 6. Set up CloudWatch alarms for RDS replication lag exceeding 30 seconds. 7. Configure SNS topics in both regions for alert notifications. 8. Implement automated DNS failover using Route 53 health checks with 30-second intervals. 9. Create S3 buckets with cross-region replication for application artifacts. 10. Deploy Application Load Balancers in both regions with target groups pointing to Lambda functions. 11. Ensure all resources are tagged with Environment=Production and DisasterRecovery=Enabled. Expected output: A Pulumi program that creates a fully functional multi-region infrastructure with automated failover capabilities, meeting RPO/RTO requirements and allowing seamless disaster recovery testing.","A financial services company needs to implement a disaster recovery solution for their critical transaction processing system. The system must maintain RPO of under 1 hour and RTO of under 4 hours, with automated failover capabilities between regions.","""Multi-region AWS deployment spanning us-east-1 (primary) and us-west-2 (DR). Infrastructure includes RDS Aurora Global Database with PostgreSQL 15.x, Lambda functions for transaction processing, DynamoDB global tables for session data, and Route 53 for DNS failover. Each region has VPCs with 3 availability zones, private subnets for databases, and public subnets for ALBs. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate credentials, Node.js 18+, and access to both AWS regions. The environment uses Parameter Store for configuration management across regions.""","[""Primary region must be us-east-1 with failover to us-west-2"", ""RDS Aurora Global Database must use PostgreSQL 15.x with encryption at rest"", ""Lambda functions must be replicated with identical configurations in both regions"", ""DynamoDB global tables must have point-in-time recovery enabled"", ""Route 53 health checks must trigger automatic DNS failover within 60 seconds"", ""All IAM roles must follow least-privilege principle with no AdminAccess policies""]"
f0d6c4,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready Amazon EKS cluster with advanced security and orchestration features. The configuration must: 1. Create an EKS cluster with Kubernetes 1.28+ and enable control plane logging for audit, authenticator, and scheduler. 2. Configure a managed node group using Bottlerocket AMI with t3.medium instances, auto-scaling from 2 to 10 nodes across 3 AZs. 3. Set up IRSA with OIDC provider and create at least one example IAM role for service accounts. 4. Deploy the EBS CSI driver as an EKS add-on with appropriate IAM permissions for dynamic volume provisioning. 5. Create a Fargate profile for kube-system and kube-node-lease namespaces with dedicated pod execution role. 6. Implement Kubernetes RBAC with a custom role binding for a developer group with limited namespace access. 7. Configure pod security standards setting restricted as baseline for all namespaces except kube-system. 8. Deploy AWS Load Balancer Controller using Helm with proper IRSA configuration. 9. Create a StorageClass for GP3 volumes with encryption and 3000 IOPS baseline. 10. Export cluster endpoint, OIDC issuer URL, and kubeconfig for external access. Expected output: A fully functional EKS cluster accessible via kubectl with production-grade security settings, auto-scaling capabilities, and integrated AWS services for load balancing and persistent storage.","A financial services company needs to modernize their monolithic payment processing application by migrating to a containerized microservices architecture on AWS. They require a production-grade Kubernetes cluster with strict security requirements, automated scaling capabilities, and integrated monitoring to support their 24/7 operations.","""Production-grade EKS cluster deployed in us-east-1 across 3 availability zones using managed node groups with Bottlerocket AMI and Fargate profiles for system components. Requires Pulumi CLI 3.x with TypeScript, AWS CLI configured with appropriate permissions, kubectl 1.28+, and Node.js 18+. VPC with private subnets for worker nodes and public subnets for load balancers. Integration with AWS Load Balancer Controller and EBS CSI driver for dynamic provisioning.""","[""The EKS cluster must use Kubernetes version 1.28 or higher"", ""Node groups must use Bottlerocket AMI for enhanced security"", ""Enable IRSA (IAM Roles for Service Accounts) for pod-level AWS permissions"", ""Implement pod security standards with restricted baseline as default"", ""Configure OIDC provider for external authentication integration"", ""Node groups must span at least 3 availability zones"", ""Enable EKS managed node group auto-scaling between 2-10 nodes"", ""Use GP3 EBS volumes with encryption enabled for persistent storage"", ""Configure Fargate profile for system workloads (kube-system, kube-node-lease)""]"
k1o5y4,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy an enterprise-grade observability platform for monitoring containerized workloads. The configuration must: 1. Set up Amazon Managed Service for Prometheus workspace with remote write configuration for EKS cluster metrics. 2. Deploy AWS Managed Grafana workspace with SSO authentication enabled via AWS IAM Identity Center. 3. Configure AWS X-Ray service map with sampling rate of 10% for distributed tracing. 4. Enable CloudWatch Container Insights on the existing EKS cluster with Fluent Bit as the log forwarder. 5. Create CloudWatch Log Groups with 30-day retention for application logs from each namespace. 6. Set up CloudWatch Alarms for CPU utilization >80%, memory usage >85%, and pod restart count >5. 7. Configure SNS topic for alert notifications with email subscription endpoint. 8. Implement CloudWatch Metrics Streams to push metrics to the Prometheus workspace. 9. Create IAM roles with least-privilege policies for all observability components. 10. Set up KMS keys for encrypting metrics, logs, and traces at rest. 11. Configure cross-region replication of observability data to us-west-2 for disaster recovery. 12. Output the Grafana workspace URL, Prometheus remote write endpoint, and X-Ray trace console URL. Expected output: A fully functional Pulumi program that provisions a production-ready observability stack with centralized monitoring, alerting, and visualization capabilities for the EKS-based microservices architecture.","A fintech startup needs centralized observability for their microservices architecture running on EKS. The company requires real-time metrics collection, distributed tracing, and log aggregation to meet regulatory compliance and maintain 99.9% uptime SLAs.","""Production observability stack deployed in us-east-1 with cross-region replication to us-west-2. Core services include Amazon Managed Grafana, Amazon Managed Service for Prometheus (AMP), AWS X-Ray, and CloudWatch Container Insights for EKS cluster monitoring. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, AWS CLI v2 configured with appropriate IAM permissions. VPC spans 3 availability zones with private subnets for EKS worker nodes. Existing EKS cluster version 1.28 with 20+ microservices already deployed.""","[""Use AWS Managed Grafana for visualization with at least 3 custom dashboards"", ""Deploy Prometheus using Amazon Managed Service for Prometheus (AMP)"", ""Configure X-Ray SDK integration for all EKS workloads"", ""Set up CloudWatch Container Insights with enhanced monitoring enabled"", ""Implement metric retention of 30 days for compliance requirements"", ""Create SNS topic with email subscription for critical alerts"", ""Use KMS customer-managed keys for all data encryption at rest"", ""Configure cross-region replication for observability data to us-west-2""]"
r2t5s9,,Pulumi,TypeScript,expert,Provisioning of Infrastructure Environments,Environment Migration,"Create a Pulumi TypeScript program to migrate a legacy payment processing system to AWS with zero-downtime cutover capabilities. The configuration must: 1. Deploy Lambda functions (Node.js 18, 1GB memory) in both regions for payment validation and processing. 2. Set up DynamoDB Global Tables with point-in-time recovery and on-demand billing. 3. Configure Application Load Balancer with weighted target groups for blue-green deployment. 4. Implement S3 buckets with lifecycle policies (90-day transition to Glacier) for PCI audit logs. 5. Create IAM roles with session-based temporary credentials and MFA enforcement. 6. Set up VPC with private subnets, NAT gateways, and VPC endpoints for AWS services. 7. Configure CloudWatch Logs with 30-day retention and metric filters for error tracking. 8. Implement Parameter Store for secure configuration management. 9. Use Pulumi outputs to generate migration runbook with endpoint URLs and health check commands. 10. Tag all resources with Environment, CostCenter, and DataClassification tags. Expected output: A Pulumi TypeScript program that provisions parallel infrastructure in two regions, ready for zero-downtime migration with automated failover capabilities and full observability.",A fintech startup needs to migrate their payment processing infrastructure from their legacy on-premises setup to AWS. The existing system handles credit card transactions with strict PCI compliance requirements and must maintain zero downtime during the migration.,"""Production-ready infrastructure deployed across us-east-1 and us-west-2 regions for disaster recovery. Core services include Lambda functions for payment processing, DynamoDB Global Tables for transaction records, S3 for audit logs, and Application Load Balancer for traffic distribution. VPC setup with private subnets in 3 availability zones per region, VPC peering between regions. Requires Pulumi 3.x with TypeScript, AWS CLI v2 configured with appropriate credentials, Node.js 18+. Infrastructure must support 10,000 concurrent transactions with sub-second response times.""","[""Use AWS KMS with customer-managed keys for all encryption at rest"", ""Implement blue-green deployment strategy with traffic shifting capabilities"", ""Configure VPC endpoints for S3 and DynamoDB to avoid internet exposure"", ""Set up CloudWatch alarms for Lambda cold starts exceeding 1 second"", ""Use Pulumi stack references to share outputs between staging and production""]"
p9n4g3,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster with enhanced security controls. The configuration must: 1. Create an EKS cluster v1.28 with private endpoint enabled and public access restricted to specific IPs. 2. Deploy worker nodes using Bottlerocket AMI in private subnets across 3 AZs. 3. Install Calico CNI plugin for network policy enforcement. 4. Configure AWS Load Balancer Controller with IAM roles for service account (IRSA). 5. Create KMS key with automatic rotation for envelope encryption of Kubernetes secrets. 6. Deploy Falco as a DaemonSet for runtime security monitoring. 7. Set up namespace isolation with dedicated service accounts and IAM roles. 8. Configure EBS CSI driver with encrypted gp3 volumes as default storage class. 9. Implement pod security standards with restricted policy as cluster default. 10. Enable control plane logging to CloudWatch for audit, authenticator, and API logs. 11. Tag all resources with Environment=production and ManagedBy=pulumi. Expected output: A fully functional EKS cluster with Calico network policies enforced, IRSA configured for workload isolation, and Falco monitoring all container runtime activities. The cluster should be ready to receive microservice deployments with automatic IAM role assumption per namespace.",A fintech startup needs to deploy their microservices platform on AWS EKS with strict security requirements for PCI DSS compliance. The infrastructure must support automated blue-green deployments and maintain complete network isolation between different service tiers.,"""Production-grade EKS cluster deployed in eu-west-1 across 3 availability zones with private node groups. Requires Pulumi 3.x with TypeScript, kubectl 1.28+, AWS CLI v2 configured. VPC with private subnets for worker nodes, public subnets for load balancers. NAT Gateways for outbound traffic. Integration with AWS Systems Manager for node access. Container insights enabled for monitoring. OIDC provider configured for IRSA. Total of 6 worker nodes (2 per AZ) with t3.large instances.""","[""All worker nodes must use Bottlerocket AMI for enhanced security"", ""Pod-to-pod communication must be restricted using Calico network policies"", ""Secrets must be encrypted using AWS KMS with automatic key rotation"", ""Each microservice namespace must have dedicated IAM roles via IRSA"", ""Control plane API access must be restricted to specific CIDR blocks"", ""All container images must be scanned for vulnerabilities before deployment""]"
h7f1g3,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy identical infrastructure across three AWS accounts with automated environment replication. The configuration must: 1. Create a reusable Pulumi component that encapsulates VPC, RDS, and ECS resources for each environment. 2. Implement cross-account deployment using AWS provider configuration with assumed roles. 3. Deploy RDS PostgreSQL instances with automated backups and 7-day retention in each account. 4. Create ECS Fargate services running a sample containerized application with auto-scaling (min: 2, max: 10 tasks). 5. Configure Application Load Balancers with health checks and SSL termination using ACM certificates. 6. Ensure all resources are tagged with Environment, ManagedBy=Pulumi, and CostCenter tags. 7. Create environment-specific KMS keys for encrypting RDS storage and S3 buckets. 8. Implement least-privilege IAM roles for ECS task execution and task roles. 9. Set up CloudWatch Log Groups with 30-day retention for ECS container logs. 10. Export critical resource IDs and endpoints as stack outputs for cross-stack references. Expected output: A Pulumi TypeScript program with a reusable component class that can be instantiated for each environment, configuration files for dev/staging/prod, and deployment instructions that ensure consistent infrastructure across all three AWS accounts.","A financial services company operates across three AWS accounts (development, staging, production) and needs identical infrastructure deployed consistently across all environments. They require automated cross-account role assumption and centralized state management to ensure configuration drift doesn't occur between environments.","""Multi-account AWS deployment spanning us-east-1 region across development (111111111111), staging (222222222222), and production (333333333333) accounts. Each environment requires VPC with 3 availability zones, private and public subnets, NAT gateways for outbound traffic. RDS PostgreSQL 15.3 in private subnets with Multi-AZ enabled. ALB in public subnets routing to ECS Fargate services. Requires Pulumi 3.x with TypeScript, AWS CLI configured with cross-account assume role permissions, Node.js 18+. State stored in central S3 backend with DynamoDB locking.""","[""Use Pulumi stack references to share outputs between environments without hardcoding values"", ""Implement cross-account IAM role assumption using AWS STS provider configuration"", ""All S3 buckets must use KMS encryption with account-specific CMKs"", ""VPC CIDR blocks must follow pattern 10.{env_number}.0.0/16 where dev=1, staging=2, prod=3"", ""Use Pulumi configuration system for environment-specific variables, no hardcoded account IDs""]"
t0e0i5,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a PCI-compliant payment processing web application. The configuration must: 1. Create a VPC with 3 public and 3 private subnets across different AZs with NAT Gateways for outbound traffic. 2. Deploy an Aurora PostgreSQL cluster with encryption at rest using KMS and automated backups with 7-day retention. 3. Set up ECS Fargate service running containerized Node.js API with auto-scaling (2-10 tasks) based on CPU utilization. 4. Configure Application Load Balancer with HTTPS listeners using ACM certificates and target group health checks. 5. Create distribution serving React frontend from with OAI and custom error pages. 6. Implement Secrets Manager for database credentials with -based rotation function. 7. Configure CloudWatch Logs groups for ECS tasks, ALB access logs, and VPC Flow Logs with specified retention periods. 8. Set up AWS WAF WebACL with rate limiting and OWASP rules attached to both ALB and . 9. Create all necessary IAM roles and security groups following zero-trust principles. 10. Output the distribution URL, ALB DNS name, and cluster endpoint. Expected output: A fully functional Pulumi TypeScript program that provisions secure, highly available infrastructure for the payment processing application with all PCI compliance requirements met, including encryption, access controls, and audit logging.",A fintech startup needs to deploy their payment processing web application with strict compliance requirements for PCI DSS. The application consists of a React frontend and Node.js API backend that must handle sensitive cardholder data with end-to-end encryption and comprehensive audit logging.,"""Production-grade infrastructure deployed in us-east-1 across 3 availability zones. Core services include ECS Fargate for containerized Node.js API, Aurora PostgreSQL Multi-AZ for transaction data, with origin for React frontend, Application Load Balancer with AWS WAF integration. VPC with public subnets for ALB/NAT Gateways and private subnets for ECS tasks and . Requires Pulumi CLI 3.x, Node.js 18+, TypeScript 5.x, AWS CLI configured with appropriate permissions. Infrastructure must support 10,000 concurrent users with sub-100ms API response times.""","[""All data at rest must use AWS KMS customer-managed keys with automatic rotation enabled"", ""API endpoints must implement rate limiting of 100 requests per minute per IP address"", ""Database connections must use SSL/TLS certificates with certificate validation enabled"", ""ALB access logs must be stored in with server-side encryption and 90-day lifecycle policy"", ""ECS tasks must run in private subnets with no direct internet access"", ""Secrets Manager must be used for all database credentials with automatic rotation every 30 days"", "" distribution must use AWS WAF with OWASP Top 10 rule set"", ""All IAM roles must follow least privilege principle with no wildcard actions"", ""VPC Flow Logs must be enabled and sent to CloudWatch Logs with 30-day retention""]"
t9v5v9,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a serverless cryptocurrency price alert system. MANDATORY REQUIREMENTS (Must complete): 1. Create a DynamoDB table named 'crypto-alerts' with partition key 'userId' (string) and sort key 'alertId' (string), plus a global secondary index on 'coinSymbol' (CORE: DynamoDB). 2. Deploy a Lambda function 'price-checker' that fetches prices from exchanges and compares against user thresholds stored in DynamoDB (CORE: Lambda). 3. Create an EventBridge rule that triggers the price-checker Lambda every minute using a cron expression. 4. Deploy a second Lambda function 'alert-processor' that sends notifications via SNS when price thresholds are met. 5. Create an SNS topic 'price-alerts' with email subscription endpoint for notifications. 6. Implement proper IAM roles for each Lambda with least-privilege access to only required DynamoDB operations. 7. Add CloudWatch Logs groups for both Lambda functions with 14-day retention. 8. Configure Lambda environment variables for exchange API endpoints (encrypted with KMS). 9. Set up DynamoDB streams to trigger alert-processor when new alerts are added. OPTIONAL ENHANCEMENTS (If time permits):  Add API Gateway HTTP API for users to manage alerts via REST endpoints (OPTIONAL: API Gateway) - enables direct alert management.  Implement SQS queue between price-checker and alert-processor for decoupling (OPTIONAL: SQS) - improves reliability under load.  Add state machine for complex alert logic like multi-condition alerts (OPTIONAL: ) - enables advanced alert rules. Expected output: A complete Pulumi TypeScript program that provisions all infrastructure components, with type-safe resource definitions and proper error handling for the Lambda function code inline.","A fintech startup needs to process cryptocurrency price alerts in real-time, checking multiple exchanges every minute and notifying users when their price thresholds are met. The system must handle 100,000+ active alerts while maintaining sub-second notification delivery.","""Serverless infrastructure deployed in us-east-1 using Lambda functions for price checking and alert processing, DynamoDB for storing user alerts and price history, EventBridge for scheduling, and SNS for notifications. Requires Node.js 18+, Pulumi 3.x, AWS CLI configured with appropriate permissions. No VPC required as all services are managed. Lambda functions connect to external cryptocurrency exchange APIs. KMS key for encrypting sensitive environment variables.""","[""Lambda functions must use ARM64 architecture for cost optimization"", ""DynamoDB tables must use on-demand billing mode with point-in-time recovery enabled"", ""All Lambda functions must have reserved concurrent executions set to prevent throttling"", ""EventBridge rules must use cron expressions with UTC timezone"", ""SNS topics must have server-side encryption using AWS managed keys"", ""Lambda environment variables containing API keys must be encrypted using KMS"", ""DynamoDB global secondary indexes must project only required attributes"", ""Lambda functions must implement exponential backoff for external API calls"", ""All resources must have Cost Allocation tags with 'Environment' and 'Service' keys""]"
t5y1i0,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a complete CI/CD pipeline for containerized applications. The configuration must: 1. Set up a CodeCommit repository with a trigger rule for the main branch. 2. Create a CodeBuild project that builds Docker images and pushes to ECR. 3. Configure an S3 bucket for build artifacts with AES256 encryption and versioning enabled. 4. Deploy an ECS Fargate cluster with a service running 2 tasks behind an Application Load Balancer. 5. Create a with Source (CodeCommit), Build (CodeBuild), and Deploy (ECS) stages. 6. Implement rules to capture pipeline state changes and send notifications to SNS. 7. Configure all necessary IAM roles and policies with minimal required permissions. 8. Set up CloudWatch Log Groups for CodeBuild and ECS with appropriate retention periods. 9. Ensure all resources are tagged with Environment=Production and ManagedBy=Pulumi. Expected output: A fully functional Pulumi program that creates an automated deployment pipeline triggered by code commits, builds containerized applications, and deploys them to ECS Fargate with proper monitoring and notifications.","A software development team needs to automate their deployment pipeline for a Node.js application. The pipeline should build, test, and deploy the application to ECS Fargate whenever changes are pushed to the main branch in CodeCommit.","""AWS infrastructure deployed in us-east-1 region using for CI/CD automation, CodeBuild for application builds, and ECS Fargate for container hosting. VPC configured with 2 public subnets for ALB and 2 private subnets for ECS tasks across 2 availability zones. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, and AWS CLI configured with appropriate credentials. Architecture includes CodeCommit repository, S3 bucket for artifacts, ECR repository for Docker images, and SNS topic for notifications.""","[""Use CodeCommit as the source repository with automatic trigger on main branch"", ""CodeBuild project must use aws/codebuild/standard:7.0 image"", ""Build artifacts must be stored in S3 with server-side encryption"", ""ECS task definition must use 512 CPU units and 1024 memory"", "" must have exactly 3 stages: Source, Build, and Deploy"", ""All IAM roles must follow least privilege with no wildcard actions"", ""Build logs must be stored in CloudWatch Logs with 30-day retention"", ""ECS service must run in private subnets with ALB in public subnets"", ""Pipeline must use for failure notifications to SNS topic""]"
y6b6l6,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement an automated multi-region disaster recovery solution. The configuration must: 1. Set up DynamoDB global tables spanning us-east-1 and us-west-2 with on-demand billing and PITR enabled. 2. Create S3 buckets in both regions with versioning and cross-region replication using RTC. 3. Deploy identical Lambda functions in both regions with 512MB memory and Node.js 18 runtime. 4. Configure Route53 hosted zone with primary and secondary record sets using failover routing. 5. Implement health checks that monitor ALB endpoints in the primary region. 6. Store region-specific endpoints in SSM Parameter Store with SecureString type. 7. Create CloudWatch alarms that trigger on health check failures. 8. Set up SNS topics in both regions for failover notifications. 9. Ensure all IAM roles follow least-privilege with no wildcard permissions. 10. Tag all resources with Environment=production and DR-Role=primary/secondary. Expected output: A Pulumi program that deploys complete multi-region infrastructure with automated failover capability, switching traffic to the secondary region within 5 minutes of primary region failure.","A financial services company needs automated disaster recovery for their critical trading platform. The system must detect regional outages and automatically failover to a secondary region within 5 minutes, maintaining data consistency for transaction records.","""Multi-region disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary) regions. Core services include Route53 for DNS failover, DynamoDB global tables for transaction data, S3 with cross-region replication for document storage, Lambda functions for business logic, and Systems Manager for configuration management. Requires Pulumi 3.x with TypeScript, AWS CLI configured with multi-region access. Each region has its own VPC with public and private subnets across 3 availability zones. Application Load Balancers in each region handle traffic distribution.""","[""Use Route53 health checks with 30-second intervals for primary region monitoring"", ""DynamoDB global tables must have point-in-time recovery enabled"", ""Lambda functions must use reserved concurrency of exactly 100 for predictable failover performance"", ""S3 buckets require cross-region replication with RTC (Replication Time Control) enabled"", ""All resources must use consistent naming with format: {service}-{region}-{environment}-{purpose}"", ""Implement automated Route53 failover routing policy with 60-second TTL"", ""Use Systems Manager Parameter Store for storing region-specific configuration"", ""Deploy identical infrastructure stacks in us-east-1 (primary) and us-west-2 (secondary)""]"
i9j2u1,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a secure secrets vault infrastructure with automated rotation capabilities. MANDATORY REQUIREMENTS (Must complete): 1. Create KMS customer-managed keys with automatic annual rotation for database and application secrets (CORE: KMS) 2. Deploy Secrets Manager secrets for credentials and API keys with 30-day automatic rotation (CORE: Secrets Manager) 3. Implement Lambda function to handle custom secret rotation logic with VPC configuration 4. Configure IAM roles with least-privilege policies for Lambda execution 5. Set up CloudWatch Log groups with 90-day retention and KMS encryption 6. Create for Secrets Manager and KMS in private subnets 7. Apply mandatory tags to all resources with validation 8. Enable deletion protection on KMS keys only 9. Configure CloudWatch alarms for rotation failures OPTIONAL ENHANCEMENTS (If time permits):  Add EventBridge rules to notify on secret access patterns (OPTIONAL: EventBridge) - improves security monitoring  Implement AWS Config rules for compliance checking (OPTIONAL: Config) - ensures continuous compliance  Add Parameter Store for non-sensitive config (OPTIONAL: ) - separates concerns between secrets and config Expected output: A Pulumi TypeScript program that creates a production-ready secrets management infrastructure with automated rotation, comprehensive access controls, and audit logging capabilities.","A financial services company needs to implement a secure secrets management system for their microservices architecture. The system must enforce strict access controls, automatic key rotation, and comprehensive audit logging while maintaining zero-trust principles across their AWS infrastructure.","""Zero-trust security infrastructure deployed in us-east-1 using AWS Secrets Manager for centralized secret storage, KMS for encryption key management, Lambda functions for secret rotation logic. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate permissions. VPC with private subnets across 2 AZs, for Secrets Manager and KMS access. Production AWS account with and Config enabled for compliance monitoring.""","[""All KMS keys must use customer-managed policies with automatic annual rotation"", ""Secrets Manager must enforce minimum 32-character passwords with complexity requirements"", ""IAM policies must follow least-privilege with no wildcard actions or resources"", ""All secrets must be encrypted at rest using separate KMS keys per environment"", ""Lambda functions must use for Secrets Manager access"", ""CloudWatch Logs must retain audit logs for exactly 90 days with encryption"", ""Resource names must follow pattern: {env}-{service}-{resource}-{random}"", ""All resources must have mandatory tags: Environment, Owner, CostCenter, SecurityLevel"", ""Deletion protection must be enabled on KMS keys but disabled on other resources""]"
f7f0e1,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to refactor and optimize an existing ECS-based order processing system. MANDATORY REQUIREMENTS (Must complete): 1. Migrate ECS tasks from EC2 to Fargate Spot instances with 70% spot allocation (CORE: ECS) 2. Replace individual task definitions with a reusable component class accepting memory/CPU parameters 3. Implement ALB target group stickiness with 300-second duration and connection draining (CORE: ALB) 4. Convert RDS single-instance to Aurora Serverless v2 with 0.5-2 ACU scaling (CORE: RDS) 5. Add resource tagging function that enforces Team, Environment, and CostCenter tags 6. Implement stack exports for cross-stack references using StackReference 7. Configure CloudWatch Log Groups with 14-day retention and KMS encryption 8. Set up parameter validation to reject deployments with >4 CPU or >8GB memory per task OPTIONAL ENHANCEMENTS (If time permits):  Add tracing to identify performance bottlenecks (OPTIONAL: ) - improves debugging  Implement FIFO queue for order buffering (OPTIONAL: ) - adds resilience during spikes  Configure SNS alerts for deployment failures (OPTIONAL: SNS) - improves monitoring Expected output: Optimized Pulumi TypeScript program with modular components, proper type definitions, and deployment time under 15 minutes. Include comments explaining each optimization and estimated cost savings.",Your company's containerized order processing system has grown from handling 1000 to 50000 daily transactions. The existing Pulumi infrastructure takes 45 minutes to deploy and costs $8000/month despite only 30% resource utilization. The DevOps team needs to optimize the infrastructure code to reduce both deployment time and operational costs.,"""Production AWS environment in us-east-2 running containerized Node.js order processing services. Current setup uses ECS on EC2 with overprovisioned t3.xlarge instances, single-AZ RDS MySQL, and Classic Load Balancer. Requires Pulumi 3.x with TypeScript, AWS CLI configured with admin access. VPC spans 3 AZs with public/private subnets, NAT gateways for outbound traffic. Target architecture should support 100k daily transactions with <2s response time while reducing costs by 60%.""","[""All Fargate tasks must use ARM64 (Graviton2) architecture for cost optimization"", ""Aurora Serverless must have deletion protection disabled and point-in-time recovery enabled"", ""Component classes must use Pulumi's ComponentResource base class with proper type constraints"", ""ALB health checks must use HTTP path /health with 5-second intervals and 2 consecutive failures"", ""All encryption keys must be customer-managed KMS keys with automatic rotation enabled"", ""Stack must include unit tests using Pulumi's testing framework with minimum 80% coverage""]"
w2n8c9,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy an automated infrastructure compliance scanning system. MANDATORY REQUIREMENTS (Must complete): 1. Set up AWS Config with recording enabled for EC2, RDS, and resources (CORE: Config) 2. Create Lambda function to analyze Config snapshots and detect non-compliant resources (CORE: Lambda) 3. Store compliance scan results in DynamoDB with partition key 'resourceId' and sort key 'timestamp' 4. Configure bucket for Config delivery with AES256 encryption 5. Implement EventBridge rule to trigger compliance scans every 6 hours 6. Create topic for critical compliance violations 7. Set up CloudWatch dashboard showing compliance metrics 8. Enable tracing for all Lambda functions OPTIONAL ENHANCEMENTS (If time permits):  Add automation for auto-remediation (OPTIONAL: ) - enables automatic fixes  Implement for complex remediation workflows (OPTIONAL: ) - handles multi-step fixes  Add Athena for SQL queries on compliance data (OPTIONAL: Athena) - enables advanced analytics Expected output: A Pulumi program that deploys a fully automated compliance scanning system capable of detecting configuration drift, storing audit trails, and alerting on violations.","A financial services company needs automated infrastructure compliance scanning to meet SOC2 requirements. Their cloud resources must be continuously monitored for configuration drift and policy violations, with automated remediation for critical issues.","""Production compliance infrastructure deployed in us-east-1 using AWS Config for continuous configuration monitoring, Lambda for custom compliance rules execution, DynamoDB for tracking violation history, and for long-term audit storage. Requires Pulumi 3.x with TypeScript, AWS CLI configured with appropriate permissions. Resources span multiple availability zones with EventBridge for orchestration. used for private communication between services.""","[""Use AWS Config for compliance tracking with custom rules"", ""Lambda functions must use Node.js 18.x runtime with 256MB memory"", ""Store compliance results in with intelligent tiering enabled"", ""DynamoDB table must use on-demand billing mode"", ""All Lambda functions must have tracing enabled"", "" bucket must have versioning enabled and lifecycle policies"", ""Use EventBridge for scheduling scans every 6 hours"", ""Implement dead letter queues for all asynchronous operations""]"
h3o7z0,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a production-ready EKS cluster with enhanced security and auto-scaling capabilities. The configuration must: 1. Create a dedicated VPC with 3 availability zones, each containing public and private subnets. 2. Deploy an EKS cluster with managed node groups using t3.large instances. 3. Configure IRSA with OIDC provider for secure pod-to-AWS service authentication. 4. Install and configure the Kubernetes cluster autoscaler as a Helm chart. 5. Create an AWS Secrets Manager secret for storing database credentials. 6. Deploy a sample namespace with IRSA-enabled service account that can read the secret. 7. Configure EKS control plane logging with 30-day retention in CloudWatch. 8. Implement proper tagging strategy with Environment, Team, and Cost-Center tags. 9. Create an IAM role for the cluster autoscaler with appropriate permissions. 10. Export cluster endpoint, certificate authority data, and kubeconfig command. Expected output: A fully functional EKS cluster accessible via kubectl, with auto-scaling enabled, IRSA configured for secure AWS service access, and centralized logging. The infrastructure should be production-ready with proper security controls and monitoring in place.",Your DevOps team needs to deploy a production-grade Kubernetes cluster for a microservices application that processes financial transactions. The cluster must support auto-scaling based on workload demands and integrate with existing AWS services for secure secrets management and observability.,"""Production EKS infrastructure deployed in us-east-1 using Amazon EKS managed Kubernetes service with EC2-based managed node groups. Requires Pulumi CLI 3.x with TypeScript, AWS CLI configured with appropriate credentials, kubectl installed. VPC spans 3 availability zones with private subnets for worker nodes and public subnets for load balancers. Integration with AWS Secrets Manager for secure credential storage and CloudWatch for centralized logging. Node groups use t3.large instances with gp3 EBS volumes.""","[""Use Pulumi TypeScript with strict type checking enabled"", ""Deploy EKS cluster version 1.28 or higher with managed node groups"", ""Enable IRSA (IAM Roles for Service Accounts) for pod-level AWS permissions"", ""Configure cluster autoscaler with minimum 2 and maximum 10 nodes per group"", ""Implement OIDC provider integration for external authentication"", ""Use AWS Secrets Manager for storing sensitive configuration data"", ""Enable EKS control plane logging for audit, authenticator, and scheduler logs""]"
i5s9y4,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to implement a multi-region disaster recovery architecture for a trading application. The configuration must: 1. Deploy DynamoDB global tables with automated backups and point-in-time recovery in both us-east-1 and us-west-2. 2. Configure S3 buckets in both regions with cross-region replication and RTC enabled for sub-15-minute replication. 3. Implement customer-managed KMS keys in each region with appropriate key policies for cross-region access. 4. Deploy identical Lambda functions in both regions with environment-specific configurations. 5. Create Route 53 hosted zone with primary and secondary record sets using health checks. 6. Configure Route 53 health checks to monitor application endpoints in both regions. 7. Set up CloudWatch alarms for DynamoDB throttling, S3 replication lag, and Lambda errors. 8. Implement SNS topics in the primary region for alerting on failover events. 9. Ensure all resources have consistent tagging: Environment, Region, DR-Role. 10. Configure IAM roles with cross-region assume permissions for disaster recovery operations. 11. Enable CloudWatch Logs with 30-day retention for all Lambda functions. 12. Implement automated backup lifecycle policies for both regions. Expected output: A complete Pulumi TypeScript program with separate stacks for each region, shared configuration for consistency, and automated failover capabilities that achieve RTO < 15 minutes and RPO < 1 minute.",A financial services company requires a disaster recovery solution for their critical trading application. The system must maintain data consistency across regions while ensuring automatic failover capabilities with minimal data loss. Regulatory compliance mandates specific backup retention periods and encryption requirements.,"""Multi-region AWS deployment spanning us-east-1 (primary) and us-west-2 (disaster recovery). Infrastructure includes DynamoDB global tables for session data, S3 buckets with cross-region replication for static assets, Lambda functions for business logic, and Route 53 for DNS failover. Requires Pulumi 3.x with TypeScript, AWS SDK v3, Node.js 18+. Each region operates in isolated VPCs with private subnets. KMS keys in each region for encryption. CloudWatch cross-region monitoring with centralized alerting via SNS.""","[""Primary region must be us-east-1 with failover to us-west-2"", ""RTO (Recovery Time Objective) must not exceed 15 minutes"", ""RPO (Recovery Point Objective) must not exceed 1 minute"", ""All data must be encrypted at rest using customer-managed KMS keys"", ""DynamoDB global tables must have point-in-time recovery enabled"", ""Route 53 health checks must monitor both regions with 30-second intervals"", ""Lambda functions must be deployed identically in both regions"", ""S3 buckets must use cross-region replication with RTC (Replication Time Control)"", ""CloudWatch alarms must trigger SNS notifications for failover events"", ""All resources must be tagged with Environment, Region, and DR-Role tags""]"
j3y4r0,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a production-grade EKS cluster with advanced container orchestration capabilities. MANDATORY REQUIREMENTS (Must complete): 1. Create EKS cluster v1.29+ with OIDC provider and managed node groups using Bottlerocket AMI (CORE: EKS) 2. Configure ECR repositories with vulnerability scanning enabled for storing application images (CORE: ECR) 3. Deploy AWS Load Balancer Controller using Helm for ALB/NLB provisioning 4. Set up IRSA with least-privilege policies for pods to access S3, DynamoDB, and 5. Implement Calico network policies with default deny-all and explicit allow rules for services 6. Configure pod disruption budgets ensuring minimum 2 replicas during rolling updates 7. Install External Secrets Operator to sync secrets to Kubernetes 8. Deploy sample microservice with HPA targeting 70% CPU utilization and 2-10 replicas OPTIONAL ENHANCEMENTS (If time permits):  Add Fargate profile for batch processing workloads (OPTIONAL: Fargate) - reduces node management overhead  Implement Karpenter for advanced node autoscaling (OPTIONAL: EC2) - improves cost optimization  Configure daemon for distributed tracing (OPTIONAL: ) - enhances observability Expected output: Complete Pulumi TypeScript program that provisions a production-ready EKS cluster with Bottlerocket nodes, ECR integration, advanced networking policies, and a sample microservice demonstrating autoscaling and secret management capabilities.","A fintech startup needs to modernize their monolithic payment processing application by breaking it into microservices running on Kubernetes. The application handles sensitive financial transactions and requires strict network isolation, automatic scaling based on transaction volume, and blue-green deployments for zero-downtime updates.","""Multi-AZ EKS cluster deployment in us-east-2 using managed node groups with Bottlerocket OS, Application Load Balancer for ingress, and ECR for container registry. Requires Pulumi 3.x with TypeScript, kubectl 1.29+, AWS CLI v2 configured with appropriate permissions. VPC spans 3 availability zones with private subnets for worker nodes and public subnets for load balancers. Cluster autoscaler and metrics server pre-installed for horizontal pod autoscaling based on custom CloudWatch metrics.""","[""EKS cluster must use Kubernetes version 1.29 or higher with OIDC provider enabled"", ""Worker nodes must use Bottlerocket AMI for enhanced security and immutability"", ""All container images must be scanned for vulnerabilities using ECR scanning before deployment"", ""Network policies must enforce strict pod-to-pod communication rules with default deny-all"", ""Implement pod disruption budgets to maintain minimum 2 replicas during node updates"", ""Use IRSA (IAM Roles for Service Accounts) for all pod AWS API access - no instance profiles"", ""Enable AWS Load Balancer Controller for native ALB/NLB integration instead of classic ELBs"", ""All secrets must be stored in and synced using External Secrets Operator""]"
i6s8v5,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a comprehensive observability platform for multi-cluster Kubernetes environments. The configuration must: 1. Create an Amazon Managed Service for Prometheus (AMP) workspace with remote write configuration for ingesting metrics from multiple EKS clusters. 2. Deploy daemon as a DaemonSet across target clusters with service map configuration. 3. Set up CloudWatch Log Groups with KMS encryption and stream to Kinesis Data Firehose for centralized log processing. 4. Configure IAM roles with cross-account assume role permissions for metric collection agents. 5. Create for Prometheus, and CloudWatch services in the monitoring VPC. 6. Deploy Grafana on ECS Fargate with AMP as datasource using IRSA authentication. 7. Implement CloudWatch Synthetics canaries to monitor critical API endpoints with 5-minute intervals. 8. Configure topics with email subscriptions for alerting on metric thresholds. 9. Set up AWS Config rules to audit logging compliance across all resources. 10. Create function to auto-remediate non-compliant log configurations. Expected output: A fully automated observability stack that provides unified metrics, traces, and logs across multiple EKS clusters with Grafana dashboards, automated alerting, and compliance monitoring.","A financial services company needs centralized observability for their microservices running across multiple EKS clusters. They require real-time metrics, distributed tracing, and log aggregation to meet regulatory compliance for transaction monitoring and incident response times.","""Multi-cluster observability infrastructure deployed in us-east-2 across 3 availability zones. Core services include Amazon Managed Service for Prometheus (AMP) for metrics collection, for distributed tracing, and CloudWatch Logs with Kinesis Data Firehose for log aggregation. Requires Pulumi 3.x with TypeScript, Node.js 18+, and kubectl configured. Infrastructure spans a dedicated monitoring VPC with private subnets only, for AWS services, and attachment for cross-VPC communication with application clusters.""","[""All logs must be encrypted at rest using customer-managed KMS keys"", ""Metrics retention must be exactly 30 days for compliance requirements"", ""Use Prometheus-compatible metrics format for existing dashboards"", ""Implement resource tagging strategy with Environment, Team, and CostCenter tags"", ""Deploy monitoring stack in isolated VPC with no internet gateway access""]"
b7s6f2,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to orchestrate a zero-downtime migration from -based infrastructure to ECS Fargate. MANDATORY REQUIREMENTS (Must complete): 1. Create target Aurora PostgreSQL cluster with read replicas (CORE: RDS Aurora) 2. Deploy containerized application on ECS Fargate with auto-scaling (CORE: ECS) 3. Set up DMS replication instance and tasks for MySQL to PostgreSQL migration 4. Configure Application Load Balancer with weighted target groups (90% old, 10% new) 5. Implement CloudWatch alarms for migration metrics (replication lag, error rate) 6. Create entries for database endpoints and credentials 7. Set up IAM roles with least privilege for DMS, ECS tasks, and ALB 8. Configure security groups allowing only necessary inter-service communication OPTIONAL ENHANCEMENTS (If time permits):  Add weighted routing for DNS-level traffic control (OPTIONAL: ) - enables gradual DNS migration  Implement AWS Lambda for automated rollback triggers (OPTIONAL: Lambda) - adds intelligent failure handling  Configure for Aurora snapshots (OPTIONAL: ) - improves disaster recovery posture Expected output: Complete Pulumi program that creates parallel infrastructure, sets up continuous data replication, and enables controlled traffic migration with monitoring and rollback capabilities.","FinanceCore runs a legacy monolithic application on instances with local MySQL databases. The company needs to migrate this infrastructure to a modern, containerized architecture while maintaining zero downtime during the transition phase.","""Production environment migration in us-east-2 region spanning multiple availability zones. Source infrastructure includes instances in Auto Scaling Groups with RDS MySQL 5.7 databases. Target architecture uses ECS Fargate containers with Aurora PostgreSQL 13.7. Requires Pulumi 3.x with TypeScript, AWS CLI v2, Node.js 18+. VPC configuration includes public subnets for ALB and private subnets for ECS tasks and databases. Migration must support gradual traffic shifting between old and new environments.""","[""Use AWS Database Migration Service (DMS) for continuous replication from source RDS to target Aurora"", ""Implement blue-green deployment pattern with weighted target groups in ALB"", ""All data must be encrypted in transit using TLS 1.2 or higher"", ""Configure automated rollback if health checks fail for more than 5 minutes"", ""Use Pulumi state backends with encryption and versioning enabled"", ""Implement canary deployment with 10% initial traffic allocation""]"
f2p5s9,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a hub-and-spoke network architecture using AWS Transit Gateway. The configuration must: 1. Create 3 VPCs with non-overlapping CIDR ranges (10.0.0.0/16, 10.1.0.0/16, 10.2.0.0/16) for production, development, and shared-services environments. 2. Deploy 2 public and 2 private subnets in each VPC across 2 availability zones. 3. Create a Transit Gateway with custom route tables for traffic inspection and segmentation. 4. Attach all VPCs to the Transit Gateway with appropriate route table associations. 5. Configure VPC Flow Logs for all VPCs storing to a centralized S3 bucket with lifecycle policies. 6. Implement Network ACLs that deny traffic from unused RFC1918 ranges (172.16.0.0/12 and 192.168.0.0/16). 7. Store Transit Gateway attachment IDs in SSM Parameter Store for reference by other stacks. 8. Configure Transit Gateway route propagation for automatic route updates. 9. Apply consistent tagging strategy across all resources. 10. Enable DNS support settings in all VPCs for private hosted zone resolution. Expected output: A Pulumi program that creates a fully functional Transit Gateway hub-and-spoke network with 3 isolated VPCs, centralized routing, flow logs collection, and proper network segmentation ready for workload deployment.",A financial services company needs to establish secure connectivity between multiple isolated VPCs hosting different business units. Each VPC contains sensitive workloads that must communicate through a centralized inspection point while maintaining network isolation and compliance requirements.,"""Multi-VPC architecture deployed in us-east-1 using AWS Transit Gateway for centralized connectivity between production, development, and shared-services VPCs. Each VPC spans 2 availability zones with public and private subnets. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, and AWS CLI configured with appropriate permissions. S3 bucket needed for VPC Flow Logs storage. Systems Manager Parameter Store for configuration management. Total of 6 subnets per VPC with /24 CIDR blocks.""","[""Use Transit Gateway for inter-VPC connectivity with route table associations"", ""Deploy exactly 3 VPCs: production, development, and shared-services"", ""Implement VPC Flow Logs with S3 storage and 30-day lifecycle policy"", ""Configure Network ACLs with explicit deny rules for RFC1918 ranges not in use"", ""Use Systems Manager Parameter Store to store Transit Gateway attachment IDs"", ""Enable DNS resolution and DNS hostnames in all VPCs"", ""Tag all resources with Environment, CostCenter, and ManagedBy tags"", ""Use custom resource options to set deletion protection on Transit Gateway"", ""Implement route propagation between Transit Gateway route tables"", ""Export all VPC and subnet IDs as stack outputs for cross-stack references""]"
j2k9g2,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy identical payment processing infrastructure across three environments (dev, staging, prod) with automated consistency validation. The configuration must: 1. Define a reusable PaymentInfrastructure ComponentResource that encapsulates VPC, RDS cluster, Lambda functions, S3 buckets, and SQS queues. 2. Implement environment-specific configuration files validated with zod schemas for database sizes, Lambda memory, and retention periods. 3. Create Stack References to share VPC endpoints and KMS key ARNs between stacks within the same environment. 4. Deploy RDS Aurora PostgreSQL clusters with automated backups and point-in-time recovery enabled. 5. Configure Lambda functions using container images with environment-specific memory allocations (dev: 512MB, staging: 1024MB, prod: 2048MB). 6. Set up S3 buckets with intelligent tiering and 90-day lifecycle transitions to Glacier. 7. Implement SQS queues with dead letter queues and environment-specific visibility timeouts. 8. Create a validation script using Pulumi Automation API to compare resource configurations across environments. 9. Generate a drift detection report showing any configuration differences between environments. 10. Export critical resource ARNs and endpoints for use by application deployment pipelines. Expected output: A complete Pulumi TypeScript project with separate stacks for each environment, reusable components, validation scripts, and automated drift detection that ensures all three environments maintain identical infrastructure patterns while allowing controlled parameter variations.","A financial services company needs to maintain identical infrastructure across development, staging, and production environments for their payment processing system. They require automated environment replication with strict parameter validation to prevent configuration drift between environments.","""Multi-environment AWS deployment across us-east-1 (production), us-west-2 (staging), and eu-west-1 (development). Each environment requires isolated VPCs with 3 availability zones, private subnets for RDS Aurora PostgreSQL clusters, Lambda functions for payment processing, S3 buckets for transaction logs, and SQS queues for async processing. Requires Pulumi 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate IAM permissions. Each environment uses separate AWS accounts with cross-account IAM roles for deployment. KMS encryption keys must be region-specific and cannot be shared across environments.""","[""Use Pulumi Stack References to share outputs between environment stacks"", ""Implement custom ComponentResource classes for reusable infrastructure patterns"", ""Environment-specific configs must be validated using zod schemas"", ""All S3 buckets must have versioning enabled and lifecycle policies"", ""RDS instances must use encrypted storage with customer-managed KMS keys"", ""Lambda functions must use container images stored in private ECR repositories"", ""Network segmentation must prevent direct communication between environments"", ""Use Pulumi Policy Pack to enforce naming conventions across all resources"", ""Implement automated drift detection using Pulumi Automation API""]"
r1i1t9,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a payment processing web application with high-availability architecture. MANDATORY REQUIREMENTS (Must complete): 1. Create a VPC with 3 availability zones, each with public and private subnets (CORE: VPC) 2. Deploy an ECS cluster running containerized Node.js API with auto-scaling (2-10 tasks) (CORE: ECS) 3. Provision Aurora PostgreSQL cluster with Multi-AZ deployment and automated backups 4. Configure Application Load Balancer with HTTPS listener and target group health checks 5. Set up bucket for React frontend static files with distribution 6. Implement IAM roles with session-based temporary credentials for ECS tasks 7. Create CloudWatch Log Groups with 30-day retention for application logs 8. Configure for and ECR to reduce costs 9. Enable VPC Flow Logs to for network traffic analysis 10. Tag all resources with Environment, Project, and CostCenter tags OPTIONAL ENHANCEMENTS (If time permits):  Add WAF rules to ALB for SQL injection protection (OPTIONAL: WAF) - enhances security posture  Implement Secrets Manager for database credentials rotation (OPTIONAL: Secrets Manager) - improves credential management  Configure rules for ECS task state changes (OPTIONAL: ) - enables event-driven monitoring Expected output: A complete Pulumi TypeScript program that deploys the entire infrastructure stack with proper error handling, resource dependencies, and stack outputs including ALB DNS name, distribution URL, and cluster endpoint.","A fintech startup needs to deploy their transaction processing web application with strict compliance requirements for PCI-DSS. The application must handle sensitive payment data with proper isolation, encryption, and audit trails while maintaining sub-second response times for their React frontend.","""Production-grade infrastructure deployment in us-east-2 region featuring ECS for containerized Node.js backend, Aurora PostgreSQL Multi-AZ for transaction storage, and for static asset hosting. Requires Pulumi CLI 3.x with TypeScript/Node.js 18+, AWS CLI configured with appropriate credentials. Infrastructure spans 3 availability zones with public subnets for ALB and private subnets for compute/database resources. NAT gateways provide outbound connectivity for private resources. Deployment includes for and ECR to minimize data transfer costs.""","[""All instances must use encrypted storage with customer-managed KMS keys"", ""ECS tasks must run in private subnets with no direct internet access"", ""Application load balancer must terminate SSL with ACM certificates only"", ""CloudWatch Logs must have encryption at rest enabled using KMS"", ""Security groups must follow least-privilege with no 0.0.0.0/0 inbound rules"", ""All buckets must have versioning enabled and block public access"", ""ECS task definitions must use specific image tags, not 'latest'""]"
d0c3t8,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to deploy a serverless payment webhook processor. MANDATORY REQUIREMENTS (Must complete): 1. Create an API Gateway REST API with /webhook/{provider} endpoint accepting POST requests (CORE: API Gateway) 2. Deploy Lambda function (Node.js 18, ARM64, 1GB memory) to process webhooks with 30-second timeout (CORE: Lambda) 3. Create DynamoDB table 'payment-transactions' with 'transactionId' partition key and TTL enabled 4. Configure SNS topic 'payment-events' for successful transaction notifications 5. Implement SQS queue 'payment-processing' with visibility timeout of 60 seconds 6. Set up dead letter queue for the SQS queue with max receive count of 3 7. Create entries for API keys and webhook secrets (SecureString type) 8. Configure CloudWatch Log groups with 30-day retention for all Lambda functions 9. Implement least-privilege IAM roles with specific actions only 10. Enable tracing for Lambda and API Gateway 11. Add CloudWatch alarms for Lambda errors exceeding 1% error rate 12. Export API endpoint URL and Lambda function ARN as stack outputs OPTIONAL ENHANCEMENTS (If time permits):  Add for payment event routing (OPTIONAL: ) - enables flexible event-driven patterns  Implement for complex payment workflows (OPTIONAL: ) - adds orchestration capabilities  Add Kinesis Data Firehose for analytics (OPTIONAL: Kinesis) - enables real-time analytics pipeline Expected output: A complete Pulumi TypeScript program that deploys all resources with proper error handling, IAM policies, and monitoring. The infrastructure should be production-ready with built-in resilience and observability.","A fintech startup needs a serverless event-driven architecture to process real-time payment notifications from multiple payment providers. The system must handle webhook events, validate signatures, enrich transaction data, and notify downstream services while maintaining PCI compliance standards.","""Production-ready serverless infrastructure deployed in us-east-1 for payment webhook processing. Uses API Gateway REST API for webhook endpoints, Lambda functions with TypeScript runtime for processing logic, DynamoDB for transaction storage and idempotency, SNS for event fanout, and SQS for reliable message delivery. Requires Pulumi 3.x with TypeScript, Node.js 18+, and AWS CLI configured. for DynamoDB and S3 to keep traffic within AWS network. for secure configuration management. CloudWatch Logs and for observability.""","[""Lambda functions must use ARM64 architecture for cost optimization"", ""DynamoDB tables must use point-in-time recovery and encryption at rest"", ""All Lambda functions must have reserved concurrent executions set"", ""Dead letter queues must be configured for all async invocations"", ""API Gateway must use request validation and API keys"", ""CloudWatch Logs retention must be exactly 30 days for compliance"", ""All IAM roles must follow least privilege with no wildcard actions"", ""Lambda functions must use environment variables from ""]"
j5q3r2,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,"Create a Pulumi TypeScript program to deploy a multi-region disaster recovery infrastructure for a healthcare analytics platform. The configuration must: 1. Set up Aurora Global Database cluster in us-east-1 (primary) with read replica in us-west-2. 2. Configure DynamoDB global tables for session storage with contributor insights enabled. 3. Deploy Lambda functions in both regions with environment variables for region-specific endpoints. 4. Create Route53 hosted zone with health check-based routing policies for automatic failover. 5. Implement CloudWatch Synthetics canaries to monitor /health endpoints every 5 minutes. 6. Configure AWS Backup plans with cross-region copy for Aurora snapshots (daily backups, 7-day retention). 7. Set up CloudWatch alarms for database replication lag exceeding 1000ms. 8. Create SNS topics in both regions for failover notifications with email subscriptions. 9. Deploy ALB in each region with target group health checks at 10-second intervals. 10. Ensure all resources are tagged with Environment=Production and DisasterRecovery=Enabled. Expected output: A Pulumi TypeScript program that provisions a complete multi-region DR infrastructure with automated failover capabilities, monitoring, and backup strategies that meet healthcare compliance requirements for data availability.",A healthcare data analytics platform requires a disaster recovery solution to ensure continuous availability of patient dashboards and reporting services. The system must automatically failover to a secondary region within 60 seconds of primary region failure while maintaining data consistency and minimizing data loss.,"""Multi-region active-passive disaster recovery infrastructure spanning us-east-1 (primary) and us-west-2 (secondary). Utilizes Aurora Global Database for relational data storage with automatic failover, DynamoDB global tables for session management, Lambda functions for business logic processing, and Route53 for DNS failover. VPC configuration includes private subnets across 3 AZs in each region with VPC peering for cross-region communication. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, and AWS CLI configured with appropriate permissions. CloudWatch cross-region monitoring dashboards aggregate metrics from both regions.""","[""Use Route53 health checks with 30-second intervals for automatic failover"", ""Implement Aurora Global Database with backtrack enabled for point-in-time recovery"", ""Configure Lambda functions with reserved concurrency of 100 in both regions"", ""Use DynamoDB global tables with point-in-time recovery enabled"", ""Set RTO (Recovery Time Objective) of 60 seconds and RPO (Recovery Point Objective) of 5 minutes"", ""Implement dead letter queues for all asynchronous processes"", ""Use AWS Backup for automated cross-region backup of all stateful resources"", ""Configure CloudWatch Synthetics canaries to monitor endpoint availability"", ""Ensure all IAM roles use condition keys restricting access to specific regions""]"
p9w5s9,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy a secure data processing infrastructure for handling sensitive financial records. MANDATORY REQUIREMENTS (Must complete): 1. Create an bucket with KMS encryption using a customer-managed key, versioning enabled, and block all public access (CORE: ) 2. Deploy PostgreSQL instance in private subnets with encryption enabled and credentials stored in Secrets Manager with 7-day rotation (CORE: ) 3. Configure Lambda function in private subnet to process data between and with execution role limited to specific bucket and database 4. Create VPC with 3 private subnets across availability zones and for , Secrets Manager, and KMS 5. Enable VPC flow logs encrypted with KMS and stored in CloudWatch Logs with 90-day retention 6. Implement CloudWatch Log groups for Lambda and with KMS encryption 7. Create KMS key with strict key policy allowing only specific IAM roles 8. Configure Secrets Manager secret for credentials with automatic rotation Lambda OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Config rules for compliance checking (OPTIONAL: Config) - ensures ongoing compliance  Implement GuardDuty for threat detection (OPTIONAL: GuardDuty) - adds security monitoring  Add EventBridge rule to trigger alerts on failed Lambda executions (OPTIONAL: EventBridge) - improves incident response Expected output: A Pulumi TypeScript program that creates a fully secured data processing infrastructure with encryption everywhere, private networking, automated credential rotation, and comprehensive audit logging suitable for financial compliance requirements.","A financial services company needs to implement a secure data processing pipeline that meets strict compliance requirements for handling sensitive customer data. The infrastructure must enforce encryption at rest and in transit, implement fine-grained access controls, and maintain detailed audit logs for regulatory compliance.","""Secure multi-AZ deployment in us-east-1 region using Lambda for data processing, for encrypted storage, PostgreSQL with encryption enabled, and Secrets Manager for credential management. VPC spans 3 availability zones with private subnets only, no internet gateway. Lambda functions access AWS services via . KMS customer-managed keys for all encryption. Requires Pulumi CLI 3.x, TypeScript 4.x, Node.js 18+, AWS CLI configured with appropriate permissions. CloudWatch Logs encrypted with KMS for audit trails.""","[""All buckets must use KMS encryption with customer-managed keys and block public access"", ""Lambda functions must run in private subnets with no direct internet access"", ""All IAM roles must follow least-privilege principles with no wildcard actions"", ""VPC flow logs must be enabled and stored encrypted for 90 days minimum"", ""Secrets Manager must be used for all database credentials with automatic rotation""]"
o9x5y6,,Pulumi,TypeScript,expert,Application Deployment,Web Application Deployment,"Create a Pulumi TypeScript program to optimize an existing ECS Fargate deployment that currently costs $8,000/month. The configuration must: 1. Refactor ECS task definitions to use ARM-based Graviton2 instances for 20% cost savings 2. Implement auto-scaling policies based on custom CloudWatch metrics (target 70% CPU) 3. Replace always-on RDS Aurora cluster with Aurora Serverless v2 with minimum 0.5 ACUs 4. Configure Application Load Balancer with connection draining and deregistration delay of 30s 5. Fix memory leaks by setting container memory limits to 2048MB with 1792MB soft limit 6. Add lifecycle policies to ECR repositories to retain only last 10 images 7. Optimize VPC endpoints to reduce NAT Gateway data transfer costs 8. Enable ECS Container Insights for performance monitoring 9. Implement blue-green deployment pattern using target group switching 10. Set proper resource tagging for cost allocation (Environment, Team, Service) Expected output: Optimized Pulumi program that reduces infrastructure costs while maintaining performance SLAs, with clear comments explaining each optimization and its impact on the overall system.","A fintech startup's payment processing system built with Pulumi has grown to handle 50K transactions daily but suffers from cold starts, inefficient resource allocation, and excessive AWS costs. The existing infrastructure needs critical optimizations to reduce monthly spend by 40% while maintaining sub-200ms response times.","""Production AWS environment in us-east-1 running containerized payment processing workloads. Current setup uses ECS Fargate with 20 tasks, RDS Aurora PostgreSQL cluster, Application Load Balancer across 3 AZs. Requires Pulumi 3.x with TypeScript, AWS CLI configured with production credentials. VPC spans 3 availability zones with public/private subnets, NAT Gateways in each AZ. Monthly data transfer exceeds 5TB. System processes financial transactions requiring PCI compliance.""","[""Must maintain 99.9% uptime during optimization rollout"", ""Cannot exceed 200ms p95 latency for API responses"", ""Database connections must use SSL/TLS encryption"", ""All changes must be reversible within 5 minutes"", ""Container images must pass Trivy security scanning"", ""Resource names must follow pattern: {environment}-{service}-{resource-type}"", ""Total migration must complete within 4-hour maintenance window""]"
o1a9s0,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy an automated infrastructure compliance scanning system. MANDATORY REQUIREMENTS (Must complete): 1. Create Lambda function with 1GB memory and 5-minute timeout to scan AWS resources against compliance rules (CORE: Lambda) 2. Store scan results and compliance policies in DynamoDB with global secondary index for query by severity (CORE: DynamoDB) 3. Schedule hourly scans using EventBridge with retry configuration 4. Configure SNS topic for critical compliance violations with email subscription 5. Implement cross-account IAM role assumption for scanning multiple AWS accounts 6. Create Lambda layers for shared compliance rule libraries 7. Set up CloudWatch Log groups with 30-day retention for all Lambda functions 8. Configure error handling with DLQ for failed scans 9. Output scan statistics dashboard URL and SNS topic ARN OPTIONAL ENHANCEMENTS (If time permits):  Add for complex multi-stage scanning workflows (OPTIONAL: ) - enables parallel scanning and conditional logic  Implement API Gateway for manual scan triggering (OPTIONAL: API Gateway) - allows on-demand compliance checks  Add S3 bucket for detailed scan reports (OPTIONAL: S3) - provides long-term audit trail storage Expected output: A fully functional Pulumi program that deploys an automated compliance scanning system capable of analyzing AWS resources against defined policies, storing results, and alerting on violations.",A financial services company needs automated infrastructure compliance scanning to meet regulatory requirements. Their current manual review process takes weeks and often misses critical security misconfigurations. They require a solution that continuously monitors their AWS infrastructure against CIS benchmarks and custom policies.,"""Multi-account AWS infrastructure deployed across us-east-1 and eu-west-1 regions. Core services include Lambda for compliance scanning logic, DynamoDB for storing scan results and policy definitions, EventBridge for scheduled scanning, SNS for alerting on critical findings. required for private Lambda execution. Cross-account IAM roles needed for scanning target accounts. Requires Pulumi CLI 3.x with TypeScript, Node.js 18+, AWS CLI configured with appropriate permissions. Infrastructure spans development, staging, and production accounts with centralized compliance monitoring.""","[""Lambda functions must use ARM64 architecture for cost optimization"", ""DynamoDB tables must use point-in-time recovery and encryption at rest"", ""All Lambda environment variables containing credentials must be encrypted with customer-managed KMS keys"", ""EventBridge rules must use dead-letter queues for failed invocations"", ""Lambda functions must have reserved concurrent executions set to prevent throttling"", ""All resources must be tagged with Environment, Owner, and ComplianceLevel tags""]"
g4j1o2,,Pulumi,TypeScript,expert,"Security, Compliance, and Governance",Security Configuration as Code,"Create a Pulumi TypeScript program to deploy an Amazon EKS cluster with advanced authentication and authorization configurations. The configuration must: 1. Create an EKS cluster with both public and private API endpoint access in a custom VPC. 2. Configure an OIDC identity provider for the cluster to enable IRSA. 3. Deploy three managed node groups: 'dev' (t3.medium, 2-4 nodes), 'prod' (m5.large, 3-6 nodes), and 'monitoring' (t3.large, 1-2 nodes). 4. Set up the aws-auth ConfigMap with role mappings for DevOpsRole, DeveloperRole, and AuditorRole IAM roles. 5. Install and configure the VPC CNI addon with custom settings for pod security groups. 6. Deploy the EBS CSI driver addon with an IRSA-enabled service account. 7. Create namespace-specific service accounts with IAM role associations for 'payments' and 'analytics' namespaces. 8. Configure cluster logging for api, audit, authenticator, controllerManager, and scheduler log types. 9. Implement node group launch templates with custom user data for Bottlerocket configuration. 10. Set up cluster autoscaler RBAC and service account with proper IAM permissions. Expected output: A fully functional EKS cluster with granular RBAC controls, IRSA-enabled service accounts, and production-ready node configurations that can be validated using kubectl auth can-i commands for different role mappings.","A healthcare technology company needs to deploy a Kubernetes-based microservices platform for their patient data processing system. The infrastructure must support role-based access control with separate permissions for developers, operators, and auditors, while maintaining HIPAA compliance requirements.","""Production-grade EKS cluster deployed in us-east-1 across 3 availability zones. Requires Pulumi 3.x with TypeScript, AWS CLI v2 configured with appropriate permissions, kubectl 1.28+, and helm 3.x. Infrastructure includes EKS control plane, managed node groups with Bottlerocket OS, IAM OIDC provider for pod-level permissions, and core add-ons (VPC CNI, EBS CSI, CoreDNS). VPC with private subnets for worker nodes and public subnets for load balancers. Integration with AWS Systems Manager for node access.""","[""EKS cluster must use Kubernetes version 1.28 or higher"", ""OIDC provider must be configured for IRSA (IAM Roles for Service Accounts)"", ""aws-auth ConfigMap must define at least 3 distinct IAM role mappings"", ""Node groups must use Bottlerocket AMI for enhanced security"", ""Cluster must have both public and private endpoint access enabled"", ""VPC CNI addon version must be explicitly specified (not latest)"", ""All node groups must use launch templates with IMDSv2 required"", ""EBS CSI driver addon must be installed with encryption enabled""]"
z4f1p6,,Pulumi,TypeScript,expert,Failure Recovery and High Availability,Failure Recovery Automation,Create a Pulumi TypeScript program to implement a multi-region disaster recovery solution for a critical database workload. MANDATORY REQUIREMENTS (Must complete): 1. Deploy RDS Aurora Global Database with primary cluster in us-east-1 and secondary in eu-west-1 (CORE: RDS Aurora) 2. Configure Route 53 health checks and failover routing between regions (CORE: Route 53) 3. Implement S3 cross-region replication for database backups between us-east-1 and eu-west-1 buckets (CORE: S3) 4. Set up CloudWatch alarms for replication lag exceeding 5 seconds 5. Create Lambda functions in both regions to test database connectivity 6. Configure automatic backups with 7-day retention and point-in-time recovery 7. Implement IAM roles with cross-region assume role permissions for failover automation 8. Tag all resources with Environment=production and DisasterRecovery=enabled 9. Enable deletion protection on production resources but allow programmatic override 10. Configure VPC peering between regions for secure replication traffic OPTIONAL ENHANCEMENTS (If time permits):  Add AWS Backup for centralized backup management (OPTIONAL: AWS Backup) - provides unified backup policies  Implement EventBridge rules for automated failover triggers (OPTIONAL: EventBridge) - enables event-driven recovery  Add for cross-region configuration sync (OPTIONAL: ) - centralizes configuration management Expected output: A Pulumi TypeScript program that creates a fully functional multi-region disaster recovery infrastructure with automated health monitoring and failover capabilities. The solution should demonstrate RPO under 1 minute and RTO under 5 minutes.,"A financial services company needs disaster recovery capabilities for their trading platform database. They require automatic failover between regions with minimal data loss and downtime. The solution must handle database replication, DNS failover, and backup synchronization across regions.","""Multi-region AWS deployment spanning us-east-1 (primary) and eu-west-1 (secondary) for disaster recovery. Uses RDS Aurora Global Database for data replication, Route 53 for DNS failover, and S3 for backup synchronization. Requires Pulumi CLI 3.x with TypeScript, AWS CLI configured with credentials for both regions. VPCs in each region with private subnets for RDS, public subnets for ALBs. VPC peering connection enables secure cross-region traffic. CloudWatch monitors replication lag and triggers automated responses.""","[""RDS Aurora clusters must use r6g.large instances with encrypted storage using AWS managed keys"", ""Route 53 health checks must evaluate both database connectivity and replication lag before triggering failover"", ""S3 buckets must use versioning and lifecycle policies to retain backups for exactly 30 days"", ""All inter-region traffic must traverse VPC peering connections, not public internet"", ""Pulumi stack exports must include primary and secondary database endpoints, S3 bucket names, and Route 53 hosted zone ID""]"
