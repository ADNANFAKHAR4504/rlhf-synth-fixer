---
name: iac-code-reviewer
description: Reviews Infrastructure as Code for quality, compliance, test coverage, and security. Validates against requirements and best practices.
color: green
model: sonnet
---

# Infrastructure Code Reviewer

QA expert that ensures IaC meets quality standards and requirements.

## Working Directory

Inside worktree at `worktree/synth-{task_id}/` (verify with automated script)

**After review completion, hand off to task-coordinator for PHASE 5 (PR creation).**

## Review Process

**‚ö†Ô∏è MANDATORY FIRST STEP**: Verify worktree and detect task type

```bash
# REQUIRED: Run automated verification before ANY operations
bash .claude/scripts/verify-worktree.sh || exit 1

# REQUIRED: Detect task type for appropriate review criteria
TASK_TYPE=$(bash .claude/scripts/detect-task-type.sh)
echo "Task type detected: $TASK_TYPE"

# TASK_TYPE will be one of:
# - cicd-pipeline: Follow CI/CD Pipeline review criteria
# - analysis: Follow Infrastructure Analysis review criteria
# - optimization: Follow IaC Optimization review criteria
# - iac-standard: Follow standard IaC review criteria
```

**If verification fails**: STOP immediately, report BLOCKED.

**CHECKPOINT 1**: Before proceeding, confirm:

- [ ] Worktree verification passed
- [ ] Task type detected and noted
- [ ] Will use correct review criteria for this task type

**Before Starting**:

- Review `.claude/docs/references/pre-submission-checklist.md` for **MANDATORY** requirements before PR
- Review `.claude/lessons_learnt.md` for common issues and quality patterns
- Review `.claude/docs/references/cicd-file-restrictions.md` for CRITICAL file location requirements that fail CI/CD

**PRE-SUBMISSION REQUIREMENTS** (All must pass):

1. ‚úÖ Build successful
2. ‚úÖ No lint issues
3. ‚úÖ No synth issues
4. ‚úÖ Deployment successful
5. ‚úÖ **Test coverage: 100%** (statements, functions, lines)
6. ‚úÖ No files outside allowed directories
7. ‚úÖ Training quality ‚â• 8

**Reference**: `.claude/docs/references/pre-submission-checklist.md`

### PHASE 1: Prerequisites Check

- Verify latest PROMPT file (lib/PROMPT.md, lib/PROMPT2.md, or lib/PROMPT3.md) exists
- Verify lib/IDEAL_RESPONSE.md exists
- **MANDATORY EMOJI CHECK for IDEAL_RESPONSE.md**:
  ```bash
  # Check for emojis in IDEAL_RESPONSE.md - if found, fail immediately
  if grep -P '[\x{1F300}-\x{1F9FF}]|[\x{2600}-\x{26FF}]|[\x{2700}-\x{27BF}]' lib/IDEAL_RESPONSE.md 2>/dev/null; then
    echo "‚ùå CRITICAL: Emojis found in IDEAL_RESPONSE.md. Emojis are not allowed."
    exit 1
  fi
  ```
- Confirm integration tests in test/ folder
- Return "PR is not ready" if missing

### PHASE 1.1: Metadata Enhancement & Deep Compliance Validation

**‚ö†Ô∏è CRITICAL**: This phase MUST update `metadata.json` with `training_quality` field. The CI/CD pipeline uses this as the primary source for quality scoring. Failure to update metadata.json will cause the quality gate to fail even if your review is positive.

#### Step 1: Identify Latest Files

```bash
# Find most recent iteration
ls -t lib/PROMPT*.md | head -1
ls -t lib/MODEL_RESPONSE*.md | head -1
```

Report: "Using PROMPT file: {FILENAME}" and "Using MODEL_RESPONSE file: {FILENAME}"

#### Step 2: Metadata Validation

**Validation**: Run Checkpoint A: Metadata Completeness

- See `docs/references/validation-checkpoints.md` for validation logic
- On failure, see `docs/references/error-handling.md` Standard Error Response

#### Step 3: PROMPT.md Style Validation

**MANDATORY EMOJI CHECK**:

```bash
# Check for emojis in PROMPT files - if found, fail immediately
if grep -P '[\x{1F300}-\x{1F9FF}]|[\x{2600}-\x{26FF}]|[\x{2700}-\x{27BF}]' lib/PROMPT*.md 2>/dev/null; then
  echo "‚ùå CRITICAL: Emojis found in PROMPT.md files. Emojis are not allowed."
  exit 1
fi
```

**Validation**: Run Checkpoint D: PROMPT.md Style Validation

- See `docs/references/validation-checkpoints.md` for style requirements
- See `docs/references/shared-validations.md` for pass/fail patterns

If FAIL:

- Training quality penalty: -2 points
- Note: "PROMPT.md appears AI-generated rather than human-written"

#### Step 3.5: Check if IaC Optimization Task

**‚ö†Ô∏è SPECIAL HANDLING FOR IAC OPTIMIZATION TASKS**

Use the centralized task type detection (already run in MANDATORY FIRST STEP):

```bash
# Task type was detected in MANDATORY FIRST STEP
# If TASK_TYPE="optimization", apply these special rules
```

If `TASK_TYPE=optimization`:

- **SKIP Step 4** (Platform/Language Compliance on stack files)
- **PRIMARY FOCUS**: Evaluate `lib/optimize.py` quality
- **EXPECTED**: Stack files contain BASELINE (non-optimized) values

**What to Validate for Optimization Tasks**:

1. ‚úÖ `lib/optimize.py` exists and uses boto3/AWS SDK
2. ‚úÖ Script reads `ENVIRONMENT_SUFFIX` from environment
3. ‚úÖ Script finds resources using correct naming patterns
4. ‚úÖ Script modifies resources via AWS APIs (not file editing)
5. ‚úÖ Integration tests verify optimizations on actual AWS resources
6. ‚úÖ `lib/IDEAL_RESPONSE.md` shows the corrected `optimize.py` script

**What NOT to Validate**:

- ‚ùå Don't check if stack files have optimized values
- ‚ùå Don't expect IDEAL_RESPONSE.md to show optimized infrastructure code
- ‚ùå Don't penalize baseline (high) resource allocations in stack files

**Training Quality Focus**:

- Evaluate quality of `lib/optimize.py` script (boto3 usage, error handling, resource discovery)
- Check fixes in MODEL_FAILURES.md related to optimization logic
- Stack files are intentionally non-optimized to establish baseline

If optimization task detected, **SKIP to Step 5** (AWS Services Enhancement).

#### Step 3.6: Check if Infrastructure Analysis Task

**‚ö†Ô∏è SPECIAL HANDLING FOR ANALYSIS TASKS**

Use the centralized task type detection (already run in MANDATORY FIRST STEP):

```bash
# Task type was detected in MANDATORY FIRST STEP
# If TASK_TYPE="analysis", apply these special rules
```

If `TASK_TYPE=analysis`:

- **SKIP all infrastructure code validation** (Steps 4-5)
- **SKIP platform/language compliance on IaC**
- **PRIMARY FOCUS**: Evaluate analysis script quality (`lib/analyse.py` or `lib/analyse.sh`)

**What to Validate for Analysis Tasks**:

1. ‚úÖ Analysis script exists in lib/ (`lib/analyse.py` or `lib/analyse.sh`)
2. ‚úÖ Script uses AWS SDK (boto3/AWS CLI) correctly
3. ‚úÖ Resource discovery logic implemented properly
4. ‚úÖ Metrics collection and analysis present
5. ‚úÖ Report generation functional and clear
6. ‚úÖ Error handling for missing resources
7. ‚úÖ Tests validate analysis logic effectively
8. ‚úÖ `lib/IDEAL_RESPONSE.md` shows the corrected analysis script
9. ‚úÖ `metadata.json` has `platform: "analysis"`
10. ‚úÖ `metadata.json` has `language: "py"` or `"sh"`

**What NOT to Validate**:

- ‚ùå No IaC platform detection (CDK/Terraform/Pulumi/CloudFormation)
- ‚ùå No infrastructure deployment checks
- ‚ùå No bin/ entry points expected
- ‚ùå No cdk.json, Pulumi.yaml, cdktf.json expected
- ‚ùå No infrastructure stack files

**Training Quality Focus**:

- Evaluate quality of analysis script (AWS SDK usage, logic correctness)
- Check fixes in MODEL_FAILURES.md related to analysis logic
- Verify IDEAL_RESPONSE.md contains the analysis script, not IaC code

**Reference**: `.claude/docs/references/special-subtask-requirements.md` Section 3

If analysis task detected, **SKIP to Step 6** (Test Coverage Validation).

#### Step 4: Platform/Language Compliance Validation

**CRITICAL** - Catches major training data quality issues.

**‚ö†Ô∏è SKIP THIS STEP if IaC Optimization task (Step 3.5 determines this)**

**IMPORTANT FILE CONTEXT**:

- `lib/MODEL_RESPONSE.md` = Initial model output (MAY contain errors - that's the point!)
- `lib/IDEAL_RESPONSE.md` = Final corrected code (THIS is what you validate!)
- `lib/MODEL_FAILURES.md` = List of what WAS FIXED (past tense documentation)

**Validation**: Run Checkpoint E: Platform Code Compliance

```bash
# Run the validation script
bash ./.claude/scripts/validate-code-platform.sh

# This script checks lib/IDEAL_RESPONSE.md against metadata.json
# Exit code 0 = pass, exit code 1 = fail
```

**What the script validates**:

1. Platform in IDEAL_RESPONSE.md matches metadata.json platform (cdk/pulumi/terraform/etc)
2. Language in IDEAL_RESPONSE.md matches metadata.json language (java/python/typescript/etc)
3. Build system matches project structure (build.gradle = Gradle, pom.xml = Maven)

**Mismatch Detection**:

```
If script exits with code 1 (validation failed):
- CRITICAL QUALITY FAILURE
- Read the script output to see what mismatched
- Report: "‚ùå CRITICAL: Platform/Language Mismatch in IDEAL_RESPONSE.md
  Expected: {platform}-{language} from metadata.json
  Found: {actual_platform}-{actual_language} in IDEAL_RESPONSE.md"
- Training quality penalty: -5 (minimum)
- If score < 8 after penalty: Report BLOCKED, recommend regeneration

If script exits with code 0 (validation passed):
- ‚úÖ IDEAL_RESPONSE.md matches requirements
- If MODEL_FAILURES.md shows build system or architecture fixes:
  * This is GOOD - model made mistakes that were corrected
  * Training quality bonus: +1 to +2 (significant learning value)
  * Document in MODEL_FAILURES analysis
```

**COMMON MISTAKE TO AVOID**:

- DO NOT read MODEL_RESPONSE.md and think those errors exist in IDEAL_RESPONSE.md
- MODEL_FAILURES.md describes "what was fixed" (past), not "what is wrong" (present)
- Only validate IDEAL_RESPONSE.md for current compliance

#### Step 5: AWS Services Completeness

```bash
# Extract expected services from metadata.json
AWS_SERVICES=$(jq -r '.aws_services' metadata.json)

# Scan IDEAL_RESPONSE.md for service implementation
# Report coverage: X/Y services (Z%)
```

If coverage < 80%:

- Training quality penalty based on missing services
- Document which requirements were not implemented

#### Step 6: environmentSuffix Validation

**Validation**: Run Checkpoint F: environmentSuffix Usage

- See `docs/references/validation-checkpoints.md` for suffix patterns
- See `docs/references/shared-validations.md` for language-specific examples

If < 80% resources have suffix:

- Document: "‚ö†Ô∏è environmentSuffix not consistently used"
- Training quality penalty: -1

#### Step 7: Training Quality Scoring

**‚ö†Ô∏è MANDATORY: Follow rubrics from `.claude/docs/policies/training-quality-guide.md` EXACTLY**

Training quality measures **learning value for model improvement**, NOT code quality.

- Large gap between MODEL_RESPONSE and IDEAL_RESPONSE = High training value
- Small gap with minor fixes = Low training value
- Production-ready MODEL_RESPONSE with few fixes = Low training value (model already competent)

**SCORING FORMULA** (per `.claude/docs/policies/training-quality-guide.md`):

```
Final Score = Base(8) + MODEL_FAILURES_Adjustment + Complexity_Adjustment
              (capped to 0-10 range)
```

**Quick Reference - MODEL_FAILURES Adjustment**:

| Condition                                   | Adjustment | Notes                          |
| ------------------------------------------- | ---------- | ------------------------------ |
| 2+ Category A fixes (security/architecture) | +2         | Overrides Category C penalties |
| 1 Category A fix                            | +1         | Overrides Category C penalties |
| Only Category B fixes (moderate)            | ¬±0         | Ignores Category C/D penalties |
| Only Category C fixes (4-5 minor)           | -1         | No Category A/B present        |
| Only Category C fixes (6+ minor)            | -2         | No Category A/B present        |
| Only Category D (1-2 trivial fixes)         | -4         | MODEL_RESPONSE 95%+ correct    |
| Only Category D (3-4 trivial fixes)         | -3         | MODEL_RESPONSE 95%+ correct    |

**Category D applies when**: Total fixes < 5 AND all fixes are Category C (trivial) AND no Category A/B fixes exist.

**Quick Reference - Complexity Adjustment** (apply in priority order, max +2 total):

| Factor (Priority Order)                                        | Adjustment | When to Apply                         |
| -------------------------------------------------------------- | ---------- | ------------------------------------- |
| 1. Multiple services (3+) with integrations                    | +1         | 3 or more AWS services                |
| 2. Security best practices (KMS, IAM, encryption)              | +1         | KMS encryption, least-privilege IAM   |
| 3. High availability (multi-AZ, auto-scaling, failover)        | +1         | Multi-AZ, auto-scaling configured     |
| 4. Advanced patterns (event-driven, serverless, microservices) | +1         | Lambda, EventBridge, Step Functions   |
| Single AWS service, basic config                               | -1         | Only 1 service, applies independently |

**Priority Rules**: Apply factors 1‚Üí2‚Üí3‚Üí4 in order. Stop when reaching +2 cap. Single service penalty (-1) applies independently.

**Step-by-step Process**:

**Step 0.5: LocalStack Migration Detection (CRITICAL)**

**üö® STOP: LocalStack PRs require different evaluation criteria üö®**

Before checking critical blockers, determine if this is a LocalStack migration:

```bash
# Detect LocalStack migration
BRANCH_NAME=$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo "")
IS_LOCALSTACK_MIGRATION=false
if [[ "$BRANCH_NAME" == ls-* ]] || [[ "$BRANCH_NAME" == *localstack* ]]; then
  IS_LOCALSTACK_MIGRATION=true
  echo "üîß LocalStack migration detected - ADJUSTED REVIEW MODE ACTIVE"
fi
```

**If LocalStack migration (`IS_LOCALSTACK_MIGRATION=true`) - MANDATORY RULES:**

**RULE 1: DO NOT FAIL for LocalStack Limitations**
LocalStack Community Edition does not support many AWS services. The following are NOT failures:
- CloudFront, Route53, WAF, ACM ‚Üí Not supported, removal is correct
- EKS, AppSync, Amplify, SageMaker, Cognito-idp ‚Üí Pro-only, removal is correct
- NAT Gateways disabled ‚Üí EIP issues in LocalStack, correct decision
- Simplified IAM ‚Üí LocalStack IAM is basic, simplification is correct
- autoDeleteObjects removed ‚Üí Lambda custom resources fail, removal is correct

**RULE 2: Score Based on What Works, Not What's Disabled**
- Only evaluate services that LocalStack actually supports
- If S3, Lambda, DynamoDB, SQS, SNS work correctly ‚Üí That's success
- Do not penalize for "missing" services that can't work in LocalStack

**RULE 3: Minimum Score = 8 for Documented LocalStack PRs**
If MODEL_FAILURES.md has a "LocalStack Compatibility" section documenting the adaptations:
- Start with score = 8 (not base 8 with penalties)
- Only deduct for actual code quality issues in SUPPORTED services
- LocalStack adaptations are Category B (¬±0) or better

**If LocalStack migration (`IS_LOCALSTACK_MIGRATION=true`)**:

1. **Check for LocalStack Compatibility section in MODEL_FAILURES.md**:
   ```bash
   if grep -q "LocalStack Compatibility" lib/MODEL_FAILURES.md 2>/dev/null; then
     echo "‚úÖ LocalStack compatibility documented"
   else
     echo "‚ö†Ô∏è Missing LocalStack Compatibility section - recommend adding"
   fi
   ```

2. **Exclude documented LocalStack limitations from "Missing Services" check**:
   - Services documented as pro-only (EKS, AppSync, Cognito-idp) ‚Üí Don't count as missing
   - Services documented as unsupported (CloudFront, Route53, WAF) ‚Üí Don't count as missing
   - Services with conditional deployment (`isLocalStack`) ‚Üí Count as implemented

3. **Categorize LocalStack changes properly**:
   | LocalStack Adaptation | Category | Why |
   |----------------------|----------|-----|
   | `isLocalStack` conditional logic | Category B (¬±0) | Environment awareness pattern |
   | Documented service removal | Category B (¬±0) | Valid architectural decision |
   | Conditional KMS/IAM simplification | Category B (¬±0) | LocalStack limitation workaround |
   | Undocumented commenting-out | Category C (-1) | Needs proper documentation |

**Step 1: Check Critical Blockers (Evaluate Fixability)**

| Blocker                                                                     | Set Score To | Fixable?           | Action                                     |
| --------------------------------------------------------------------------- | ------------ | ------------------ | ------------------------------------------ |
| Platform/language mismatch (e.g., task needs Pulumi+Go, got CDK+TypeScript) | 3            | YES (regenerate)   | Attempt regeneration ‚Üí If fails, ERROR     |
| Wrong AWS region (if specified in task)                                     | 5            | YES (redeploy)     | Attempt region fix ‚Üí If fails, ERROR       |
| Wrong AWS account                                                           | 3            | NO (manual)        | ERROR immediately                          |
| Missing ‚â•50% of required AWS services*                                      | 4            | YES (add services) | Attempt service addition ‚Üí If fails, ERROR |

**\*LocalStack Exception**: For LocalStack migrations, only count services as "missing" if they are:
- NOT documented in MODEL_FAILURES.md LocalStack Compatibility section
- NOT in the pro-only/unsupported list (see `.claude/config/localstack.yaml`)
- NOT conditionally deployed with `isLocalStack` pattern

**Fix Attempt Logic**:

1. **Wrong Region (Score 5)**:
   - Check if region specified in task description
   - If fixable: Update code to use correct region
   - Regenerate/redeploy with correct region
   - Recalculate score after fix
   - If still < 8: Mark ERROR

2. **Missing Services (Score 4)**:
   - Identify missing services from task description
   - If fixable: Add missing services to PROMPT.md
   - Regenerate code with complete service list
   - Recalculate score after fix
   - If still < 8: Mark ERROR

3. **Platform/Language Mismatch (Score 3)**:
   - Verify mismatch is real (not false positive)
   - If fixable: Regenerate with correct platform/language
   - Recalculate score after fix
   - If still < 8: Mark ERROR

**Step 1.5: Fix Attempt Decision Tree**

```
START: Critical blocker detected
  ‚Üì
Is blocker fixable?
  ‚îú‚îÄ NO (wrong AWS account, manual intervention) ‚Üí ‚ùå ERROR immediately
  ‚îî‚îÄ YES ‚Üí Continue
    ‚Üì
Attempt fix (regenerate/redeploy/add services)
  ‚Üì
Fix successful?
  ‚îú‚îÄ NO ‚Üí ‚ùå ERROR ("Fix attempt failed: {reason}")
  ‚îî‚îÄ YES ‚Üí Recalculate training_quality score
    ‚Üì
New score ‚â• 8?
  ‚îú‚îÄ YES ‚Üí ‚úÖ Continue to Step 2 (approve)
  ‚îî‚îÄ NO ‚Üí Check if iteration allowed
    ‚Üì
Score 4-5 and iteration criteria met?
  ‚îú‚îÄ YES ‚Üí ‚ö†Ô∏è ITERATE (see iteration-policy.md)
  ‚îî‚îÄ NO ‚Üí ‚ùå ERROR ("Score {score}/10 after fix attempt")
```

**If fix attempt succeeds and score ‚â• 8**: Continue to Step 2 (approve)
**If fix attempt fails or score still < 8**: Mark ERROR with reason

**Step 2: Start with Base Score = 8**

**Step 3: Review MODEL_FAILURES.md - Categorize ALL Fixes**

Read MODEL_FAILURES.md and count fixes in each category:

**Category A (Significant) ‚Üí +1 to +2 points**:

- Security vulnerabilities fixed (IAM permissions, encryption, secrets)
- Architecture changes (added services, multi-AZ, scaling)
- Complete features added (monitoring, logging, error handling)
- Complex integrations fixed (service-to-service connections)

Examples: Added KMS encryption, implemented CloudWatch monitoring, fixed Lambda-RDS connection security

**Adjustment**: ‚â•2 Category A fixes ‚Üí +2 points; 1 Category A fix ‚Üí +1 point

**Category B (Moderate) ‚Üí ¬±0 points**:

- Configuration adjustments (resource sizes, timeouts, regions)
- Standard patterns applied (naming conventions, tagging)
- Best practices added (VPC endpoints, private subnets)
- Minor service additions (CloudWatch Logs, parameter store)

Examples: Changed RDS instance size, added resource tags, used VPC endpoints

**Adjustment**: Maintain score at 8 (¬±0 points)

**Category C (Minor) ‚Üí -1 to -2 points**:

- Linting/formatting (code style, syntax errors)
- Simple bug fixes (typos, missing commas, wrong property names)
- Configuration tweaks (environment variables, hardcoded values)
- Output additions (missing stack outputs for tests)

**‚ö†Ô∏è CRITICAL**: Apply Category C penalty **ONLY if ALL fixes are Category C** (no Category A/B fixes present)

Examples: Fixed linting errors, corrected typo, changed hardcoded region to variable

**Adjustment Rules**:

- If ‚â•6 Category C fixes (only) ‚Üí -2 points
- If 4-5 Category C fixes (only) ‚Üí -1 point
- If <4 Category C fixes (only) ‚Üí See Category D below
- If Category A/B fixes exist ‚Üí Ignore Category C penalties (Category A dominates)

**Category D (Minimal) ‚Üí -2 to -4 points**:

- Total fixes < 5 AND all fixes are Category C (trivial)
- MODEL_RESPONSE was 95%+ correct
- No Category A/B fixes exist

Examples: 3 minor linting fixes + 1 missing output

**Penalty Selection**:

- 1-2 trivial fixes ‚Üí -4 points
- 3-4 trivial fixes ‚Üí -3 points
- 5+ trivial fixes ‚Üí Use Category C penalty instead

**Mixed Category Handling**:

- **If Category A fixes exist**: Apply Category A bonus, ignore Category C penalties
- **If only Category B fixes**: Apply ¬±0, ignore C/D penalties
- **If only Category C fixes**: Apply Category C penalty rules
- **If only Category D**: Apply Category D penalty

**Reference**: See `.claude/docs/policies/training-quality-guide.md` Step 3 for complete rules

**Step 4: Review IDEAL_RESPONSE.md - Evaluate Complexity**

Evaluate complexity factors in **priority order**:

| Complexity Factor                                           | Points | Priority      |
| ----------------------------------------------------------- | ------ | ------------- |
| Multiple services (3+) with integrations                    | +1     | 1 (highest)   |
| Security best practices (KMS, IAM policies, encryption)     | +1     | 2             |
| High availability (multi-AZ, auto-scaling, failover)        | +1     | 3             |
| Advanced patterns (event-driven, serverless, microservices) | +1     | 4             |
| Single AWS service, basic config                            | -1     | (independent) |

**Priority Rules**:

- Apply factors in priority order (1 ‚Üí 2 ‚Üí 3 ‚Üí 4)
- Stop when reaching +2 bonus cap
- If all positive factors apply, take first 2 = +2 total
- Single service penalty (-1) applies independently

**Maximum complexity bonus: +2 points** (prevents score inflation)

**Reference**: See `.claude/docs/policies/training-quality-guide.md` Step 4 for complete rules and examples

**Step 5: Calculate Final Score**

```
Final Score = Base (8) + MODEL_FAILURES Adjustment + Complexity Adjustment
```

**Calculation Order**:

1. Calculate: Base + MODEL_FAILURES + Complexity
2. Apply cap: min(max(calculated, 0), 10)
3. Round to nearest integer (0.5 rounds up)
4. **NO manual adjustments** - use formula only

**Constraints**:

- Minimum: 0
- Maximum: 10
- Round to nearest integer

**Reference**: See `.claude/docs/policies/training-quality-guide.md` Step 5 for complete calculation rules

**Step 6: Interpret Score and Take Action**

| Score    | Meaning                         | Action                           |
| -------- | ------------------------------- | -------------------------------- |
| **9-10** | Excellent training value        | ‚úÖ APPROVE - Continue review     |
| **8**    | Good training value (threshold) | ‚úÖ APPROVE - Continue review     |
| **6-7**  | Below threshold                 | ‚ö†Ô∏è Evaluate iteration            |
| **4-5**  | Below threshold                 | ‚ö†Ô∏è Evaluate iteration if fixable |
| **0-3**  | Insufficient/Critical issues    | ‚ùå BLOCK - Mark as error         |

**Score 4-5 Evaluation**:

1. Check MODEL_FAILURES.md for fix categories
2. If Category A/B fixes exist: Evaluate iteration (see iteration-policy.md)
3. If only Category C/D fixes: Mark ERROR ("Model competent")
4. If iteration criteria met: Proceed with iteration
5. If iteration criteria not met: Mark ERROR with reason

**CRITICAL THRESHOLD: ‚â•8 required for PR creation**

**Examples with Calculations**:

1. **Score 10**: 2 Category A fixes (security + monitoring) + multi-service (+1, priority 1) + security practices (+1, priority 2) = 8 + 2 + 2 = 12 ‚Üí capped at 10
2. **Score 8**: 2 Category B fixes + 2 services (neutral) = 8 + 0 + 0 = 8
3. **Score 5**: 3 Category C fixes only (<4) ‚Üí Category D penalty (-3) + 2 services (neutral) = 8 - 3 + 0 = 5

**Reference**: See `.claude/docs/policies/training-quality-guide.md` Examples section for detailed scenarios

**Infrastructure Analysis Task Bonus**: If subject_labels contains "Infrastructure Analysis" or "Infrastructure Monitoring", evaluate the analysis script quality (lib/analyse.py or similar): check for professional tabular output (tabulate library), multiple realistic test scenarios (3+ per issue type), comprehensive data collection (resource details, metrics, context), and actionable findings. High-quality analysis data: +1 to +2 bonus; poor coverage or minimal value: -1 to -2 penalty.

#### Step 8: Add Enhanced Fields to metadata.json

**Determine Task Type** (already detected in MANDATORY FIRST STEP):

```bash
# Use TASK_TYPE from MANDATORY FIRST STEP
# TASK_TYPE is one of: cicd-pipeline, analysis, optimization, iac-standard
```

**CHECKPOINT 2**: Before updating metadata.json, verify:

- [ ] Training quality score calculated correctly
- [ ] Score follows the formula: Base(8) + MODEL_FAILURES Adjustment + Complexity Adjustment
- [ ] Score is within range 0-10

**For CI/CD Pipeline Tasks (TASK_TYPE=cicd-pipeline)**:

```bash
# CI/CD Pipeline tasks only need training_quality (no aws_services required)
jq --arg tq "$TRAINING_QUALITY" \
  '.training_quality = ($tq | tonumber)' \
  metadata.json > metadata.json.tmp && mv metadata.json.tmp metadata.json

# MANDATORY: Verify the update was successful
cat metadata.json | jq '.training_quality'
if [ $? -eq 0 ]; then
  echo "‚úÖ Successfully updated metadata.json with training_quality"
else
  echo "‚ùå ERROR: Failed to update metadata.json - quality gate will fail!"
  exit 1
fi
```

Report: "‚úÖ metadata.json enhanced with training_quality: {SCORE}/10 (CI/CD Pipeline task - aws_services not required)"

**‚ö†Ô∏è REMEMBER**: The SCORE:X line at the end of your PR comment MUST use the SAME score value!

**For Standard IaC Tasks**:

Scan IDEAL_RESPONSE.md and create a JSON array of unique AWS services mentioned. Examples:

- RDS ‚Üí "RDS"
- Amazon S3 ‚Üí "S3"
- AWS Lambda ‚Üí "Lambda"
- DynamoDB ‚Üí "DynamoDB"

```bash
# Create AWS services array from IDEAL_RESPONSE.md
# Extract unique AWS service names and format as JSON array
# Example: AWS_SERVICES_ARRAY='["S3", "Lambda", "DynamoDB", "IAM"]'

# CRITICAL: Must be a valid JSON array (not a string)
AWS_SERVICES_ARRAY='["service1", "service2", "service3"]'  # Replace with actual extracted services

# Update metadata.json with training quality and AWS services
jq --arg tq "$TRAINING_QUALITY" --argjson services "$AWS_SERVICES_ARRAY" \
  '.training_quality = ($tq | tonumber) | .aws_services = $services' \
  metadata.json > metadata.json.tmp && mv metadata.json.tmp metadata.json

# MANDATORY: Verify the update was successful
cat metadata.json | jq '.training_quality, .aws_services'
if [ $? -eq 0 ]; then
  echo "‚úÖ Successfully updated metadata.json with training_quality and aws_services"
else
  echo "‚ùå ERROR: Failed to update metadata.json - quality gate will fail!"
  exit 1
fi
```

**Validation**: Verify `aws_services` is an array:

```bash
jq -e '.aws_services | type == "array"' metadata.json || echo "‚ùå ERROR: aws_services must be an array"
```

Report: "‚úÖ metadata.json enhanced with training_quality: {SCORE}/10 and aws_services array"

**‚ö†Ô∏è REMEMBER**: The SCORE:X line at the end of your PR comment MUST use the SAME score value as training_quality in metadata.json!

#### Step 9: File Location Validation

**CRITICAL CI/CD CHECK**: Verify all files are in allowed locations

```bash
# Check changed files against allowed locations
git diff --name-only origin/main...HEAD

# Verify no violations exist
# See .claude/docs/references/cicd-file-restrictions.md for rules
```

**Common Violations**:

- ‚ùå `README.md` at root ‚Üí Must be `lib/README.md`
- ‚ùå `PROMPT.md` at root ‚Üí Must be `lib/PROMPT.md`
- ‚ùå `IDEAL_RESPONSE.md` at root ‚Üí Must be `lib/IDEAL_RESPONSE.md`
- ‚ùå `MODEL_FAILURES.md` at root ‚Üí Must be `lib/MODEL_FAILURES.md`
- ‚ùå Files in `.github/`, `scripts/`, `docs/`, etc. ‚Üí Not allowed

**If violations found**:

- Training quality penalty: -3 points (Critical issue)
- Report: "‚ùå BLOCKED: Files in wrong locations will FAIL CI/CD"
- List violating files and correct locations
- Do NOT proceed to PR creation

#### Step 10: Final Quality Gate

**CHECKPOINT 3**: Before reporting "Ready" status, verify ALL items:

```
FINAL CHECKLIST (ALL MUST PASS):
‚òê training_quality ‚â• 8 (verified in metadata.json)
‚òê Platform matches metadata.json
‚òê Language matches metadata.json
‚òê PROMPT.md is human-style (no emojis, no AI markers)
‚òê environmentSuffix used in resource names
‚òê All required metadata fields present
‚òê AWS services implemented (for iac-standard tasks)
‚òê No Retain policies
‚òê Tests exist and pass
‚òê All files in allowed locations (Step 9)
‚òê metadata.json updated with training_quality
‚òê PR comment will end with SCORE:X format

STOP AND VERIFY: Review the checklist above. Do NOT proceed if any item fails.

If ALL checked:
- Report: "‚úÖ READY for PR creation"
- Hand off to task-coordinator PHASE 5

If ANY unchecked:
- Report: "‚ùå NOT READY"
- List specific failing items
- Provide fix recommendations
- Do NOT proceed to PR creation
```

**Quality Validation Report Template**:

```markdown
## Code Review Summary

### Validation Results

- ‚úÖ/‚ùå Platform/Language: {PLATFORM}-{LANGUAGE}
- ‚úÖ/‚ùå PROMPT Style: {human/ai-generated}
- ‚úÖ/‚ùå environmentSuffix: {X}%
- ‚úÖ/‚ùå AWS Services: {Y}/{Z} services
- ‚úÖ/‚ùå Training Quality: {SCORE}/10

### Training Quality Assessment

**Final Score: {SCORE}/10**

**Scoring Breakdown** (per training-quality-guide.md):

- Base Score: 8
- MODEL_FAILURES Adjustment: {+X or -Y}
  - Category A fixes (significant): {count} ‚Üí {+1 or +2}
  - Category B fixes (moderate): {count} ‚Üí ¬±0
  - Category C fixes (minor): {count} ‚Üí {-1 or -2 if ‚â•4}
  - Category D (minimal): {yes/no} ‚Üí {-2 to -4}
- Complexity Adjustment: {+X or -Y} (max +2)
  - Multiple services (3+): {yes/no} ‚Üí {+1}
  - Security best practices: {yes/no} ‚Üí {+1}
  - High availability: {yes/no} ‚Üí {+1}
  - Advanced patterns: {yes/no} ‚Üí {+1}
  - Basic/single service: {yes/no} ‚Üí {-1}
- Critical Blockers: {None or BLOCKER_TYPE ‚Üí score set to X}

**Calculation**: {show math: 8 + X + Y = Z, capped at 0-10}

**Justification** (2-3 sentences):
{Explain training value: What did the model learn? What was the gap between MODEL_RESPONSE and IDEAL_RESPONSE? Was model already too competent?}

**Detailed Fixes by Category**:

**Category A (Significant) - Training Value HIGH**:

- {List each with explanation, or "None"}

**Category B (Moderate) - Training Value MEDIUM**:

- {List each with explanation, or "None"}

**Category C (Minor) - Training Value LOW**:

- {List each if relevant, or "None"}

**Category D (Minimal) - Insufficient Training Value**:

- {Explain if MODEL_RESPONSE was 95%+ correct, or "N/A"}

{If < 8: Specific recommendations to reach ‚â•8 per training-quality-guide.md}

### Status: {‚úÖ READY / ‚ùå NOT READY}

{Next steps or blocking issues}

---

SCORE:{REPLACE_WITH_YOUR_CALCULATED_SCORE}
```

**‚ö†Ô∏è IMPORTANT**: In the template above, replace `{REPLACE_WITH_YOUR_CALCULATED_SCORE}` with your actual numeric score (0-10). The `SCORE:X` line MUST be the absolute last line of your comment.

---

## ‚ö†Ô∏è MANDATORY OUTPUT FORMAT ‚ö†Ô∏è

**CRITICAL REQUIREMENT**: Your GitHub review comment MUST end with exactly:

```
SCORE:8
```

(Replace 8 with your actual calculated score from 0-10)

**FINAL CHECKPOINT BEFORE POSTING:**

```
‚òê I have calculated my training quality score (0-10)
‚òê I have updated metadata.json with training_quality = MY_SCORE
‚òê My PR comment ends with SCORE:MY_SCORE
‚òê The SCORE:X value EXACTLY MATCHES training_quality in metadata.json
‚òê There is NO text after the SCORE:X line
```

**‚ö†Ô∏è CRITICAL: SCORE MUST MATCH metadata.json ‚ö†Ô∏è**

The `SCORE:X` in your comment and `training_quality` in metadata.json MUST be the same value!

**Example**: If you calculated score = 8:

1. Update metadata.json: `"training_quality": 8`
2. End comment with: `SCORE:8`

**If they don't match, the CI/CD pipeline may fail or produce inconsistent results.**

**If ANY checkbox is unchecked, FIX IT before posting.**

**Valid Examples:**

```
SCORE:10
SCORE:8
SCORE:6
```

**Invalid Examples (WILL CAUSE BUILD FAILURE):**

```
SCORE: 8      ‚Üê Space after colon - WRONG
Score: 8/10   ‚Üê Wrong format - WRONG
**SCORE:8**   ‚Üê Markdown formatting - WRONG
SCORE:8.5     ‚Üê Decimal - OK but will be truncated
```

**Why This Matters:**

- The CI/CD pipeline extracts this score using strict pattern matching
- Without this exact format, the score defaults to 0 and the build FAILS
- Scores > 10 are rejected as false matches
- This is in addition to updating metadata.json

**Verification:**
Before posting your comment, verify:

1. metadata.json has been updated with training_quality field
2. Your comment ends with `SCORE:X` line where X is 0-10

````

### PHASE 2: Compliance Analysis

**Cost Optimization**: Focus on meaningful differences only.

- Generate compliance report: Requirement | Status (‚úÖ/‚ö†Ô∏è/‚ùå) | Action
- Compare lib/IDEAL_RESPONSE.md with lib/TapStack.\* implementation
  - **Skip detailed comparison if files are identical** (check hashes first: `md5sum`)
  - Only report actual differences
  - **If differences found: suggest updating IDEAL_RESPONSE.md (NOT MODEL_RESPONSE.md)**
- Calculate compliance percentage
- **FOR SCORING ONLY**: Compare lib/IDEAL_RESPONSE.md and latest MODEL_RESPONSE file
  - **PURPOSE**: Understand what the model got wrong and what was fixed (for training quality score)
  - **NOT FOR**: Finding current errors (those are already fixed in IDEAL_RESPONSE.md!)
  - **DO NOT suggest updating MODEL_RESPONSE.md** - it is read-only historical record
  - **Focus on infrastructure differences**: resources, configuration, security, architecture
  - Avoid listing trivial formatting/comment differences
  - Document significant fixes for MODEL_FAILURES analysis and training quality bonus

### PHASE 3: Test Coverage

**CRITICAL REQUIREMENT: 100% Coverage**

**Unit Test Coverage Validation**:

```bash
# Extract coverage metrics
STMT_COV=$(jq -r '.total.statements.pct' coverage/coverage-summary.json)
FUNC_COV=$(jq -r '.total.functions.pct' coverage/coverage-summary.json)
LINE_COV=$(jq -r '.total.lines.pct' coverage/coverage-summary.json)

# Validate 100% requirement
if [ "$STMT_COV" != "100" ] || [ "$FUNC_COV" != "100" ] || [ "$LINE_COV" != "100" ]; then
  echo "‚ùå Coverage below 100%"
  echo "Statements: ${STMT_COV}%"
  echo "Functions: ${FUNC_COV}%"
  echo "Lines: ${LINE_COV}%"
  exit 1
fi
````

**Pass Criteria**:

- Statement coverage: **100%** (not 99%, not 99.9%, exactly 100%)
- Function coverage: **100%**
- Line coverage: **100%**
- All unit tests passing

**If coverage < 100%**:

- BLOCK PR creation
- Report specific coverage gaps
- Training quality penalty: -3 points
- Cannot proceed to PHASE 4

**Integration Test Coverage**:

- Analyze integration test coverage (must use cfn-outputs, no mocks)
- Generate coverage report focusing on gaps: Requirement | Covered? | Test | Notes
  - **Prioritize uncovered resources** - list missing first
  - Briefly summarize what's covered
- Provide Ready/Pending recommendation

**Reference**: `.claude/docs/references/pre-submission-checklist.md` Section 5

### PHASE 4: Final Training Quality Gate

**CRITICAL**: Validate before reporting "Ready"

**Validation**: Run Checkpoint J: Training Quality Threshold

- See `docs/references/validation-checkpoints.md` for threshold check
- Minimum: 8, Target: 9

If training_quality < 8:

- Report: "NOT READY - Training quality below threshold"
- **Apply iteration-policy.md decision logic**:

  ```
  If score 6-7 AND first iteration AND can add significant features:
    - Recommend specific features to add (1-2 AWS services or patterns)
    - Examples: CloudWatch monitoring, KMS encryption, multi-AZ, error handling
    - Expected post-iteration score: ‚â•8
    - Hand off to task-coordinator for iteration

  If score <6 OR already iterated OR only minor fixes possible:
    - Recommend: Mark as ERROR
    - Reason: "Insufficient training value" OR "Max iteration reached" OR "Model already competent"
    - Do NOT iterate
  ```

- Do NOT proceed to PHASE 5 until training_quality ‚â• 8 after any iteration

**Report "Ready" only when**:

- All phases passed
- Training quality ‚â• 8
- All metadata fields validated
- **Unit test coverage = 100%** (statements, functions, lines)
- Integration tests passing
- All files in allowed directories
- Requirements met

**Pre-Submission Validation**:
Before reporting "Ready", run final validation:

```bash
# Recommended: Run pre-submission check script
bash .claude/scripts/pre-submission-check.sh
```

This validates all 6 critical requirements. If any fail, report BLOCKED and list specific issues.

## Focus Areas

- **Best Practices**: Design patterns, naming, configuration
- **Security**: Access control, encryption, secrets management
- **Performance**: Resource sizing, scaling, efficiency
