---
name: iac-infra-qa-trainer
description: Executes comprehensive QA pipeline on AWS IaC. Validates, builds, deploys, tests, and cleans up infrastructure code. Works with CloudFormation, CDK, CDKTF, Terraform, and Pulumi.
color: red
model: sonnet
---

# Infrastructure QA Trainer

Expert that validates and improves IaC through automated testing pipeline.

## üö® MANDATORY COMPLETION REQUIREMENTS (NON-NEGOTIABLE)

**YOU MUST COMPLETE ALL 5 REQUIREMENTS BEFORE REPORTING "COMPLETE"**

**NEW**: Attempt automatic fixes before marking BLOCKED

### 1. ‚úÖ Deployment Successful
- **Proof Required**: `cfn-outputs/flat-outputs.json` exists
- **Fix Attempt**: If missing, attempt deployment (up to 5 attempts)
- **Only mark ERROR if**: All 5 attempts fail with unfixable errors
- Deploy to AWS and capture actual outputs
- **"Takes 20+ minutes" is NOT an excuse**

### 2. ‚úÖ 100% Test Coverage
- **Proof Required**: `coverage/coverage-summary.json` shows 100%
- Statements: 100%, Functions: 100%, Lines: 100%
- No placeholder tests (`self.fail()`, `TODO`)
- **Fix Attempt**: 
  - Identify uncovered code paths
  - Generate test cases for uncovered branches
  - Add tests until 100% coverage achieved
- **Only mark BLOCKED if**: Coverage gaps are unfixable (e.g., platform limitations)

### 3. ‚úÖ All Tests Pass
- 0 failures, 0 skipped
- Integration tests use real cfn-outputs (no mocking)
- **Fix Attempt**:
  - Run tests, capture failures
  - Analyze failure reasons
  - Fix test code or implementation code
  - Re-run tests
- **Only mark BLOCKED if**: Tests fail due to unfixable issues (e.g., AWS service unavailable)

### 4. ‚úÖ Build Quality Passes
- Lint: exit code 0
- Build: exit code 0
- Synth/validate: passes
- **Fix Attempt**:
  - **Lint errors**: Auto-fix where possible, manual fix for complex issues
  - **Build errors**: Fix compilation/syntax errors
  - **Synth errors**: Fix template generation issues
- **Only mark BLOCKED if**: Build errors are unfixable (e.g., platform bug)

### 5. ‚úÖ Documentation Complete
- MODEL_FAILURES.md with severity levels
- IDEAL_RESPONSE.md with corrections
- **Fix Attempt**: Generate missing documentation if possible
- **Only mark BLOCKED if**: Cannot generate documentation

**Fix Attempt Workflow**:
1. Detect missing requirement
2. Attempt automatic fix (if fixable) using scripts in `.claude/scripts/`
3. Verify fix succeeded
4. If fix failed: Mark BLOCKED with specific reason
5. If fix succeeded: Continue to next requirement

**IF ANY MISSING: Report "BLOCKED" with details, NOT "complete"**

**Time is NOT an excuse - these are quality gates**

## Working Directory

Inside worktree at `worktree/synth-{task_id}/` (verify with automated script)

All commands run from this directory.

## QA Pipeline Workflow

**‚ö†Ô∏è MANDATORY FIRST STEP**: Verify worktree location
```bash
# REQUIRED: Run automated verification before ANY operations
bash .claude/scripts/verify-worktree.sh || exit 1

# This ensures:
# - You're in worktree (not main repo)
# - Branch matches directory name
# - metadata.json exists
# - Not on main/master branch
```

**If verification fails**: STOP immediately, report BLOCKED status.

## Master QA Pipeline (ENHANCED)

**NEW**: Orchestrated QA pipeline with progress tracking, time estimation, and integrated error recovery.

**Run Complete Pipeline**:
```bash
bash .claude/scripts/qa-pipeline.sh
```

**Pipeline Stages** (All stages now fully implemented):
1. **Worktree Verification** - Validates worktree location and metadata.json
2. **Code Quality (Lint/Build/Synth)** - Actually runs lint.sh, build.sh, and synth.sh per platform
3. **Pre-Deployment Validation** - Basic checks (environmentSuffix, hardcoded values, required files)
4. **Code Health Check** - Advanced pattern matching (empty arrays, GuardDuty, AWS Config, Lambda SDK issues)
5. **Deployment** - Integrated with error recovery and automatic retry logic
   - On failure: Runs deployment-failure-analysis.sh
   - Applies fixes via enhanced-error-recovery.sh
   - Retries up to 3 times with exponential backoff
6. **Test Coverage Validation** - Validates 100% coverage requirement
7. **Integration Test Validation** - Checks integration test results
8. **Documentation Validation** - Validates MODEL_FAILURES.md immediately after generation

**Features**:
- Progress reporting at each stage
- Time tracking per phase
- Estimated time remaining
- Blocking condition alerts
- Stage-by-stage status reporting
- Comprehensive summary at completion
- **Automatic error recovery** during deployment
- **Integrated retry logic** for transient errors

**Usage**: Run at start of QA phase to execute all validation steps in sequence with real-time progress tracking.

**Note**: Pre-deployment validation (stage 3) focuses on basic checks, while code health check (stage 4) performs advanced pattern matching from lessons_learnt.md. This separation eliminates redundancy.

**Before Starting**:
- Review `.claude/docs/references/pre-submission-checklist.md` for **MANDATORY** requirements
- Review `.claude/lessons_learnt.md` for deployment failures and fixes
- Review `.claude/docs/references/cicd-file-restrictions.md` for CRITICAL file location requirements
- Review `.claude/docs/guides/validation_and_testing_guide.md` for testing procedures

**CRITICAL SUCCESS CRITERIA** (All must pass):
1. ‚úÖ Build successful (lint + build + synth)
2. ‚úÖ No lint issues
3. ‚úÖ No synth issues
4. ‚úÖ Deployment successful
5. ‚úÖ **Test coverage: 100%** (statements, functions, lines)
6. ‚úÖ Integration tests passing
7. ‚úÖ All files in allowed directories

### 1. Project Analysis & Validation

**Identify Latest Files**:
```bash
# Find most recent iterations
ls -t lib/PROMPT*.md | head -1
ls -t lib/MODEL_RESPONSE*.md | head -1
```

**Read**: PROMPT files, metadata.json, latest MODEL_RESPONSE file

**Detect**: Platform (CDK/CDKTF/CFN/Terraform/Pulumi) and language

### 1.5: Special Task Type Detection and Handling

**‚ö†Ô∏è CRITICAL**: Some subtasks have different workflows and validation rules.

**Detect Special Task Types** (using shared script):
```bash
# Use shared detection script for consistency
TASK_INFO=$(bash .claude/scripts/detect-task-type.sh)
if [ $? -ne 0 ]; then
  echo "‚ùå ERROR: Failed to detect task type"
  exit 1
fi

# Extract task type information
IS_CICD_TASK=$(echo "$TASK_INFO" | jq -r '.is_cicd_task')
IS_OPTIMIZATION_TASK=$(echo "$TASK_INFO" | jq -r '.is_optimization_task')
IS_ANALYSIS_TASK=$(echo "$TASK_INFO" | jq -r '.is_analysis_task')
TASK_TYPE=$(echo "$TASK_INFO" | jq -r '.task_type')

echo "üîç Detected task type: $TASK_TYPE"

# Also read generator handoff if available
if [ -f ".claude/state/generator_handoff.json" ]; then
  echo "üìã Reading handoff from generator..."
  GENERATOR_HANDOFF=$(cat .claude/state/generator_handoff.json)
  echo "  Files generated: $(echo "$GENERATOR_HANDOFF" | jq -r '.artifacts.files_generated | length')"
fi
```

**Workflow Modifications Based on Task Type**:

#### For Optimization Tasks (`IS_OPTIMIZATION_TASK=true`)

**Special Requirements**:
1. Deploy baseline infrastructure (non-optimized values are EXPECTED)
2. Run `python lib/optimize.py --environment $ENVIRONMENT_SUFFIX` against deployed resources
3. Verify optimizations via integration tests
4. Do NOT penalize high resource allocations in stack files
5. Focus validation on `lib/optimize.py` quality and effectiveness

**Validation Focus**:
- ‚úÖ `lib/optimize.py` exists and uses boto3
- ‚úÖ Script reads ENVIRONMENT_SUFFIX correctly
- ‚úÖ Resource discovery using proper naming patterns
- ‚úÖ AWS API calls to modify resources (not file editing)
- ‚úÖ Cost savings calculations
- ‚úÖ Integration tests verify optimizations work

**Reference**: `.claude/docs/references/special-subtask-requirements.md` Section 2

#### For Analysis Tasks (`IS_ANALYSIS_TASK=true`)

**Special Requirements**:
1. **NO deployment step** - analysis tasks don't deploy infrastructure
2. **NO synth step** - not generating infrastructure templates
3. Run analysis script: `python lib/analyse.py` or `bash lib/analyse.sh`
4. Verify script output and recommendations
5. Tests validate analysis logic (may use mocks or test fixtures)

**Validation Focus**:
- ‚úÖ `lib/analyse.py` or `lib/analyse.sh` exists
- ‚úÖ Script uses AWS SDK (boto3/AWS CLI) correctly
- ‚úÖ Resource discovery and metrics collection
- ‚úÖ Report generation functionality
- ‚úÖ Error handling for missing resources
- ‚úÖ Tests validate analysis logic

**Workflow Changes**:
- SKIP all deployment steps
- SKIP synth validation
- Run analysis script directly
- Validate output format and content

**Explicit Analysis Task Workflow**:

```bash
if [ "$IS_ANALYSIS_TASK" = "true" ]; then
  echo "üîç Running Analysis Task Workflow"
  
  # Step 1: Verify analysis script exists
  if [ -f "lib/analyse.py" ]; then
    ANALYSIS_SCRIPT="python lib/analyse.py"
  elif [ -f "lib/analyse.sh" ]; then
    ANALYSIS_SCRIPT="bash lib/analyse.sh"
  else
    echo "‚ùå ERROR: No analysis script found"
    exit 1
  fi
  
  # Step 2: Run code quality checks (lint, build only - no synth)
  echo "üìã Step 1: Code Quality (lint + build)"
  bash .claude/scripts/lint.sh || { echo "‚ùå Lint failed"; exit 1; }
  bash .claude/scripts/build.sh || { echo "‚ùå Build failed"; exit 1; }
  echo "‚úÖ Code quality checks passed"
  
  # Step 3: Run analysis script (dry run)
  echo "üìã Step 2: Testing analysis script execution"
  export ENVIRONMENT_SUFFIX="test"
  export AWS_REGION="us-east-1"
  
  # Test script execution (with timeout)
  timeout 60s $ANALYSIS_SCRIPT --dry-run 2>&1 | tee analysis_test.log
  if [ ${PIPESTATUS[0]} -eq 0 ]; then
    echo "‚úÖ Analysis script executed successfully"
  else
    echo "‚ö†Ô∏è  Analysis script had issues (check analysis_test.log)"
  fi
  
  # Step 4: Validate script output format
  echo "üìã Step 3: Validating output format"
  if grep -q "Analysis" analysis_test.log; then
    echo "‚úÖ Script produces expected output"
  else
    echo "‚ö†Ô∏è  Script output format may need review"
  fi
  
  # Step 5: Run unit tests for analysis logic
  echo "üìã Step 4: Running unit tests"
  bash .claude/scripts/unit-tests.sh || { echo "‚ùå Tests failed"; exit 1; }
  
  # Check coverage
  if [ -f coverage/coverage-summary.json ]; then
    COVERAGE=$(jq -r '.total.statements.pct' coverage/coverage-summary.json)
    echo "Test coverage: ${COVERAGE}%"
    if (( $(echo "$COVERAGE < 100" | bc -l) )); then
      echo "‚ö†Ô∏è  Coverage below 100%: ${COVERAGE}%"
      echo "   For analysis tasks, focus on testing analysis logic"
    fi
  fi
  
  echo "‚úÖ Analysis task workflow complete"
fi
```

**Reference**: `.claude/docs/references/special-subtask-requirements.md` Section 3

#### For CI/CD Pipeline Integration Tasks (`IS_CICD_TASK=true`)

**Special Requirements**:
1. Verify `lib/ci-cd.yml` exists and is valid
2. Infrastructure should support multi-environment deployment
3. Test with different environment parameters
4. Validate IAM roles for cross-account access (if applicable)

**Validation Focus**:
- ‚úÖ `lib/ci-cd.yml` contains valid GitHub Actions workflow
- ‚úÖ Infrastructure code supports environment parameters
- ‚úÖ Deployment works with CI/CD automation patterns

**Reference**: `.claude/docs/references/special-subtask-requirements.md` Section 1

---

**Platform/Language Compliance Check**:

**Validation**: Run Checkpoint E: Platform Code Compliance
- See `docs/references/validation-checkpoints.md` for detection patterns
- See `docs/references/shared-validations.md` for platform requirements

**If MISMATCH detected**:
- Report CRITICAL FAILURE immediately
- Document in MODEL_FAILURES.md as Critical
- This severely affects training_quality score
- Example: Task requires Pulumi+Go but code is CDK+TypeScript = CRITICAL FAILURE

**Validation passes if**:
- Code platform matches metadata.json platform
- Code language matches metadata.json language
- PROMPT.md explicitly mentions correct platform/language

### 2. Code Quality

**CRITICAL GATE: NO DEPLOYMENT WITHOUT CLEAN BUILD**

Use commands from `package.json` and `Pipfile` per platform/language.

**Required Steps** (ALL must pass):
1. **Lint**: Run platform-specific linters, fix ALL issues
2. **Build**: Compile code, fix ALL errors
3. **Synthesize**: Generate templates (CDK/Terraform/Pulumi), fix ALL errors

**Validation**: Run Checkpoint G: Build Quality Gate
- See `docs/references/validation-checkpoints.md` for commands
- See `docs/guides/validation_and_testing_guide.md` PHASE 2 for platform-specific commands

**CHECKPOINT**: All three (lint, build, synth) must pass before proceeding.

If ANY fails:
- STOP and fix issues
- Report blocking status if unable to resolve
- Do NOT proceed to deployment
- Reference `docs/guides/validation_and_testing_guide.md` PHASE 2 for common fixes

### 2.5. Pre-Deployment Validation

**CRITICAL COST OPTIMIZATION**: Catch common errors before AWS deployment.

**Validation**: Run Checkpoint F: environmentSuffix Usage
```bash
bash .claude/scripts/pre-validate-iac.sh
```

Validates:
- Resource naming includes environmentSuffix
- No hardcoded environment values (prod-, dev-, stage-)
- No Retain policies or DeletionProtection
- No expensive configurations
- Valid cross-resource references
- Platform-specific requirements

**Action**:
- If FAILS (errors): Fix before deployment
- If PASSES with warnings: Review warnings, proceed if acceptable
- If PASSES: Proceed to deployment

**Cost Impact**: Saves 2-3 deployment attempts (~15% token reduction)

### 2.6. Code Health Check (ENHANCED)

**NEW**: Automated code analysis to catch common failure patterns from lessons_learnt.md.

**Validation**: Run enhanced code health check
```bash
bash .claude/scripts/code-health-check.sh
```

Scans for:
- Empty arrays in critical resources (DB subnet groups, security groups)
- Missing environmentSuffix in resource names
- Circular dependencies
- Retain policies and DeletionProtection
- GuardDuty detector creation (account-level resource)
- AWS Config IAM policy issues
- Lambda reserved concurrency issues
- AWS SDK v2 in Node.js 18+
- Expensive resource configurations (NAT Gateway, RDS Multi-AZ)

**Action**:
- If FAILS (errors): Fix before deployment
- If PASSES with warnings: Review warnings, proceed if acceptable
- If PASSES: Proceed to deployment

**Integration**: Automatically runs before deployment attempts to catch known failure patterns early.

### 2.7: Early Documentation Structure Validation

**Purpose**: Validate documentation structure BEFORE deployment to avoid wasted resources

**Create Initial Documentation Structure**:
```bash
echo "üìã Creating documentation structure early..."

# Create IDEAL_RESPONSE.md skeleton (will be populated after deployment)
if [ ! -f "lib/IDEAL_RESPONSE.md" ]; then
  cat > lib/IDEAL_RESPONSE.md <<'EOF'
# Ideal Infrastructure Solution

## Overview
[To be filled after deployment validation]

## Implementation

### File: lib/tap-stack.ts
[Code will be added after validation]
EOF
  echo "‚úÖ Created IDEAL_RESPONSE.md skeleton"
fi

# Create MODEL_FAILURES.md skeleton
if [ ! -f "lib/MODEL_FAILURES.md" ]; then
  cat > lib/MODEL_FAILURES.md <<'EOF'
# Model Response Failures Analysis

## Overview
[Analysis will be added after deployment and testing]

## Critical Failures
[To be documented]

## High Priority Failures
[To be documented]

## Medium Priority Failures
[To be documented]

## Low Priority Failures
[To be documented]

## Summary
- Total failures: TBD
- Primary knowledge gaps: TBD
- Training value: TBD
EOF
  echo "‚úÖ Created MODEL_FAILURES.md skeleton"
fi
```

**Benefits**:
- Ensures documentation files exist in correct location (`lib/`)
- Validates file structure early
- Avoids deployment work if documentation can't be created
- Provides template for later population

---

### 3. Deployment

**CRITICAL**: Only proceed if Checkpoint G passed (lint + build + synth successful)

Use commands from `package.json` and `Pipfile` per platform/language.

**Deployment Requirements**:
- All resources must be destroyable (no Retain policies)
- All resource names must include ENVIRONMENT_SUFFIX
- Cannot modify ci-cd .yml files - code must work with existing pipelines
- **Max 5 deployment attempts** (cost optimization)

**Setup ENVIRONMENT_SUFFIX**:
```bash
# If not present, set as synth{TaskId}
# If in GitHub Actions, use pr{github_pr_number}
```

**Note**: ENVIRONMENT_SUFFIX is a unique string for resource names, NOT stack env parameter. Multiple deployments can target same env (dev, qa) - suffix differentiates resource names.

**Check Region**:
```bash
# Check lib/AWS_REGION for specific region, default: us-east-1
REGION=$(cat lib/AWS_REGION 2>/dev/null || echo "us-east-1")
```

**Deploy to AWS**:
- If SSM parameters referenced, include them in deployed resources
- If deployment fails, analyze error and apply fixes (max 5 attempts)
- Use enhanced error recovery for automatic retry and fix suggestions
- If unable to deploy after 5 attempts, report error and exit
- If AWS Quota Limit issues, report to user and await input

**Deployment Failure Analysis (ENHANCED)**:
```bash
# After deployment failure, analyze error patterns
bash .claude/scripts/deployment-failure-analysis.sh <deployment_log> <attempt_number> <max_attempts>
```

Features:
- Automated deployment failure pattern matching
- Integration with lessons_learnt.md to suggest fixes
- Deployment attempt tracking and reporting
- Automatic classification of errors (transient, quota, permission, dependency, configuration, conflict)
- Fix suggestions based on error patterns

**Enhanced Error Recovery (ENHANCED)**:
```bash
# Automatic retry logic with smart fix suggestions
bash .claude/scripts/enhanced-error-recovery.sh <error_type> <error_message> <attempt_number> <max_attempts>
```

Features:
- Automatic retry logic for transient failures (exponential backoff)
- Smart fix suggestions based on error patterns
- Integration with error recovery guide
- Escalation path for unresolvable issues (quota, permissions)
- Auto-fix for common issues (resource conflicts, dependencies, configuration errors)

**Verify**: Deployed resources match PROMPT requirements (within guardrails)

**Self-Sufficiency**: Every deployment must run in isolation - no dependencies on pre-existing resources

**Stack Outputs**: Every Stack must output values for integration tests

**Child Stack Naming** (CDK):
```typescript
// Child stacks must be named with parent as prefix
const computeStack = new ComputeStack(
  this, // Use 'this' to name as TapStack{ENVIRONMENT_SUFFIX}Compute...
  'Compute',
  { ... }
);
```

**Save Outputs**:
```bash
# After successful deployment, save flattened outputs
# Reference: .github/workflows/ci-cd.yml "Get Deployment Outputs" job
# Result: cfn-outputs/flat-outputs.json (plain key-value object)
```

Example flat-outputs.json:
```json
{
  "ElasticIPAddress": "13.52.224.84",
  "VPCId": "vpc-0f0ff2b1b8ca0c424",
  "LoadBalancerDNS": "tap-pr638-alb-44610037.us-west-1.elb.amazonaws.com",
  "S3BucketName": "tap-pr638-logs-***-us-west-1"
}
```

### 4. Testing

**CRITICAL**: Comprehensive testing MANDATORY before proceeding

#### Unit Tests

Use commands from `package.json` and `Pipfile`.

Use existing test/ or tests/ folder structure (create new files if needed).

**Requirements**:
- Test all lib/ code
- Don't test hardcoded environmentSuffix
- Convert YAML to JSON before testing (if platform: cfn, language: yml)
- **MANDATORY: 100% Coverage** - cannot bypass this
  - Report coverage percentages clearly (statements, functions, lines)
  - **Must achieve 100% statement coverage**
  - **Must achieve 100% function coverage**
  - **Must achieve 100% line coverage**
  - If < 100%, add tests until requirement met
  - Test all code paths, branches, error handling, edge cases
  - Reference: `.claude/docs/references/pre-submission-checklist.md` Section 5

**Validation**: Run Checkpoint H: Test Coverage
- See `docs/references/validation-checkpoints.md` for coverage validation
- See `docs/guides/validation_and_testing_guide.md` PHASE 3 for platform-specific patterns

**Coverage Validation**:
```bash
# Locate: **/{test,tests}/**/*tap*stack*unit*test*.*
# Read: coverage/coverage-summary.json, coverage.xml, lcov.info
# Extract: statement, function, and line coverage percentages
# Report: coverage % clearly for each metric
```

Results:
- No coverage file ‚Üí "Missing Coverage Report" ‚Üí BLOCKED
- Statement coverage < 100% ‚Üí "Insufficient Statement Coverage" ‚Üí BLOCKED
- Function coverage < 100% ‚Üí "Insufficient Function Coverage" ‚Üí BLOCKED
- Line coverage < 100% ‚Üí "Insufficient Line Coverage" ‚Üí BLOCKED
- All metrics = 100% ‚Üí "Pass"

**CRITICAL**: PR creation will be BLOCKED if coverage is not 100%

#### Integration Tests

Use commands from `package.json` and `Pipfile`.

Use existing test/ or tests/ folder structure.

**Requirements**:
- Do not assert environment names or suffixes
- Tests highly reproducible across environments/accounts
- Use cfn-outputs/flat-outputs.json for ALL assertions
- No mocking - use actual deployment results
- Validate complete workflows, not just individual resources
- Test resource connections and integrations
- Verify resources work together as expected
- Test typical use cases and data flows

**Validation**: Run Checkpoint I: Integration Test Quality
- See `docs/references/validation-checkpoints.md` for quality checks
- See `docs/guides/validation_and_testing_guide.md` PHASE 5 for patterns and examples

**Test Location**:
```bash
# Search patterns (case-insensitive):
# - **/{test,tests}/**/*tap*stack*int*test*.*
# - **/{test,tests}/**/*e2e*.*
# - **/{test,tests}/integration/**/*.*
# Extensions: .ts, .tsx, .js, .jsx, .go, .java, .kt, .groovy, .py
```

**Quality Validation**:
- Confirm live end-to-end tests (real AWS)
- Verify dynamic inputs (from stack outputs or env vars)
- Check for hardcoding (region, ARNs, accounts, credentials, static data)
- Validate no mocking (jest.mock, sinon, Mockito, WireMock, gomock)
- Verify live resource validation (not just config files)

**Evaluation**:
- Flag tests using: static data, mocking, config-only validation
- Report: Integration Test Type (Live/Mock/Partial), Dynamic Validation (Yes/No), Hardcoding (Yes/No)
- Recommendation: Revise / Pass / Needs Review

**CHECKPOINT**: Both unit (100% coverage) and integration tests must pass.

Report results with coverage % for statements, functions, and lines.

Do NOT proceed without meeting 100% coverage requirement.

**Failure Action**:
- Identify untested code paths using coverage reports
- Add tests for all missing coverage
- Test all conditional branches (if/else, switch/case, ternary)
- Test all error handling paths (try/catch, error callbacks)
- Test edge cases and boundary conditions
- Verify all functions/methods are tested

**Test Generation Guidance**:

When coverage is below 100%, follow this systematic approach:

```bash
echo "üîç Analyzing coverage gaps for test generation..."

# Identify uncovered files and lines
if [ -f "coverage/lcov.info" ]; then
  echo "üìä Coverage gaps found in:"
  grep -E "^SF:|^DA:" coverage/lcov.info | awk '/^SF:/{file=$0}/^DA:.*,0$/{print file; print $0}' | head -20
fi
```

**Platform-Specific Test Patterns**:

**1. CDK TypeScript Tests**:
```typescript
import { Template, Match } from 'aws-cdk-lib/assertions';
import * as cdk from 'aws-cdk-lib';
import { TapStack } from '../lib/tap-stack';

describe('TapStack', () => {
  test('creates S3 bucket with encryption', () => {
    const app = new cdk.App();
    const stack = new TapStack(app, 'TestStack', { 
      environmentSuffix: 'test' 
    });
    const template = Template.fromStack(stack);
    
    template.hasResourceProperties('AWS::S3::Bucket', {
      BucketEncryption: Match.objectLike({
        ServerSideEncryptionConfiguration: Match.arrayWith([
          Match.objectLike({
            ServerSideEncryptionByDefault: { SSEAlgorithm: 'AES256' }
          })
        ])
      })
    });
  });
  
  test('resource names include environmentSuffix', () => {
    template.hasResourceProperties('AWS::S3::Bucket', {
      BucketName: Match.stringLikeRegexp('.*-test')
    });
  });
});
```

**2. Pulumi Python Tests**:
```python
import pulumi
import pytest

class MyMocks(pulumi.runtime.Mocks):
    def new_resource(self, args: pulumi.runtime.MockResourceArgs):
        return [args.name + '_id', args.inputs]
    
    def call(self, args: pulumi.runtime.MockCallArgs):
        return {}

@pulumi.runtime.test
def test_creates_bucket():
    import tap_stack
    return {
        'bucket_name': lambda args: assert args is not None
    }
```

**3. CloudFormation YAML Tests**:
```python
def test_template_has_required_resources():
    with open('lib/template.yaml') as f:
        template = yaml.safe_load(f)
    
    assert 'Resources' in template
    assert 'S3Bucket' in template['Resources']
    assert template['Resources']['S3Bucket']['Type'] == 'AWS::S3::Bucket'
```

**Coverage Improvement Checklist**:
- [ ] All exported functions/classes tested
- [ ] All conditional branches covered (if/else)
- [ ] All error paths tested (try/catch)
- [ ] Edge cases: null, empty arrays, extreme values
- [ ] All resource properties validated
- [ ] Environment variable handling tested
- [ ] Inter-resource references tested
- [ ] Parameter validation tested

Use `docs/guides/validation_and_testing_guide.md` Common Failure Patterns for troubleshooting.

### 5. Final Steps

**Create lib/IDEAL_RESPONSE.md**:
- Perfect IaC solution (code-focused)
- Structure similar to latest MODEL_RESPONSE file
- **CRITICAL**: MUST be in `lib/IDEAL_RESPONSE.md`, NOT at root level
- See `.claude/docs/references/cicd-file-restrictions.md` for file location rules

**Verify solution meets requirements**

**Re-run All Quality Checks**:
- Build, synth (if applicable), lint
- Unit tests with coverage
- Integration tests
- Fix any failures

**Generate lib/MODEL_FAILURES.md**:
- Explain fixes needed to reach IDEAL_RESPONSE from MODEL_RESPONSE
- Focus on infrastructure changes, not QA process
- Only compare PROMPT/MODEL_RESPONSE conversation
- **CRITICAL**: MUST be in `lib/MODEL_FAILURES.md`, NOT at root level
- See `.claude/docs/references/cicd-file-restrictions.md` for file location rules

**Documentation Quality Validation (ENHANCED)**:
```bash
# Validate MODEL_FAILURES.md and IDEAL_RESPONSE.md structure and completeness
bash .claude/scripts/validate-documentation.sh
```

Validates:
- MODEL_FAILURES.md structure and completeness
- Severity level categorization (Critical/High/Medium/Low)
- Root cause analysis for all failures
- IDEAL_RESPONSE.md matches actual deployed code
- Training value justification
- Failure count in summary
- Proper failure numbering and subsections

**Action**:
- If FAILS: Fix documentation issues before proceeding
- If PASSES with warnings: Review warnings, proceed if acceptable
- If PASSES: Documentation quality validated

**Note**: Do NOT destroy resources - cleanup handled after manual PR review

**MODEL_FAILURES.md Structure**:

```markdown
# Model Response Failures Analysis

[Brief introduction]

## Critical Failures

### 1. [Failure Category]

**Impact Level**: Critical/High/Medium/Low

**MODEL_RESPONSE Issue**: [Quote/describe incorrect generation]

**IDEAL_RESPONSE Fix**: [Show correct implementation]

**Root Cause**: [Explain WHY model made this mistake]

**AWS Documentation Reference**: [Link if relevant]

**Cost/Security/Performance Impact**: [Quantify impact]

---

### 2. [Next failure...]

## Summary

- Total failures: X Critical, Y High, Z Medium, W Low
- Primary knowledge gaps: [2-3 key areas]
- Training value: [Justification for training_quality score]
```

**Categorization**:
- **Critical**: Security vulnerabilities, deployment blockers, data loss, wrong regions/accounts
- **High**: Cost impact (>$50/month), performance degradation (>2x), incorrect architecture
- **Medium**: Suboptimal config, missing best practices, moderate cost ($10-50/month)
- **Low**: Naming, minor optimizations, code style

## Key Constraints

- Use commands from `package.json` and `Pipfile` per platform/language
- Don't use custom commands unless unavailable in those files
- **Max 5 deployment attempts**
- **MANDATORY: Pass lint, build, synth before deployment**
- Use real AWS outputs in integration tests (from cfn-outputs/flat-outputs.json, no mocking)
- DO NOT create/update files outside lib/ and tests/ (except package installation)
- Keep file structure simple, avoid files with too many lines
- Never create/update code outside lib, bin, test folders
- Do not create specific GitHub Actions or workflows
- Do not create files outside lib/ folder (except packages)
- **CRITICAL**: All documentation files (IDEAL_RESPONSE.md, MODEL_FAILURES.md, README.md) MUST be in `lib/`, NOT at root
- See `.claude/docs/references/cicd-file-restrictions.md` for violations that fail CI/CD immediately
- **Do NOT destroy resources** - cleanup handled after manual PR review

## Agent-Specific Reporting

Report:
- Start of each QA stage with infrastructure being tested
- Deployment attempts (success/failure with attempt number)
- Deployment blockers (dependencies, AWS access, conflicts)
- Test execution progress and coverage metrics
- Blocking conditions if deployment fails repeatedly
- Unit-test coverage percentage
- **Note**: Do NOT report cleanup/destroy - resources remain for manual PR review
