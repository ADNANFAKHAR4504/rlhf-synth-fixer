We need to design a production-grade Azure DevOps CI/CD pipeline (azure-pipelines.yml) that deploys a globally distributed IoT edge computing platform across multiple Azure regions. The system relies on IoT Hub for device management, Azure Functions for event and telemetry processing, Cosmos DB for storage, and AKS for analytics workloads. The pipeline must use Managed Identity for authentication—no service principals—and fully automate validation, build, test, security, deployment, monitoring, and rollback stages for all environments: dev, staging, and production.

The pipeline should begin with a Validation stage that performs thorough code and configuration checks. Jobs should include lintCode to run ESLint and the TypeScript compiler on Azure Functions, terraform validate for infrastructure definitions, hadolint for Dockerfiles, and shellcheck for bash scripts. A separate checkDependencies job should execute npm audit and Snyk scans for both Node.js Functions and C# IoT Edge modules to detect vulnerable dependencies. Another job, validateIoTConfig, must verify IoT Hub device provisioning templates, device twin schemas, and layered edge deployment manifests. All these jobs should run on the ubuntu-latest image.

After successful validation, the Build stage begins, depending on Validation. This includes buildFunctions, which compiles TypeScript Functions for telemetry ingestion, device commands, and data aggregation, caches npm packages, and publishes functions.zip as a pipeline artifact. The buildEdgeModules job should build three IoT Edge modules—SensorProcessor, DataFilter, and LocalAnalytics—using Docker tasks, pushing them to $(acrName).azurecr.io/iot-edge/module:$(Build.BuildId). The buildAnalyticsContainers job should containerize TimeSeriesAnalyzer, AnomalyDetector, and PredictiveModel workloads for AKS and push them to the same ACR. Infrastructure preparation should run via Terraform with a terraform plan job, using TerraformInstaller@0 (v1.6.0) and TerraformTaskV4 commands (init, plan), producing and publishing the plan artifact.

The Test stage, which depends on Build, should cover unit, integration, edge runtime, and performance testing. unitTests must run Jest for Functions and MSTest for C# modules, publishing results in JUnit format. integrationTests will deploy a test IoT Hub and Functions via ARM templates, validating end-to-end device messaging and twin updates. The edgeRuntimeTests job should use the iotedgedev tool to simulate edge modules locally, testing inter-module communication and offline resilience. The performanceTests job should simulate 100k devices, measuring throughput, latency, and scaling efficiency, running on a Standard_D4s_v3 agent pool.

A Security stage follows, depending on Build, focusing on vulnerability management and compliance. The containerScan job must use Trivy for container vulnerability scanning (failing on HIGH severity) and Grype to produce SBOMs. The sastScan job should run PSScriptAnalyzer on PowerShell scripts, Semgrep on TypeScript code, and Checkov on Terraform files. A secretScan job using TruffleHog must fail the pipeline on any secret detection. Finally, complianceScan should ensure Azure Policy adherence, validating NSG rules and encryption configurations.

Upon passing security and testing, the pipeline proceeds to the DeployDev stage, using Managed Identity for all AzureCLI@2 tasks. The deployInfra job applies Terraform to create IoT Hub, Event Hubs, Cosmos DB, and Storage Accounts for the dev environment. The deployFunctions job deploys the compiled function package to Azure Functions. deployEdge must push layered IoT Edge deployments targeting dev-\* devices, and deployAKS should apply AKS manifests for analytics workloads.

The next stage, IntegrationTest, should simulate real device scenarios using 1000 test devices to validate telemetry ingestion, edge offline synchronization, and analytics data flow. Once validated, a CanaryDeploy stage begins, deploying to the staging subscription. It must use Azure Traffic Manager for traffic splitting (90/10 between stable/canary) and Flagger for progressive delivery on AKS. The monitorCanary job should run for 30 minutes, monitoring error rates, latency, and IoT Hub throttling, triggering an automatic rollback if error rate exceeds 1%.

After successful canary evaluation, the StagingDeploy stage promotes traffic to 100% canary, finalizes AKS updates, and runs extensive staging E2E tests with 10k devices, including failover and disaster recovery validation, on a Standard_D8s_v3 pool. Before production rollout, a ProductionApproval stage requires explicit approvals: one from the security team and one from the IoT operations team, each with a 72-hour timeout.

The DeployProduction stage deploys to six Azure regions (eastus, westus, northeurope, southeastasia, australiaeast, brazilsouth) sequentially using Terraform workspaces per region. Each region must deploy IoT Hubs, Event Hubs, AKS clusters, and Cosmos DB configured for multi-region writes, with Traffic Manager handling global routing. The stage should use a runOnce deployment strategy with preDeploy, deploy, routeTraffic, and postRouteTraffic hooks, retrying failed tasks twice.

Following production rollout, a Monitoring stage must configure Application Insights, Log Analytics, and Azure Monitor alerts for telemetry throughput, device connectivity, Function errors, and AKS health. Dashboards should visualize device fleet health, and notifications should be sent to Microsoft Teams and PagerDuty through webhooks.

A final Rollback stage must allow manual triggering to revert the production environment to a previous version. This includes rolling back IoT Hub edge deployments, Functions, and Traffic Manager configurations using provided scripts like rollback-iot.sh, along with clearing relevant caches.

The pipeline must enforce policies such as container scan failures blocking deployment, mandatory secret scanning, managed identity authentication only, and externalizing any scripts over five lines (stored in /scripts). It should trigger automatically on main, develop, and feature/\* branches, with pull requests triggering validation-only runs and scheduled nightly full executions.

All variables such as azureSubscription, acrName, aksCluster, iotHubName, devAksCluster, stagingAksCluster, and prodAksRegions should be configurable at runtime. Artifact management, coverage reporting, and test publishing must follow Azure DevOps best practices.

Please generate a unified multi-stage Azure DevOps pipeline YAML (ci-cd.yml) that implements the complete flow described above, adhering strictly to the given structure, Managed Identity constraint, security enforcement, and deployment sequencing across environments and regions.
