# CircleCI Configuration for Multi-tenant B2B SaaS Platform
# Version: 2.1
# Authentication: AWS OIDC (no static keys)
# Platform: EKS, RDS Aurora, ElastiCache, Cognito

# ==================== OIDC CONTEXT REQUIREMENTS ====================
#
# This configuration requires three CircleCI Contexts for environment isolation.
# Each context must be configured in CircleCI Project Settings > Contexts.
#
# IMPORTANT: All AWS authentication uses OIDC (OpenID Connect) - NO static access keys!
# Configure OIDC trust relationship in IAM for each role before use.
#
# Required CircleCI Contexts and Environment Variables:
#
# Context: aws-dev (Development Environment)
# -----------------------------------------------
#   - OIDC_ROLE_ARN_DEV: arn:aws:iam::123456789012:role/circleci-dev-role
#     (IAM role for dev environment with OIDC trust policy)
#   - AWS_DEFAULT_REGION: us-east-1
#     (Primary AWS region for all deployments)
#   - CLUSTER_NAME: saas-platform
#     (EKS cluster name prefix; full name: saas-platform-dev)
#   - ECR_REGISTRY: 123456789012.dkr.ecr.us-east-1.amazonaws.com
#     (ECR registry URL for container image storage)
#   - PRIVATE_REGISTRY: registry.company.com
#     (Private Docker registry URL for base images)
#   - TENANT_ID: default-tenant
#     (Default tenant identifier for dev deployments)
#   - HOSTED_ZONE_ID: Z1234567890ABC
#     (Route53 hosted zone ID for DNS management)
#   - DATADOG_API_KEY: <sensitive-key>
#     (Datadog API key for monitoring integration)
#   - SENTRY_DSN: https://<key>@sentry.io/<project>
#     (Sentry DSN for error tracking and alerting)
#
# Context: aws-staging (Staging Environment)
# -----------------------------------------------
#   - OIDC_ROLE_ARN_STAGING: arn:aws:iam::123456789012:role/circleci-staging-role
#   - AWS_DEFAULT_REGION: us-east-1
#   - CLUSTER_NAME: saas-platform
#   - ECR_REGISTRY: 123456789012.dkr.ecr.us-east-1.amazonaws.com
#   - PRIVATE_REGISTRY: registry.company.com
#   - TENANT_ID: staging-tenant
#   - HOSTED_ZONE_ID: Z1234567890ABC
#   - DATADOG_API_KEY: <sensitive-key>
#   - SENTRY_DSN: https://<key>@sentry.io/<project>
#
# Context: aws-prod (Production Environment)
# -----------------------------------------------
#   - OIDC_ROLE_ARN_PROD: arn:aws:iam::987654321098:role/circleci-prod-role
#     (Production may use separate AWS account for security isolation)
#   - AWS_DEFAULT_REGION: us-east-1
#   - CLUSTER_NAME: saas-platform
#   - ECR_REGISTRY: 987654321098.dkr.ecr.us-east-1.amazonaws.com
#   - PRIVATE_REGISTRY: registry.company.com
#   - TENANT_ID: prod-tenant
#   - HOSTED_ZONE_ID: Z9876543210XYZ
#   - DATADOG_API_KEY: <sensitive-key>
#   - SENTRY_DSN: https://<key>@sentry.io/<project>
#
# IAM Role Trust Policy Example (for OIDC):
# {
#   "Version": "2012-10-17",
#   "Statement": [{
#     "Effect": "Allow",
#     "Principal": {
#       "Federated": "arn:aws:iam::ACCOUNT_ID:oidc-provider/oidc.circleci.com/org/ORG_ID"
#     },
#     "Action": "sts:AssumeRoleWithWebIdentity",
#     "Condition": {
#       "StringLike": {
#         "oidc.circleci.com/org/ORG_ID:sub": "org/ORG_ID/project/PROJECT_ID/user/*"
#       }
#     }
#   }]
# }
#

version: 2.1

# ==================== ORBS ====================
orbs:
  aws-cli: circleci/aws-cli@4.1
  kubernetes: circleci/kubernetes@1.3
  terraform: circleci/terraform@3.2
  snyk: snyk/snyk@2.0
  node: circleci/node@5.1
  docker: circleci/docker@2.4

# ==================== EXECUTORS ====================
executors:
  # Node.js executor for application builds
  node-executor:
    docker:
      - image: ${PRIVATE_REGISTRY}/node:20-alpine
    resource_class: medium
    working_directory: ~/project

  # Docker executor for container builds
  docker-executor:
    machine:
      image: ubuntu-2204:current
    resource_class: medium
    working_directory: ~/project

  # Terraform executor for infrastructure
  terraform-executor:
    docker:
      - image: ${PRIVATE_REGISTRY}/hashicorp/terraform:1.6
    resource_class: medium
    working_directory: ~/project

# ==================== COMMANDS ====================
commands:
  # OIDC authentication wrapper for AWS CLI
  assume-aws-role:
    description: "Assume AWS role using OIDC authentication"
    parameters:
      role-arn:
        type: string
        description: "AWS IAM Role ARN from context"
    steps:
      - aws-cli/setup:
          role-arn: << parameters.role-arn >>
          role-session-name: circleci-${CIRCLE_BUILD_NUM}
          aws-region: ${AWS_DEFAULT_REGION}
          profile-name: deployment

  # Install kubectl and Helm
  install-kubectl:
    description: "Install kubectl v1.28 and Helm v3.13"
    steps:
      - run:
          name: Install kubectl
          command: |
            curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
            chmod +x kubectl
            sudo mv kubectl /usr/local/bin/
            kubectl version --client
      - run:
          name: Install Helm
          command: |
            curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
            chmod 700 get_helm.sh
            ./get_helm.sh --version v3.13.0
            rm get_helm.sh
            helm version

  # Setup tenant-specific Kubernetes context
  setup-tenant-context:
    description: "Configure kubeconfig and namespace for tenant isolation"
    parameters:
      tenant-id:
        type: string
        description: "Tenant identifier"
      environment:
        type: string
        description: "Deployment environment"
      role-arn:
        type: string
        description: "AWS IAM Role ARN for EKS authentication"
    steps:
      - run:
          name: Configure tenant context
          command: |
            aws eks update-kubeconfig \
              --name ${CLUSTER_NAME}-<< parameters.environment >> \
              --region ${AWS_DEFAULT_REGION} \
              --role-arn << parameters.role-arn >>
            kubectl config set-context --current \
              --namespace=tenant-<< parameters.tenant-id >>-<< parameters.environment >>

# ==================== JOBS ====================
jobs:
  # -------------------- VALIDATION JOBS --------------------
  code-validation:
    executor: node-executor
    steps:
      - checkout
      - node/install-packages:
          pkg-manager: npm
      - run:
          name: Lint code
          command: npm run lint
      - run:
          name: TypeScript check
          command: npm run type-check
      - run:
          name: Prettier check
          command: npm run prettier:check
      - store_test_results:
          path: reports/validation
      - store_artifacts:
          path: reports/validation
          # Note: Artifact retention is configured at CircleCI organization/project level, not in YAML

  infrastructure-validation:
    executor: terraform-executor
    steps:
      - checkout
      - assume-aws-role:
          role-arn: ${OIDC_ROLE_ARN_DEV}
      - run:
          name: Terraform format check
          command: |
            cd infrastructure
            terraform fmt -check -recursive
      - run:
          name: Terraform validate
          command: |
            cd infrastructure
            terraform init -backend=false
            terraform validate -json > ../reports/terraform-validate.json
            terraform validate 2>&1 | tee ../reports/terraform-validate.log
      - run:
          name: TFLint
          command: |
            curl -L "https://github.com/terraform-linters/tflint/releases/latest/download/tflint_linux_amd64.zip" \
              > tflint.zip
            unzip tflint.zip
            ./tflint --recursive --format=json > ../reports/tflint-report.json
            ./tflint --recursive 2>&1 | tee ../reports/tflint.log
      - run:
          name: TFSec security scan
          command: |
            docker run --rm -v "$(pwd)":/src aquasec/tfsec:latest /src --format=json > reports/tfsec-report.json
            docker run --rm -v "$(pwd)":/src aquasec/tfsec:latest /src 2>&1 | tee reports/tfsec.log
      - run:
          name: Checkov scan
          command: |
            pip install checkov
            checkov -d infrastructure --output-file reports/checkov-report.json
      - store_artifacts:
          path: reports
          # Note: Artifact retention is configured at CircleCI organization/project level, not in YAML

  # -------------------- SECURITY SCANNING --------------------
  dependency-scan:
    executor: node-executor
    steps:
      - checkout
      - node/install-packages
      - snyk/scan:
          severity-threshold: high
          fail-on-issues: true
          monitor-on-build: true
      - run:
          name: NPM audit
          command: |
            npm audit --audit-level=high
            npm audit --json > reports/npm-audit.json || true
      - store_artifacts:
          path: reports/npm-audit.json
          # Note: Artifact retention is configured at CircleCI organization/project level, not in YAML

  # -------------------- BUILD JOBS --------------------
  build-services:
    executor: docker-executor
    parameters:
      service-name:
        type: string
    steps:
      - checkout
      - assume-aws-role:
          role-arn: ${OIDC_ROLE_ARN_DEV}
      - run:
          name: Login to ECR
          command: |
            aws ecr get-login-password --region ${AWS_DEFAULT_REGION} | \
              docker login --username AWS --password-stdin ${ECR_REGISTRY}
      - run:
          name: Build multi-arch Docker image
          command: |
            docker buildx create --use
            docker buildx build \
              --platform linux/amd64,linux/arm64 \
              --cache-from type=registry,ref=${ECR_REGISTRY}/<< parameters.service-name >>:cache \
              --cache-to type=registry,ref=${ECR_REGISTRY}/<< parameters.service-name >>:cache \
              -t ${ECR_REGISTRY}/<< parameters.service-name >>:${CIRCLE_SHA1} \
              -t ${ECR_REGISTRY}/<< parameters.service-name >>:latest \
              --push \
              ./services/<< parameters.service-name >>
      - run:
          name: Trivy security scan
          command: |
            docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
              -v $(pwd)/reports:/reports \
              aquasec/trivy:latest image \
              --format json \
              --output /reports/trivy-<< parameters.service-name >>.json \
              ${ECR_REGISTRY}/<< parameters.service-name >>:${CIRCLE_SHA1}
      - store_artifacts:
          path: reports
          # Note: Artifact retention is configured at CircleCI organization/project level, not in YAML

  # -------------------- INFRASTRUCTURE DEPLOYMENT --------------------
  terraform-deploy:
    executor: terraform-executor
    parameters:
      environment:
        type: string
    steps:
      - checkout
      # Environment-specific OIDC role ARN selection for secure authentication
      # Each environment uses a dedicated IAM role with principle of least privilege
      - when:
          condition:
            equal: [dev, << parameters.environment >>]
          steps:
            - assume-aws-role:
                role-arn: ${OIDC_ROLE_ARN_DEV}
      - when:
          condition:
            equal: [staging, << parameters.environment >>]
          steps:
            - assume-aws-role:
                role-arn: ${OIDC_ROLE_ARN_STAGING}
      - when:
          condition:
            equal: [production, << parameters.environment >>]
          steps:
            - assume-aws-role:
                role-arn: ${OIDC_ROLE_ARN_PROD}
      - terraform/init:
          path: infrastructure
          backend_config_file: backend-<< parameters.environment >>.conf
      - run:
          name: Select workspace
          command: |
            cd infrastructure
            terraform workspace select << parameters.environment >> || \
              terraform workspace new << parameters.environment >>
      - terraform/plan:
          path: infrastructure
          var_file: environments/<< parameters.environment >>.tfvars
          out: tfplan-<< parameters.environment >>
      - terraform/apply:
          path: infrastructure
          plan: tfplan-<< parameters.environment >>
      - store_artifacts:
          path: infrastructure/tfplan-<< parameters.environment >>
          # Note: Artifact retention is configured at CircleCI organization/project level, not in YAML

  # -------------------- TESTING JOBS --------------------
  unit-tests:
    executor: node-executor
    parallelism: 4
    steps:
      - checkout
      - node/install-packages
      - run:
          name: Run unit tests
          command: |
            TESTFILES=$(circleci tests glob "src/**/*.test.ts" | circleci tests split --split-by=timings)
            npm run test:unit -- --coverage --ci --reporters=jest-junit ${TESTFILES}
          environment:
            JEST_JUNIT_OUTPUT_DIR: reports/junit
      - store_test_results:
          path: reports/junit
      - store_artifacts:
          path: coverage
          # Note: Artifact retention is configured at CircleCI organization/project level, not in YAML

  integration-tests:
    executor: docker-executor
    steps:
      - checkout
      - run:
          name: Setup test environment
          command: |
            docker-compose -f docker-compose.test.yml up -d
            ./scripts/wait-for-services.sh
      - run:
          name: Setup database and cache proxies
          command: |
            # Start database proxy for tenant isolation testing
            ./scripts/start-db-proxy.sh
            # Start cache proxy for ElastiCache testing
            ./scripts/start-cache-proxy.sh
      - run:
          name: Setup local Cognito
          command: |
            # Start local Cognito mock for authentication testing
            ./scripts/start-local-cognito.sh
      - run:
          name: Run integration tests
          command: |
            docker-compose -f docker-compose.test.yml \
              run --rm test-runner npm run test:integration
      - run:
          name: Rollback on integration test failure
          command: ./scripts/rollback-integration-failure.sh
          when: on_fail
      - run:
          name: Pact contract tests
          command: |
            docker-compose -f docker-compose.test.yml \
              run --rm test-runner npm run test:contract
      - store_test_results:
          path: reports/integration
      - store_artifacts:
          path: reports
          # Note: Artifact retention is configured at CircleCI organization/project level, not in YAML

  e2e-tests:
    executor: docker-executor
    parallelism: 3
    steps:
      - checkout
      - run:
          name: Run E2E tests
          command: |
            SPECS=$(circleci tests glob "e2e/**/*.spec.ts" | circleci tests split)
            docker run --rm \
              -v $(pwd):/app \
              -e SPECS="${SPECS}" \
              ${PRIVATE_REGISTRY}/e2e-runner:latest
      - run:
          name: Rollback on E2E test failure
          command: ./scripts/rollback-e2e-failure.sh
          when: on_fail
      - store_artifacts:
          path: reports
          # Note: Artifact retention is configured at CircleCI organization/project level, not in YAML

  # -------------------- SECURITY VALIDATION --------------------
  security-scan:
    executor: docker-executor
    steps:
      - checkout
      - run:
          name: Semgrep scan
          command: |
            docker run --rm -v $(pwd):/src \
              returntocorp/semgrep:latest \
              --config=auto --json -o reports/semgrep.json /src
      - run:
          name: Trufflehog secrets scan
          command: |
            docker run --rm -v $(pwd):/src \
              trufflesecurity/trufflehog:latest \
              filesystem /src --json > reports/trufflehog.json
      - run:
          name: Grype vulnerability scan
          command: |
            curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh -s -- -b /tmp/
            /tmp/grype dir:. -o json > reports/grype.json
      - run:
          name: Checkov K8s manifests scan
          command: |
            pip install checkov
            checkov -d k8s-manifests --framework kubernetes -o json > reports/checkov-k8s.json
      - store_artifacts:
          path: reports
          # Note: Artifact retention is configured at CircleCI organization/project level, not in YAML

  # -------------------- PERFORMANCE TESTING --------------------
  performance-tests:
    executor: docker-executor
    parallelism: 2
    steps:
      - checkout
      - assume-aws-role:
          role-arn: ${OIDC_ROLE_ARN_DEV}
      - run:
          name: K6 load tests
          command: |
            docker run --rm \
              -v $(pwd)/tests/performance:/scripts \
              -v $(pwd)/reports:/reports \
              grafana/k6:latest run \
              --out json=/reports/k6-results.json \
              /scripts/load-test.js
      - run:
          name: Artillery stress tests
          command: |
            npm install -g artillery
            artillery run tests/performance/stress-test.yml \
              --output reports/artillery-report.json
            artillery report --output reports/artillery.html \
              reports/artillery-report.json
      - store_artifacts:
          path: reports
          # Note: Artifact retention is configured at CircleCI organization/project level, not in YAML

  # -------------------- DEPLOYMENT JOBS --------------------
  deploy-dev:
    executor: docker-executor
    steps:
      - checkout
      - assume-aws-role:
          role-arn: ${OIDC_ROLE_ARN_DEV}
      - install-kubectl
      - setup-tenant-context:
          tenant-id: "${TENANT_ID}"
          environment: "dev"
          role-arn: ${OIDC_ROLE_ARN_DEV}
      - run:
          name: Deploy to dev
          command: |
            helm upgrade --install \
              saas-platform ./charts/saas-platform \
              -f ./charts/values/dev.yaml \
              --set image.tag=${CIRCLE_SHA1} \
              --wait --timeout 10m
      - run:
          name: Health check after deployment
          command: ./scripts/health-check.sh dev
          no_output_timeout: 5m
      - run:
          name: Rollback on health check failure
          command: ./scripts/rollback-dev.sh
          when: on_fail
      - run:
          name: Configure tenant isolation
          command: |
            ./scripts/configure-tenant-isolation.sh dev ${TENANT_ID}

  deploy-staging-blue-green:
    executor: docker-executor
    steps:
      - checkout
      - assume-aws-role:
          role-arn: ${OIDC_ROLE_ARN_STAGING}
      - install-kubectl
      - setup-tenant-context:
          tenant-id: "${TENANT_ID}"
          environment: "staging"
          role-arn: ${OIDC_ROLE_ARN_STAGING}
      - run:
          name: Deploy blue-green to staging
          command: |
            ./scripts/deploy-blue-green.sh staging ${CIRCLE_SHA1}
      - run:
          name: Health check after deployment
          command: ./scripts/health-check.sh staging
          no_output_timeout: 5m
      - run:
          name: Rollback on health check failure
          command: ./scripts/rollback-staging.sh
          when: on_fail
      - run:
          name: Validate deployment
          command: |
            ./scripts/validate-deployment.sh staging
      - run:
          name: Switch traffic
          command: |
            aws route53 change-resource-record-sets \
              --hosted-zone-id ${HOSTED_ZONE_ID} \
              --change-batch file://route53-staging-switch.json

  deploy-production:
    executor: docker-executor
    parameters:
      deployment-type:
        type: enum
        enum: ["rolling", "canary"]
        default: "rolling"
    steps:
      - checkout
      - assume-aws-role:
          role-arn: ${OIDC_ROLE_ARN_PROD}
      - install-kubectl
      - setup-tenant-context:
          tenant-id: "${TENANT_ID}"
          environment: "production"
          role-arn: ${OIDC_ROLE_ARN_PROD}
      # Deployment strategy selection: rolling update vs canary deployment
      # Rolling: Immediate full deployment with zero-downtime rolling update
      # Canary: Gradual traffic shifting with health checks and monitoring
      - when:
          condition:
            equal: [rolling, << parameters.deployment-type >>]
          steps:
            - run:
                name: Rolling update
                command: |
                  kubectl set image deployment/api-gateway \
                    api-gateway=${ECR_REGISTRY}/api-gateway:${CIRCLE_SHA1} \
                    --record
                  kubectl rollout status deployment/api-gateway
            - run:
                name: Health check after rolling update
                command: ./scripts/health-check.sh production
                no_output_timeout: 5m
            - run:
                name: Rollback on health check failure
                command: ./scripts/rollback-production.sh application
                when: on_fail
      - when:
          condition:
            equal: [canary, << parameters.deployment-type >>]
          steps:
            # Canary deployment process: Gradual traffic increase with monitoring
            # 10% -> 50% -> 100% traffic shift with health checks and audits at each stage
            - run:
                name: Canary deployment 10%
                command: |
                  ./scripts/deploy-canary.sh production ${CIRCLE_SHA1} 10
            - run:
                name: Health check after 10%
                command: |
                  ./scripts/health-check.sh production
            - run:
                name: Rollback on 10% health check failure
                command: ./scripts/rollback-production.sh application
                when: on_fail
            - run:
                name: Audit after 10%
                command: |
                  ./scripts/audit-deployment.sh production 10
            - run:
                name: Sleep for monitoring
                command: sleep 300
            - run:
                name: Canary deployment 50%
                command: |
                  ./scripts/deploy-canary.sh production ${CIRCLE_SHA1} 50
            - run:
                name: Health check after 50%
                command: |
                  ./scripts/health-check.sh production
            - run:
                name: Rollback on 50% health check failure
                command: ./scripts/rollback-production.sh application
                when: on_fail
            - run:
                name: Audit after 50%
                command: |
                  ./scripts/audit-deployment.sh production 50
            - run:
                name: Sleep for monitoring
                command: sleep 300
            - run:
                name: Canary deployment 100%
                command: |
                  ./scripts/deploy-canary.sh production ${CIRCLE_SHA1} 100
            - run:
                name: Final health check
                command: |
                  ./scripts/health-check.sh production
            - run:
                name: Rollback on final health check failure
                command: ./scripts/rollback-production.sh application
                when: on_fail
            - run:
                name: Final audit
                command: |
                  ./scripts/audit-deployment.sh production 100

  # -------------------- VALIDATION JOBS --------------------
  validate-deployment:
    executor: node-executor
    parameters:
      environment:
        type: string
    steps:
      - checkout
      - run:
          name: Integration validation
          command: |
            npm run test:integration:<< parameters.environment >>
      - run:
          name: E2E validation
          command: |
            npm run test:e2e:<< parameters.environment >>
      - run:
          name: API validation
          command: |
            npm run test:api:<< parameters.environment >>
      - run:
          name: Rollback on API validation failure
          command: ./scripts/rollback-validation-failure.sh << parameters.environment >>
          when: on_fail
      - run:
          name: Tenant isolation check
          command: |
            ./scripts/test-tenant-isolation.sh << parameters.environment >>
      - store_test_results:
          path: reports/validation
      - store_artifacts:
          path: reports/validation
          # Note: Artifact retention is configured at CircleCI organization/project level, not in YAML

  security-compliance-check:
    executor: docker-executor
    parameters:
      environment:
        type: string
    steps:
      - checkout
      # Environment-specific OIDC role ARN selection for secure authentication
      # Each environment uses a dedicated IAM role with principle of least privilege
      - when:
          condition:
            equal: [dev, << parameters.environment >>]
          steps:
            - assume-aws-role:
                role-arn: ${OIDC_ROLE_ARN_DEV}
      - when:
          condition:
            equal: [staging, << parameters.environment >>]
          steps:
            - assume-aws-role:
                role-arn: ${OIDC_ROLE_ARN_STAGING}
      - when:
          condition:
            equal: [production, << parameters.environment >>]
          steps:
            - assume-aws-role:
                role-arn: ${OIDC_ROLE_ARN_PROD}
      - run:
          name: Prowler PCI-DSS scan
          command: |
            docker run --rm \
              -e AWS_REGION=${AWS_DEFAULT_REGION} \
              toniblyx/prowler:latest \
              -g pci_dss \
              -f json \
              > reports/prowler-<< parameters.environment >>.json
      - run:
          name: AWS Config compliance check
          command: |
            aws configservice describe-compliance-by-config-rule \
              --compliance-types NON_COMPLIANT \
              --output json > reports/aws-config-<< parameters.environment >>.json
      - run:
          name: Custom tenant isolation validation
          command: |
            ./scripts/validate-tenant-isolation.sh << parameters.environment >>
      - store_artifacts:
          path: reports
          # Note: Artifact retention is configured at CircleCI organization/project level, not in YAML

  smoke-tests:
    executor: node-executor
    parameters:
      environment:
        type: string
    steps:
      - checkout
      - run:
          name: Run smoke tests
          command: |
            npm run test:smoke:<< parameters.environment >>
      - run:
          name: Rollback on smoke test failure
          command: ./scripts/rollback-smoke-failure.sh << parameters.environment >>
          when: on_fail
      - store_test_results:
          path: reports/smoke
      - store_artifacts:
          path: reports/smoke
          # Note: Artifact retention is configured at CircleCI organization/project level, not in YAML

  # -------------------- MONITORING SETUP --------------------
  setup-monitoring:
    executor: docker-executor
    parameters:
      environment:
        type: string
    steps:
      - checkout
      # Environment-specific OIDC role ARN selection for secure authentication
      # Each environment uses a dedicated IAM role with principle of least privilege
      - when:
          condition:
            equal: [dev, << parameters.environment >>]
          steps:
            - assume-aws-role:
                role-arn: ${OIDC_ROLE_ARN_DEV}
      - when:
          condition:
            equal: [staging, << parameters.environment >>]
          steps:
            - assume-aws-role:
                role-arn: ${OIDC_ROLE_ARN_STAGING}
      - when:
          condition:
            equal: [production, << parameters.environment >>]
          steps:
            - assume-aws-role:
                role-arn: ${OIDC_ROLE_ARN_PROD}
      - run:
          name: Configure Datadog
          command: |
            helm upgrade --install datadog-agent datadog/datadog \
              -f monitoring/datadog-values-<< parameters.environment >>.yaml \
              --set datadog.apiKey=${DATADOG_API_KEY} \
              --namespace monitoring
      - run:
          name: Setup CloudWatch dashboards
          command: |
            aws cloudformation deploy \
              --template-file monitoring/cloudwatch-dashboards.yaml \
              --stack-name cloudwatch-dashboards-<< parameters.environment >> \
              --parameter-overrides Environment=<< parameters.environment >>
      - run:
          name: Configure PagerDuty
          command: |
            ./scripts/configure-pagerduty.sh << parameters.environment >>
      - run:
          name: Setup Sentry
          command: |
            ./scripts/configure-sentry.sh << parameters.environment >> ${SENTRY_DSN}

  # -------------------- ROLLBACK JOB --------------------
  rollback:
    executor: docker-executor
    parameters:
      environment:
        type: string
      rollback-type:
        type: enum
        enum: ["application", "infrastructure", "full"]
    steps:
      - checkout
      # Environment-specific OIDC role ARN selection for secure authentication
      # Each environment uses a dedicated IAM role with principle of least privilege
      - when:
          condition:
            equal: [dev, << parameters.environment >>]
          steps:
            - assume-aws-role:
                role-arn: ${OIDC_ROLE_ARN_DEV}
      - when:
          condition:
            equal: [staging, << parameters.environment >>]
          steps:
            - assume-aws-role:
                role-arn: ${OIDC_ROLE_ARN_STAGING}
      - when:
          condition:
            equal: [production, << parameters.environment >>]
          steps:
            - assume-aws-role:
                role-arn: ${OIDC_ROLE_ARN_PROD}
      - install-kubectl
      # Rollback scope selection based on rollback-type parameter
      # application: Rollback only application deployments
      # infrastructure: Rollback only infrastructure changes
      # full: Complete rollback of both application and infrastructure
      - when:
          condition:
            or:
              - equal: [application, << parameters.rollback-type >>]
              - equal: [full, << parameters.rollback-type >>]
          steps:
            - run:
                name: Rollback Kubernetes deployments
                command: |
                  kubectl rollout undo deployment --all -n tenant-${TENANT_ID}-<< parameters.environment >>
                  kubectl rollout status deployment --all -n tenant-${TENANT_ID}-<< parameters.environment >>
      - when:
          condition:
            or:
              - equal: [infrastructure, << parameters.rollback-type >>]
              - equal: [full, << parameters.rollback-type >>]
          steps:
            - run:
                name: Rollback Terraform changes
                command: |
                  cd infrastructure
                  terraform init -backend-config=backend-<< parameters.environment >>.conf
                  terraform workspace select << parameters.environment >>
                  ./scripts/terraform-rollback.sh << parameters.environment >>
      - run:
          name: Rollback Route53 changes
          command: |
            aws route53 change-resource-record-sets \
              --hosted-zone-id ${HOSTED_ZONE_ID} \
              --change-batch file://route53-rollback-<< parameters.environment >>.json
      - run:
          name: Validate rollback
          command: |
            ./scripts/validate-rollback.sh << parameters.environment >>

# ==================== WORKFLOWS ====================
workflows:
  version: 2

  # Main CI/CD pipeline workflow
  # Implements a comprehensive build-test-deploy-verify pipeline
  # with progressive delivery: dev -> staging (blue-green) -> production (rolling/canary)
  build-test-deploy:
    jobs:
      # Phase 1: Parallel validation (code quality, security, infrastructure)
      # These jobs run in parallel to catch issues early
      - code-validation:
          context: aws-dev
      - infrastructure-validation:
          context: aws-dev
      - dependency-scan:
          context: aws-dev

      # Phase 2: Parallel service builds (8 microservices)
      # Each service builds independently to maximize parallelism
      - build-services:
          name: build-api-gateway
          service-name: api-gateway
          context: aws-dev
          requires:
            - code-validation
            - dependency-scan
      - build-services:
          name: build-user-service
          service-name: user-service
          context: aws-dev
          requires:
            - code-validation
            - dependency-scan
      - build-services:
          name: build-tenant-service
          service-name: tenant-service
          context: aws-dev
          requires:
            - code-validation
            - dependency-scan
      - build-services:
          name: build-billing-service
          service-name: billing-service
          context: aws-dev
          requires:
            - code-validation
            - dependency-scan
      - build-services:
          name: build-notification-service
          service-name: notification-service
          context: aws-dev
          requires:
            - code-validation
            - dependency-scan
      - build-services:
          name: build-analytics-service
          service-name: analytics-service
          context: aws-dev
          requires:
            - code-validation
            - dependency-scan
      - build-services:
          name: build-audit-service
          service-name: audit-service
          context: aws-dev
          requires:
            - code-validation
            - dependency-scan
      - build-services:
          name: build-integration-service
          service-name: integration-service
          context: aws-dev
          requires:
            - code-validation
            - dependency-scan

      # Phase 3: Comprehensive testing (unit, integration, e2e, security)
      # Tests run in parallel where possible, with proper dependencies
      - unit-tests:
          context: aws-dev
          requires:
            - code-validation
      - integration-tests:
          context: aws-dev
          requires:
            - build-api-gateway
            - build-user-service
            - build-tenant-service
            - build-billing-service
            - build-notification-service
            - build-analytics-service
            - build-audit-service
            - build-integration-service
      - e2e-tests:
          context: aws-dev
          requires:
            - integration-tests
      - security-scan:
          context: aws-dev
          requires:
            - code-validation

      # Phase 4: Infrastructure deployment (dev environment)
      - terraform-deploy:
          name: deploy-infrastructure-dev
          environment: dev
          context: aws-dev
          requires:
            - infrastructure-validation

      # Phase 5: Application deployment (dev)
      - deploy-dev:
          context: aws-dev
          requires:
            - deploy-infrastructure-dev
            - unit-tests
            - integration-tests
            - e2e-tests
            - security-scan

      # Phase 6: Dev environment validation and monitoring
      - validate-deployment:
          name: validate-dev
          environment: dev
          context: aws-dev
          requires:
            - deploy-dev
      - security-compliance-check:
          name: compliance-check-dev
          environment: dev
          context: aws-dev
          requires:
            - deploy-dev
      - smoke-tests:
          name: smoke-test-dev
          environment: dev
          context: aws-dev
          requires:
            - deploy-dev
      - setup-monitoring:
          name: monitoring-dev
          environment: dev
          context: aws-dev
          requires:
            - deploy-dev

      # Phase 7: Performance testing (post-deployment)
      - performance-tests:
          context: aws-dev
          requires:
            - validate-dev
            - compliance-check-dev

      # Phase 8: Staging environment promotion
      # Only proceeds if dev validation passes
      - terraform-deploy:
          name: deploy-infrastructure-staging
          environment: staging
          context: aws-staging
          requires:
            - validate-dev
            - performance-tests

      - deploy-staging-blue-green:
          context: aws-staging
          requires:
            - deploy-infrastructure-staging

      # Phase 9: Staging validation
      - validate-deployment:
          name: validate-staging
          environment: staging
          context: aws-staging
          requires:
            - deploy-staging-blue-green
      - security-compliance-check:
          name: compliance-check-staging
          environment: staging
          context: aws-staging
          requires:
            - deploy-staging-blue-green
      - smoke-tests:
          name: smoke-test-staging
          environment: staging
          context: aws-staging
          requires:
            - deploy-staging-blue-green
      - setup-monitoring:
          name: monitoring-staging
          environment: staging
          context: aws-staging
          requires:
            - deploy-staging-blue-green

      # Phase 10: Production approval gate
      # Manual approval required before production deployment
      - hold-production:
          type: approval
          requires:
            - validate-staging
            - compliance-check-staging
            - smoke-test-staging

      # Phase 11: Production deployment
      - terraform-deploy:
          name: deploy-infrastructure-prod
          environment: production
          context: aws-prod
          requires:
            - hold-production

      - deploy-production:
          name: deploy-prod-rolling
          deployment-type: rolling
          context: aws-prod
          requires:
            - deploy-infrastructure-prod
          filters:
            branches:
              only: main

      # Phase 12: Production validation and monitoring
      - validate-deployment:
          name: validate-prod
          environment: production
          context: aws-prod
          requires:
            - deploy-prod-rolling
      - security-compliance-check:
          name: compliance-check-prod
          environment: production
          context: aws-prod
          requires:
            - deploy-prod-rolling
      - smoke-tests:
          name: smoke-test-prod
          environment: production
          context: aws-prod
          requires:
            - deploy-prod-rolling
      - setup-monitoring:
          name: monitoring-prod
          environment: production
          context: aws-prod
          requires:
            - deploy-prod-rolling

  # Emergency rollback workflow
  # Manually triggered for production incidents
  emergency-rollback:
    jobs:
      - hold-rollback:
          type: approval
      - rollback:
          environment: production
          rollback-type: full
          context: aws-prod
          requires:
            - hold-rollback
