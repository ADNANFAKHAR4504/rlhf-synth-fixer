We’re building a fully automated CI/CD workflow in Azure DevOps for a retail e-commerce platform that spans multiple Azure regions and relies heavily on AKS for microservices, Azure SQL for transactional workloads, Redis for session caching, and Azure Front Door for global traffic routing. The goal is to create a resilient, multi-stage pipeline that enforces strict quality, security, and compliance requirements while deploying through development, staging, and production environments using Workload Identity Federation as the authentication model. All container images should be built and pushed to ACR, external scripts longer than five lines must be isolated into separate files, and every step in the workflow—from validation to rollback—should operate with production-grade rigor.

The pipeline should begin with a validation stage that runs code quality checks across all six .NET 8 microservices (ProductCatalog, OrderService, PaymentGateway, InventoryManager, CustomerService, and NotificationHub). This includes linting and quality gates using SonarQube, followed by infrastructure validation where Bicep templates are checked with az bicep build, and security/compliance scanning is executed using tfsec and Checkov. Dependency scanning should also occur here using Snyk for NuGet packages and WhiteSource Bolt for license compliance. All validation jobs should run on Ubuntu-based agents.

Once validation completes, the build stage should run with a dependency on it. Microservices must be restored, built, tested, and packaged using DotNetCoreCLI@2, with code coverage requirements enforced (>80%) and test results published. NuGet caching should be enabled for performance. Containers must then be built using Docker@2 with BuildKit enabled, multi-stage Dockerfiles, and tags pushed to $(acrName).azurecr.io/retail/service:$(Build.BuildId) after being scanned using integrated Trivy scanning. Infrastructure build jobs should run Terraform init/plan using TerraformTaskV4 with an azurerm backend and produce a plan for AKS with zone redundancy, Azure SQL with geo-replication, Redis Premium clustering, Front Door Premium, and Application Gateway WAF. The front-end build should compile the React/TypeScript SPA, run webpack’s production build, validate performance using Lighthouse CI, and upload the result to the Azure Storage static website.

Following the build stage, the testing stage should run in parallel jobs. Unit tests should execute with xUnit and coverage reporting. Integration tests must deploy a temporary environment using ARM templates, validate service-to-service communication including gRPC calls, and use Testcontainers for Azure SQL and Redis. Performance tests should run JMeter for 10k concurrent users and publish HTML reports. Contract testing must use Pact to verify consumer-driven API expectations.

A dedicated security stage should run after the build completes. All six service containers should be scanned using both Trivy and Grype, and the pipeline must fail immediately on any CRITICAL vulnerability. The stage should also run Semgrep (with custom retail rules), CodeQL, and other source-level analysis. Secret scanning should be performed using detect-secrets and TruffleHog across the full git history, blocking the pipeline on any leaked credential. Network security validation must ensure NSG rules, WAF rules, and Azure Firewall settings are compliant, while compliance validation should verify Azure Policy for PCI-DSS, encryption at rest, and logging configurations.

Once Test and Security stages succeed, the pipeline should deploy into the development environment. This includes deploying Bicep-managed infrastructure with AzureCLI@2, performing SQL migrations using SqlAzureDacpacDeployment@1 with backups, deploying all six microservices to the dev AKS cluster using KubernetesManifest@1 with Helm charts and autoscaling enabled, and configuring monitoring through Application Insights, Log Analytics, and Azure Monitor alerts.

With the development environment live, an integration testing stage must run Postman API tests using Newman, browser-based end-to-end tests with Playwright validating full purchase flows, and load tests with K6 scaling to 5k users to observe how the infrastructure responds.

Deployment to staging should use a canary model that requires manual approval. The stage should deploy AKS clusters in both East US and West Europe using Istio for traffic splitting, routing 10% of traffic initially, and monitoring key metrics such as 99.5%+ success rate, <500ms p95 latency, SQL deadlocks, and Redis hit rate. If thresholds fail, rollback must occur automatically using monitoring scripts. After canary validation, a blue-green deployment should spin up a parallel "Green" environment, warm caches, switch traffic through Application Gateway, keep the "Blue" infrastructure online for two hours, then remove it after successful validation.

Once staging passes all tests, a comprehensive validation stage should run full regression tests using Selenium Grid, accessibility tests with Pa11y and axe-core for WCAG 2.1 AA compliance, and a deep security sweep using OWASP ZAP and SQLMap.

Before production deployment, the pipeline should enforce manual approvals from security (with a 168-hour timeout), operations (72 hours), and business stakeholders (48 hours). Production deployment should then create infrastructure across all four required regions—East US, West Europe, Southeast Asia, and Australia East—using Bicep templates. AKS must be zone-redundant, Azure SQL must be configured with active geo-replication and auto-failover groups, Redis Premium must be fully clustered with geo-replication, and Front Door Premium must be configured with WAF, DDoS protection, and global routing.

Application rollout in production must follow a phased canary process of 25% → 50% → 100%, validating health probes and service metrics between phases. Database migrations must support automated rollback on error and maintain connection stability throughout. After deployment, Front Door must be configured with backend pools across all regions, correct geo-filtering, session affinity, and caching rules.

A post-deployment stage should run smoke tests in production, validate checkout flow and payment processing, create custom dashboards, enable multi-step availability tests, configure SLO-based alerting (99.95% availability, p99 <1s latency, <0.1% error rate), and integrate notifications with PagerDuty and Slack. The final step should notify Teams, update Jira release notes, and refresh the public status page.

A rollback stage must be available as a manual trigger only. It should allow operators to choose a prior version, revert Front Door routing, undo AKS deployments, restore databases using backups, invalidate caches, and re-run critical path tests.

All pipeline triggers should include main-branch CI/CD runs, PR validation for early stages, and a nightly scheduled run. Variables such as the federated identity service connection, ACR name, AKS cluster names for each environment, database identifiers, Redis caches, and Front Door instance names should be incorporated throughout. The entire workflow must maintain PCI-DSS compliance and enforce all required SLO, accessibility, and security constraints.

Generate a complete Azure DevOps YAML pipeline (ci-cd.yml) implementing everything described above, using best-practice patterns for multi-stage pipelines, job dependencies, federated identity authentication, deployment strategies, environment protections, and reusable external scripts.
