# MODEL_FAILURES - Issues in Initial Implementation

This document identifies issues in the MODEL_RESPONSE.md implementation and explains the corrections needed for the IDEAL_RESPONSE.md.

## Issue 1: Missing Pod Security Standards Configuration

**Severity**: High
**Category**: Missing Required Feature

**Problem**: The task explicitly requires "Configure pod security standards at the cluster level with 'restricted' as the baseline", but this configuration is completely missing from the implementation.

**Impact**: Pod security standards provide baseline security policies that restrict pod capabilities. Without this, pods could run with elevated privileges, access host namespaces, or use dangerous volume types - all security risks for financial services workloads.

**Fix**: Add pod security policy labels to the namespaces:
- For dev/staging: use `pod-security.kubernetes.io/enforce: baseline`
- For prod: use `pod-security.kubernetes.io/enforce: restricted`
- Add audit and warn modes for better visibility

## Issue 2: Inefficient NAT Gateway Configuration

**Severity**: Medium
**Category**: Cost Optimization

**Problem**: The VPC is configured with 3 NAT Gateways (one per AZ), costing approximately $96/month ($32 x 3). For synthetic testing tasks, this is unnecessary expense.

**Impact**: High AWS costs for ephemeral infrastructure that will be destroyed after testing.

**Fix**: Reduce to 1 NAT Gateway for synthetic tasks. Production deployments can use 3 for high availability, but test environments don't require this level of redundancy.

## Issue 3: Incomplete CloudWatch Logs Configuration

**Severity**: Medium
**Category**: Missing Configuration

**Problem**: While Fluent Bit is configured to write to CloudWatch Logs, there's no explicit CloudWatch Log Group resource created with retention policies. The configuration relies on `auto_create_group: true` in Fluent Bit, which doesn't allow control over retention and encryption.

**Impact**:
- Logs may be retained indefinitely, increasing costs
- No explicit KMS encryption for log data at rest
- No tagging for cost tracking

**Fix**: Create explicit CloudWatch Log Group with:
- 7-day retention period for synthetic tasks
- KMS encryption using the same key as EKS secrets
- Proper tags for Environment and CostCenter

## Issue 4: Fluent Bit Configuration Not Loaded from SSM

**Severity**: High
**Category**: Implementation Gap

**Problem**: The Fluent Bit configuration is stored in SSM Parameter Store but never actually used by the Fluent Bit DaemonSet. The DaemonSet has hardcoded configuration in the container, making the SSM parameter pointless.

**Impact**: Task requirement "Store Fluent Bit configuration in AWS Systems Manager Parameter Store" is not properly fulfilled. Configuration changes require pod restarts, and the SSM parameter serves no functional purpose.

**Fix**: Two options:
1. Use a ConfigMap populated from SSM Parameter (requires init container or external controller)
2. Configure Fluent Bit to use AWS SDK to fetch config from SSM on startup
3. Use AWS Secrets Manager instead with EKS Secrets CSI driver

For this implementation, use option 1 with a Kubernetes Job that fetches the SSM parameter and creates a ConfigMap, then mount the ConfigMap in the Fluent Bit pods.

## Issue 5: Missing VPC Endpoint for Cost Optimization

**Severity**: Low
**Category**: Cost Optimization Best Practice

**Problem**: The implementation uses NAT Gateways for all egress traffic. For AWS service traffic (CloudWatch Logs, ECR, etc.), VPC endpoints would be more cost-effective and performant.

**Impact**:
- Higher NAT Gateway data transfer costs
- Slightly higher latency for AWS service calls

**Fix**: Add VPC endpoints for commonly used services:
- `com.amazonaws.ap-southeast-1.logs` (CloudWatch Logs)
- `com.amazonaws.ap-southeast-1.ecr.api` (ECR API)
- `com.amazonaws.ap-southeast-1.ecr.dkr` (ECR Docker)
- `com.amazonaws.ap-southeast-1.s3` (S3 Gateway Endpoint - free)

## Issue 6: KMS Key Missing Alias Format

**Severity**: Low
**Category**: Code Quality

**Problem**: The KMS key uses `alias:` prefix in the alias name, but the CDK Key construct already adds this prefix. This results in an alias like `alias:alias:eks-secrets-suffix`.

**Impact**: Minor - the alias works but has incorrect naming format.

**Fix**: Remove `alias:` prefix from the alias parameter - just use `eks-secrets-${environmentSuffix}`.

## Issue 7: Incomplete Output Export Names

**Severity**: Medium
**Category**: Integration Testing Support

**Problem**: CfnOutputs don't have explicit `exportName` properties, making them harder to reference in integration tests via `cfn-outputs/flat-outputs.json`.

**Impact**: Integration tests may have difficulty accessing outputs if the output key names are auto-generated by CDK with stack prefixes.

**Fix**: Add explicit exportName to each CfnOutput:
- `ClusterEndpoint` → `EksClusterEndpoint${environmentSuffix}`
- `OIDCProviderArn` → `EksOIDCProviderArn${environmentSuffix}`
- `KubectlConfig` → `EksKubectlConfig${environmentSuffix}`

## Issue 8: Missing VPC and Security Group Outputs

**Severity**: Medium
**Category**: Missing Outputs

**Problem**: The outputs only include cluster-related information, but integration tests may need VPC ID, subnet IDs, and security group IDs for validation.

**Impact**: Integration tests cannot fully validate the network configuration.

**Fix**: Add outputs for:
- VPC ID
- Private Subnet IDs
- Cluster Security Group ID
- Node Security Group ID

## Issue 9: Node Groups Not Following AZ Best Practice

**Severity**: Low
**Category**: Best Practice

**Problem**: Node groups are manually created with specific AZ assignments. CDK's default behavior of letting EKS distribute nodes across AZs would be cleaner and more resilient.

**Impact**: More verbose code, potential for AZ imbalance if one AZ is unhealthy.

**Fix**: Use a single node group with automatic AZ distribution:
```typescript
this.cluster.addNodegroupCapacity('NodeGroup', {
  nodegroupName: `eks-ng-${environmentSuffix}`,
  instanceTypes: [new ec2.InstanceType('m5.large')],
  minSize: 3,
  maxSize: 9,
  desiredSize: 3,
  // Let EKS distribute across all AZs automatically
});
```

However, since the task explicitly says "three managed node groups spread across availability zones", we should keep three separate node groups but could simplify the configuration.

## Issue 10: Missing Cluster Name Output

**Severity**: Low
**Category**: Missing Output

**Problem**: While kubectl config command is provided, the actual cluster name isn't exported as a separate output.

**Impact**: Integration tests that need just the cluster name must parse the kubectl command string.

**Fix**: Add explicit cluster name output:
```typescript
new cdk.CfnOutput(this, 'ClusterName', {
  value: this.cluster.clusterName,
  exportName: `EksClusterName${environmentSuffix}`,
});
```

## Summary

Total Issues: 10
- High Severity: 2 (Pod Security Standards, Fluent Bit SSM integration)
- Medium Severity: 4 (NAT Gateway cost, CloudWatch Logs, Outputs, VPC outputs)
- Low Severity: 4 (VPC Endpoints, KMS alias, Node groups, Cluster name)

These issues represent important learning opportunities about:
1. Kubernetes security best practices (Pod Security Standards)
2. AWS cost optimization (NAT Gateways, VPC Endpoints)
3. Proper secret/config management (SSM Parameter Store integration)
4. Production-ready observability (CloudWatch Logs with retention/encryption)
5. Integration testing support (comprehensive outputs)
