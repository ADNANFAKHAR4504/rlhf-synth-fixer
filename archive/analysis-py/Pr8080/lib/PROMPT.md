You are an AWS Cloud Architect and Monitoring Engineer. I want a single, self-contained Python script (saved as lib/analyse.py) that uses boto3 to perform infrastructure analysis, configuration-drift detection, and compliance evaluation for an AWS account according to the scenario described below. The script should be written professionally and ready to run either as a standalone analysis tool or as the handler logic for a scheduled Lambda function (it must include a clear lambda_handler(event, context) entrypoint). Do not produce any IaC, YAML, or deployment code — this script only analyzes and reports.

The environment to analyze is an AWS monitoring stack that relies on AWS Config to record EC2, RDS, S3, and IAM resources, a scheduled EventBridge rule that triggers analysis every six hours, an S3 bucket with versioning for storing JSON compliance reports, a DynamoDB table (partition key resourceId, sort key timestamp) using on-demand billing for storing detailed drift records, and an SNS topic used for alerts when drift exceeds a 15% threshold. The operations team optionally stores canonical baselines in AWS Systems Manager Parameter Store; if present the script should read baseline configuration data from SSM and fall back gracefully if it is absent (for example by using the oldest available Config snapshot as baseline or marking the baseline as missing). The organization enforces that Lambda execution environments used for analysis are expected to run with up to 3 GB memory and a 5-minute timeout, but the script should itself remain efficient and stream results so it can run within those limits when executed as a Lambda. Assume the account enforces VPC endpoints for AWS APIs — the script should work through standard boto3 clients (no direct network assumptions beyond normal AWS SDK behavior). The script must not modify or create AWS infrastructure or IAM policies; it should assume the executing role already has least-privilege permissions to read AWS Config, S3, DynamoDB, SNS, SSM, CloudWatch, and relevant resources.

Your generated lib/analyse.py should, at minimum, implement the following behaviors in code and comments so an engineer can maintain and extend it easily:

Initialize boto3 clients/resources for AWS Config (configservice), S3, DynamoDB, SNS, SSM, and optionally CloudWatch for metrics. Include an optional environment variable or config block at the top to set region, S3 report bucket name, DynamoDB table name, SNS topic ARN, SSM baseline parameter name, drift threshold (default 15%), and a “run id” or timestamp prefix for report filenames.

Fetch and inspect recent AWS Config snapshots / resource configuration histories for the target resource types (EC2, RDS, S3, IAM). Use appropriate boto3 configservice APIs (e.g., retrieving resource config history, compliance details, or aggregated snapshots) to obtain resource states and their timestamps.

Compute drift by comparing the latest configurations against a baseline: attempt to load baseline configuration from SSM Parameter Store if the configured SSM parameter exists; otherwise, derive a baseline from a historical Config snapshot and clearly annotate that fallback in the report. The drift calculation should be explicit and explainable (for each resource, identify which keys/fields differ, count number of changed resources, and compute a drift percentage = changed_resources / total_tracked_resources).

Evaluate compliance against the provided constraints (express each constraint as a check implemented in code): example checks should include S3 buckets having versioning enabled and lifecycle rules, DynamoDB billing mode is ON_DEMAND, Lambda configuration expectations (memory >= 3GB and timeout >= 300s when analyzing Lambda resources), EventBridge run frequency (verify that scheduled rule exists and is configured for a 6-hour cadence), and SNS topics having at least one subscription endpoint defined (email endpoints if configured). For each check produce a pass/fail state and explanatory message.

Produce structured JSON output that contains: metadata (run_id, timestamp, region, invoking_principal` if available via STS), per-resource drift details (resourceId, resourceType, baselineSnapshotId or ssmBaselineRef, diff details), aggregate statistics (total resources, changed resources, drift_percentage), and compliance findings (list of checks with status and remediation hints). The JSON should be human-readable (pretty-printed) and also compactly storable.

Persist results into the environment: write the full JSON report to the configured S3 bucket as a versioned object with a timestamped key and atomic write semantics; write summary rows into DynamoDB using partition key resourceId (for each resource or aggregated summary) and sort key timestamp (ISO8601), and ensure that DynamoDB operations respect on-demand usage patterns (batch writes where appropriate). When writing to DynamoDB, include a concise item schema in code comments matching the required keys.

Trigger alerting: if the computed drift percentage for the run exceeds the configured threshold (default 15%), publish a concise alert message to the configured SNS topic with the run summary and a presigned S3 URL or S3 object location for the detailed report. The SNS publish should include structured attributes (e.g., drift_percentage, affected_resources) so subscribers can filter.

Robustness and operational details: include clear error handling, exponential backoff retries for AWS API throttling, idempotent writes, and safe handling of partial failures (for example, if DynamoDB write fails, still write to S3 and include an error note in the S3 report). The script should log at multiple levels (INFO, WARNING, ERROR) and expose a verbose flag or environment variable to increase log detail. Add comments describing the least-privilege IAM permissions that the executing role should have (read/describe/list for Config, S3 PutObject/GetObject, DynamoDB PutItem/BatchWriteItem, SNS Publish, SSM GetParameter, STS:GetCallerIdentity).

Extensibility hooks: provide well-documented functions so future engineers can add Step Functions orchestration, CloudWatch metric emission, or more advanced relation-aware drift rules (for example, if an IAM role used by an EC2 instance changes, mark that EC2 instance as transitively affected). Include a simple mapping of relationships in code comments and a stub function that demonstrates how to enrich findings with related resource context.

Testing and local run: include a if **name** == "**main**": block that allows running the analysis locally with environment variables to point to the S3/DynamoDB/SNS resources; when run locally, the script should print the same JSON summary to stdout in addition to uploading results.

Documentation: top-of-file docstring describing purpose, expected environment (region, existing AWS resources names/ARNs), assumptions (script will not create resources), and exact files/resources the script will touch. Keep code readable, modular, and commented — focus on clear engineering style rather than clever one-liners.

Be explicit in the narrative that the script is for analysis only (no resource creation) and that least-privilege roles should be provided by IaC elsewhere. The prompt you produce should read like an AWS Cloud Architect describing the concrete monitoring script they want, blending Problem, Environment, Constraints, and Background into a single narrative so another AI can generate exactly one Python file that implements all of the above behavior.

Now generate the Python boto3 analysis script and save it as lib/analyse.py.
