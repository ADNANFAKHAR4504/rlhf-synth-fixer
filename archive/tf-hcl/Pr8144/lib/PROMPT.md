I'm building a payment processing system for a financial services company that handles real-time transaction data. We need a DynamoDB table to store payment transaction records that supports both instant lookups by transaction ID and batch reporting queries where the finance team needs to pull transactions by date ranges and filter by payment amounts. The system processes around 50,000 transactions daily with unpredictable spikes during business hours, so we need on-demand capacity that scales automatically without manual intervention.

Create a DynamoDB table named exactly 'payment-transaction' using on-demand billing mode to handle the variable traffic patterns. The table needs a partition key called 'transaction_id' as a String type that will store unique transaction identifiers like TXN-20240115-ABC123. Add a sort key called 'timestamp' as a Number type that stores Unix epoch timestamps so we can efficiently query transactions in chronological order for a specific transaction ID. Make sure all attribute names follow snake_case naming convention consistently throughout the configuration.

For the finance team's reporting needs, add a global secondary index named 'date-index' that allows querying transactions by date and amount. This index should use 'date' as the partition key in String format like '2024-01-15' and 'amount' as the sort key in Number format storing dollar amounts as integers in cents like 1599 for $15.99. The global secondary index must project ALL attributes so the reporting queries don't need to fetch data from the base table, which would add latency and cost. Configure this index to also use on-demand billing mode to match the base table's capacity settings.

Enable point-in-time recovery on the table to protect against accidental deletions or data corruption. This is a regulatory requirement for financial transaction data where we must be able to restore the table to any point in time within the last 35 days. Configure server-side encryption using AWS managed keys, not customer managed KMS keys, to keep the infrastructure simpler while still meeting compliance requirements for data at rest encryption. Set up Time to Live functionality on an attribute field called 'expiration_time' so we can automatically delete transaction records after our required retention period expires without running manual cleanup jobs.

Organize all the code in a single main.tf file. The file should start with the terraform and provider blocks specifying AWS provider version 5.0 or higher since we need the latest DynamoDB features and syntax. Include the DynamoDB table resource with all the configuration for keys, indexes, point-in-time recovery, encryption settings, and TTL attribute configuration. Define all necessary attributes in the attribute definitions block including transaction_id, timestamp, date, amount, and expiration_time with their correct types. Make sure the global secondary index configuration is complete with the index name, hash key, range key, and projection type set to ALL.

Add resource tags to the DynamoDB table with Environment set to exactly 'prod' and Department set to exactly 'finance' as these are required for cost allocation and access control policies in our AWS organization. Don't use any customer managed KMS keys for encryption, only use AWS managed keys. Don't change the table name from 'payment-transaction' as our application code is already configured to use this exact name. Don't use provisioned billing mode or auto-scaling configurations since the requirement explicitly needs on-demand capacity mode.

Output the table ARN so we can reference it in IAM policies and application configuration files. Also output the name of the global secondary index so our reporting Lambda functions can reference it when making Query API calls. The entire configuration should work with a simple terraform init and terraform apply without requiring any variable files or additional input, using only the AWS credentials from the environment.