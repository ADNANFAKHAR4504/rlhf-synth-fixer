Design a production-ready Terraform configuration in HCL that deploys scalable ML training infrastructure for distributed GPU workloads on AWS, contained entirely within a single file named **tap-stack.tf**, without creating or modifying any provider blocks. The configuration must build a secure networking layer including a VPC with three private subnets across different Availability Zones, appropriate route tables and optional NAT gateways, security groups that allow internal cluster communication only, and VPC endpoints for S3 and DynamoDB to ensure private traffic. Implement an EC2 Fleet that uses only **p3.2xlarge Spot instances**, maintains a constant target capacity of six instances with automatic Spot replacement, and is powered by a Launch Template using the latest AWS Deep Learning AMI (Ubuntu) or a custom AMI via variable. The instances must use an IAM Instance Profile granting least-privilege access to S3, DynamoDB, CloudWatch, and SSM, and user_data should install NVIDIA drivers, configure the CloudWatch agent, and publish GPU utilization metrics. Provision two S3 buckets—`ml-training-data-<env>` and `ml-model-artifacts-<env>`—both with block public access, versioning, server-side encryption, and lifecycle rules transitioning objects to Glacier after 30 days. Create a DynamoDB table for experiment tracking using on-demand billing, PITR enabled, and keys (`experiment_id`, `run_id`). Define all IAM roles and inline policies needed for S3, DynamoDB, CloudWatch Logs/Metrics, and SSM operations. Configure CloudWatch Log Groups for training logs plus custom metrics for GPU utilization, optionally with alarms. Add SSM Parameter Store SecureString entries for model hyperparameters—learning rate, batch size, and epochs—under `/ml/hparams/`. Finally, output the S3 bucket names, DynamoDB table name, EC2 Fleet ID, and EC2 IAM role name. This single-file Terraform configuration must follow AWS security and scalability best practices, enforce least-privilege access, and fully support distributed GPU training workloads. 
