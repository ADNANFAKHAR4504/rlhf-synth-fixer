Hey, I'm working on a fleet telematics service that monitors around 234,000 commercial vehicles in real-time, handling everything from OBD-II diagnostics like engine RPM, coolant temp, fuel levels, and DTCs, to predictive maintenance alerts, driver hours-of-service (HOS) compliance with DOT regulations, vehicle location tracking with geofencing, and fuel efficiency analytics. The goal is to ensure multi-environment consistency across dev, staging, and prod setups, so the topology stays identical—same resource types and counts everywhere—while allowing capacities, like shard counts or memory allocations, to scale up via tfvars files. We're using Terraform to manage this all in AWS, sticking to native services without any Kubernetes or ECS orchestration.
I need you to generate a production-ready Terraform file called tap*stack.tf that sets up the entire pipeline. Start with the terraform block specifying required_version >= 1.5 and required_providers for AWS ~> 5.0, but keep provider config out since it's handled in a separate provider.tf file that uses a variable aws_region. Declare that aws_region variable as a string in tap_stack.tf, along with other variables like env, project_name, owner, cost_center, and common_tags for tagging consistency. You'll also need VPC-related vars like vpc_cidr, public_subnet_cidrs, private_subnet_cidrs, and enable_nat_gateway; Kinesis ones like diagnostics_stream_name, hos_stream_name, gps_stream_name, stream_mode, diagnostics_shard_count, hos_shard_count, gps_shard_count, and retention_hours; DynamoDB details such as diagnostics_table, thresholds_table, predictions_table, driver_logs_table, compliance_rules_table, locations_table, geofences_table, compliance_status_table, billing_mode, rcu, wcu, ttl_enabled, and ttl_attribute; Lambda configs including processor_memory, anomaly_memory, maintenance_memory, driver_notifier_memory, fleet_memory, predictive_memory, hos_memory, location_memory, fuel_memory, coaching_memory, timeout_s, runtime, and reserved_concurrency; Redis vars like node_type, num_cache_clusters, engine_version, and snapshot_retention_limit; Aurora settings with cluster_identifier, database_name, master_username, instance_class, min_capacity, max_capacity, and backup_retention_days; SageMaker predictive_endpoint_name as a data source reference; SNS topics like alerts_topic, maintenance_topic, violations_topic, summary_topic, geofence_topic, and coaching_topic; SQS queues including maintenance_queue, driver_queue, fleet_queue, training_queue, visibility_timeout, and retention_seconds; EventBridge compliance_schedule_expression; S3 buckets with reports_bucket, data_lake_bucket, and lifecycle_archive_days; Firehose diagnostics_firehose_name, buffer_interval_s, buffer_size_mb, and data_format_conversion_enabled; Glue crawler_name, database_name, and crawler_schedule; Athena workgroup_name and output_bucket; and CloudWatch log_retention_days. Give these variables types, descriptions, and sane defaults where it makes sense.
Use locals for things like naming conventions, tagging, and per-environment capacity maps to keep everything deterministic and idempotent. Define all resources and data sources directly in this single file—no placeholders or pseudo-code, just actual aws*\* resources. Set up a VPC with public and private subnets, NAT gateways, security groups, and place resources like Lambda functions for Aurora and Redis access inside the VPC for security. Enable VPC endpoints for DynamoDB, Kinesis, S3, SNS, SQS, and SageMaker Runtime to keep traffic internal.
The core pipeline starts with three Kinesis Data Streams—one for diagnostics, one for HOS driver status updates (on-duty, off-duty, driving, sleeper berth), and one for GPS coordinates—all in the specified stream_mode with configurable shard counts and retention hours. Use for_each to create these streams with a common configuration pattern for efficiency. A Lambda processor subscribed to the diagnostics stream validates OBD-II data with Python 3.12 parsing, writes to a DynamoDB vehicle_diagnostics table with a composite key of vehicle_id and timestamp, enables streams, and sets TTL on the timestamp attribute for 30-day automatic cleanup. That DynamoDB stream triggers another Lambda for anomaly detection, which checks against thresholds from a DynamoDB alert_thresholds table for vehicle-specific calibrations, does trend analysis using rolling window data stored as sorted sets in ElastiCache Redis, and publishes anomalies to an SNS vehicle-alerts topic.
From there, SNS fans out to SQS queues: maintenance_alerts for service scheduling, driver_notifications for in-cab warnings, and fleet_manager_queue for dashboard updates. Each queue has Lambda consumers—a maintenance one creating work orders in an Aurora PostgreSQL maintenance_schedule table, a driver one sending notifications via a mocked external telematics API through API Gateway, and a fleet one updating Redis with alert counters in hash structures for real-time dashboards. For predictive maintenance, a Lambda on the diagnostics stream pulls historical patterns from Aurora's vehicle_history table, invokes a SageMaker endpoint (referenced via data source) for ML predictions, writes to DynamoDB predicted_failures with confidence scores, and publishes high-confidence ones (>80%) to SNS maintenance-required topic.
On the HOS side, the HOS stream's Lambda validates against DOT rules from DynamoDB compliance_rules, updates driver_logs table, detects violations like driving over 11 hours, and publishes to SNS hos-violations for immediate alerts, which trigger a Lambda to generate reports stored in S3 compliance_reports bucket with partitioning by fleet_id/date and object lock enabled for regulatory compliance. An EventBridge scheduled rule kicks off Step Functions daily for DOT reporting: it queries Aurora and DynamoDB for logs, inspections, and history; generates FMCSA-compliant PDFs via an external API; uploads to S3; updates DynamoDB compliance_status; and sends summaries to SNS compliance-summary.
For location tracking, the GPS stream's Lambda updates DynamoDB vehicle_locations with geohash for spatial queries (add a GSI on geohash), GEOADDs to Redis for geospatial indexing, checks against DynamoDB geofences, and publishes violations to SNS geofence-alerts. Fuel analytics come from a Lambda on diagnostics calculating MPG and efficiency, writing to Aurora fuel_analytics, comparing to Redis benchmarks, spotting inefficient patterns, and publishing coaching recs to SNS driver-coaching, which triggers a Lambda sending materials via SQS driver_training_queue.
Archival happens via Kinesis Data Firehose on diagnostics, delivering raw data to S3 data lake with Lambda transformation to Parquet format, partitioned by date/vehicle_type, buffer settings configurable. A Glue Crawler catalogs this nightly on schedule, and an Athena workgroup with encrypted queries and output bucket enables ad-hoc analytics for optimization.
Security and best practices are key: all vehicle data encrypted with KMS, least-privilege IAM roles for Kinesis Get/Put, DynamoDB read/write with streams and batch ops, Redis/Aurora access via VPC and Secrets Manager for credentials, SageMaker InvokeEndpoint, SNS publish, SQS receive/delete, S3 read/write with SSE-KMS and lifecycle to Glacier after archive_days, Firehose PutBatch, Glue StartCrawler, Athena StartQueryExecution, and Step Functions execution. Set up event source mappings for streams with parallelization, batch windows, and DLQ error handling. SQS with DLQ, message retention for SLAs, and SNS with KMS encryption plus subscriptions including HTTP for externals and filter policies on severity.
Monitoring via CloudWatch: alarms for Kinesis shard capacity (incoming bytes), Lambda anomaly duration (p95), DynamoDB throughput exceeded, Redis memory, Aurora connection count, SQS oldest message age, Step Functions failures, Firehose errors; plus logs with retention and metric filters for HOS violations and predictions.
Finally, define outputs for key stuff like Kinesis ARNs, DynamoDB names/ARNs, SNS ARNs, SQS URLs, Aurora endpoints, Redis endpoint, Step Functions ARN, Lambda ARNs, S3 names, Firehose ARN, Glue names, Athena workgroup, and VPC/subnet/SG IDs. At the end of the file, include three example var files: dev.tfvars, staging.tfvars, and prod.tfvars, showing how capacities differ—like smaller shards in dev, ramping up in prod—while keeping the topology identical, no changes to stream types, HOS rules, or reporting logic. Add comprehensive comments explaining things like DOT HOS regs and the predictive ML workflow.
Make sure the whole thing follows AWS best practices: encryption everywhere, least-privilege, deterministic naming, and idempotency. This should be ready to deploy in prod environment, but parity ensures it works seamlessly in dev and staging too. Thanks—looking forward to solid, working code!
