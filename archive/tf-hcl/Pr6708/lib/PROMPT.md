# Asynchronous Payment Processing Pipeline with Fault-Tolerant Architecture

## Business Context

Hey, we need to implement a production-grade asynchronous payment processing pipeline for our financial services platform that handles millions of daily transactions requiring fraud detection API validation with guaranteed ordering and zero data loss. The system must handle variable load patterns with peak volumes during business hours while maintaining strict transaction ordering, automatic retry logic, and comprehensive audit trails for compliance. **We'll use Terraform with HCL** to build this fault-tolerant infrastructure in ap-southeast-1 with multi-AZ resilience and complete observability.

## Technical Requirements

### VPC Network Architecture

Create a VPC with CIDR 10.0.0.0/16 containing three private subnets across three availability zones using 10.0.1.0/24, 10.0.2.0/24, and 10.0.3.0/24 with DNS hostnames and DNS support enabled. This is a fully private architecture without Internet Gateway or NAT Gateway relying entirely on VPC endpoints for AWS service connectivity. Set up a single route table for all private subnets with local routing only. Enable VPC Flow Logs capturing all traffic types with sixty-second aggregation intervals to an S3 bucket with KMS encryption for security auditing. Deploy five VPC endpoints including a gateway endpoint for DynamoDB with route table associations and four interface endpoints for SQS, Lambda, CloudWatch Logs, and SNS with private DNS enabled and security group restrictions allowing HTTPS traffic from the VPC CIDR range only.

### S3 Storage for Audit Logs

Create an S3 bucket named "s3-audit-logs-dev-ACCOUNT_ID" for storing VPC Flow Logs and CloudTrail data with server-side encryption using a dedicated KMS key. Enable versioning for immutable audit trails and implement all four public access block settings including block_public_acls, block_public_policy, ignore_public_acls, and restrict_public_buckets. Configure a bucket policy denying unencrypted uploads while allowing root account access and service principals for vpc-flow-logs.amazonaws.com and cloudtrail.amazonaws.com. Set force_destroy to true for testing cleanup and add lifecycle rules with the required filter block to transition flow logs to Glacier after thirty days and expire after ninety days.

### KMS Encryption Infrastructure

Create three customer-managed KMS keys for different service categories including one for S3 and CloudTrail, one for SQS and SNS, and one for DynamoDB and CloudWatch Logs. Each key must enable automatic rotation and include comprehensive key policies allowing root account access, deployment user access, and service principals like s3.amazonaws.com, logs.amazonaws.com, sqs.amazonaws.com, sns.amazonaws.com, dynamodb.amazonaws.com, and cloudtrail.amazonaws.com with GenerateDataKey and Decrypt permissions. Set deletion_window_in_days to seven for testing cleanup and create aliases like "alias/payment-s3-dev", "alias/payment-sqs-dev", and "alias/payment-dynamodb-dev" for easier reference.

### CloudTrail Audit Logging

Set up a CloudTrail trail named "cloudtrail-payment-processing-dev" logging all management events to the S3 audit bucket with log file validation enabled for integrity verification. Configure server-side encryption using the S3 KMS key and enable include_global_service_events to capture IAM and STS events in addition to regional API calls. Add CloudWatch Logs integration with a dedicated log group encrypted by the DynamoDB KMS key for real-time monitoring and alarming on security events. Configure the trail to log read and write events across all regions for comprehensive API activity tracking.

### SQS FIFO Queue Configuration

Create two FIFO SQS queues named "payment-processing.fifo" and "payment-processing-dlq.fifo" with server-side encryption using the SQS KMS key. The main processing queue must configure content_based_deduplication set to false for message group ID based deduplication, visibility_timeout of 360 seconds which is six times the Lambda timeout, and message_retention_seconds of 345600 for four days. Configure the dead letter queue with fourteen-day retention as explicitly required and set up the main queue's redrive policy with maxReceiveCount of three directing failed messages to the DLQ. Both queues need deduplication_scope set to messageGroup and fifo_throughput_limit set to perMessageGroupId for high-throughput processing within message groups.

### Lambda Processing Functions

Create two Lambda functions in Python 3.11 for payment processing and dead letter queue handling with 256 MB memory. The primary payment processor requires 60-second timeout with environment variables for the DynamoDB table name, SQS queue URL, and fraud detection API configuration. Configure vpc_config block placing the Lambda in all three private subnets with a security group allowing outbound HTTPS to VPC endpoints for SQS, DynamoDB, CloudWatch Logs, and SNS access. Set up an event source mapping connecting the main SQS FIFO queue to this Lambda with batch_size of ten, maximum_batching_window_in_seconds of five, and reserved concurrent executions of fifty to limit parallel processing and prevent overwhelming downstream fraud detection APIs. The second Lambda handles dead letter queue processing with manual invocation capability using 120-second timeout and the same VPC configuration for investigating failed transactions. Package both functions using archive_file data source referencing lib/lambda_payment_processor.py and lib/lambda_dlq_processor.py, create dedicated CloudWatch log groups with one-day retention and KMS encryption, and add explicit depends_on to IAM role and policy attachments for eventual consistency handling.

### DynamoDB Transaction State Storage

Set up a DynamoDB table named "payment-status-dev" with transaction_id as partition key of string type and on-demand billing mode for automatic scaling. Enable point_in_time_recovery for transaction data protection and configure server_side_encryption using the DynamoDB KMS key. Add a global secondary index named "status-timestamp-index" with payment_status as partition key and timestamp as sort key to support queries by transaction status for monitoring dashboards. Set deletion_protection_enabled to false for testing teardown and configure with standard table class for production workloads.

### CloudWatch Monitoring and Dashboards

Create four CloudWatch metric alarms monitoring critical system health. The first alarm triggers when the main SQS queue ApproximateNumberOfMessagesVisible exceeds 1000 for two consecutive five-minute periods indicating processing backlog. The second alarm monitors Lambda error rate above one percent calculated using Errors metric divided by Invocations over five minutes. The third alarm detects any messages in the dead letter queue using ApproximateNumberOfMessagesVisible greater than zero requiring immediate investigation. The fourth alarm tracks Lambda throttling when ConcurrentExecutions approaches the reserved limit of fifty. All alarms publish to the SNS topic for operations alerting. Build a CloudWatch dashboard named "payment-processing-monitoring-dev" with widgets displaying SQS metrics including NumberOfMessagesSent, NumberOfMessagesReceived, ApproximateAgeOfOldestMessage, and ApproximateNumberOfMessagesVisible for both queues. Add Lambda widgets for Invocations, Errors, Duration, ConcurrentExecutions, and Throttles. Include DynamoDB ConsumedReadCapacityUnits and ConsumedWriteCapacityUnits with custom metric widgets for transaction processing throughput and status distribution. Create log groups for both Lambda functions, VPC Flow Logs, and CloudTrail with one-day retention for testing and KMS encryption.

### SNS Alerting Configuration

Set up an SNS topic named "sns-payment-alerts-dev" with KMS encryption using the SQS encryption key and configure an email subscription using a test email address provided via variables. The topic policy must allow CloudWatch alarms to publish notifications with service principal cloudwatch.amazonaws.com and allow publishing from specific alarm ARNs. Add display_name for clearer email notifications and configure delivery status logging for troubleshooting subscription issues.

### Security Groups and Network ACLs

Create two security groups with separate aws_security_group_rule resources to avoid circular dependencies. The first security group "vpc-endpoints-dev" allows inbound TCP traffic on port 443 from VPC CIDR 10.0.0.0/16 for interface endpoints and outbound TCP on port 443 to VPC CIDR for responses. The second security group "lambda-functions-dev" allows outbound TCP on port 443 to VPC CIDR for accessing VPC endpoints with no inbound rules since Lambda doesn't accept incoming connections. Use explicit protocol tcp, from_port 443, to_port 443, and cidr_blocks in each rule resource.

### IAM Roles and Policies

Create four IAM roles following least privilege with explicit resource ARNs. The payment processor Lambda role needs SQS ReceiveMessage, DeleteMessage, and GetQueueAttributes on the main FIFO queue ARN, DynamoDB PutItem, UpdateItem, and GetItem on the payment-status table ARN, KMS Decrypt and GenerateDataKey on all three KMS key ARNs, CloudWatch Logs CreateLogStream and PutLogEvents on the specific log group ARN, and EC2 network interface permissions for VPC Lambda including CreateNetworkInterface, DescribeNetworkInterfaces, and DeleteNetworkInterface. The DLQ processor Lambda role requires SQS permissions on the DLQ ARN, DynamoDB Query and Scan permissions, KMS and CloudWatch Logs access, and VPC network interface permissions. The VPC Flow Logs role needs CloudWatch Logs CreateLogGroup, CreateLogStream, PutLogEvents, and DescribeLogGroups for the flow logs log group. The CloudTrail role needs S3 PutObject permission on the audit bucket with a condition for encrypted uploads only. Define all policies using aws_iam_policy_document data sources with specific resource ARNs and add explicit depends_on chains ensuring roles exist before policy attachments and both exist before creating Lambda functions or CloudTrail.

## Provider Configuration

Configure Terraform 1.5 or higher with AWS provider version constrained to 5.x using pessimistic operator (~> 5.0). Include random provider for generating identifiers and archive provider for Lambda function packaging. Deploy all resources to ap-southeast-1 with default_tags applying Environment set to "dev", DataClassification set to "sensitive", Compliance set to "pci-dss", Owner set to "platform-team", and ManagedBy set to "terraform" automatically across all resources. Define variables with type constraints for environment with default "dev", alert_email for SNS subscription with type string, lambda_memory with default 256, sqs_visibility_timeout with default 360, and dlq_retention_days with default 14.

## Resource Naming

Follow the deterministic naming pattern {resource-type}-{purpose}-{environment} for all resources like "lambda-payment-processor-dev" or "dynamodb-payment-status-dev". SQS FIFO queues must end with the .fifo suffix as required by AWS like "payment-processing.fifo". S3 buckets need AWS account ID appended for global uniqueness like "s3-audit-logs-dev-ACCOUNT_ID". CloudWatch alarms use descriptive names like "alarm-sqs-queue-depth-dev" and dashboards like "payment-processing-monitoring-dev". Don't use random_string resources in naming since that causes integration test failures.

## Data Source Restrictions

Only use data.aws_caller_identity.current for account ID, data.aws_region.current for region name, data.aws_availability_zones.available for selecting three availability zones, and data.archive_file for Lambda function packaging referencing lib/lambda_payment_processor.py and lib/lambda_dlq_processor.py with source_dir or source_file pointing to Python code. Don't use data sources referencing existing infrastructure like existing VPCs, subnets, security groups, or KMS keys since we're creating all resources fresh for isolated testing environments.

## File Organization

Structure with lib/provider.tf containing Terraform required_version greater than or equal to 1.5, AWS provider configuration with version constraint ~> 5.0 and default_tags block, random and archive provider declarations, and variable definitions for environment, alert_email, lambda_memory, sqs_visibility_timeout, and dlq_retention_days with descriptions and type constraints. The lib/main.tf file contains data sources for caller identity, region, availability zones, and archive files, VPC with three private subnets and five VPC endpoints, three KMS keys with policies and aliases, S3 bucket for audit logs with versioning and lifecycle, VPC Flow Logs configuration, CloudTrail with CloudWatch Logs integration, two SQS FIFO queues with dead letter configuration, two Lambda functions with VPC configuration and event source mapping, DynamoDB table with GSI and PITR, four IAM roles with least privilege policies, four CloudWatch alarms and one dashboard, SNS topic with email subscription, two security groups with separate rule resources, and comprehensive outputs totaling 45-50 outputs. Create lib/lambda_payment_processor.py with the payment processing handler implementing exponential backoff retry logic for fraud detection API calls, transaction validation, DynamoDB state updates, and error handling. Create lib/lambda_dlq_processor.py with the dead letter queue handler for processing failed messages, logging detailed failure analysis, and alerting on patterns requiring manual intervention.

## Cleanup Configuration

Set deletion_window_in_days to seven on all three KMS keys, deletion_protection_enabled to false on the DynamoDB table, force_destroy to true on the S3 bucket, and retention_in_days to one on all CloudWatch log groups for testing environments. Configure recovery_window_in_days to zero on any Secrets Manager secrets if added. SQS queues, Lambda functions, SNS topics, IAM roles, VPC endpoints, security groups, CloudWatch alarms, CloudTrail, and VPC Flow Logs delete cleanly without special configuration. The VPC and subnets require all dependent resources deleted first which Terraform handles automatically through dependency graph analysis.

## Integration Testing Outputs

Provide comprehensive outputs including VPC ID, three private subnet IDs with availability zone information, five VPC endpoint IDs and DNS names, three KMS key IDs and ARNs with aliases, S3 bucket name and ARN, VPC Flow Logs ID and log group name, CloudTrail trail ARN and log group name, main SQS queue URL and ARN, dead letter queue URL and ARN, both Lambda function names and ARNs, Lambda log group names and ARNs, Lambda security group ID, VPC endpoint security group ID, DynamoDB table name and ARN, GSI name, four CloudWatch alarm names and ARNs, CloudWatch dashboard name, SNS topic ARN and subscription ARN, and all four IAM role names and ARNs. Mark sensitive outputs like queue URLs and database details with sensitive equals true and include detailed descriptions for each output explaining its purpose in the architecture. Tests require outputs for every resource to validate configurations with minimum 45-50 total outputs covering all infrastructure components for complete end-to-end validation.