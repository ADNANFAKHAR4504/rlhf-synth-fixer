Hey there, I'm a cloud architect working on a property management platform that handles maintenance requests for around 120,000 rental units. We need to automate vendor dispatch, tenant notifications, and compliance tracking in a way that's consistent across our dev, staging, and prod environments to ensure smooth deployments and avoid any surprises in production. Since we're all about multi-environment consistency, I want you to act as an expert Terraform engineer and create a production-ready Terraform configuration in a single file called tap_stack.tf. This should enforce identical topology across environments—same resource types and counts everywhere—while allowing capacities like DynamoDB RCUs, Lambda memory, or Redis nodes to vary based on environment-specific tfvars files.
The goal is to build a robust AWS infrastructure for our workflow pipeline. It starts with tenants submitting maintenance requests via a mobile app to an API Gateway REST API at the /request endpoint. A Lambda validator checks if the request is complete, then writes it to a DynamoDB maintenance_requests table with status 'new' and streams enabled—all encrypted with KMS for security. From there, DynamoDB streams trigger a Lambda request router that queries a vendor_availability table for matching vendors based on skills and geographic proximity, which is handled through an ElastiCache Redis geospatial index using commands like GEORADIUS. The router also pulls priority rules from a priority_matrix table to differentiate emergency vs. standard requests, then publishes the assignment to an SNS request-assigned topic.
That SNS topic fans out to two SQS queues: one for vendor notifications to send work orders, and another for tenant acknowledgments. Lambda consumers on each queue handle sending notifications via an external SMS/email API, which we'll mock with an API Gateway HTTP proxy integration. Vendors can update status via a webhook at /vendor/status on the same API Gateway, triggering a Lambda vendor status processor that updates the maintenance_requests table, logs to an Aurora PostgreSQL maintenance_audit table, and publishes to an SNS status-updates topic.
The status-updates SNS triggers a Lambda workflow controller that manages transitions—if it's 'in_progress', it starts an EventBridge schedule for check-ins; if 'completed', it kicks off a Lambda quality check using rules from a quality_rules DynamoDB table, sends a survey to the tenant via SNS, and archives data to an S3 bucket partitioned by date and property_id, with a lifecycle policy for expiration.
For compliance, an EventBridge scheduled rule runs a daily Lambda to query Aurora for overdue requests, calculate penalties from a penalty_rates DynamoDB table, update vendor_scores in DynamoDB, and publish violations to an SNS compliance-alerts topic. That triggers a Lambda to generate PDF reports via an external API (again, proxied through API Gateway) and store them in an S3 compliance_reports bucket with retention policies.
On top of that, for critical emergencies detected via DynamoDB streams with a filter pattern for 'emergency' status, an EventBridge rule triggers a Step Functions state machine. This orchestrates escalation: a Lambda tries assigning to the primary vendor with a timeout, fails over to the next via another Lambda query, runs a parallel branch to notify the property manager via SNS, and finally updates the request priority in DynamoDB. Use a templatefile for the state machine definition, including Parallel states, Wait with timestamps, and Retry/Catch for failures.
There's also an hourly EventBridge-scheduled Lambda that updates Redis with vendor availability zones and workload counts by querying the Aurora vendor_assignments table for our analytics dashboard.
Everything runs in a VPC with public and private subnets, NAT gateways, and security groups—place VPC resources like Lambda functions for Aurora and Redis access accordingly, and include VPC endpoints for DynamoDB, S3, SNS, SQS, and Step Functions. Stick to AWS services only, no Kubernetes or ECS. Follow best practices: full encryption with KMS on all maintenance data, least-privilege IAM roles (e.g., API Gateway to invoke Lambda with API keys, Lambda to read/write DynamoDB with conditional updates, access Redis via VPC for GEOADD/GEORADIUS, connect to Aurora via Secrets Manager with pooling, publish to SNS, receive/delete from SQS, read/write S3 with tagging, start Step Functions, put events to EventBridge). API Gateway needs API key authentication, request validators, method response models, Lambda proxy integrations, and rate limiting with throttle settings. Lambdas should be in Python 3.12 with archive_file for packaging, appropriate memory and timeouts, and run in VPC for database access. SNS topics get KMS encryption, SQS queues have visibility timeouts matching Lambda timeouts, message retention, and DLQs with alarms. DynamoDB tables use specified billing modes, capacities, and stream view types. Redis supports multi-AZ if enabled, Aurora has autoscaling with min/max capacity, backup retention, and preferred windows. S3 buckets have server-side encryption and no event notifications needed.
Monitor it all with CloudWatch: alarms for API Gateway error rates, Lambda p95 durations (especially the assignment function), DynamoDB conditional check failures for concurrent assignments, Redis geo-query latency, Aurora connection counts, SQS message ages, Step Functions failed executions, and S3 report generation errors. Include log retention, metric filters for SLA compliance rates, and dashboards for ops metrics.
In tap_stack.tf, start with a terraform block for required_version >=1.5 and required_providers with AWS ~>5.0. Declare all variables with types, descriptions, and sane defaults where it makes sense—like env, aws_region (string, since provider config is in a separate provider.tf), project_name, owner, cost_center, common_tags; VPC details like cidr, subnet cidrs, AZs; API Gateway params like name, stage, throttle, api_key_required; DynamoDB for each table's capacities, billing, etc.; Lambda memories, timeouts, runtime; Redis node type, count, version, multi-AZ; Aurora cluster id, username, db name, instance class, capacities, backups; SNS topic names; SQS queue names, timeouts, retention; EventBridge expressions and patterns; S3 bucket names, partitions, lifecycle days; Step Functions name, timeout, retries; CloudWatch log retention, alarm thresholds.
Use locals for naming conventions, tagging, and per-env capacity maps to keep things deterministic and idempotent. Define all resources and data sources directly—no placeholders, actual aws\*\* resources. No provider blocks here, as they're in provider.tf.
At the end, include three example var files: dev.tfvars, staging.tfvars, prod.tfvars, with values that differ only in capacities, rates, etc.—like lower throttles in dev, higher in prod—but keep the topology identical, no changes to endpoints, routing logic, or rules.
Finally, add outputs for key stuff: API Gateway invoke URL and API key ID, DynamoDB table names, SNS ARNs, SQS URLs, Aurora endpoints, Redis endpoint, Step Functions ARN, Lambda ARNs, S3 buckets, EventBridge rule names, VPC/subnet/SG IDs.
Make sure the code is comprehensive, with comments on tricky parts like the vendor assignment algorithm and SLA enforcement. Deliver just the full tap_stack.tf content, ready to use.
