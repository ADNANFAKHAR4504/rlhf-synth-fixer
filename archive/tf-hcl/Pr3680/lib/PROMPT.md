A video platform needs a Terraform-defined media pipeline that can process roughly five thousand uploads a day, create multiple streaming renditions, track progress end to end, and keep data secure while scaling reliably across Availability Zones. Build this in Terraform as a single file named tap_stack.tf. Use S3 for original and output storage with KMS encryption and blocked public access. Orchestrate jobs with a Lambda function that submits AWS Elemental MediaConvert transcodes, updates a DynamoDB table to track per-asset state and renditions, and reacts to events. Wire EventBridge rules to capture MediaConvert state changes and other pipeline signals, feeding SQS queues that buffer work for the Lambda workers; include dead-letter queues for failures. Expose CloudWatch metrics, logs, and alarms for queue depth, Lambda errors/throttles, and MediaConvert job failures so the team can see progress and troubleshoot quickly. Keep IAM least-privilege for Lambda, MediaConvert, S3, DynamoDB, SQS, and EventBridge, and ensure at-rest encryption across all services with KMS. Make sure the design scales across AZs by using regional services and, where needed, subnets spread across at least two AZs for Lambda concurrency resilience. The response must be only the Terraform code for tap_stack.tf, with no explanations, no comments, and nothing extra.