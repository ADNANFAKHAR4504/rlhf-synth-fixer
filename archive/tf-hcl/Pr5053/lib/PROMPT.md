Hey, I'm setting up an event-driven file processing system for a content management platform that handles around 1,500 file uploads daily. We need an S3 bucket where users can upload files, and as soon as something lands there, it should automatically trigger a Lambda function via EventBridge to process the content. The Lambda should be Node.js 18 runtime, and it needs to store metadata about each file in DynamoDB - things like upload timestamp, file size, processing status, and the S3 key. Once processing is done, we want SNS to send notifications to our team so they know new content is ready. This whole setup needs to be cost-efficient since we're processing a decent volume daily, so we can't have resources running constantly when they're not needed.

For security, all the S3 objects need server-side encryption enabled by default - just use AES256 with AWS-managed keys, nothing fancy. The bucket should block all public access since this is internal content management stuff. I need IAM roles set up properly - the Lambda function needs a role that allows it to read from the S3 bucket, write metadata to DynamoDB, and publish messages to the SNS topic. Make sure we're following least privilege here, so the Lambda can only access what it actually needs. Don't give it broad permissions like s3:* or dynamodb:* - be specific with actions like s3:GetObject, dynamodb:PutItem, and sns:Publish. The EventBridge rule also needs permissions to invoke the Lambda function, so that'll need a resource-based policy on the Lambda side.

So for monitoring, I want CloudWatch metrics tracking the number of events processed, Lambda invocation counts, error rates, and DynamoDB write capacity usage. Set up a CloudWatch alarm that triggers if the Lambda error rate goes above 5% in a 5-minute period - that's a sign something's broken in the processing logic. Also create an alarm if DynamoDB consumed write capacity exceeds 80% of provisioned capacity, because we don't want throttling issues. All these alarms should publish to the same SNS topic we're using for notifications, so we get alerts via email. I'd also like the Lambda function to have a CloudWatch log group with a 7-day retention policy - we don't need to keep logs forever for this use case, and that'll save on costs.

The DynamoDB table should have a simple schema with a primary key based on the file's S3 key or a generated upload ID. Enable point-in-time recovery on the table since we're tracking important metadata, and use on-demand billing mode instead of provisioned capacity - that's way more cost-efficient for our 1,500 daily uploads pattern where traffic might be bursty.

For file organization, split it into two files under the lib/ directory. Put the Terraform and provider configuration blocks in lib/provider.tf - that's just the terraform block with required providers and the provider "aws" blocks with regions and default tags. Everything else goes in lib/main.tf - data sources for getting account ID and region, then variables, then locals for common_tags and reusable config, then a random_string resource for unique naming (8 chars, no special chars, lowercase only), then all the infrastructure resources organized with clear comment headers for S3, Lambda, EventBridge, SNS, DynamoDB, IAM, and CloudWatch sections, and finally all the outputs at the bottom.

Oh and make sure all resource names include the random suffix so we don't get conflicts during deployment. Use AWS provider version 5.x since we need the latest S3 resource separation patterns. Deploy everything in us-west-2 unless the region variable says otherwise.

For testing, I'll need outputs for the S3 bucket name, Lambda function ARN, SNS topic ARN, DynamoDB table name, and EventBridge rule name so I can validate the event-driven workflow end-to-end. Let me know if anything's unclear.