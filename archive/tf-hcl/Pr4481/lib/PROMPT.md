Hey, I need to set up an S3 bucket for static asset storage that'll also serve as a website endpoint. We're talking about a production setup here, so versioning needs to be enabled from the start - can't risk losing any assets if someone accidentally overwrites a file. The bucket should be configured for website hosting with index.html as the default document, and I need this to be fully functional right out of the gate.

For security, we need encryption enabled using AWS-managed keys, so just the standard SSE-S3 setup. Here's the tricky part though - most of the bucket should be completely locked down with public access blocked, but we need to allow public read access specifically for objects under the 'public/' prefix. That's where our public assets like images and CSS files will live. Oh, and we need an IAM role that EC2 instances can assume to upload objects to this bucket, so the assume role policy should be configured for the EC2 service only. We also need centralized logging enabled, which means creating a separate logging bucket to capture all access logs from the main bucket.

From a cost optimization perspective, we should set up lifecycle policies to automatically transition objects to Standard-IA storage class after 30 days since most of our assets don't get accessed frequently after the initial launch period. That'll save us a good chunk on storage costs over time. Also really important - we need to clean up incomplete multipart uploads automatically after 7 days because those can rack up charges if left hanging around. I've seen that happen before and it's not pretty on the AWS bill.

For CORS configuration, we need to allow GET requests from https://example.com with a max age of 3600 seconds for the preflight cache. This is critical for our frontend application to be able to fetch assets directly from the bucket without running into cross-origin issues. Make sure the CORS rules are explicit about the allowed origins and methods.

For file organization, just put everything in main.tf. I mean everything - the terraform and provider blocks at the top with AWS provider version 5.x specified, then data sources for getting the account ID and region, then variables for things like region and project name, then locals for common_tags and reusable config, then a random_string resource for unique naming with 8 characters lowercase only and no special chars, then the logging bucket, then the main static assets bucket with all its configurations like versioning and encryption and website setup and lifecycle rules organized with clear comment headers, then the IAM role and policies, and finally all the outputs at the bottom. Keep it all in one file, it's easier to manage for a focused infrastructure piece like this.

The bucket name absolutely has to include a random suffix for uniqueness - we deploy this across multiple environments and accounts, so hardcoding names will cause conflicts. All resources need to be tagged with Environment and Project for cost allocation tracking. Make sure you're using data sources to get the current AWS account ID rather than hardcoding it anywhere, since this needs to work in any AWS account we deploy to.

For outputs, I'll need the bucket name and the website endpoint URL so our deployment pipeline can grab those values and configure our CDN and application configs accordingly. The website endpoint should be the actual S3 website URL format, not just the bucket regional endpoint. Let me know if anything's unclear or if you need more details on any of these requirements.