AWSTemplateFormatVersion: "2010-09-09"
Description: "Automated Daily Backup System with S3, Lambda, and EventBridge - LocalStack Compatible"

Parameters:
  EnvironmentSuffix:
    Type: String
    Description: Environment suffix for resource naming (e.g., dev, staging, prod)
    Default: dev
    AllowedPattern: "^[a-zA-Z0-9]+$"
    ConstraintDescription: Must contain only alphanumeric characters

  BackupBucketName:
    Type: String
    Description: Name for the S3 backup bucket (must be globally unique)
    Default: "backup-system-prod-12345"
    AllowedPattern: "^[a-z0-9][a-z0-9-]*[a-z0-9]$"
    ConstraintDescription: Must be a valid S3 bucket name
    MinLength: 3
    MaxLength: 63

  RetentionDays:
    Type: Number
    Description: Number of days to retain backups
    Default: 30
    MinValue: 1
    MaxValue: 365

Resources:
  # S3 Backup Bucket (Simplified for LocalStack)
  BackupS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub "${BackupBucketName}-${EnvironmentSuffix}"
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldBackups
            Status: Enabled
            ExpirationInDays: !Ref RetentionDays
          - Id: AbortIncompleteMultipartUploads
            Status: Enabled
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 1
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      Tags:
        - Key: Environment
          Value: !Ref EnvironmentSuffix
        - Key: Purpose
          Value: DailyBackups
        - Key: iac-rlhf-amazon
          Value: "true"

  # IAM Role for Lambda (Simplified for LocalStack)
  BackupLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${EnvironmentSuffix}-backup-lambda-role"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: "sts:AssumeRole"
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
      Policies:
        - PolicyName: BackupS3Access
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "s3:PutObject"
                  - "s3:PutObjectAcl"
                  - "s3:GetObject"
                  - "s3:GetObjectVersion"
                Resource:
                  - !Sub "${BackupS3Bucket.Arn}/*"
              - Effect: Allow
                Action:
                  - "s3:ListBucket"
                Resource:
                  - !GetAtt BackupS3Bucket.Arn
              - Effect: Allow
                Action:
                  - "cloudwatch:PutMetricData"
                Resource: "*"
      Tags:
        - Key: Environment
          Value: !Ref EnvironmentSuffix
        - Key: Purpose
          Value: BackupLambdaExecution
        - Key: iac-rlhf-amazon
          Value: "true"

  # CloudWatch Log Group for Lambda (Simplified for LocalStack)
  BackupLambdaLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/lambda/${EnvironmentSuffix}-backup-function"
      RetentionInDays: 30
      Tags:
        - Key: Environment
          Value: !Ref EnvironmentSuffix
        - Key: Purpose
          Value: BackupLogs
        - Key: iac-rlhf-amazon
          Value: "true"

  # Lambda Function for Backups (Updated for LocalStack compatibility)
  BackupLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub "${EnvironmentSuffix}-backup-function"
      Description: "Daily backup function to upload documents to S3"
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt BackupLambdaRole.Arn
      Timeout: 900 # 15 minutes
      MemorySize: 512
      Environment:
        Variables:
          BACKUP_BUCKET: !Ref BackupS3Bucket
          ENVIRONMENT: !Ref EnvironmentSuffix
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          from datetime import datetime
          import logging
          from botocore.exceptions import ClientError

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          s3_client = boto3.client('s3')
          cloudwatch = boto3.client('cloudwatch')

          def send_metric(metric_name, value, unit='Count'):
              """Send custom metric to CloudWatch"""
              try:
                  cloudwatch.put_metric_data(
                      Namespace='BackupSystem',
                      MetricData=[
                          {
                              'MetricName': metric_name,
                              'Value': value,
                              'Unit': unit,
                              'Dimensions': [
                                  {
                                      'Name': 'Environment',
                                      'Value': os.environ['ENVIRONMENT']
                                  }
                              ]
                          }
                      ]
                  )
              except Exception as e:
                  logger.error(f"Failed to send metric: {e}")

          def generate_sample_documents(count=500):
              """Generate sample business documents for backup"""
              documents = []
              document_types = ['invoice', 'receipt', 'contract', 'report', 'memo']
              
              for i in range(count):
                  doc_type = document_types[i % len(document_types)]
                  document = {
                      'document_id': f'{doc_type}_{i:04d}',
                      'type': doc_type,
                      'content': f'Sample {doc_type} content for document {i}',
                      'created_date': datetime.utcnow().isoformat(),
                      'file_size': len(f'Sample {doc_type} content for document {i}'),
                      'checksum': f'md5_{i:04d}',
                      'metadata': {
                          'department': ['finance', 'hr', 'operations', 'sales'][i % 4],
                          'priority': ['low', 'medium', 'high'][i % 3],
                          'confidential': i % 5 == 0
                      }
                  }
                  documents.append(document)
              
              return documents

          def lambda_handler(event, context):
              """Main handler for backup processing"""
              bucket_name = os.environ['BACKUP_BUCKET']
              
              backup_date = datetime.utcnow().strftime('%Y-%m-%d')
              documents_uploaded = 0
              
              try:
                  logger.info(f"Starting backup process for {backup_date}")
                  
                  # Generate sample documents for backup (in real scenario, fetch from business systems)
                  sample_documents = generate_sample_documents(500)
                  
                  # Create daily backup manifest with business context
                  manifest = {
                      'backup_date': backup_date,
                      'backup_timestamp': datetime.utcnow().isoformat(),
                      'total_documents': len(sample_documents),
                      'backup_summary': {
                          'document_types': {},
                          'departments': {},
                          'total_size_bytes': 0
                      },
                      'documents': sample_documents
                  }
                  
                  # Calculate summary statistics
                  for doc in sample_documents:
                      doc_type = doc['type']
                      department = doc['metadata']['department']
                      
                      manifest['backup_summary']['document_types'][doc_type] = \
                          manifest['backup_summary']['document_types'].get(doc_type, 0) + 1
                      manifest['backup_summary']['departments'][department] = \
                          manifest['backup_summary']['departments'].get(department, 0) + 1
                      manifest['backup_summary']['total_size_bytes'] += doc['file_size']
                  
                  # Upload manifest to S3 (simplified for LocalStack - no KMS)
                  manifest_key = f'backups/{backup_date}/manifest.json'
                  
                  s3_client.put_object(
                      Bucket=bucket_name,
                      Key=manifest_key,
                      Body=json.dumps(manifest, indent=2),
                      ContentType='application/json',
                      Metadata={
                          'backup-date': backup_date,
                          'document-count': str(len(sample_documents)),
                          'backup-type': 'daily-business-documents',
                          'environment': os.environ['ENVIRONMENT']
                      }
                  )
                  
                  # Upload individual document samples (simulate business document backup)
                  for i, doc in enumerate(sample_documents[:10]):  # Upload first 10 as samples
                      doc_key = f'backups/{backup_date}/documents/{doc["document_id"]}.json'
                      
                      s3_client.put_object(
                          Bucket=bucket_name,
                          Key=doc_key,
                          Body=json.dumps(doc, indent=2),
                          ContentType='application/json',
                          Metadata={
                              'document-type': doc['type'],
                              'department': doc['metadata']['department'],
                              'backup-date': backup_date
                          }
                      )
                  
                  documents_uploaded = len(sample_documents)
                  logger.info(f"Successfully uploaded backup for {backup_date} with {documents_uploaded} documents")
                  
                  # Send success metrics to CloudWatch
                  send_metric('BackupSuccess', 1)
                  send_metric('DocumentsBackedUp', documents_uploaded)
                  send_metric('BackupSizeBytes', manifest['backup_summary']['total_size_bytes'], 'Bytes')
                  
                  return {
                      'statusCode': 200,
                      'body': json.dumps({
                          'message': 'Backup completed successfully',
                          'backup_date': backup_date,
                          'documents_uploaded': documents_uploaded,
                          'total_size_bytes': manifest['backup_summary']['total_size_bytes'],
                          'manifest_key': manifest_key
                      })
                  }
                  
              except ClientError as e:
                  logger.error(f"AWS Client Error during backup: {e}")
                  send_metric('BackupFailure', 1)
                  raise e
              except Exception as e:
                  logger.error(f"Unexpected error during backup: {e}")
                  send_metric('BackupFailure', 1)
                  raise e

      Tags:
        - Key: Environment
          Value: !Ref EnvironmentSuffix
        - Key: Purpose
          Value: DailyBackupProcessor
        - Key: iac-rlhf-amazon
          Value: "true"

  # EventBridge Rule for Daily Trigger
  DailyBackupRule:
    Type: AWS::Events::Rule
    Properties:
      Name: !Sub "${EnvironmentSuffix}-daily-backup-trigger"
      Description: "Triggers daily backup at 2 AM UTC"
      ScheduleExpression: "cron(0 2 * * ? *)" # Daily at 2 AM UTC
      State: ENABLED
      Targets:
        - Arn: !GetAtt BackupLambdaFunction.Arn
          Id: "1"
      Tags:
        - Key: Environment
          Value: !Ref EnvironmentSuffix
        - Key: Purpose
          Value: BackupScheduler
        - Key: iac-rlhf-amazon
          Value: "true"

  # Permission for EventBridge to invoke Lambda
  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref BackupLambdaFunction
      Action: "lambda:InvokeFunction"
      Principal: events.amazonaws.com
      SourceArn: !GetAtt DailyBackupRule.Arn

  # CloudWatch Alarms (Simplified for LocalStack)
  BackupFailureAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub "${EnvironmentSuffix}-backup-failures"
      AlarmDescription: "Alert when backup fails"
      MetricName: BackupFailure
      Namespace: BackupSystem
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      Dimensions:
        - Name: Environment
          Value: !Ref EnvironmentSuffix
      TreatMissingData: notBreaching

  LambdaDurationAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub "${EnvironmentSuffix}-backup-duration-high"
      AlarmDescription: "Alert when backup takes too long"
      MetricName: Duration
      Namespace: AWS/Lambda
      Statistic: Average
      Period: 300
      EvaluationPeriods: 2
      Threshold: 600000 # 10 minutes in milliseconds
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: FunctionName
          Value: !Ref BackupLambdaFunction
      TreatMissingData: notBreaching

Outputs:
  BackupBucketName:
    Description: "Name of the S3 backup bucket"
    Value: !Ref BackupS3Bucket
    Export:
      Name: !Sub "${AWS::StackName}-backup-bucket"

  BackupLambdaArn:
    Description: "ARN of the backup Lambda function"
    Value: !GetAtt BackupLambdaFunction.Arn
    Export:
      Name: !Sub "${AWS::StackName}-backup-lambda-arn"

  EventBridgeRuleName:
    Description: "Name of the EventBridge rule for daily backups"
    Value: !Ref DailyBackupRule
    Export:
      Name: !Sub "${AWS::StackName}-daily-backup-rule"

