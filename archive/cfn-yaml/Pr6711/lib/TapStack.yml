AWSTemplateFormatVersion: '2010-09-09'
Description: 'Financial Analytics Data Processing Pipeline - Production-grade real-time market data processing with comprehensive security, monitoring, and compliance'

Parameters:
  Environment:
    Type: String
    Default: 'production'
    AllowedValues: ['development', 'staging', 'production']
    Description: 'Environment name for resource tagging and naming'
  
  CostCenter:
    Type: String
    Description: 'Cost center code for billing and resource allocation'
    AllowedPattern: '^[A-Z]{2}-[0-9]{4}$'
    ConstraintDescription: 'Must be in format XX-0000 (e.g., FI-1001)'
    Default: 'FI-1001'
  
  DataConsumerAccountId:
    Type: String
    Description: 'AWS Account ID for cross-account data access (optional)'
    Default: ''
    AllowedPattern: '^(\d{12})?$'
    ConstraintDescription: 'Must be a valid 12-digit AWS Account ID or empty'
  
  AlertEmail:
    Type: String
    Default: 'alerts@example.com'
    Description: 'Email address for pipeline alerts'
    AllowedPattern: '[^@]+@[^@]+\.[^@]+'
    ConstraintDescription: 'Must be a valid email address'
  
  VpcCidrBlock:
    Type: String
    Description: 'CIDR block of the VPC for security group rules'
    Default: '10.0.0.0/16'
    AllowedPattern: '^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$'
    ConstraintDescription: 'Must be a valid CIDR block'
  
  KinesisShardCount:
    Type: Number
    Description: 'Number of shards for Kinesis Data Stream'
    Default: 10
    MinValue: 1
    MaxValue: 100
  
  DataRetentionDays:
    Type: Number
    Description: 'Days before transitioning data to Glacier'
    Default: 90
    MinValue: 1
    MaxValue: 365
  
Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: "Environment Configuration"
        Parameters:
          - Environment
          - CostCenter
          - AlertEmail
      - Label:
          default: "Network Configuration"
        Parameters:
          - VpcCidrBlock
      - Label:
          default: "Security Configuration"
        Parameters:
          - DataConsumerAccountId
      - Label:
          default: "Performance Configuration"
        Parameters:
          - KinesisShardCount
          - DataRetentionDays

Conditions:
  HasDataConsumerAccount: !Not [!Equals [!Ref DataConsumerAccountId, '']]
  IsProduction: !Equals [!Ref Environment, 'production']

Resources:
  # ======================
  # KMS Encryption Keys
  # ======================
  DataEncryptionKey:
    Type: AWS::KMS::Key
    Properties:
      Description: !Sub 'KMS key for encrypting financial data at rest - ${Environment}'
      KeyPolicy:
        Version: '2012-10-17'
        Statement:
          - Sid: Enable IAM User Permissions
            Effect: Allow
            Principal:
              AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'
            Action: 'kms:*'
            Resource: '*'
          - Sid: Allow services to use the key
            Effect: Allow
            Principal:
              Service:
                - s3.amazonaws.com
                - kinesis.amazonaws.com
                - glue.amazonaws.com
                - lambda.amazonaws.com
                - dynamodb.amazonaws.com
                - cloudwatch.amazonaws.com
                - sns.amazonaws.com
            Action:
              - 'kms:Decrypt'
              - 'kms:GenerateDataKey'
              - 'kms:CreateGrant'
              - 'kms:DescribeKey'
            Resource: '*'
          - !If 
            - HasDataConsumerAccount
            - 
              Sid: Allow cross-account access if configured
              Effect: Allow
              Principal:
                AWS: !Sub 'arn:aws:iam::${DataConsumerAccountId}:root'
              Action:
                - 'kms:Decrypt'
                - 'kms:DescribeKey'
                - 'kms:ListGrants'
              Resource: '*'
              Condition:
                StringEquals:
                  'kms:ViaService': 
                    - !Sub 's3.${AWS::Region}.amazonaws.com'
                    - !Sub 'dynamodb.${AWS::Region}.amazonaws.com'
            - !Ref AWS::NoValue
      EnableKeyRotation: true
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: Purpose
          Value: 'Financial Data Encryption'
        - Key: iac-rlhf-amazon
          Value: 'true'

  DataEncryptionKeyAlias:
    Type: AWS::KMS::Alias
    Properties:
      AliasName: !Sub 'alias/financial-pipeline-${Environment}-${AWS::Region}'
      TargetKeyId: !Ref DataEncryptionKey

  # ======================
  # S3 Buckets with Complete Lifecycle
  # ======================
  RawDataBucket:
    Type: AWS::S3::Bucket
    DependsOn:
      - RawDataBucketInvokePermission
      - DataValidationFunctionPermission
    Properties:
      BucketName: !Sub 'financial-raw-data-${Environment}-${AWS::AccountId}-${AWS::Region}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: 'aws:kms'
              KMSMasterKeyID: !Ref DataEncryptionKey
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: TransitionToGlacier
            Status: Enabled
            Transitions:
              - StorageClass: GLACIER
                TransitionInDays: !Ref DataRetentionDays
              - StorageClass: DEEP_ARCHIVE
                TransitionInDays: 365
          - Id: DeleteOldVersions
            Status: Enabled
            NoncurrentVersionExpirationInDays: 730
          - Id: AbortIncompleteMultipartUploads
            Status: Enabled
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      NotificationConfiguration:
        LambdaConfigurations:
          - Event: 's3:ObjectCreated:*'
            Function: !GetAtt DataValidationFunction.Arn
            Filter:
              S3Key:
                Rules:
                  - Name: prefix
                    Value: 'market-data/'
      CorsConfiguration:
        CorsRules:
          - AllowedMethods: ['GET', 'HEAD']
            AllowedOrigins: ['*']
            AllowedHeaders: ['*']
            MaxAge: 3000
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: DataClassification
          Value: 'Confidential'
        - Key: iac-rlhf-amazon
          Value: 'true'

  ProcessedDataBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'financial-processed-data-${Environment}-${AWS::AccountId}-${AWS::Region}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: 'aws:kms'
              KMSMasterKeyID: !Ref DataEncryptionKey
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: TransitionToGlacier
            Status: Enabled
            Transitions:
              - StorageClass: INTELLIGENT_TIERING
                TransitionInDays: 30
              - StorageClass: GLACIER
                TransitionInDays: !Ref DataRetentionDays
              - StorageClass: DEEP_ARCHIVE
                TransitionInDays: 365
          - Id: DeleteOldVersions
            Status: Enabled
            NoncurrentVersionExpirationInDays: 730
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: DataClassification
          Value: 'Confidential'
        - Key: iac-rlhf-amazon
          Value: 'true'

  ArchivedDataBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'financial-archived-data-${Environment}-${AWS::AccountId}-${AWS::Region}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: 'aws:kms'
              KMSMasterKeyID: !Ref DataEncryptionKey
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: GlacierDeepArchive
            Status: Enabled
            Transitions:
              - StorageClass: GLACIER
                TransitionInDays: !Ref DataRetentionDays
              - StorageClass: DEEP_ARCHIVE
                TransitionInDays: 365
          - Id: ExpireOldData
            Status: Enabled
            ExpirationInDays: 2555  # 7 years for compliance
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: DataClassification
          Value: 'Archive'
        - Key: iac-rlhf-amazon
          Value: 'true'

  # S3 Bucket Policies for Cross-Account Access
  ProcessedDataBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Condition: HasDataConsumerAccount
    Properties:
      Bucket: !Ref ProcessedDataBucket
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: CrossAccountReadAccess
            Effect: Allow
            Principal:
              AWS: !Sub 'arn:aws:iam::${DataConsumerAccountId}:root'
            Action:
              - 's3:GetObject'
              - 's3:GetObjectVersion'
              - 's3:ListBucket'
              - 's3:GetBucketLocation'
            Resource:
              - !GetAtt ProcessedDataBucket.Arn
              - !Sub '${ProcessedDataBucket.Arn}/*'
            Condition:
              StringEquals:
                's3:x-amz-server-side-encryption': 'aws:kms'
                's3:x-amz-server-side-encryption-aws-kms-key-id': !GetAtt DataEncryptionKey.Arn

  # ======================
  # Kinesis Data Stream with Enhanced Configuration
  # ======================
  MarketDataStream:
    Type: AWS::Kinesis::Stream
    Properties:
      Name: !Sub 'financial-market-stream-${Environment}-${AWS::Region}'
      ShardCount: !Ref KinesisShardCount
      RetentionPeriodHours: 168  # 7 days
      StreamEncryption:
        EncryptionType: KMS
        KeyId: !Ref DataEncryptionKey
      StreamModeDetails:
        StreamMode: PROVISIONED
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: Purpose
          Value: 'Real-time Market Data Ingestion'
        - Key: iac-rlhf-amazon
          Value: 'true'

  # ======================
  # DynamoDB Tables with Enhanced Configuration
  # ======================
  ProcessingJobTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Sub 'processing-jobs-${Environment}-${AWS::Region}'
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: JobId
          AttributeType: S
        - AttributeName: Timestamp
          AttributeType: N
        - AttributeName: Status
          AttributeType: S
        - AttributeName: DataSource
          AttributeType: S
      KeySchema:
        - AttributeName: JobId
          KeyType: HASH
        - AttributeName: Timestamp
          KeyType: RANGE
      GlobalSecondaryIndexes:
        - IndexName: StatusIndex
          KeySchema:
            - AttributeName: Status
              KeyType: HASH
            - AttributeName: Timestamp
              KeyType: RANGE
          Projection:
            ProjectionType: ALL
        - IndexName: DataSourceIndex
          KeySchema:
            - AttributeName: DataSource
              KeyType: HASH
            - AttributeName: Timestamp
              KeyType: RANGE
          Projection:
            ProjectionType: ALL
      StreamSpecification:
        StreamViewType: NEW_AND_OLD_IMAGES
      SSESpecification:
        SSEEnabled: true
        SSEType: KMS
        KMSMasterKeyId: !Ref DataEncryptionKey
      PointInTimeRecoverySpecification:
        PointInTimeRecoveryEnabled: true
      TimeToLiveSpecification:
        Enabled: true
        AttributeName: TTL
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: iac-rlhf-amazon
          Value: 'true'

  DataLineageTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Sub 'data-lineage-${Environment}-${AWS::Region}'
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: DatasetId
          AttributeType: S
        - AttributeName: ProcessingTimestamp
          AttributeType: N
        - AttributeName: SourceLocation
          AttributeType: S
        - AttributeName: TransformationType
          AttributeType: S
      KeySchema:
        - AttributeName: DatasetId
          KeyType: HASH
        - AttributeName: ProcessingTimestamp
          KeyType: RANGE
      GlobalSecondaryIndexes:
        - IndexName: SourceIndex
          KeySchema:
            - AttributeName: SourceLocation
              KeyType: HASH
            - AttributeName: ProcessingTimestamp
              KeyType: RANGE
          Projection:
            ProjectionType: ALL
        - IndexName: TransformationIndex
          KeySchema:
            - AttributeName: TransformationType
              KeyType: HASH
            - AttributeName: ProcessingTimestamp
              KeyType: RANGE
          Projection:
            ProjectionType: INCLUDE
            NonKeyAttributes:
              - DatasetId
              - SourceLocation
              - TargetLocation
      SSESpecification:
        SSEEnabled: true
        SSEType: KMS
        KMSMasterKeyId: !Ref DataEncryptionKey
      PointInTimeRecoverySpecification:
        PointInTimeRecoveryEnabled: true
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: iac-rlhf-amazon
          Value: 'true'

  # ======================
  # VPC Network 
  # ======================
  DataPipelineVpc:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: !Ref VpcCidrBlock
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
        - Key: Name
          Value: !Sub 'financial-vpc-${Environment}-${AWS::Region}'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: iac-rlhf-amazon
          Value: 'true'

  DataPipelinePrivateSubnetA:
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Select 
        - 0
        - !GetAZs ''
      CidrBlock: !Select 
        - 0
        - !Cidr 
          - !Ref VpcCidrBlock
          - 4
          - 8
      MapPublicIpOnLaunch: false
      VpcId: !Ref DataPipelineVpc
      Tags:
        - Key: Name
          Value: !Sub 'financial-subnet-a-${Environment}-${AWS::Region}'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: iac-rlhf-amazon
          Value: 'true'

  DataPipelinePrivateSubnetB:
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Select 
        - 1
        - !GetAZs ''
      CidrBlock: !Select 
        - 1
        - !Cidr 
          - !Ref VpcCidrBlock
          - 4
          - 8
      MapPublicIpOnLaunch: false
      VpcId: !Ref DataPipelineVpc
      Tags:
        - Key: Name
          Value: !Sub 'financial-subnet-b-${Environment}-${AWS::Region}'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: iac-rlhf-amazon
          Value: 'true'

  DataPipelineRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref DataPipelineVpc
      Tags:
        - Key: Name
          Value: !Sub 'financial-rt-${Environment}-${AWS::Region}'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: iac-rlhf-amazon
          Value: 'true'

  DataPipelineSubnetARouteAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref DataPipelinePrivateSubnetA
      RouteTableId: !Ref DataPipelineRouteTable

  DataPipelineSubnetBRouteAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref DataPipelinePrivateSubnetB
      RouteTableId: !Ref DataPipelineRouteTable

  # ======================
  # IAM Roles with Enhanced Permissions
  # ======================
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 'financial-lambda-role-${Environment}-${AWS::Region}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole'
      Policies:
        - PolicyName: DataProcessingPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Sid: S3Access
                Effect: Allow
                Action:
                  - 's3:GetObject'
                  - 's3:GetObjectVersion'
                  - 's3:PutObject'
                  - 's3:DeleteObject'
                Resource:
                  - !Sub 'arn:aws:s3:::financial-raw-data-${Environment}-${AWS::AccountId}-${AWS::Region}/*'
                  - !Sub 'arn:aws:s3:::financial-processed-data-${Environment}-${AWS::AccountId}-${AWS::Region}/*'
                  - !Sub 'arn:aws:s3:::glue-scripts-${Environment}-${AWS::AccountId}-${AWS::Region}/*'
              - Sid: S3ListBucket
                Effect: Allow
                Action:
                  - 's3:ListBucket'
                  - 's3:GetBucketLocation'
                Resource:
                  - !Sub 'arn:aws:s3:::financial-raw-data-${Environment}-${AWS::AccountId}-${AWS::Region}'
                  - !Sub 'arn:aws:s3:::financial-processed-data-${Environment}-${AWS::AccountId}-${AWS::Region}'
                  - !Sub 'arn:aws:s3:::glue-scripts-${Environment}-${AWS::AccountId}-${AWS::Region}'
              - Sid: DynamoDBAccess
                Effect: Allow
                Action:
                  - 'dynamodb:PutItem'
                  - 'dynamodb:GetItem'
                  - 'dynamodb:UpdateItem'
                  - 'dynamodb:Query'
                  - 'dynamodb:BatchWriteItem'
                Resource:
                  - !GetAtt ProcessingJobTable.Arn
                  - !GetAtt DataLineageTable.Arn
                  - !Sub '${ProcessingJobTable.Arn}/index/*'
                  - !Sub '${DataLineageTable.Arn}/index/*'
              - Sid: KinesisAccess
                Effect: Allow
                Action:
                  - 'kinesis:GetRecords'
                  - 'kinesis:GetShardIterator'
                  - 'kinesis:DescribeStream'
                  - 'kinesis:ListStreams'
                  - 'kinesis:ListShards'
                Resource: !GetAtt MarketDataStream.Arn
              - Sid: KMSAccess
                Effect: Allow
                Action:
                  - 'kms:Decrypt'
                  - 'kms:GenerateDataKey'
                  - 'kms:DescribeKey'
                Resource: !GetAtt DataEncryptionKey.Arn
              - Sid: SQSAccess
                Effect: Allow
                Action:
                  - 'sqs:SendMessage'
                  - 'sqs:ReceiveMessage'
                  - 'sqs:DeleteMessage'
                  - 'sqs:GetQueueAttributes'
                Resource: !GetAtt ValidationDeadLetterQueue.Arn
              - Sid: SNSPublish
                Effect: Allow
                Action:
                  - 'sns:Publish'
                Resource:
                  - !Ref PipelineAlertTopic
                  - !Ref DataQualityAlertTopic
              - Sid: CloudWatchMetrics
                Effect: Allow
                Action:
                  - 'cloudwatch:PutMetricData'
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: '*'
              - Sid: ExplicitDenyUnencryptedUploads
                Effect: Deny
                Action:
                  - 's3:PutObject'
                Resource:
                  - !Sub 'arn:aws:s3:::financial-raw-data-${Environment}-${AWS::AccountId}-${AWS::Region}/*'
                  - !Sub 'arn:aws:s3:::financial-processed-data-${Environment}-${AWS::AccountId}-${AWS::Region}/*'
                  - !Sub 'arn:aws:s3:::financial-archived-data-${Environment}-${AWS::AccountId}-${AWS::Region}/*'
                  - !Sub 'arn:aws:s3:::glue-scripts-${Environment}-${AWS::AccountId}-${AWS::Region}/*'
                Condition:
                  StringNotEquals:
                    's3:x-amz-server-side-encryption': 'aws:kms'
                    's3:x-amz-server-side-encryption-aws-kms-key-id': !GetAtt DataEncryptionKey.Arn
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: iac-rlhf-amazon
          Value: 'true'

  GlueServiceRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 'financial-glue-role-${Environment}-${AWS::Region}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole'
      Policies:
        - PolicyName: GlueDataAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Sid: S3DataAccess
                Effect: Allow
                Action:
                  - 's3:GetObject'
                  - 's3:PutObject'
                  - 's3:DeleteObject'
                Resource:
                  - !Sub '${RawDataBucket.Arn}/*'
                  - !Sub '${ProcessedDataBucket.Arn}/*'
                  - !Sub '${ArchivedDataBucket.Arn}/*'
                  - !Sub '${GlueScriptsBucket.Arn}/*'
              - Sid: S3ListAccess
                Effect: Allow
                Action:
                  - 's3:ListBucket'
                  - 's3:GetBucketLocation'
                Resource:
                  - !GetAtt RawDataBucket.Arn
                  - !GetAtt ProcessedDataBucket.Arn
                  - !GetAtt ArchivedDataBucket.Arn
                  - !GetAtt GlueScriptsBucket.Arn
              - Sid: KMSAccess
                Effect: Allow
                Action:
                  - 'kms:Decrypt'
                  - 'kms:GenerateDataKey'
                  - 'kms:CreateGrant'
                Resource: !GetAtt DataEncryptionKey.Arn
              - Sid: DynamoDBLogging
                Effect: Allow
                Action:
                  - 'dynamodb:PutItem'
                  - 'dynamodb:UpdateItem'
                Resource:
                  - !GetAtt ProcessingJobTable.Arn
                  - !GetAtt DataLineageTable.Arn
              - Sid: CloudWatchLogs
                Effect: Allow
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                  - 'cloudwatch:PutMetricData'
                Resource: '*'
              - Sid: ExplicitDenyUnencryptedS3
                Effect: Deny
                Action: 's3:PutObject'
                Resource: '*'
                Condition:
                  StringNotEquals:
                    's3:x-amz-server-side-encryption': 'aws:kms'
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: iac-rlhf-amazon
          Value: 'true'

  CrossAccountDataConsumerRole:
    Type: AWS::IAM::Role
    Condition: HasDataConsumerAccount
    Properties:
      RoleName: !Sub 'financial-data-consumer-role-${Environment}-${AWS::Region}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              AWS: !Sub 'arn:aws:iam::${DataConsumerAccountId}:root'
            Action: 'sts:AssumeRole'
            Condition:
              StringEquals:
                'sts:ExternalId': !Sub '${AWS::StackName}-${Environment}'
      Policies:
        - PolicyName: DataConsumerAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Sid: S3ReadAccess
                Effect: Allow
                Action:
                  - 's3:GetObject'
                  - 's3:GetObjectVersion'
                  - 's3:ListBucket'
                Resource:
                  - !GetAtt ProcessedDataBucket.Arn
                  - !Sub '${ProcessedDataBucket.Arn}/*'
              - Sid: KMSDecrypt
                Effect: Allow
                Action:
                  - 'kms:Decrypt'
                  - 'kms:DescribeKey'
                Resource: !GetAtt DataEncryptionKey.Arn
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: iac-rlhf-amazon
          Value: 'true'

  # ======================
  # Lambda Functions with Complete Implementation
  # ======================
  DataValidationFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'data-validation-${Environment}-${AWS::Region}'
      Runtime: python3.9
      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: 300
      MemorySize: 1024
      Environment:
        Variables:
          PROCESSING_TABLE: !Ref ProcessingJobTable
          LINEAGE_TABLE: !Ref DataLineageTable
          DLQ_URL: !Ref ValidationDeadLetterQueue
          PROCESSED_BUCKET: !Ref ProcessedDataBucket
          SNS_ALERT_TOPIC: !Ref DataQualityAlertTopic
          ENVIRONMENT: !Ref Environment
      DeadLetterConfig:
        TargetArn: !GetAtt ValidationDeadLetterQueue.Arn
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          from datetime import datetime
          import uuid
          from decimal import Decimal
          import logging
          
          # Configure logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          
          # Initialize AWS clients
          s3_client = boto3.client('s3')
          dynamodb = boto3.resource('dynamodb')
          sqs = boto3.client('sqs')
          sns = boto3.client('sns')
          cloudwatch = boto3.client('cloudwatch')
          
          def validate_market_data_schema(data):
              """Validate market data against predefined schema"""
              required_fields = ['symbol', 'price', 'volume', 'timestamp', 'exchange']
              
              # Check for required fields
              for field in required_fields:
                  if field not in data:
                      return False, f"Missing required field: {field}"
              
              # Validate data types and ranges
              if not isinstance(data['price'], (int, float)) or data['price'] <= 0:
                  return False, "Invalid price value"
              
              if not isinstance(data['volume'], (int, float)) or data['volume'] < 0:
                  return False, "Invalid volume value"
              
              # Validate exchange code
              valid_exchanges = ['NYSE', 'NASDAQ', 'LSE', 'TSE', 'HKSE', 'SSE', 'NSE']
              if data['exchange'] not in valid_exchanges:
                  return False, f"Invalid exchange: {data['exchange']}"
              
              # Validate timestamp format
              try:
                  if isinstance(data['timestamp'], str):
                      datetime.fromisoformat(data['timestamp'])
              except:
                  return False, "Invalid timestamp format"
              
              return True, "Valid"
          
          def send_alert(subject, message):
              """Send alert via SNS"""
              try:
                  sns.publish(
                      TopicArn=os.environ['SNS_ALERT_TOPIC'],
                      Subject=subject,
                      Message=message
                  )
              except Exception as e:
                  logger.error(f"Failed to send alert: {str(e)}")
          
          def handler(event, context):
              processing_table = dynamodb.Table(os.environ['PROCESSING_TABLE'])
              lineage_table = dynamodb.Table(os.environ['LINEAGE_TABLE'])
              
              batch_results = {'success': 0, 'failure': 0}
              
              for record in event['Records']:
                  bucket = record['s3']['bucket']['name']
                  key = record['s3']['object']['key']
                  job_id = str(uuid.uuid4())
                  
                  try:
                      # Get object from S3
                      response = s3_client.get_object(Bucket=bucket, Key=key)
                      data = json.loads(response['Body'].read())
                      
                      # Support batch processing
                      if isinstance(data, list):
                          for item in data:
                              is_valid, message = validate_market_data_schema(item)
                              if is_valid:
                                  batch_results['success'] += 1
                              else:
                                  batch_results['failure'] += 1
                      else:
                          is_valid, message = validate_market_data_schema(data)
                          if is_valid:
                              batch_results['success'] += 1
                          else:
                              batch_results['failure'] += 1
                      
                      # Record processing job
                      processing_table.put_item(
                          Item={
                              'JobId': job_id,
                              'Timestamp': Decimal(str(datetime.now().timestamp())),
                              'Status': 'SUCCESS' if batch_results['failure'] == 0 else 'PARTIAL',
                              'SourceFile': f"s3://{bucket}/{key}",
                              'Message': f"Processed {batch_results['success']} success, {batch_results['failure']} failures",
                              'DataSource': data.get('exchange', 'UNKNOWN') if not isinstance(data, list) else 'BATCH',
                              'TTL': int((datetime.now().timestamp() + 2592000))  # 30 days TTL
                          }
                      )
                      
                      # Update lineage
                      lineage_table.put_item(
                          Item={
                              'DatasetId': job_id,
                              'ProcessingTimestamp': Decimal(str(datetime.now().timestamp())),
                              'SourceLocation': f"s3://{bucket}/{key}",
                              'TargetLocation': f"s3://{os.environ['PROCESSED_BUCKET']}/validated/{key}",
                              'TransformationType': 'VALIDATION',
                              'RecordsProcessed': batch_results['success'] + batch_results['failure']
                          }
                      )
                      
                      # Send metrics
                      cloudwatch.put_metric_data(
                          Namespace='FinancialPipeline',
                          MetricData=[
                              {
                                  'MetricName': 'ValidatedRecords',
                                  'Value': batch_results['success'],
                                  'Unit': 'Count',
                                  'Dimensions': [
                                      {'Name': 'Environment', 'Value': os.environ['ENVIRONMENT']}
                                  ]
                              },
                              {
                                  'MetricName': 'ValidationFailures',
                                  'Value': batch_results['failure'],
                                  'Unit': 'Count',
                                  'Dimensions': [
                                      {'Name': 'Environment', 'Value': os.environ['ENVIRONMENT']}
                                  ]
                              }
                          ]
                      )
                      
                      # Send alert if failure rate is high
                      if batch_results['failure'] > 0:
                          failure_rate = batch_results['failure'] / (batch_results['success'] + batch_results['failure'])
                          if failure_rate > 0.1:  # More than 10% failure
                              send_alert(
                                  f"High Validation Failure Rate - {os.environ['ENVIRONMENT']}",
                                  f"File: {key}\nFailure Rate: {failure_rate:.2%}\nFailed: {batch_results['failure']}"
                              )
                      
                  except Exception as e:
                      logger.error(f"Error processing {key}: {str(e)}")
                      
                      # Log error to DynamoDB
                      processing_table.put_item(
                          Item={
                              'JobId': job_id,
                              'Timestamp': Decimal(str(datetime.now().timestamp())),
                              'Status': 'ERROR',
                              'SourceFile': f"s3://{bucket}/{key}",
                              'Message': str(e),
                              'DataSource': 'ERROR',
                              'TTL': int((datetime.now().timestamp() + 2592000))
                          }
                      )
                      
                      # Send to DLQ
                      sqs.send_message(
                          QueueUrl=os.environ['DLQ_URL'],
                          MessageBody=json.dumps({
                              'jobId': job_id,
                              'source': f"s3://{bucket}/{key}",
                              'error': str(e),
                              'timestamp': datetime.now().isoformat()
                          })
                      )
                      
                      raise
              
              return {
                  'statusCode': 200,
                  'body': json.dumps({
                      'message': 'Processing complete',
                      'results': batch_results
                  })
              }
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: iac-rlhf-amazon
          Value: 'true'

  RawDataBucketInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref DataValidationFunction
      Action: 'lambda:InvokeFunction'
      Principal: s3.amazonaws.com
      SourceAccount: !Ref AWS::AccountId
      SourceArn: !Sub 'arn:aws:s3:::financial-raw-data-${Environment}-${AWS::AccountId}-${AWS::Region}'

  # Lambda permission for S3 to invoke
  DataValidationFunctionPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt DataValidationFunction.Arn
      Action: 'lambda:InvokeFunction'
      Principal: s3.amazonaws.com
      SourceAccount: !Ref AWS::AccountId
      SourceArn: !Sub 'arn:aws:s3:::financial-raw-data-${Environment}-${AWS::AccountId}-${AWS::Region}'

  # Enhanced Kinesis Consumer Lambda
  KinesisConsumerFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'kinesis-consumer-${Environment}-${AWS::Region}'
      Runtime: python3.9
      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: 60
      MemorySize: 512
      Environment:
        Variables:
          RAW_BUCKET: !Ref RawDataBucket
          PROCESSING_TABLE: !Ref ProcessingJobTable
          LINEAGE_TABLE: !Ref DataLineageTable
          ENVIRONMENT: !Ref Environment
      Code:
        ZipFile: |
          import json
          import boto3
          import base64
          import os
          from datetime import datetime
          import uuid
          from decimal import Decimal
          import logging
          
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          
          s3_client = boto3.client('s3')
          dynamodb = boto3.resource('dynamodb')
          cloudwatch = boto3.client('cloudwatch')
          
          def handler(event, context):
              processing_table = dynamodb.Table(os.environ['PROCESSING_TABLE'])
              lineage_table = dynamodb.Table(os.environ['LINEAGE_TABLE'])
              
              records_processed = 0
              batch_size = len(event['Records'])
              
              for record in event['Records']:
                  try:
                      # Decode Kinesis data
                      payload = base64.b64decode(record['kinesis']['data']).decode('utf-8')
                      data = json.loads(payload)
                      
                      # Generate S3 key with partitioning by exchange and date
                      now = datetime.now()
                      exchange = data.get('exchange', 'unknown').lower()
                      partition_key = f"market-data/exchange={exchange}/year={now.year}/month={now.month:02d}/day={now.day:02d}/hour={now.hour:02d}"
                      file_name = f"{record['kinesis']['sequenceNumber']}_{uuid.uuid4()}.json"
                      s3_key = f"{partition_key}/{file_name}"
                      
                      # Write to S3 with server-side encryption
                      s3_client.put_object(
                          Bucket=os.environ['RAW_BUCKET'],
                          Key=s3_key,
                          Body=json.dumps(data),
                          ServerSideEncryption='aws:kms',
                          Metadata={
                              'source': 'kinesis',
                              'stream': record['eventSourceARN'].split('/')[-1],
                              'sequence': record['kinesis']['sequenceNumber']
                          }
                      )
                      
                      job_id = str(uuid.uuid4())
                      
                      # Log to processing table
                      processing_table.put_item(
                          Item={
                              'JobId': job_id,
                              'Timestamp': Decimal(str(now.timestamp())),
                              'Status': 'INGESTED',
                              'SourceFile': f"kinesis://{record['eventSourceARN']}/{record['kinesis']['sequenceNumber']}",
                              'Message': f"Written to s3://{os.environ['RAW_BUCKET']}/{s3_key}",
                              'DataSource': exchange.upper(),
                              'TTL': int(now.timestamp() + 2592000)
                          }
                      )
                      
                      # Update lineage
                      lineage_table.put_item(
                          Item={
                              'DatasetId': job_id,
                              'ProcessingTimestamp': Decimal(str(now.timestamp())),
                              'SourceLocation': f"kinesis://{record['kinesis']['sequenceNumber']}",
                              'TargetLocation': f"s3://{os.environ['RAW_BUCKET']}/{s3_key}",
                              'TransformationType': 'INGESTION',
                              'RecordsProcessed': 1
                          }
                      )
                      
                      records_processed += 1
                      
                  except Exception as e:
                      logger.error(f"Error processing Kinesis record: {str(e)}")
                      # Continue processing other records
                      continue
              
              # Send batch metrics
              cloudwatch.put_metric_data(
                  Namespace='FinancialPipeline',
                  MetricData=[
                      {
                          'MetricName': 'KinesisRecordsProcessed',
                          'Value': records_processed,
                          'Unit': 'Count',
                          'Dimensions': [
                              {'Name': 'Environment', 'Value': os.environ['ENVIRONMENT']}
                          ]
                      },
                      {
                          'MetricName': 'KinesisProcessingSuccess',
                          'Value': (records_processed / batch_size) * 100 if batch_size > 0 else 0,
                          'Unit': 'Percent',
                          'Dimensions': [
                              {'Name': 'Environment', 'Value': os.environ['ENVIRONMENT']}
                          ]
                      }
                  ]
              )
              
              logger.info(f"Processed {records_processed}/{batch_size} records")
              
              return {
                  'statusCode': 200,
                  'batchItemFailures': []  # Return failed items for retry
              }
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: iac-rlhf-amazon
          Value: 'true'

  # Event Source Mapping for Kinesis with Enhanced Configuration
  KinesisEventSourceMapping:
    Type: AWS::Lambda::EventSourceMapping
    Properties:
      EventSourceArn: !GetAtt MarketDataStream.Arn
      FunctionName: !GetAtt KinesisConsumerFunction.Arn
      StartingPosition: LATEST
      MaximumBatchingWindowInSeconds: 10
      ParallelizationFactor: 1
      MaximumRecordAgeInSeconds: 3600
      BisectBatchOnFunctionError: true
      MaximumRetryAttempts: 3
      TumblingWindowInSeconds: 60
      FunctionResponseTypes:
        - ReportBatchItemFailures

  # ======================
  # SQS Dead Letter Queue
  # ======================
  ValidationDeadLetterQueue:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: !Sub 'validation-dlq-${Environment}-${AWS::Region}'
      MessageRetentionPeriod: 1209600  # 14 days
      KmsMasterKeyId: !Ref DataEncryptionKey
      RedriveAllowPolicy:
        redrivePermission: allowAll
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: Purpose
          Value: 'Failed Data Validation'
        - Key: iac-rlhf-amazon
          Value: 'true'

  # ======================
  # Glue Resources 
  # ======================
  GlueDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: !Sub 'financial_data_${Environment}'
        Description: 'Financial market data catalog for analytics and reporting'

  RawDataCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Sub 'raw-data-crawler-${Environment}-${AWS::Region}'
      Role: !GetAtt GlueServiceRole.Arn
      DatabaseName: !Ref GlueDatabase
      Targets:
        S3Targets:
          - Path: !Sub 's3://${RawDataBucket}/market-data/'
            Exclusions:
              - '**/_temporary/**'
              - '**/.sparkStaging/**'
      SchemaChangePolicy:
        UpdateBehavior: LOG
        DeleteBehavior: LOG
      RecrawlPolicy:
        RecrawlBehavior: CRAWL_NEW_FOLDERS_ONLY
      Schedule:
        ScheduleExpression: 'cron(0 * * * ? *)'
      Configuration: |
        {
          "Version": 1.0,
          "CrawlerOutput": {
            "Partitions": {"AddOrUpdateBehavior": "InheritFromTable"}
          }
        }
      Tags:
        Environment: !Ref Environment
        CostCenter: !Ref CostCenter
        'iac-rlhf-amazon': 'true'

  ProcessedDataCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Sub 'processed-data-crawler-${Environment}-${AWS::Region}'
      Role: !GetAtt GlueServiceRole.Arn
      DatabaseName: !Ref GlueDatabase
      Targets:
        S3Targets:
          - Path: !Sub 's3://${ProcessedDataBucket}/parquet/'
      LakeFormationConfiguration:
        UseLakeFormationCredentials: false
      SchemaChangePolicy:
        UpdateBehavior: LOG
        DeleteBehavior: LOG
      RecrawlPolicy:
        RecrawlBehavior: CRAWL_NEW_FOLDERS_ONLY
      Schedule:
        ScheduleExpression: 'cron(0 * * * ? *)'
      Tags:
        Environment: !Ref Environment
        CostCenter: !Ref CostCenter
        'iac-rlhf-amazon': 'true'

  # Glue Scripts Bucket
  GlueScriptsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'glue-scripts-${Environment}-${AWS::AccountId}-${AWS::Region}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: 'aws:kms'
              KMSMasterKeyID: !Ref DataEncryptionKey
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: iac-rlhf-amazon
          Value: 'true'

  # Custom resource to upload Glue script
  GlueScriptUploader:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'glue-script-uploader-${Environment}-${AWS::Region}'
      Runtime: python3.9
      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: 60
      Code:
        ZipFile: |
          import json
          import boto3
          import cfnresponse
          
          s3 = boto3.client('s3')
          
          def handler(event, context):
              try:
                  if event['RequestType'] == 'Create' or event['RequestType'] == 'Update':
                      bucket = event['ResourceProperties']['Bucket']
                      key = event['ResourceProperties']['Key']
                      script_content = event['ResourceProperties']['ScriptContent']
                      
                      s3.put_object(
                          Bucket=bucket,
                          Key=key,
                          Body=script_content.encode('utf-8'),
                          ServerSideEncryption='aws:kms'
                      )
                      
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {'Message': 'Script uploaded successfully'})
                  elif event['RequestType'] == 'Delete':
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {'Message': 'Delete complete'})
              except Exception as e:
                  print(f"Error: {str(e)}")
                  cfnresponse.send(event, context, cfnresponse.FAILED, {'Error': str(e)})
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: iac-rlhf-amazon
          Value: 'true'

  GlueScriptUpload:
    Type: Custom::GlueScriptUpload
    Properties:
      ServiceToken: !GetAtt GlueScriptUploader.Arn
      Bucket: !Ref GlueScriptsBucket
      Key: 'scripts/json_to_parquet.py'
      ScriptContent: |
        import sys
        from awsglue.transforms import *
        from awsglue.utils import getResolvedOptions
        from pyspark.context import SparkContext
        from awsglue.context import GlueContext
        from awsglue.job import Job
        from awsglue.dynamicframe import DynamicFrame
        from datetime import datetime
        import boto3
        
        # Get job parameters
        args = getResolvedOptions(sys.argv, [
            'JOB_NAME',
            'source_database',
            'source_table',
            'target_path',
            'processing_table',
            'lineage_table'
        ])
        
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
        
        # Initialize DynamoDB client for logging
        dynamodb = boto3.resource('dynamodb')
        processing_table = dynamodb.Table(args.get('processing_table', 'processing-jobs'))
        lineage_table = dynamodb.Table(args.get('lineage_table', 'data-lineage'))
        
        try:
            # Read from Glue Catalog
            datasource = glueContext.create_dynamic_frame.from_catalog(
                database=args['source_database'],
                table_name=args['source_table'],
                transformation_ctx="datasource"
            )
            
            # Apply mappings for financial data schema
            mapped_df = ApplyMapping.apply(
                frame=datasource,
                mappings=[
                    ("symbol", "string", "symbol", "string"),
                    ("price", "double", "price", "decimal(10,2)"),
                    ("volume", "long", "volume", "bigint"),
                    ("timestamp", "string", "timestamp", "timestamp"),
                    ("exchange", "string", "exchange", "string"),
                    ("bid", "double", "bid", "decimal(10,2)"),
                    ("ask", "double", "ask", "decimal(10,2)"),
                    ("high", "double", "high", "decimal(10,2)"),
                    ("low", "double", "low", "decimal(10,2)"),
                    ("open", "double", "open", "decimal(10,2)"),
                    ("close", "double", "close", "decimal(10,2)")
                ],
                transformation_ctx="mapped_df"
            )
            
            # Convert to DataFrame for additional transformations
            df = mapped_df.toDF()
            
            # Add processing metadata
            from pyspark.sql.functions import lit, current_timestamp
            df_with_metadata = df.withColumn("processing_timestamp", current_timestamp()) \
                                 .withColumn("processing_date", lit(datetime.now().strftime("%Y-%m-%d")))
            
            # Repartition for optimal file sizes (aim for ~128MB files)
            record_count = df_with_metadata.count()
            num_partitions = max(1, int(record_count / 100000))  # ~100k records per partition
            df_partitioned = df_with_metadata.repartition(num_partitions)
            
            # Convert back to DynamicFrame
            final_df = DynamicFrame.fromDF(df_partitioned, glueContext, "final_df")
            
            # Write to S3 in Parquet format with partitioning
            glueContext.write_dynamic_frame.from_options(
                frame=final_df,
                connection_type="s3",
                connection_options={
                    "path": args['target_path'],
                    "partitionKeys": ["exchange", "processing_date"]
                },
                format="parquet",
                format_options={
                    "compression": "snappy"
                },
                transformation_ctx="write_parquet"
            )
            
            # Log successful processing
            job_id = args['JOB_NAME'] + "_" + datetime.now().strftime("%Y%m%d%H%M%S")
            processing_table.put_item(
                Item={
                    'JobId': job_id,
                    'Timestamp': int(datetime.now().timestamp()),
                    'Status': 'SUCCESS',
                    'SourceFile': f"{args['source_database']}.{args['source_table']}",
                    'Message': f"Processed {record_count} records to Parquet",
                    'DataSource': 'GLUE_ETL',
                    'TTL': int(datetime.now().timestamp() + 2592000)
                }
            )
            
            lineage_table.put_item(
                Item={
                    'DatasetId': job_id,
                    'ProcessingTimestamp': int(datetime.now().timestamp()),
                    'SourceLocation': f"glue://{args['source_database']}.{args['source_table']}",
                    'TargetLocation': args['target_path'],
                    'TransformationType': 'JSON_TO_PARQUET',
                    'RecordsProcessed': record_count
                }
            )
            
            print(f"Successfully processed {record_count} records")
            
        except Exception as e:
            print(f"Error in Glue job: {str(e)}")
            
            # Log error
            processing_table.put_item(
                Item={
                    'JobId': args['JOB_NAME'] + "_error_" + datetime.now().strftime("%Y%m%d%H%M%S"),
                    'Timestamp': int(datetime.now().timestamp()),
                    'Status': 'ERROR',
                    'SourceFile': f"{args['source_database']}.{args['source_table']}",
                    'Message': str(e),
                    'DataSource': 'GLUE_ETL',
                    'TTL': int(datetime.now().timestamp() + 2592000)
                }
            )
            raise
        
        job.commit()

  # ======================
  # Lake Formation Permissions
  # ======================
  JsonToParquetJob:
    Type: AWS::Glue::Job
    Properties:
      Name: !Sub 'json-to-parquet-${Environment}-${AWS::Region}'
      Role: !GetAtt GlueServiceRole.Arn
      Command:
        Name: glueetl
        ScriptLocation: !Sub 's3://${GlueScriptsBucket}/scripts/json_to_parquet.py'
        PythonVersion: '3'
      DefaultArguments:
        '--job-bookmark-option': 'job-bookmark-enable'
        '--enable-metrics': ''
        '--enable-continuous-cloudwatch-log': 'true'
        '--enable-spark-ui': 'true'
        '--spark-event-logs-path': !Sub 's3://${GlueScriptsBucket}/spark-logs/'
        '--TempDir': !Sub 's3://${GlueScriptsBucket}/temp/'
        '--source_database': !Ref GlueDatabase
        '--source_table': 'market_data'
        '--target_path': !Sub 's3://${ProcessedDataBucket}/parquet/'
        '--processing_table': !Ref ProcessingJobTable
        '--lineage_table': !Ref DataLineageTable
        '--encryption-type': 'sse-kms'
        '--kms-key-id': !Ref DataEncryptionKey
        '--enable-auto-scaling': 'true'
      ExecutionProperty:
        MaxConcurrentRuns: 2
      GlueVersion: '3.0'
      MaxRetries: 1
      Timeout: 2880  # 48 hours
      NumberOfWorkers: 10
      WorkerType: G.2X
      Tags:
        Environment: !Ref Environment
        CostCenter: !Ref CostCenter
        'iac-rlhf-amazon': 'true'

  # Glue Trigger for scheduled ETL
  GlueETLTrigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Sub 'json-to-parquet-trigger-${Environment}'
      Type: SCHEDULED
      Schedule: 'rate(1 hour)'  # Run every hour
      Actions:
        - JobName: !Ref JsonToParquetJob
          Arguments:
            '--job-bookmark-option': 'job-bookmark-enable'
      StartOnCreation: !If [IsProduction, true, false]
      Tags:
        Environment: !Ref Environment
        CostCenter: !Ref CostCenter
        'iac-rlhf-amazon': 'true'

  # ======================
  # VPC Endpoints with Complete Configuration
  # ======================
  S3VPCEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcId: !Ref DataPipelineVpc
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.s3'
      VpcEndpointType: Gateway
      RouteTableIds:
        - !Ref DataPipelineRouteTable

  DynamoDBVPCEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcId: !Ref DataPipelineVpc
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.dynamodb'
      VpcEndpointType: Gateway
      RouteTableIds:
        - !Ref DataPipelineRouteTable

  KinesisVPCEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcId: !Ref DataPipelineVpc
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.kinesis-streams'
      VpcEndpointType: Interface
      SubnetIds:
        - !Ref DataPipelinePrivateSubnetA
        - !Ref DataPipelinePrivateSubnetB
      SecurityGroupIds:
        - !Ref VPCEndpointSecurityGroup
      PrivateDnsEnabled: true

  GlueVPCEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcId: !Ref DataPipelineVpc
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.glue'
      VpcEndpointType: Interface
      SubnetIds:
        - !Ref DataPipelinePrivateSubnetA
        - !Ref DataPipelinePrivateSubnetB
      SecurityGroupIds:
        - !Ref VPCEndpointSecurityGroup
      PrivateDnsEnabled: true

  LambdaVPCEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcId: !Ref DataPipelineVpc
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.lambda'
      VpcEndpointType: Interface
      SubnetIds:
        - !Ref DataPipelinePrivateSubnetA
        - !Ref DataPipelinePrivateSubnetB
      SecurityGroupIds:
        - !Ref VPCEndpointSecurityGroup
      PrivateDnsEnabled: true

  VPCEndpointSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub 'vpc-endpoints-sg-${Environment}-${AWS::Region}'
      GroupDescription: Security group for VPC endpoints with HTTPS access
      VpcId: !Ref DataPipelineVpc
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 443
          ToPort: 443
          CidrIp: !Ref VpcCidrBlock
          Description: 'HTTPS access from VPC'
      Tags:
        - Key: Name
          Value: !Sub 'vpc-endpoints-sg-${Environment}'
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: iac-rlhf-amazon
          Value: 'true'

  # ======================
  # SNS Topics with Enhanced Configuration
  # ======================
  PipelineAlertTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Sub 'pipeline-alerts-${Environment}-${AWS::Region}'
      DisplayName: !Sub 'Financial Pipeline Alerts - ${Environment}'
      KmsMasterKeyId: !Ref DataEncryptionKey
      Subscription:
        - Endpoint: !Ref AlertEmail
          Protocol: email
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: iac-rlhf-amazon
          Value: 'true'

  DataQualityAlertTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Sub 'data-quality-alerts-${Environment}-${AWS::Region}'
      DisplayName: !Sub 'Data Quality Alerts - ${Environment}'
      KmsMasterKeyId: !Ref DataEncryptionKey
      Subscription:
        - Endpoint: !Ref AlertEmail
          Protocol: email
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: CostCenter
          Value: !Ref CostCenter
        - Key: iac-rlhf-amazon
          Value: 'true'

  # ======================
  # CloudWatch Alarms with Complete Coverage
  # ======================
  KinesisShardIteratorAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub 'kinesis-iterator-age-${Environment}-${AWS::Region}'
      AlarmDescription: Alert when Kinesis iterator age exceeds threshold
      MetricName: GetRecords.IteratorAgeMilliseconds
      Namespace: AWS/Kinesis
      Statistic: Maximum
      Period: 300
      EvaluationPeriods: 2
      Threshold: 60000  # 60 seconds
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: StreamName
          Value: !Ref MarketDataStream
      AlarmActions:
        - !Ref PipelineAlertTopic
      OKActions:
        - !Ref PipelineAlertTopic
      TreatMissingData: notBreaching

  LambdaErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub 'lambda-validation-errors-${Environment}-${AWS::Region}'
      AlarmDescription: Alert on Lambda validation function errors
      MetricName: Errors
      Namespace: AWS/Lambda
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 10
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: FunctionName
          Value: !Ref DataValidationFunction
      AlarmActions:
        - !Ref PipelineAlertTopic
      TreatMissingData: notBreaching

  LambdaThrottleAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub 'lambda-throttles-${Environment}-${AWS::Region}'
      AlarmDescription: Alert on Lambda throttling
      MetricName: Throttles
      Namespace: AWS/Lambda
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 5
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: FunctionName
          Value: !Ref DataValidationFunction
      AlarmActions:
        - !Ref PipelineAlertTopic
      TreatMissingData: notBreaching

  KinesisLambdaErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub 'kinesis-consumer-errors-${Environment}-${AWS::Region}'
      AlarmDescription: Alert on Kinesis consumer Lambda errors
      MetricName: Errors
      Namespace: AWS/Lambda
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 5
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: FunctionName
          Value: !Ref KinesisConsumerFunction
      AlarmActions:
        - !Ref PipelineAlertTopic
      TreatMissingData: notBreaching

  DLQMessageAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub 'dlq-messages-${Environment}-${AWS::Region}'
      AlarmDescription: Alert when messages appear in DLQ
      MetricName: ApproximateNumberOfMessagesVisible
      Namespace: AWS/SQS
      Statistic: Maximum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: QueueName
          Value: !GetAtt ValidationDeadLetterQueue.QueueName
      AlarmActions:
        - !Ref DataQualityAlertTopic
      TreatMissingData: notBreaching

  DataQualityMetricAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub 'data-quality-failures-${Environment}-${AWS::Region}'
      AlarmDescription: Alert on high validation failure rate
      MetricName: ValidationFailures
      Namespace: FinancialPipeline
      Statistic: Sum
      Period: 900
      EvaluationPeriods: 2
      Threshold: 100
      ComparisonOperator: GreaterThanThreshold
      AlarmActions:
        - !Ref DataQualityAlertTopic
      TreatMissingData: notBreaching

  GlueJobFailureAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub 'glue-job-failures-${Environment}-${AWS::Region}'
      AlarmDescription: Alert on Glue job failures
      MetricName: glue.driver.aggregate.numFailedTasks
      Namespace: AWS/Glue
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: JobName
          Value: !Ref JsonToParquetJob
      AlarmActions:
        - !Ref PipelineAlertTopic
      TreatMissingData: notBreaching

  HighIngestionRateAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub 'high-ingestion-rate-${Environment}-${AWS::Region}'
      AlarmDescription: Alert on unusually high data ingestion rate
      MetricName: IncomingRecords
      Namespace: AWS/Kinesis
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 2
      Threshold: 1000000  # 1M records in 5 minutes
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: StreamName
          Value: !Ref MarketDataStream
      AlarmActions:
        - !Ref PipelineAlertTopic
      TreatMissingData: notBreaching

  # ======================
  # CloudWatch Dashboard with Fixed Metric Format
  # ======================
  PipelineDashboard:
    Type: AWS::CloudWatch::Dashboard
    Properties:
      DashboardName: !Sub 'financial-pipeline-${Environment}-${AWS::Region}'
      DashboardBody: !Sub |
        {
          "widgets": [
            {
              "type": "metric",
              "properties": {
                "title": "Kinesis Incoming Records",
                "metrics": [
                  ["AWS/Kinesis", "IncomingRecords", "StreamName", "${MarketDataStream}"]
                ],
                "period": 60,
                "stat": "Sum",
                "region": "${AWS::Region}",
                "view": "timeSeries"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Kinesis Incoming Bytes",
                "metrics": [
                  ["AWS/Kinesis", "IncomingBytes", "StreamName", "${MarketDataStream}"]
                ],
                "period": 60,
                "stat": "Sum",
                "region": "${AWS::Region}",
                "view": "timeSeries"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Validation Lambda Avg Duration",
                "metrics": [
                  ["AWS/Lambda", "Duration", "FunctionName", "${DataValidationFunction}"]
                ],
                "period": 300,
                "stat": "Average",
                "region": "${AWS::Region}",
                "view": "timeSeries"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Validation Lambda Max Duration",
                "metrics": [
                  ["AWS/Lambda", "Duration", "FunctionName", "${DataValidationFunction}"]
                ],
                "period": 300,
                "stat": "Maximum",
                "region": "${AWS::Region}",
                "view": "timeSeries"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Kinesis Consumer Avg Duration",
                "metrics": [
                  ["AWS/Lambda", "Duration", "FunctionName", "${KinesisConsumerFunction}"]
                ],
                "period": 300,
                "stat": "Average",
                "region": "${AWS::Region}",
                "view": "timeSeries"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Kinesis Consumer Max Duration",
                "metrics": [
                  ["AWS/Lambda", "Duration", "FunctionName", "${KinesisConsumerFunction}"]
                ],
                "period": 300,
                "stat": "Maximum",
                "region": "${AWS::Region}",
                "view": "timeSeries"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Lambda Errors",
                "metrics": [
                  ["AWS/Lambda", "Errors", "FunctionName", "${DataValidationFunction}"],
                  [".", ".", "FunctionName", "${KinesisConsumerFunction}"]
                ],
                "period": 300,
                "stat": "Sum",
                "region": "${AWS::Region}",
                "view": "timeSeries",
                "yAxis": {"left": {"min": 0}}
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Validation Failures",
                "metrics": [
                  ["FinancialPipeline", "ValidationFailures", "Environment", "${Environment}"]
                ],
                "period": 300,
                "stat": "Sum",
                "region": "${AWS::Region}",
                "view": "timeSeries",
                "yAxis": {"left": {"min": 0}}
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Dead Letter Queue",
                "metrics": [
                  ["AWS/SQS", "ApproximateNumberOfMessagesVisible", "QueueName", "${ValidationDeadLetterQueue.QueueName}"]
                ],
                "period": 300,
                "stat": "Maximum",
                "region": "${AWS::Region}",
                "view": "timeSeries",
                "yAxis": {"left": {"min": 0}}
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Raw Data Bucket Size",
                "metrics": [
                  ["AWS/S3", "BucketSizeBytes", "BucketName", "${RawDataBucket}", "StorageType", "StandardStorage"]
                ],
                "period": 86400,
                "stat": "Average",
                "region": "${AWS::Region}",
                "view": "singleValue"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Processed Data Bucket Size",
                "metrics": [
                  ["AWS/S3", "BucketSizeBytes", "BucketName", "${ProcessedDataBucket}", "StorageType", "StandardStorage"]
                ],
                "period": 86400,
                "stat": "Average",
                "region": "${AWS::Region}",
                "view": "singleValue"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Archived Data Bucket Size",
                "metrics": [
                  ["AWS/S3", "BucketSizeBytes", "BucketName", "${ArchivedDataBucket}", "StorageType", "StandardStorage"]
                ],
                "period": 86400,
                "stat": "Average",
                "region": "${AWS::Region}",
                "view": "singleValue"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Processing Job Table Read",
                "metrics": [
                  ["AWS/DynamoDB", "ConsumedReadCapacityUnits", "TableName", "${ProcessingJobTable}"]
                ],
                "period": 300,
                "stat": "Sum",
                "region": "${AWS::Region}",
                "view": "timeSeries"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Processing Job Table Write",
                "metrics": [
                  ["AWS/DynamoDB", "ConsumedWriteCapacityUnits", "TableName", "${ProcessingJobTable}"]
                ],
                "period": 300,
                "stat": "Sum",
                "region": "${AWS::Region}",
                "view": "timeSeries"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Data Lineage Table Read",
                "metrics": [
                  ["AWS/DynamoDB", "ConsumedReadCapacityUnits", "TableName", "${DataLineageTable}"]
                ],
                "period": 300,
                "stat": "Sum",
                "region": "${AWS::Region}",
                "view": "timeSeries"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Data Lineage Table Write",
                "metrics": [
                  ["AWS/DynamoDB", "ConsumedWriteCapacityUnits", "TableName", "${DataLineageTable}"]
                ],
                "period": 300,
                "stat": "Sum",
                "region": "${AWS::Region}",
                "view": "timeSeries"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Glue Completed Tasks",
                "metrics": [
                  ["AWS/Glue", "glue.driver.aggregate.numCompletedTasks", "JobName", "${JsonToParquetJob}", "JobRunId", "ALL"]
                ],
                "period": 300,
                "stat": "Sum",
                "region": "${AWS::Region}",
                "view": "timeSeries"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Glue Failed Tasks",
                "metrics": [
                  ["AWS/Glue", "glue.driver.aggregate.numFailedTasks", "JobName", "${JsonToParquetJob}", "JobRunId", "ALL"]
                ],
                "period": 300,
                "stat": "Sum",
                "region": "${AWS::Region}",
                "view": "timeSeries"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Glue Job Elapsed Time",
                "metrics": [
                  ["AWS/Glue", "glue.driver.aggregate.elapsedTime", "JobName", "${JsonToParquetJob}", "JobRunId", "ALL"]
                ],
                "period": 300,
                "stat": "Average",
                "region": "${AWS::Region}",
                "view": "timeSeries"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Validated Records",
                "metrics": [
                  ["FinancialPipeline", "ValidatedRecords", "Environment", "${Environment}"]
                ],
                "period": 300,
                "stat": "Sum",
                "region": "${AWS::Region}",
                "view": "timeSeries"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Kinesis Records Processed",
                "metrics": [
                  ["FinancialPipeline", "KinesisRecordsProcessed", "Environment", "${Environment}"]
                ],
                "period": 300,
                "stat": "Sum",
                "region": "${AWS::Region}",
                "view": "timeSeries"
              }
            },
            {
              "type": "metric",
              "properties": {
                "title": "Pipeline Health Score",
                "metrics": [
                  ["FinancialPipeline", "KinesisProcessingSuccess", "Environment", "${Environment}"]
                ],
                "period": 300,
                "stat": "Average",
                "region": "${AWS::Region}",
                "view": "singleValue"
              }
            }
          ]
        }

# ======================
# Stack Outputs
# ======================
Outputs:
  StackName:
    Description: 'CloudFormation Stack Name'
    Value: !Ref AWS::StackName
    Export:
      Name: !Sub '${AWS::StackName}-StackName'

  Region:
    Description: 'AWS Region'
    Value: !Ref AWS::Region
    Export:
      Name: !Sub '${AWS::StackName}-Region'

  KinesisStreamArn:
    Description: 'ARN of the Kinesis Data Stream for market data ingestion'
    Value: !GetAtt MarketDataStream.Arn
    Export:
      Name: !Sub '${AWS::StackName}-KinesisStreamArn'

  KinesisStreamName:
    Description: 'Name of the Kinesis Data Stream'
    Value: !Ref MarketDataStream
    Export:
      Name: !Sub '${AWS::StackName}-KinesisStreamName'

  RawDataBucketName:
    Description: 'S3 bucket for raw market data'
    Value: !Ref RawDataBucket
    Export:
      Name: !Sub '${AWS::StackName}-RawDataBucket'

  RawDataBucketArn:
    Description: 'ARN of the raw data S3 bucket'
    Value: !GetAtt RawDataBucket.Arn
    Export:
      Name: !Sub '${AWS::StackName}-RawDataBucketArn'

  ProcessedDataBucketName:
    Description: 'S3 bucket for processed data'
    Value: !Ref ProcessedDataBucket
    Export:
      Name: !Sub '${AWS::StackName}-ProcessedDataBucket'

  ProcessedDataBucketArn:
    Description: 'ARN of the processed data S3 bucket'
    Value: !GetAtt ProcessedDataBucket.Arn
    Export:
      Name: !Sub '${AWS::StackName}-ProcessedDataBucketArn'

  ArchivedDataBucketName:
    Description: 'S3 bucket for archived data'
    Value: !Ref ArchivedDataBucket
    Export:
      Name: !Sub '${AWS::StackName}-ArchivedDataBucket'

  GlueDatabaseName:
    Description: 'Glue database for data catalog'
    Value: !Ref GlueDatabase
    Export:
      Name: !Sub '${AWS::StackName}-GlueDatabase'

  GlueJobName:
    Description: 'Name of the Glue ETL job'
    Value: !Ref JsonToParquetJob
    Export:
      Name: !Sub '${AWS::StackName}-GlueJobName'

  ProcessingJobTableName:
    Description: 'DynamoDB table for processing job metadata'
    Value: !Ref ProcessingJobTable
    Export:
      Name: !Sub '${AWS::StackName}-ProcessingJobTable'

  DataLineageTableName:
    Description: 'DynamoDB table for data lineage tracking'
    Value: !Ref DataLineageTable
    Export:
      Name: !Sub '${AWS::StackName}-DataLineageTable'

  ValidationFunctionArn:
    Description: 'ARN of the data validation Lambda function'
    Value: !GetAtt DataValidationFunction.Arn
    Export:
      Name: !Sub '${AWS::StackName}-ValidationFunctionArn'

  KinesisConsumerFunctionArn:
    Description: 'ARN of the Kinesis consumer Lambda function'
    Value: !GetAtt KinesisConsumerFunction.Arn
    Export:
      Name: !Sub '${AWS::StackName}-KinesisConsumerFunctionArn'

  DeadLetterQueueUrl:
    Description: 'URL of the validation dead letter queue'
    Value: !Ref ValidationDeadLetterQueue
    Export:
      Name: !Sub '${AWS::StackName}-DLQUrl'

  DeadLetterQueueArn:
    Description: 'ARN of the validation dead letter queue'
    Value: !GetAtt ValidationDeadLetterQueue.Arn
    Export:
      Name: !Sub '${AWS::StackName}-DLQArn'

  DashboardURL:
    Description: 'CloudWatch Dashboard URL'
    Value: !Sub 'https://console.aws.amazon.com/cloudwatch/home?region=${AWS::Region}#dashboards:name=${PipelineDashboard}'
    Export:
      Name: !Sub '${AWS::StackName}-DashboardURL'

  PipelineAlertTopicArn:
    Description: 'SNS Topic ARN for pipeline alerts'
    Value: !Ref PipelineAlertTopic
    Export:
      Name: !Sub '${AWS::StackName}-PipelineAlertTopic'

  DataQualityAlertTopicArn:
    Description: 'SNS Topic ARN for data quality alerts'
    Value: !Ref DataQualityAlertTopic
    Export:
      Name: !Sub '${AWS::StackName}-DataQualityAlertTopic'

  DataEncryptionKeyArn:
    Description: 'KMS key ARN for data encryption'
    Value: !GetAtt DataEncryptionKey.Arn
    Export:
      Name: !Sub '${AWS::StackName}-KMSKeyArn'

  DataEncryptionKeyAlias:
    Description: 'KMS key alias for data encryption'
    Value: !Ref DataEncryptionKeyAlias
    Export:
      Name: !Sub '${AWS::StackName}-KMSKeyAlias'

  CrossAccountRoleArn:
    Description: 'ARN of the cross-account data consumer role (if configured)'
    Value: !If 
      - HasDataConsumerAccount
      - !GetAtt CrossAccountDataConsumerRole.Arn
      - 'Not Configured'
    Export:
      Name: !Sub '${AWS::StackName}-CrossAccountRoleArn'

  VPCEndpointSecurityGroupId:
    Description: 'Security Group ID for VPC endpoints'
    Value: !Ref VPCEndpointSecurityGroup
    Export:
      Name: !Sub '${AWS::StackName}-VPCEndpointSG'