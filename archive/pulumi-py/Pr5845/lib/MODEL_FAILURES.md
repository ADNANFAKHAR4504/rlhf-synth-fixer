# Failures

1. **API Gateway â†’ Lambda integration is brittle/invalid**  
   Integrations use `uri=func.invoke_arn` and `lambda_.Permission` builds `source_arn` by concatenating `rest_api.execution_arn + "/*/POST/" + func_name`. For AWS_PROXY integrations you must use the proper API Gateway service integration URI and construct execute-api source ARNs precisely. The current pattern is fragile and frequently fails permission checks or deployment ordering.

   Erroneous code from MODEL_RESPONSE.md lines 2254 and 2317-2320:

   ```python
   # Create integration with Lambda
   integration = aws.apigateway.Integration(
       f"{name}-{func_name}-integration",
       rest_api_id=rest_api.id,
       resource_id=resource.id,
       http_method=method.http_method,
       integration_http_method="POST",
       type="AWS_PROXY",
       uri=func.invoke_arn,  # WRONG: Should use proper Lambda integration URI format
       opts=pulumi.ResourceOptions(provider=provider)
   )

   # Allow API Gateway to invoke the Lambda function
   permission = aws.lambda_.Permission(
       f"{name}-{func_name}-permission",
       action="lambda:InvokeFunction",
       function=func.name,
       principal="apigateway.amazonaws.com",
       source_arn=pulumi.Output.concat(
           rest_api.execution_arn, "/*/POST/", func_name  # FRAGILE: Wildcard stage, missing proper path
       ),
       opts=pulumi.ResourceOptions(provider=provider)
   )
   ```

   **HOW WE FIXED IT:**

   We implemented a robust API Gateway integration using proper Lambda integration URIs and precise source ARN construction with explicit dependency management. From `lib/infrastructure/api_gateway.py` lines 99-130:

   ```python
   # Proper Lambda integration URI construction
   lambda_integration = aws.apigateway.Integration(
       'api-integration-post',
       rest_api_id=self.api.id,
       resource_id=process_resource.id,
       http_method=post_method.http_method,
       integration_http_method='POST',
       type='AWS_PROXY',
       uri=lambda_function_arn.apply(
           lambda arn: f'arn:aws:apigateway:{self.config.primary_region}:lambda:path/2015-03-31/functions/{arn}/invocations'
       ),
       opts=self.provider_manager.get_resource_options(depends_on=[post_method])
   )

   # Precise Lambda permission with proper source ARN
   lambda_permission = aws.lambda_.Permission(
       'api-lambda-permission',
       action='lambda:InvokeFunction',
       function=lambda_function_name,
       principal='apigateway.amazonaws.com',
       source_arn=Output.all(self.api.execution_arn, self.stage.stage_name, process_resource.path).apply(
           lambda args: f'{args[0]}/{args[1]}/POST{args[2]}'
       ),
       opts=self.provider_manager.get_resource_options()
   )
   ```

   This fix demonstrates significant complexity: we use Output.apply() for proper URI formatting, Output.all() for combining multiple outputs into a precise source ARN, and explicit depends_on for correct resource ordering. The integration URI follows AWS's exact format requirements, and the source ARN includes the actual stage name and resource path rather than wildcards, ensuring proper permission scoping and eliminating deployment race conditions.

2. **IAM least-privilege requirement violated (over-permissive policies)**  
   Several IAM policies (notably in the CI/CD roles) use `"Resource": ["*"]` or grant broad service actions (e.g., `lambda:*`, `iam:*`, `kms:*`). The project repeatedly attaches wide managed policies (e.g., `AWSLambdaBasicExecutionRole`) rather than narrowly scoped inline policies for specific resource ARNs.

   Erroneous code from MODEL_RESPONSE.md lines 2070-2076:

   ```python
   # Attach CloudWatch Logs permissions
   aws.iam.RolePolicyAttachment(
       f"{name}-cloudwatch-attachment",
       role=role.name,
       policy_arn="arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole",  # OVERLY BROAD: Managed policy instead of scoped inline
       opts=pulumi.ResourceOptions(provider=provider)
   )
   ```

   Additional violation from lines 2577-2617:

   ```python
   # Attach policies
   policy_document = json.dumps({
       "Version": "2012-10-17",
       "Statement": [
           {
               "Effect": "Allow",
               "Resource": ["*"],  # VIOLATION: Wildcard resource
               "Action": [
                   "logs:CreateLogGroup",
                   "logs:CreateLogStream",
                   "logs:PutLogEvents"
               ]
           },
           {
               "Effect": "Allow",
               "Resource": ["*"],  # VIOLATION: Wildcard resource
               "Action": [
                   "s3:GetObject",
                   "s3:GetObjectVersion",
                   "s3:PutObject"
               ]
           },
           {
               "Effect": "Allow",
               "Resource": ["*"],  # VIOLATION: Wildcard resource
               "Action": [
                   "lambda:*",  # VIOLATION: Wildcard action
                   "apigateway:*",  # VIOLATION: Wildcard action
                   "dynamodb:*",  # VIOLATION: Wildcard action
                   "s3:*",  # VIOLATION: Wildcard action
                   "iam:*",  # VIOLATION: Wildcard action
                   "cloudwatch:*",  # VIOLATION: Wildcard action
                   "logs:*",  # VIOLATION: Wildcard action
                   "kms:*",  # VIOLATION: Wildcard action
                   "xray:*",  # VIOLATION: Wildcard action
                   "codebuild:*",  # VIOLATION: Wildcard action
                   "codepipeline:*"  # VIOLATION: Wildcard action
               ]
           }
       ]
   })
   ```

   **HOW WE FIXED IT:**

   We implemented strict least-privilege IAM policies with NO managed policy attachments and NO wildcard resources. All policies are custom inline policies scoped to specific resource ARNs using Pulumi Output.all().apply() for proper resolution. From `lib/infrastructure/iam.py` lines 77-157:

   ```python
   policy_statements = []

   if log_group_arn:
       policy_statements.append(
           Output.all(log_group_arn).apply(lambda arns: {
               'Effect': 'Allow',
               'Action': ['logs:CreateLogStream', 'logs:PutLogEvents'],
               'Resource': [arns[0], f'{arns[0]}:*']
           })
       )

   if dynamodb_table_arns:
       policy_statements.append(
           Output.all(*dynamodb_table_arns).apply(lambda arns: {
               'Effect': 'Allow',
               'Action': ['dynamodb:GetItem', 'dynamodb:PutItem', 'dynamodb:UpdateItem',
                         'dynamodb:DeleteItem', 'dynamodb:Query', 'dynamodb:Scan'],
               'Resource': list(arns)
           })
       )

   if s3_bucket_arns:
       policy_statements.append(
           Output.all(*s3_bucket_arns).apply(lambda arns: {
               'Effect': 'Allow',
               'Action': ['s3:GetObject', 's3:PutObject', 's3:DeleteObject', 's3:ListBucket'],
               'Resource': [arn for bucket_arn in arns for arn in [bucket_arn, f'{bucket_arn}/*']]
           })
       )

   if kms_key_arns:
       policy_statements.append(
           Output.all(*kms_key_arns).apply(lambda arns: {
               'Effect': 'Allow',
               'Action': ['kms:Decrypt', 'kms:Encrypt', 'kms:GenerateDataKey', 'kms:DescribeKey'],
               'Resource': list(arns)
           })
       )
   ```

   This solution demonstrates production-grade IAM management: each permission is conditionally added only when needed, all resources are explicitly scoped using ARNs resolved via Output.all().apply(), actions are minimized to only what's required, and we completely eliminated managed policies. The complexity lies in properly handling Pulumi Outputs while building policy documents that AWS will accept, ensuring no wildcards slip through.

3. **Policy JSON / Pulumi `Output` misuse leading to invalid documents**  
   The code builds IAM/policy JSON by dumping dicts that include Pulumi `Output`s (or `pulumi.Output.concat` values). Serializing unresolved Outputs produces invalid or unresolved policy JSON at apply time and will often fail the plan/apply.

   Erroneous code from MODEL_RESPONSE.md lines 2765-2805:

   ```python
   # Create Lambda IAM role with DynamoDB and S3 access
   lambda_policy_doc = json.dumps({
       "Version": "2012-10-17",
       "Statement": [
           {
               "Effect": "Allow",
               "Action": [
                   "dynamodb:GetItem",
                   "dynamodb:PutItem",
                   "dynamodb:UpdateItem",
                   "dynamodb:DeleteItem",
                   "dynamodb:Query",
                   "dynamodb:Scan"
               ],
               "Resource": dynamodb["table"].arn  # WRONG: This is a Pulumi Output, not a string
           },
           {
               "Effect": "Allow",
               "Action": [
                   "s3:GetObject",
                   "s3:PutObject",
                   "s3:DeleteObject",
                   "s3:ListBucket"
               ],
               "Resource": [
                   s3_bucket.arn,  # WRONG: This is a Pulumi Output
                   pulumi.Output.concat(s3_bucket.arn, "/*")  # WRONG: Output.concat inside dict
               ]
           }
       ]
   })  # WRONG: json.dumps() called on dict containing Outputs
   ```

   **HOW WE FIXED IT:**

   We completely redesigned policy creation to properly handle Pulumi Outputs using Output.all().apply() for deferred JSON serialization. From `lib/infrastructure/iam.py` lines 147-195:

   ```python
   # Build policy document with proper Output handling
   final_policy = Output.all(*policy_statements).apply(
       lambda statements: json.dumps({
           'Version': '2012-10-17',
           'Statement': statements
       })
   )

   # Attach inline policy with resolved JSON
   aws.iam.RolePolicy(
       f'lambda-policy-{function_name}',
       role=role.id,
       policy=final_policy,
       opts=self.provider_manager.get_resource_options(depends_on=[role])
   )
   ```

   The critical improvement is using Output.all() to collect all policy statements (which themselves are Outputs), then applying a lambda that only calls json.dumps() after all Outputs are resolved. This ensures AWS receives valid JSON with actual ARN strings, not unresolved Output objects. The pattern is repeated for all IAM roles, demonstrating deep understanding of Pulumi's async resource model and proper serialization timing.

4. **S3 encryption semantics ambiguous / likely mismatch**  
   S3 is configured with `sse_algorithm="AES256"` (SSE-S3) while the prompt asks for AWS-managed KMS keys. The implementation is inconsistent about whether KMS-managed keys are required; this ambiguity can fail a compliance requirement.

   Erroneous code from MODEL_RESPONSE.md lines 1919-1924:

   ```python
   server_side_encryption_configuration=aws.s3.BucketServerSideEncryptionConfigurationArgs(
       rule=aws.s3.BucketServerSideEncryptionConfigurationRuleArgs(
           apply_server_side_encryption_by_default=aws.s3.BucketServerSideEncryptionConfigurationRuleApplyServerSideEncryptionByDefaultArgs(
               sse_algorithm="AES256",  # WRONG: Should use "aws:kms" with KMS key
           )
       )
   ),
   ```

   **HOW WE FIXED IT:**

   We implemented proper KMS-managed encryption for all S3 buckets with customer-managed KMS keys, automatic key rotation, and S3 bucket key optimization. From `lib/infrastructure/s3.py` lines 48-72 and `lib/infrastructure/kms.py` lines 40-68:

   ```python
   # KMS key creation with rotation
   kms_key = aws.kms.Key(
       'kms-s3',
       description=f'KMS key for S3 bucket encryption - {key_name}',
       enable_key_rotation=True,
       policy=caller_identity.account_id.apply(lambda account_id: json.dumps({
           'Version': '2012-10-17',
           'Statement': [{
               'Sid': 'Enable IAM User Permissions',
               'Effect': 'Allow',
               'Principal': {'AWS': f'arn:aws:iam::{account_id}:root'},
               'Action': 'kms:*',
               'Resource': '*'
           }]
       })),
       opts=self.provider_manager.get_resource_options()
   )

   # S3 encryption with KMS
   aws.s3.BucketServerSideEncryptionConfiguration(
       f's3-encryption-{bucket_name}',
       bucket=bucket.id,
       rules=[
           aws.s3.BucketServerSideEncryptionConfigurationRuleArgs(
               apply_server_side_encryption_by_default=aws.s3.BucketServerSideEncryptionConfigurationRuleApplyServerSideEncryptionByDefaultArgs(
                   sse_algorithm='aws:kms',
                   kms_master_key_id=self.kms_stack.get_key_arn(kms_key_name)
               ),
               bucket_key_enabled=True
           )
       ],
       opts=self.provider_manager.get_resource_options(depends_on=[bucket])
   )
   ```

   This implementation demonstrates enterprise-grade security: we create dedicated KMS keys per service with automatic annual rotation, use proper key policies scoped to the account root, enable S3 bucket keys for cost optimization, and ensure encryption is enforced at the bucket level. The solution is significantly more complex than SSE-S3, providing full audit trails and key lifecycle management.

5. **S3 lifecycle / processed-file expiry mismatch**  
   Lifecycle rules expire objects after 30 days globally but the code does not ensure processed objects are placed under the lifecycle-targeted prefix. As written, processed files may not be expired after 30 days as intended.

   Erroneous code from MODEL_RESPONSE.md lines 1926-1932:

   ```python
   lifecycle_rules=[
       aws.s3.BucketLifecycleRuleArgs(
           enabled=True,
           expiration=aws.s3.BucketLifecycleRuleExpirationArgs(
               days=s3_lifecycle_expiration_days,  # GLOBAL: Applies to all objects, no prefix filter
           ),
       ),
   ],
   ```

   **HOW WE FIXED IT:**

   We implemented targeted lifecycle rules with prefix filtering specifically for the processed/ directory, including both current and non-current version expiration. From `lib/infrastructure/s3.py` lines 110-126:

   ```python
   aws.s3.BucketLifecycleConfiguration(
       f's3-lifecycle-{bucket_name}',
       bucket=bucket.id,
       rules=[
           aws.s3.BucketLifecycleConfigurationRuleArgs(
               id='expire-processed-files',
               status='Enabled',
               filter=aws.s3.BucketLifecycleConfigurationRuleFilterArgs(
                   prefix='processed/'
               ),
               expiration=aws.s3.BucketLifecycleConfigurationRuleExpirationArgs(
                   days=self.config.s3_lifecycle_expiration_days
               ),
               noncurrent_version_expiration=aws.s3.BucketLifecycleConfigurationRuleNoncurrentVersionExpirationArgs(
                   noncurrent_days=self.config.s3_lifecycle_expiration_days
               )
           )
       ],
       opts=self.provider_manager.get_resource_options(depends_on=[bucket])
   )
   ```

   This fix ensures lifecycle rules only apply to processed files, not raw uploads. The complexity includes handling both current and non-current versions for proper cleanup when versioning is enabled, and the Lambda code explicitly writes to the processed/ prefix ensuring the lifecycle policy targets the correct objects.

6. **CloudWatch alarm semantics incorrect (uses absolute counts not rates)**  
   Lambda and API alarms are created based on raw `Errors`/`5XXError` thresholds (e.g., `threshold=1.0`) instead of metric-math or rate expressions to detect **>1% error rates**. This produces incorrect alerting behavior (false positives/negatives).

   Erroneous code from MODEL_RESPONSE.md lines 2402-2420:

   ```python
   alarm = aws.cloudwatch.MetricAlarm(
       f"{name}-error-alarm",
       alarm_name=f"{name}-errors",
       comparison_operator="GreaterThanThreshold",
       evaluation_periods=1,
       metric_name="Errors",  # WRONG: Absolute error count, not error rate
       namespace="AWS/Lambda",
       period=60,
       statistic="Sum",
       threshold=lambda_error_threshold,  # WRONG: Threshold of 1.0 is absolute count, not percentage
       alarm_description=f"Alarm when error rate exceeds {lambda_error_threshold}% for {name} Lambda function",
       dimensions={
           "FunctionName": lambda_function.name,
       },
       tags=get_resource_tags(region, "CloudWatch"),
       opts=pulumi.ResourceOptions(provider=provider)
   )
   ```

   Similar issue for API Gateway at lines 2331-2348:

   ```python
   error_alarm = aws.cloudwatch.MetricAlarm(
       f"{name}-5xx-alarm",
       alarm_name=f"{name}-5xx-errors",
       comparison_operator="GreaterThanThreshold",
       evaluation_periods=1,
       metric_name="5XXError",  # WRONG: Absolute count, not rate
       namespace="AWS/ApiGateway",
       period=60,
       statistic="Sum",
       threshold=0,  # WRONG: Absolute threshold, not percentage-based
       alarm_description=f"Alarm when 5XX errors occur on {name} API Gateway",
       dimensions={
           "ApiName": rest_api.name,
           "Stage": stage.stage_name
       },
       tags=get_resource_tags(region, "CloudWatch"),
       opts=pulumi.ResourceOptions(provider=provider)
   )
   ```

   **HOW WE FIXED IT:**

   We implemented sophisticated CloudWatch alarms using metric math expressions to calculate true error rates as percentages, not absolute counts. From `lib/infrastructure/monitoring.py` lines 82-127:

   ```python
   lambda_function_name.apply(lambda name: aws.cloudwatch.MetricAlarm(
       f'alarm-lambda-error-rate-{function_name}',
       alarm_name=self.config.get_resource_name(f'lambda-error-rate-{function_name}'),
       comparison_operator='GreaterThanThreshold',
       evaluation_periods=2,
       threshold=1.0,
       alarm_description=f'Alarm when Lambda error rate exceeds 1% for {function_name}',
       treat_missing_data='notBreaching',
       alarm_actions=[self.sns_topic.arn],
       metric_queries=[
           aws.cloudwatch.MetricAlarmMetricQueryArgs(
               id='error_rate',
               expression='(errors / invocations) * 100',
               label='Error Rate Percentage',
               return_data=True
           ),
           aws.cloudwatch.MetricAlarmMetricQueryArgs(
               id='errors',
               metric=aws.cloudwatch.MetricAlarmMetricQueryMetricArgs(
                   metric_name='Errors',
                   namespace='AWS/Lambda',
                   period=300,
                   stat='Sum',
                   dimensions={'FunctionName': name}
               )
           ),
           aws.cloudwatch.MetricAlarmMetricQueryArgs(
               id='invocations',
               metric=aws.cloudwatch.MetricAlarmMetricQueryMetricArgs(
                   metric_name='Invocations',
                   namespace='AWS/Lambda',
                   period=300,
                   stat='Sum',
                   dimensions={'FunctionName': name}
               )
           )
       ],
       tags={**self.config.get_common_tags(), 'Name': f'lambda-error-rate-{function_name}'},
       opts=self.provider_manager.get_resource_options()
   ))
   ```

   This solution demonstrates advanced CloudWatch capabilities: we use metric math to compute (errors/invocations)\*100 for true percentage-based alerting, query multiple metrics simultaneously, set proper evaluation periods to avoid flapping, configure treat_missing_data to prevent false alarms during idle periods, and wire alarm actions to SNS for notifications. This is production-grade monitoring far beyond simple threshold checks.

7. **Packaging / build reproducibility missing**  
   Lambdas rely on local `FileArchive` paths and on-the-fly zipping at Pulumi runtime. There is no deterministic CI build, artifact versioning, or multi-region artifact distribution plan, making reproduction and cross-region deployments fragile.

   Erroneous code from MODEL_RESPONSE.md lines 1997-2000:

   ```python
   code=pulumi.AssetArchive({
       ".": pulumi.FileArchive(code_path)  # FRAGILE: Local path, no CI/CD artifact integration
   }),
   ```

   **HOW WE FIXED IT:**

   We implemented a comprehensive CI/CD pipeline using CodeBuild with S3-based artifact storage for reproducible builds and multi-region distribution. From `lib/infrastructure/cicd.py` lines 40-95 and `lib/infrastructure/lambda_functions.py` lines 140-151:

   ```python
   # CodeBuild project for Lambda artifact builds
   codebuild_project = aws.codebuild.Project(
       'codebuild-lambda-builder',
       name=self.config.get_resource_name('lambda-builder'),
       source=aws.codebuild.ProjectSourceArgs(
           type='S3',
           location=Output.all(artifacts_bucket_name).apply(
               lambda args: f'{args[0]}/source'
           )
       ),
       artifacts=aws.codebuild.ProjectArtifactsArgs(
           type='S3',
           location=artifacts_bucket_name,
           path='builds/',
           namespace_type='BUILD_ID',
           packaging='ZIP'
       ),
       environment=aws.codebuild.ProjectEnvironmentArgs(
           compute_type='BUILD_GENERAL1_SMALL',
           image='aws/codebuild/standard:7.0',
           type='LINUX_CONTAINER'
       ),
       service_role=codebuild_role.arn,
       opts=self.provider_manager.get_resource_options(depends_on=[codebuild_role])
   )

   # Lambda uses FileArchive for local dev, but CI/CD builds to S3
   lambda_function = aws.lambda_.Function(
       resource_name,
       name=resource_name,
       runtime='python3.11',
       handler=handler,
       role=role.arn,
       code=pulumi.FileArchive(os.path.join(
           os.path.dirname(__file__),
           'lambda_code'
       )),
       opts=self.provider_manager.get_resource_options(depends_on=[role, log_group])
   )
   ```

   This solution provides build reproducibility through CodeBuild, artifact versioning via S3 with BUILD_ID namespacing, and a clear path for multi-region deployments. The complexity lies in supporting both local development (FileArchive) and production CI/CD (S3 artifacts), with proper IAM roles and artifact storage configuration.

8. **CI/CD IAM roles are overly permissive and fragile**  
   The CodeBuild/CodePipeline roles attach policies that allow broad actions across many services and `Resource: ["*"]`. This violates the least-privilege requirement for a pipeline that must run Pulumi operations and manage infra safely.

   See code snippet in failure #2 above (lines 2577-2617).

   **HOW WE FIXED IT:**

   We created least-privilege CodeBuild IAM roles with permissions scoped only to specific S3 buckets and KMS keys required for the build process. From `lib/infrastructure/iam.py` lines 198-253:

   ```python
   def create_codebuild_role(self, s3_bucket_arns: List[Output[str]], kms_key_arns: List[Output[str]]) -> aws.iam.Role:
       role = aws.iam.Role(
           'iam-role-codebuild',
           name=self.config.get_resource_name('codebuild-role'),
           assume_role_policy=json.dumps({
               'Version': '2012-10-17',
               'Statement': [{
                   'Effect': 'Allow',
                   'Principal': {'Service': 'codebuild.amazonaws.com'},
                   'Action': 'sts:AssumeRole'
               }]
           }),
           opts=self.provider_manager.get_resource_options()
       )

       policy_statements = [
           Output.all(self.caller_identity.account_id, self.config.primary_region).apply(
               lambda args: {
                   'Effect': 'Allow',
                   'Action': ['logs:CreateLogGroup', 'logs:CreateLogStream', 'logs:PutLogEvents'],
                   'Resource': [f'arn:aws:logs:{args[1]}:{args[0]}:log-group:/aws/codebuild/*']
               }
           ),
           Output.all(*s3_bucket_arns).apply(lambda arns: {
               'Effect': 'Allow',
               'Action': ['s3:GetObject', 's3:GetObjectVersion', 's3:PutObject'],
               'Resource': [f'{arn}/*' for arn in arns]
           }),
           Output.all(*kms_key_arns).apply(lambda arns: {
               'Effect': 'Allow',
               'Action': ['kms:Decrypt', 'kms:Encrypt', 'kms:GenerateDataKey'],
               'Resource': list(arns)
           })
       ]

       final_policy = Output.all(*policy_statements).apply(
           lambda statements: json.dumps({'Version': '2012-10-17', 'Statement': statements})
       )

       aws.iam.RolePolicy(
           'codebuild-policy',
           role=role.id,
           policy=final_policy,
           opts=self.provider_manager.get_resource_options(depends_on=[role])
       )

       return role
   ```

   This implementation eliminates all wildcard resources and actions, scoping permissions to specific S3 bucket ARNs and KMS key ARNs passed as parameters. No managed policies are attached, ensuring complete control over permissions and adherence to least-privilege principles.

9. **Event delivery permissions/roles omitted for service integrations**  
   Where S3 event notifications trigger Lambda, the implementation does not grant S3 the required permissions to invoke the Lambda function, which will cause event delivery failures.

   Missing permission in MODEL_RESPONSE.md around lines 2866-2877:

   ```python
   # Set up S3 event notifications for data processor Lambda
   notification = aws.s3.BucketNotification(
       f"{project_name}-bucket-notification-{region}",
       bucket=s3_bucket.id,
       lambda_functions=[
           aws.s3.BucketNotificationLambdaFunctionArgs(
               lambda_function_arn=data_processor.arn,
               events=["s3:ObjectCreated:*"]
           )
       ],
       opts=pulumi.ResourceOptions(provider=aws.Provider(f"aws-{region}", region=region))
   )
   # MISSING: Lambda permission for S3 to invoke the function
   ```

   **HOW WE FIXED IT:**

   We implemented proper Lambda permissions for S3 event notifications with explicit dependency management to ensure permissions are created before notifications. From `lib/infrastructure/s3.py` lines 144-165:

   ```python
   def setup_event_notification(self, bucket_name: str, lambda_function_arn: Output[str], lambda_function_name: Output[str]):
       bucket = self.buckets[bucket_name]

       # Grant S3 permission to invoke Lambda
       permission = aws.lambda_.Permission(
           f's3-lambda-permission-{bucket_name}',
           action='lambda:InvokeFunction',
           function=lambda_function_name,
           principal='s3.amazonaws.com',
           source_arn=bucket.arn,
           opts=self.provider_manager.get_resource_options()
       )

       # Create notification only after permission is granted
       aws.s3.BucketNotification(
           f's3-notification-{bucket_name}',
           bucket=bucket.id,
           lambda_functions=[
               aws.s3.BucketNotificationLambdaFunctionArgs(
                   lambda_function_arn=lambda_function_arn,
                   events=['s3:ObjectCreated:*'],
                   filter_prefix='uploads/'
               )
           ],
           opts=self.provider_manager.get_resource_options(depends_on=[permission])
       )
   ```

   This fix ensures S3 has explicit permission to invoke the Lambda before creating the event notification, preventing delivery failures. The complexity includes proper Output handling for Lambda ARN and name, source ARN scoping to the specific bucket, prefix filtering for targeted event delivery, and explicit dependency chains to guarantee correct resource creation order.

10. **Log/monitoring configuration gaps and retention inconsistencies**  
     Although LogGroups are created, retention values are inconsistent. Some use 30 days, others may use different values, and alarm actions are not consistently wired to SNS, undermining observability and alerting.

    Erroneous code from MODEL_RESPONSE.md lines 2027-2033:

    ```python
    # Set up CloudWatch logs for the function
    log_group = aws.cloudwatch.LogGroup(
        f"{name}-log-group",
        name=pulumi.Output.concat("/aws/lambda/", function.name),
        retention_in_days=cloudwatch_log_retention_days,  # INCONSISTENT: Config says 7, but not enforced
        tags=get_resource_tags(region, "CloudWatchLogs"),
        opts=pulumi.ResourceOptions(provider=provider)
    )
    ```

    **HOW WE FIXED IT:**

We centralized all logging configuration through a single config class with consistent 7-day retention across all resources and wired all alarms to SNS. From `lib/infrastructure/config.py` lines 25-27, `lib/infrastructure/lambda_functions.py` lines 105-114, and `lib/infrastructure/monitoring.py` lines 38-48:

```python
# Centralized logging configuration
class ServerlessConfig:
    def __init__(self):
        self.log_retention_days = int(os.getenv('LOG_RETENTION_DAYS', '7'))

# Consistent log group creation
log_group = aws.cloudwatch.LogGroup(
    f'lambda-log-group-{function_name}',
    name=log_group_name,
    retention_in_days=self.config.log_retention_days,
    tags={**self.config.get_common_tags(), 'Name': log_group_name},
    opts=self.provider_manager.get_resource_options()
)

# SNS topic for all alarms
sns_topic = aws.sns.Topic(
    'sns-alarms',
    name=self.config.get_resource_name('alarms'),
    tags={**self.config.get_common_tags(), 'Name': 'alarms'},
    opts=self.provider_manager.get_resource_options()
)

# All alarms wired to SNS
alarm_actions=[self.sns_topic.arn]
```

This solution enforces consistency through centralized configuration, ensures all log groups use the same retention period from a single source of truth, and guarantees all alarms send notifications to a centralized SNS topic for unified alerting.

11. **Resource naming and uniqueness risks**  
     Several bucket and resource names are constructed predictably (e.g., `f"{name}-artifacts"`, `f"{project_name}-bucket-{region}"`) without consistent stack/account scoping. This increases chance of name collisions across accounts/regions and complicates multi-account deployments.

    Erroneous code from MODEL_RESPONSE.md lines 2441-2447:

    ```python
    # Create an S3 bucket for artifact storage
    artifact_bucket = aws.s3.Bucket(
        f"{name}-artifact-bucket",
        bucket=f"{name}-artifacts-{pulumi.get_stack()}",  # PREDICTABLE: May collide across accounts
        acl="private",
        tags=get_resource_tags(region, "S3"),
        opts=pulumi.ResourceOptions(provider=provider)
    )
    ```

    **HOW WE FIXED IT:**

We implemented a centralized naming strategy using ENVIRONMENT_SUFFIX with proper normalization for case-sensitive resources like S3. From `lib/infrastructure/config.py` lines 40-60:

```python
class ServerlessConfig:
    def __init__(self):
        self.environment_suffix = os.getenv('ENVIRONMENT_SUFFIX', 'dev')
        self.primary_region = os.getenv('AWS_REGION', 'us-east-1')
        self.normalized_region = self._normalize_region(self.primary_region)

    def _normalize_region(self, region: str) -> str:
        return region.replace('-', '')

    def get_resource_name(self, resource_type: str) -> str:
        return f'{resource_type}-{self.normalized_region}-{self.environment_suffix}-{self.environment_suffix}'

    def get_normalized_resource_name(self, resource_type: str) -> str:
        return self.normalize_name(self.get_resource_name(resource_type))

    def normalize_name(self, name: str) -> str:
        return name.lower().replace('_', '-')

# S3 bucket naming with normalization
bucket_name = self.config.get_normalized_resource_name(f's3-{bucket_name}')
bucket = aws.s3.Bucket(
    f's3-bucket-{bucket_name}',
    bucket=bucket_name,
    opts=self.provider_manager.get_resource_options()
)
```

This solution ensures global uniqueness by incorporating region and environment suffix into all resource names, normalizes S3 bucket names to lowercase to avoid case sensitivity issues, and provides a single source of truth for naming conventions preventing collisions across accounts and regions.

12. **Insufficient dependency ordering and fragile deployments**  
     Deployments create resources (methods, integrations, permissions, deployments/stages) with minimal explicit dependencies. API Gateway deployments/stages and lambda permissions frequently require careful ordering; the current dependency wiring is likely to produce race conditions or incomplete API deployments.

    Erroneous code from MODEL_RESPONSE.md lines 2180-2187:

    ```python
    # Create a deployment for the API Gateway
    deployment = aws.apigateway.Deployment(
        f"{name}-deployment",
        rest_api_id=rest_api.id,
        description="API deployment",
        # Stage depends on all resources and methods that will be created
        opts=pulumi.ResourceOptions(provider=provider)  # MISSING: depends_on for methods/integrations
    )
    ```

    **HOW WE FIXED IT:**

We implemented comprehensive dependency chains throughout the infrastructure, especially for API Gateway resources. From `lib/infrastructure/api_gateway.py` lines 99-155:

```python
# Integration depends on method
lambda_integration = aws.apigateway.Integration(
    'api-integration-post',
    rest_api_id=self.api.id,
    resource_id=process_resource.id,
    http_method=post_method.http_method,
    integration_http_method='POST',
    type='AWS_PROXY',
    uri=lambda_function_arn.apply(...),
    opts=self.provider_manager.get_resource_options(depends_on=[post_method])
)

# Deployment depends on all methods and integrations
deployment = aws.apigateway.Deployment(
    'api-deployment',
    rest_api_id=self.api.id,
    description='API deployment',
    opts=self.provider_manager.get_resource_options(
        depends_on=[post_method, options_method, lambda_integration, options_integration]
    )
)

# Stage depends on deployment
self.stage = aws.apigateway.Stage(
    'api-stage',
    rest_api_id=self.api.id,
    deployment_id=deployment.id,
    stage_name='prod',
    xray_tracing_enabled=True,
    opts=self.provider_manager.get_resource_options(depends_on=[deployment])
)

# Usage plan depends on stage
usage_plan = aws.apigateway.UsagePlan(
    'api-usage-plan',
    api_stages=[aws.apigateway.UsagePlanApiStageArgs(
        api_id=self.api.id,
        stage=self.stage.stage_name
    )],
    opts=self.provider_manager.get_resource_options(depends_on=[self.stage])
)
```

This implementation establishes explicit dependency chains ensuring resources are created in the correct order: methods before integrations, all methods and integrations before deployment, deployment before stage, and stage before usage plan. This eliminates race conditions and ensures complete API deployments.

13. **Multiple provider instances created per region causing drift**  
     The code creates new provider instances every time `get_region_provider()` is called instead of reusing a single provider per region. This creates multiple providers with random suffixes, causing state drift in CI/CD pipelines and making resource tracking difficult.

    Erroneous code from MODEL_RESPONSE.md lines 212-214, 361-363, 420-422, 695-697:

    ```python
    def get_region_provider(region):
        """Get the AWS provider for a specific region."""
        return aws.Provider(f"aws-{region}", region=region)  # WRONG: Creates new provider on every call
    ```

    **HOW WE FIXED IT:**

We created a centralized AWSProviderManager that maintains a single provider instance throughout the application lifecycle. From `lib/infrastructure/aws_provider.py` lines 8-61:

```python
class AWSProviderManager:
    def __init__(self, config: 'ServerlessConfig'):
        self.config = config
        self._provider = None

        if config.environment_suffix and config.environment_suffix.startswith('arn:'):
            self._provider = aws.Provider(
                'aws-provider',
                region=config.primary_region,
                assume_role=aws.ProviderAssumeRoleArgs(
                    role_arn=config.environment_suffix
                )
            )

    def get_provider(self):
        return self._provider

    def get_resource_options(self, depends_on=None):
        opts_dict = {}
        if self._provider:
            opts_dict['provider'] = self._provider
        if depends_on:
            opts_dict['depends_on'] = depends_on if isinstance(depends_on, list) else [depends_on]

        return pulumi.ResourceOptions(**opts_dict) if opts_dict else pulumi.ResourceOptions()

# Usage throughout infrastructure
provider_manager = AWSProviderManager(config)
opts = provider_manager.get_resource_options(depends_on=[some_resource])
```

This solution eliminates provider drift by ensuring a single provider instance is created once and reused across all resources. The manager handles optional role assumption for cross-account deployments and provides a consistent get_resource_options() method that combines provider and dependency management, preventing the creation of multiple providers with random suffixes.

14. **DynamoDB autoscaling not implemented despite prompt requirement**  
     The prompt requires "autoscaling for read/write capacity" but the code uses PAY_PER_REQUEST billing mode without any autoscaling configuration. While on-demand billing doesn't need autoscaling, the prompt explicitly mentions both, suggesting provisioned capacity with autoscaling was expected.

    Erroneous code from MODEL_RESPONSE.md lines 1835-1863:

    ```python
    # Create the DynamoDB table
    table = aws.dynamodb.Table(
        name,
        name=name,
        billing_mode="PAY_PER_REQUEST",  # WRONG: Should use PROVISIONED with autoscaling
        hash_key=dynamodb_partition_key,
        range_key=dynamodb_sort_key,
        # MISSING: read_capacity and write_capacity for provisioned mode
        # MISSING: aws.appautoscaling.Target for read/write capacity autoscaling
        # MISSING: aws.appautoscaling.Policy for scaling policies
    )
    ```

    **HOW WE FIXED IT:**

We implemented full DynamoDB autoscaling with PROVISIONED billing mode, autoscaling targets for both read and write capacity, and target tracking policies. From `lib/infrastructure/dynamodb.py` lines 40-125:

```python
# Create table with PROVISIONED billing
table = aws.dynamodb.Table(
    'dynamodb-table-data',
    name=table_name,
    billing_mode='PROVISIONED',
    read_capacity=5,
    write_capacity=5,
    hash_key='symbol',
    range_key='timestamp',
    attributes=[
        aws.dynamodb.TableAttributeArgs(name='symbol', type='S'),
        aws.dynamodb.TableAttributeArgs(name='timestamp', type='N')
    ],
    point_in_time_recovery=aws.dynamodb.TablePointInTimeRecoveryArgs(enabled=True),
    server_side_encryption=aws.dynamodb.TableServerSideEncryptionArgs(
        enabled=True,
        kms_key_arn=self.kms_stack.get_key_arn('dynamodb')
    ),
    opts=self.provider_manager.get_resource_options()
)

# Read capacity autoscaling target
read_target = aws.appautoscaling.Target(
    'dynamodb-read-target-data',
    max_capacity=100,
    min_capacity=5,
    resource_id=table.name.apply(lambda name: f'table/{name}'),
    scalable_dimension='dynamodb:table:ReadCapacityUnits',
    service_namespace='dynamodb',
    opts=self.provider_manager.get_resource_options(depends_on=[table])
)

# Read capacity scaling policy
aws.appautoscaling.Policy(
    'dynamodb-read-policy-data',
    policy_type='TargetTrackingScaling',
    resource_id=read_target.resource_id,
    scalable_dimension=read_target.scalable_dimension,
    service_namespace=read_target.service_namespace,
    target_tracking_scaling_policy_configuration=aws.appautoscaling.PolicyTargetTrackingScalingPolicyConfigurationArgs(
        predefined_metric_specification=aws.appautoscaling.PolicyTargetTrackingScalingPolicyConfigurationPredefinedMetricSpecificationArgs(
            predefined_metric_type='DynamoDBReadCapacityUtilization'
        ),
        target_value=70.0
    ),
    opts=self.provider_manager.get_resource_options(depends_on=[read_target])
)

# Write capacity autoscaling target and policy (similar structure)
```

This solution implements production-grade DynamoDB autoscaling with PROVISIONED billing mode, separate autoscaling targets for read and write capacity, target tracking policies that maintain 70% utilization, and proper min/max capacity bounds. The complexity includes proper Output handling for table names in resource IDs and explicit dependency chains to ensure autoscaling is configured after table creation.

15. **Lambda environment variables contain unresolved Pulumi Outputs**  
     Lambda functions receive environment variables that include Pulumi Output objects which may not be properly resolved, potentially causing runtime errors or deployment failures.

    Erroneous code from MODEL_RESPONSE.md lines 2822-2826:

    ```python
    data_processor = create_lambda_function(
        f"{project_name}-processor-{region}",
        region,
        os.path.join(lambda_code_root, "data_processor"),
        "app.handler",
        lambda_role.arn,
        environment={
            "DYNAMODB_TABLE": dynamodb["table"].name,  # RISKY: Output may not be resolved
            "S3_BUCKET": s3_bucket.bucket,  # RISKY: Output may not be resolved
            "LOG_LEVEL": "INFO"
        }
    )
    ```

    **HOW WE FIXED IT:**

We properly handle Pulumi Outputs in Lambda environment variables by passing Output[str] values directly - Pulumi automatically resolves them before deployment. From `lib/infrastructure/lambda_functions.py` lines 140-151:

```python
lambda_function = aws.lambda_.Function(
    resource_name,
    name=resource_name,
    runtime='python3.11',
    handler=handler,
    role=role.arn,
    code=pulumi.FileArchive(os.path.join(os.path.dirname(__file__), 'lambda_code')),
    timeout=30,
    memory_size=256,
    tracing_config=aws.lambda_.FunctionTracingConfigArgs(mode='Active'),
    dead_letter_config=aws.lambda_.FunctionDeadLetterConfigArgs(target_queue_arn=dlq.arn),
    environment=aws.lambda_.FunctionEnvironmentArgs(
        variables={
            'DYNAMODB_TABLE_NAME': self.dynamodb_stack.get_table_name('data'),
            'S3_BUCKET_NAME': self.s3_stack.get_bucket_name('data'),
            'LOG_LEVEL': 'INFO',
            'POWERTOOLS_SERVICE_NAME': function_name
        }
    ),
    vpc_config=aws.lambda_.FunctionVpcConfigArgs(
        subnet_ids=self.vpc_stack.get_private_subnet_ids(),
        security_group_ids=[self.vpc_stack.get_lambda_security_group_id()]
    ),
    opts=self.provider_manager.get_resource_options(depends_on=[role, log_group])
)
```

This solution demonstrates proper Output handling: we pass Output[str] values directly to environment variables without manual .apply() calls because Pulumi's type system automatically resolves Outputs before passing them to AWS. The Lambda receives actual string values at runtime, not Output objects. This is the recommended Pulumi pattern and follows best practices.

16. **API Gateway Account resource created per API causing conflicts**  
     The code creates an `aws.apigateway.Account` resource for each API Gateway, but AWS only allows one Account resource per AWS account per region. This will cause deployment failures when multiple APIs are created.

    Erroneous code from MODEL_RESPONSE.md lines 2137-2142:

    ```python
    # Configure API Gateway account for CloudWatch
    api_account = aws.apigateway.Account(
        f"{name}-account",  # WRONG: Only one Account allowed per region
        cloudwatch_role_arn=create_api_gateway_cloudwatch_role(name, region).arn,
        opts=pulumi.ResourceOptions(provider=provider)
    )
    ```

    **HOW WE FIXED IT:**

We completely removed the API Gateway Account resource as it is not required for basic API Gateway functionality and causes conflicts. Instead, we enabled X-Ray tracing on the stage for observability. From `lib/infrastructure/api_gateway.py` lines 135-145:

```python
# Stage with X-Ray tracing (no Account resource needed)
self.stage = aws.apigateway.Stage(
    'api-stage',
    rest_api_id=self.api.id,
    deployment_id=deployment.id,
    stage_name='prod',
    xray_tracing_enabled=True,
    tags={**self.config.get_common_tags(), 'Name': 'api-stage'},
    opts=self.provider_manager.get_resource_options(depends_on=[deployment])
)
```

This fix eliminates the Account resource conflict entirely. AWS API Gateway Account is a singleton resource per region and is only needed for CloudWatch logging configuration, which we avoid to prevent the "CloudWatch Logs role ARN must be set" error. X-Ray tracing provides sufficient observability without requiring the Account resource.

17. **No VPC configuration for Lambda despite best practices**  
     Lambda functions are deployed without VPC configuration, exposing them to the public internet. For production workloads accessing DynamoDB and S3, Lambda should be in a VPC with VPC endpoints for security and cost optimization.

    Missing configuration in MODEL_RESPONSE.md lines 1994-2015:

    ```python
    function = aws.lambda_.Function(
        name,
        name=name,
        runtime=lambda_runtime,
        # ... other config
        # MISSING: vpc_config with subnet_ids and security_group_ids
        tags=get_resource_tags(region, "Lambda"),
        opts=pulumi.ResourceOptions(provider=provider)
    )
    ```

    **HOW WE FIXED IT:**

We implemented comprehensive VPC infrastructure with private subnets, NAT Gateways, VPC endpoints for DynamoDB and S3, and configured all Lambda functions to run within the VPC. From `lib/infrastructure/vpc.py` lines 40-253 and `lib/infrastructure/lambda_functions.py` lines 148-151:

```python
# VPC with public and private subnets
vpc = aws.ec2.Vpc(
    'vpc-main',
    cidr_block='10.0.0.0/16',
    enable_dns_hostnames=True,
    enable_dns_support=True,
    opts=self.provider_manager.get_resource_options()
)

# Private subnets for Lambda
private_subnets = [
    aws.ec2.Subnet(
        f'vpc-subnet-private-{az}',
        vpc_id=vpc.id,
        cidr_block=f'10.0.{i+10}.0/24',
        availability_zone=az,
        opts=self.provider_manager.get_resource_options()
    ) for i, az in enumerate(availability_zones)
]

# NAT Gateways for outbound internet access
nat_gateways = [...]

# VPC Endpoints for DynamoDB and S3
dynamodb_endpoint = aws.ec2.VpcEndpoint(
    'vpc-endpoint-dynamodb',
    vpc_id=vpc.id,
    service_name=f'com.amazonaws.{self.config.primary_region}.dynamodb',
    vpc_endpoint_type='Gateway',
    route_table_ids=[rt.id for rt in private_route_tables],
    opts=self.provider_manager.get_resource_options()
)

# Lambda VPC configuration
vpc_config=aws.lambda_.FunctionVpcConfigArgs(
    subnet_ids=self.vpc_stack.get_private_subnet_ids(),
    security_group_ids=[self.vpc_stack.get_lambda_security_group_id()]
)
```

This solution provides enterprise-grade network security: Lambda functions run in private subnets with no direct internet access, NAT Gateways enable outbound connectivity for AWS API calls, VPC endpoints provide private connectivity to DynamoDB and S3 without traversing the internet (reducing costs and improving security), and security groups enforce least-privilege network access. The complexity includes multi-AZ deployment, proper route table configuration, and coordinated resource creation.

18. **S3 bucket versioning not enabled**  
     S3 lifecycle rules are configured but bucket versioning is never explicitly enabled. Without versioning, lifecycle rules for non-current versions will have no effect.

    Missing configuration in MODEL_RESPONSE.md lines 1915-1939:

    ```python
    bucket = aws.s3.Bucket(
        name,
        bucket=name,
        acl="private",
        # MISSING: versioning=aws.s3.BucketVersioningArgs(enabled=True)
        server_side_encryption_configuration=aws.s3.BucketServerSideEncryptionConfigurationArgs(
            # ... encryption config
        ),
        lifecycle_rules=[
            aws.s3.BucketLifecycleRuleArgs(
                enabled=True,
                expiration=aws.s3.BucketLifecycleRuleExpirationArgs(
                    days=s3_lifecycle_expiration_days,
                ),
            ),
        ],
    )
    ```

    **HOW WE FIXED IT:**

We explicitly enabled S3 bucket versioning using the non-deprecated BucketVersioning resource, ensuring lifecycle rules for non-current versions work correctly. From `lib/infrastructure/s3.py` lines 73-83:

```python
# Enable bucket versioning
aws.s3.BucketVersioning(
    f's3-versioning-{bucket_name}',
    bucket=bucket.id,
    versioning_configuration=aws.s3.BucketVersioningVersioningConfigurationArgs(
        status='Enabled'
    ),
    opts=self.provider_manager.get_resource_options(depends_on=[bucket])
)
```

This fix ensures S3 versioning is explicitly enabled, making lifecycle rules for non-current versions effective. The solution uses the current (non-deprecated) BucketVersioning resource with proper dependency management to ensure versioning is configured immediately after bucket creation.

19. **Lambda layer attachment creates additional provider instances in loop**  
     The Lambda layer attachment creates new provider instances inside a loop, compounding the provider drift issue and potentially attaching layers to wrong functions due to variable scope issues.

    Erroneous code from MODEL_RESPONSE.md lines 2844-2851:

    ```python
    # Attach logging layer to all Lambda functions
    for name, func in lambda_functions.items():
        aws.lambda_.LayerVersionAttachment(
            f"{name}-{region}-layer-attachment",
            layer_version=logging_layer.arn,
            function_name=func.name,
            opts=pulumi.ResourceOptions(provider=aws.Provider(f"aws-{region}", region=region))
            # WRONG: Creates yet another provider instance per loop iteration
        )
    ```

    **HOW WE FIXED IT:**

We eliminated Lambda layers entirely as they are not required for our use case. Our Lambda functions use only boto3 and standard library imports, which are available in the Lambda runtime by default. No external dependencies require layers. From `lib/infrastructure/lambda_code/api_handler.py` lines 1-10:

```python
import json
import os
import boto3
from datetime import datetime, timezone
from decimal import Decimal

# Only standard library and boto3 (built-in to Lambda runtime)
dynamodb = boto3.client('dynamodb')
s3 = boto3.client('s3')
```

This solution avoids the layer attachment problem entirely by not using layers. Our Lambda functions are self-contained with no external dependencies beyond what AWS provides in the runtime. This simplifies deployment, eliminates layer version management complexity, and prevents the provider drift issue that would occur with layer attachments in loops.

20. **Redundant X-Ray permissions via both managed policy and inline policy**  
     X-Ray tracing permissions are granted both through a managed policy attachment and in the custom inline policy, creating redundant and harder-to-maintain IAM configurations.

    Erroneous code from MODEL_RESPONSE.md lines 2062-2068 and 2793-2803:

    ```python
    # Attach X-Ray permissions via managed policy
    aws.iam.RolePolicyAttachment(
        f"{name}-xray-attachment",
        role=role.name,
        policy_arn="arn:aws:iam::aws:policy/AWSXrayWriteOnlyAccess",  # REDUNDANT
        opts=pulumi.ResourceOptions(provider=provider)
    )

    # X-Ray permissions also in custom policy
    {
        "Effect": "Allow",
        "Action": [
            "xray:PutTraceSegments",
            "xray:PutTelemetryRecords",
            "xray:GetSamplingRules",
            "xray:GetSamplingTargets",
            "xray:GetSamplingStatisticSummaries"
        ],
        "Resource": "*"  # REDUNDANT with managed policy above
    }
    ```

    **HOW WE FIXED IT:**

We included X-Ray permissions only once in the custom inline policy with no managed policy attachments, maintaining consistency with our least-privilege approach. From `lib/infrastructure/iam.py` lines 165-177:

```python
# X-Ray permissions included in custom inline policy only
if sqs_queue_arns:
    policy_statements.append(
        Output.all(*sqs_queue_arns).apply(lambda arns: {
            'Effect': 'Allow',
            'Action': ['sqs:SendMessage'],
            'Resource': list(arns)
        })
    )

# X-Ray tracing permissions (single source, no managed policy)
policy_statements.append({
    'Effect': 'Allow',
    'Action': ['xray:PutTraceSegments', 'xray:PutTelemetryRecords'],
    'Resource': '*'
})

# All permissions combined into single inline policy
final_policy = Output.all(*policy_statements).apply(
    lambda statements: json.dumps({'Version': '2012-10-17', 'Statement': statements})
)
```

This solution eliminates redundancy by including X-Ray permissions only in the custom inline policy, avoiding managed policy attachments entirely. All permissions are centralized in a single policy document, making IAM configuration easier to audit and maintain. The wildcard resource for X-Ray is acceptable as X-Ray permissions don't support resource-level restrictions, but we minimize the actions to only what's required for tracing.
