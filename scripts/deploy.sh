#!/bin/bash

# Exit on any error
set -e

echo "üöÄ Starting deployment process with clean slate approach..."
echo "üìã This script will:"
echo "   1. Bootstrap prerequisites"
echo "   2. Destroy existing resources (if any)"
echo "   3. Clean up Secrets Manager secrets in deletion state"
echo "   4. Deploy fresh resources"
echo ""

# Read platform and language from metadata.json
if [ ! -f "metadata.json" ]; then
  echo "‚ùå metadata.json not found, exiting with failure"
  exit 1
fi

PLATFORM=$(jq -r '.platform // "unknown"' metadata.json)
LANGUAGE=$(jq -r '.language // "unknown"' metadata.json)

echo "Project: platform=$PLATFORM, language=$LANGUAGE"

# Set default environment variables if not provided
export ENVIRONMENT_SUFFIX=${ENVIRONMENT_SUFFIX:-dev}
export REPOSITORY=${REPOSITORY:-$(basename "$(pwd)")}
export COMMIT_AUTHOR=${COMMIT_AUTHOR:-$(git config user.name 2>/dev/null || echo "unknown")}
export AWS_REGION=${AWS_REGION:-us-east-1}
export TERRAFORM_STATE_BUCKET=${TERRAFORM_STATE_BUCKET:-}
export TERRAFORM_STATE_BUCKET_REGION=${TERRAFORM_STATE_BUCKET_REGION:-us-east-1}
export PULUMI_BACKEND_URL=${PULUMI_BACKEND_URL:-}
export PULUMI_ORG=${PULUMI_ORG:-organization}
export PULUMI_CONFIG_PASSPHRASE=${PULUMI_CONFIG_PASSPHRASE:-}

# Ensure non-interactive Terraform by providing defaults if not set by CI secrets
export TF_VAR_db_username=${TF_VAR_db_username:-temp_admin}
export TF_VAR_db_password=${TF_VAR_db_password:-TempPassword123!}

echo "Environment configuration:"
echo "  Environment suffix: $ENVIRONMENT_SUFFIX"
echo "  Repository: $REPOSITORY"
echo "  Commit author: $COMMIT_AUTHOR"
echo "  AWS region: $AWS_REGION"
echo "Using TF_VAR_db_username: (set)"
echo "Using TF_VAR_db_password: (set)"
if [ -n "$TERRAFORM_STATE_BUCKET" ]; then
  echo "  Terraform state bucket: $TERRAFORM_STATE_BUCKET"
  echo "  Terraform state bucket region: $TERRAFORM_STATE_BUCKET_REGION"
fi
if [ -n "$PULUMI_BACKEND_URL" ]; then
  echo "  Pulumi backend URL: $PULUMI_BACKEND_URL"
  echo "  Pulumi organization: $PULUMI_ORG"
fi

echo "=== Bootstrap Phase ==="
./scripts/bootstrap.sh

# Destroy existing resources first
echo "=== Destroy Phase ==="
echo "üßπ Running destroy script to clean up existing resources before deployment..."
if [ -f "./scripts/destroy.sh" ]; then
  ./scripts/destroy.sh
  DESTROY_EXIT_CODE=$?
  if [ $DESTROY_EXIT_CODE -eq 0 ]; then
    echo "‚úÖ Destroy phase completed successfully"
  else
    echo "‚ö†Ô∏è Destroy phase completed with warnings (exit code: $DESTROY_EXIT_CODE)"
  fi
else
  echo "‚ö†Ô∏è Destroy script not found, skipping destroy phase"
fi

# Wait a moment to ensure resources are fully cleaned up
echo "‚è≥ Waiting 5 seconds to ensure resources are fully cleaned up..."
sleep 5

# Force cleanup of specific problematic resources
echo "=== Force Cleanup of Orphaned Resources ==="
echo "üîß Checking for and removing orphaned AWS resources..."

# Temporarily disable exit-on-error for cleanup
set +e

# Get the environment suffix for resource names
if [ -n "$ENVIRONMENT_SUFFIX" ]; then
  CLEANUP_SUFFIX="$ENVIRONMENT_SUFFIX"
else
  CLEANUP_SUFFIX="dev"
fi

echo "Cleaning up resources with suffix: $CLEANUP_SUFFIX"

# Force delete RDS instances if they exist
echo "üóëÔ∏è Checking for RDS instances..."
for instance in "streamflix-aurora-instance-1-${CLEANUP_SUFFIX}" "streamflix-aurora-instance-2-${CLEANUP_SUFFIX}"; do
  if aws rds describe-db-instances --db-instance-identifier "$instance" --region eu-west-2 >/dev/null 2>&1; then
    echo "  Deleting RDS instance: $instance"
    aws rds delete-db-instance --db-instance-identifier "$instance" --skip-final-snapshot --delete-automated-backups --region eu-west-2 >/dev/null 2>&1 || true
  fi
done

# Force delete ElastiCache replication group
echo "üóëÔ∏è Checking for ElastiCache replication group..."
REDIS_GROUP="streamflix-redis-${CLEANUP_SUFFIX}"
if aws elasticache describe-replication-groups --replication-group-id "$REDIS_GROUP" --region eu-west-2 >/dev/null 2>&1; then
  echo "  Deleting ElastiCache replication group: $REDIS_GROUP"
  aws elasticache delete-replication-group --replication-group-id "$REDIS_GROUP" --no-retain-primary-cluster --region eu-west-2 >/dev/null 2>&1 || true
fi

# Get EFS file system ID and delete mount targets
echo "üóëÔ∏è Checking for EFS mount targets..."
EFS_ID=$(aws efs describe-file-systems --region eu-west-2 --query "FileSystems[?Name=='streamflix-efs-${CLEANUP_SUFFIX}'].FileSystemId" --output text 2>/dev/null || echo "")
if [ -n "$EFS_ID" ] && [ "$EFS_ID" != "None" ]; then
  echo "  Found EFS file system: $EFS_ID"
  # Get and delete all mount targets for this file system
  MOUNT_TARGETS=$(aws efs describe-mount-targets --file-system-id "$EFS_ID" --region eu-west-2 --query "MountTargets[].MountTargetId" --output text 2>/dev/null || echo "")
  if [ -n "$MOUNT_TARGETS" ]; then
    for mt in $MOUNT_TARGETS; do
      echo "  Deleting mount target: $mt"
      aws efs delete-mount-target --mount-target-id "$mt" --region eu-west-2 >/dev/null 2>&1 || true
    done
  fi
fi

# Wait for resources to be deleted
echo "‚è≥ Waiting 10 seconds for AWS resources to be fully deleted..."
sleep 10

# Re-enable exit-on-error
set -e

# Clean up any Secrets Manager secrets that might be in deletion state
echo "=== Aggressive Secrets Cleanup Phase ==="
echo "üî• FORCEFULLY REMOVING ALL AWS SECRETS MANAGER SECRETS..."
echo "   This will use multiple strategies to ensure complete deletion"

# Temporarily disable exit-on-error for secret cleanup
set +e

# Determine the suffix to use for secrets
if [ -f "metadata.json" ]; then
  TASK_ID=$(jq -r '.task_id // ""' metadata.json 2>/dev/null || echo "")
else
  TASK_ID=""
fi

if [ -n "$TASK_ID" ]; then
  SECRET_SUFFIX="pr${TASK_ID}"
elif [ -n "$ENVIRONMENT_SUFFIX" ]; then
  SECRET_SUFFIX="$ENVIRONMENT_SUFFIX"
else
  SECRET_SUFFIX="dev"
fi

echo "Force deleting ALL secrets with suffix: $SECRET_SUFFIX in region: $AWS_REGION"

# Function to aggressively delete a secret with multiple strategies
force_delete_secret() {
  local secret_name=$1
  local max_retries=5
  local retry_count=0

  echo "  üîç Processing secret: $secret_name"

  while [ $retry_count -lt $max_retries ]; do
    retry_count=$((retry_count + 1))

    # Check if the secret exists
    SECRET_EXISTS=$(aws secretsmanager describe-secret \
      --secret-id "$secret_name" \
      --region "$AWS_REGION" 2>&1 || echo "ResourceNotFoundException")

    if echo "$SECRET_EXISTS" | grep -q "ResourceNotFoundException"; then
      echo "  ‚úÖ Secret does not exist or is fully deleted"
      return 0
    fi

    echo "  ‚ö†Ô∏è  Secret exists: $secret_name (attempt $retry_count/$max_retries)"

    # Strategy 1: Direct force delete
    echo "  üîÑ Attempting force delete..."
    DELETE_RESULT=$(aws secretsmanager delete-secret \
      --secret-id "$secret_name" \
      --force-delete-without-recovery \
      --region "$AWS_REGION" 2>&1 || echo "DeleteFailed")

    if echo "$DELETE_RESULT" | grep -q "DeletionDate"; then
      echo "  ‚úÖ Secret marked for immediate deletion"
      return 0
    fi

    # Strategy 2: If already scheduled for deletion, try restore then delete
    if echo "$DELETE_RESULT" | grep -q "InvalidRequestException"; then
      echo "  üîÑ Secret already scheduled, trying restore-delete strategy..."

      # Try to restore the secret first
      aws secretsmanager restore-secret \
        --secret-id "$secret_name" \
        --region "$AWS_REGION" 2>/dev/null || true

      sleep 1

      # Then immediately delete it
      aws secretsmanager delete-secret \
        --secret-id "$secret_name" \
        --force-delete-without-recovery \
        --region "$AWS_REGION" 2>/dev/null || true
    fi

    # Strategy 3: Update the secret then delete (sometimes works)
    if [ $retry_count -eq 3 ]; then
      echo "  üîÑ Trying update-delete strategy..."
      aws secretsmanager put-secret-value \
        --secret-id "$secret_name" \
        --secret-string "dummy" \
        --region "$AWS_REGION" 2>/dev/null || true

      sleep 1

      aws secretsmanager delete-secret \
        --secret-id "$secret_name" \
        --force-delete-without-recovery \
        --region "$AWS_REGION" 2>/dev/null || true
    fi

    sleep 2
  done

  echo "  ‚ùå Failed to delete secret after $max_retries attempts"
  return 1
}

# List of specific secret patterns to force delete
SECRETS_TO_DELETE=(
  "streamflix/api/keys-${SECRET_SUFFIX}"
  "streamflix/db/credentials-${SECRET_SUFFIX}"
  "streamflix-api-secret-${SECRET_SUFFIX}"
  "streamflix-db-secret-${SECRET_SUFFIX}"
)

# Force delete each specific secret
echo "Force deleting specific secrets..."
for secret in "${SECRETS_TO_DELETE[@]}"; do
  force_delete_secret "$secret" || {
    echo "  ‚ö†Ô∏è  Failed to process $secret, continuing..."
  }
done

# Also search for and delete any other secrets with our suffix
echo ""
echo "Searching for ALL secrets with suffix: ${SECRET_SUFFIX}..."

# Get all secrets including those scheduled for deletion
ALL_SECRETS=$(aws secretsmanager list-secrets \
  --include-planned-deletion \
  --region "$AWS_REGION" 2>/dev/null || echo '{"SecretList": []}')

MATCHING_SECRETS=$(echo "$ALL_SECRETS" | jq -r ".SecretList[] | select(.Name | contains(\"${SECRET_SUFFIX}\")) | .Name")

if [ -n "$MATCHING_SECRETS" ]; then
  echo "Found the following secrets to delete:"
  echo "$MATCHING_SECRETS" | while IFS= read -r secret; do
    echo "  ‚Ä¢ $secret"
  done
  echo ""

  echo "$MATCHING_SECRETS" | while IFS= read -r secret; do
    if [ -n "$secret" ]; then
      force_delete_secret "$secret" || {
        echo "  ‚ö†Ô∏è  Failed to process $secret, continuing..."
      }
    fi
  done
else
  echo "No additional secrets found with suffix: ${SECRET_SUFFIX}"
fi

# Double-check that problematic secrets are gone
echo ""
echo "üîç Final verification of critical secrets..."
FAILED_COUNT=0
for secret in "${SECRETS_TO_DELETE[@]}"; do
  SECRET_CHECK=$(aws secretsmanager describe-secret \
    --secret-id "$secret" \
    --region "$AWS_REGION" 2>&1 || echo "ResourceNotFoundException")

  if echo "$SECRET_CHECK" | grep -q "ResourceNotFoundException"; then
    echo "  ‚úÖ Confirmed deleted: $secret"
  else
    echo "  ‚ö†Ô∏è  WARNING: Secret may still exist: $secret"
    echo "  üî• FINAL AGGRESSIVE DELETION ATTEMPT..."

    # Try multiple strategies in rapid succession
    aws secretsmanager restore-secret --secret-id "$secret" --region "$AWS_REGION" 2>/dev/null || true
    sleep 0.5
    aws secretsmanager delete-secret --secret-id "$secret" --force-delete-without-recovery --region "$AWS_REGION" 2>/dev/null || true
    sleep 0.5
    aws secretsmanager put-secret-value --secret-id "$secret" --secret-string "DELETE_ME" --region "$AWS_REGION" 2>/dev/null || true
    sleep 0.5
    aws secretsmanager delete-secret --secret-id "$secret" --force-delete-without-recovery --region "$AWS_REGION" 2>/dev/null || true

    FAILED_COUNT=$((FAILED_COUNT + 1))
  fi
done

echo ""
if [ $FAILED_COUNT -eq 0 ]; then
  echo "‚úÖ Secrets cleanup phase completed successfully!"
  echo "   All critical secrets have been removed."
else
  echo "‚ö†Ô∏è  Secrets cleanup completed with warnings"
  echo "   $FAILED_COUNT secret(s) may still be in deletion state"
  echo "   Deployment will proceed, but may encounter issues"
fi

echo "‚è≥ Waiting 5 seconds to ensure AWS propagation..."
sleep 5

# Re-enable exit-on-error for deployment phase
set -e

# Deploy step
echo "=== Deploy Phase ==="
echo "üöÄ Starting fresh deployment..."
if [ "$PLATFORM" = "cdk" ]; then
  echo "‚úÖ CDK project detected, running CDK deploy..."
  npm run cdk:deploy

elif [ "$PLATFORM" = "cdktf" ]; then
  echo "‚úÖ CDKTF project detected, running CDKTF deploy..."
  
  if [ "$LANGUAGE" = "go" ]; then
    echo "üîß Ensuring .gen exists for CDKTF Go deploy"

    if [ -f "terraform.tfstate" ]; then
      echo "‚ö†Ô∏è Found legacy terraform.tfstate. Removing for clean CI run..."
      rm -f terraform.tfstate
    fi

    if [ -d "cdktf.out" ]; then
      echo "üóëÔ∏è Removing cdktf.out for clean CI run..."
      rm -rf cdktf.out
    fi

    if [ ! -d ".gen" ] || [ ! -d ".gen/aws" ]; then
      echo "Running cdktf get to generate .gen..."
      npm run cdktf:get || npx --yes cdktf get
    fi
    if [ ! -d ".gen/aws" ]; then
      echo "‚ùå .gen/aws missing after cdktf get; aborting"
      exit 1
    fi
    # Go modules are prepared during build; avoid cache-clearing and extra tidying here
  fi
  npm run cdktf:deploy

elif [ "$PLATFORM" = "cfn" ] && [ "$LANGUAGE" = "yaml" ]; then
  echo "‚úÖ CloudFormation YAML project detected, deploying with AWS CLI..."
  npm run cfn:deploy-yaml

elif [ "$PLATFORM" = "cfn" ] && [ "$LANGUAGE" = "json" ]; then
  echo "‚úÖ CloudFormation JSON project detected, deploying with AWS CLI..."
  npm run cfn:deploy-json

elif [ "$PLATFORM" = "tf" ]; then
  echo "‚úÖ Terraform HCL project detected, running Terraform deploy..."
  
  if [ -z "$TERRAFORM_STATE_BUCKET" ]; then
    echo "‚ùå TERRAFORM_STATE_BUCKET environment variable is required for Terraform projects"
    exit 1
  fi
  
  STATE_KEY="prs/${ENVIRONMENT_SUFFIX}/terraform.tfstate"
  echo "Using state key: $STATE_KEY"
  
  cd lib
  

  # Always remove any stale Terraform plan to avoid cross-run reuse
  rm -f tfplan
  
  # Check if plan file exists
  if [ -f "tfplan" ]; then
    echo "‚úÖ Terraform plan file found, proceeding with deployment..."
    # Try to deploy with the plan file
    if ! npm run tf:deploy; then
      echo "‚ö†Ô∏è Deployment with plan file failed, checking for state lock issues..."
      
      # Extract lock ID from error output if present
      LOCK_ID=$(terraform apply -auto-approve -lock=true -lock-timeout=10s -input=false tfplan 2>&1 | grep -oE 'ID:\s+[0-9a-f-]{36}' | cut -d' ' -f2 || echo "")
      
      if [ -n "$LOCK_ID" ]; then
        echo "üîì Detected stuck lock ID: $LOCK_ID. Attempting to force unlock..."
        terraform force-unlock -force "$LOCK_ID" || echo "Force unlock failed"
        echo "üîÑ Retrying deployment after unlock..."
        npm run tf:deploy || echo "Deployment still failed after unlock attempt"
      else
        echo "‚ùå Deployment failed but no lock ID detected. Manual intervention may be required."
      fi
    fi
  else
    echo "‚ö†Ô∏è Terraform plan file not found, creating new plan and deploying..."
    terraform plan -lock-timeout=120s -lock=false -input=false -out=tfplan || echo "Plan creation failed, attempting direct apply..."
    
    # Try direct apply with lock timeout, and handle lock issues
    if ! terraform apply -auto-approve -lock=true -lock-timeout=300s -input=false tfplan; then
      echo "‚ö†Ô∏è Direct apply with plan failed, trying without plan..."
      if ! terraform apply -auto-approve -lock=true -lock-timeout=300s -input=false; then
        echo "‚ùå All deployment attempts failed. Check for state lock issues."
        # List any potential locks
        terraform show -json 2>&1 | grep -i lock || echo "No lock information available"
      fi
    fi
  fi
  
  cd ..

elif [ "$PLATFORM" = "pulumi" ]; then
  echo "‚úÖ Pulumi project detected, running Pulumi deploy..."
  
  if [ -z "$PULUMI_BACKEND_URL" ]; then
    echo "‚ùå PULUMI_BACKEND_URL environment variable is required for Pulumi projects"
    exit 1
  fi
  
  echo "Using environment suffix: $ENVIRONMENT_SUFFIX"
  echo "Selecting or creating Pulumi stack Using ENVIRONMENT_SUFFIX=$ENVIRONMENT_SUFFIX"
  
  if [ "$LANGUAGE" = "go" ]; then
    echo "üîß Go Pulumi project detected"
    pulumi login "$PULUMI_BACKEND_URL"
    cd lib
    echo "Selecting or creating Pulumi stack..."
    pulumi stack select "${PULUMI_ORG}/TapStack/TapStack${ENVIRONMENT_SUFFIX}" --create
    echo "Deploying infrastructure ..."
    pulumi up --yes --refresh --stack "${PULUMI_ORG}/TapStack/TapStack${ENVIRONMENT_SUFFIX}"
    cd ..
  else
    echo "üîß Python Pulumi project detected"
    export PYTHONPATH=.:bin
    pipenv run pulumi-create-stack
    echo "Deploying infrastructure ..."
    pipenv run pulumi-deploy
  fi

else
  echo "‚ÑπÔ∏è Unknown deployment method for platform: $PLATFORM, language: $LANGUAGE"
  echo "üí° Supported combinations: cdk+typescript, cdk+python, cfn+yaml, cfn+json, cdktf+typescript, cdktf+python, tf+hcl, pulumi+python, pulumi+java"
  exit 1
fi

echo ""
echo "‚úÖ Full deployment cycle completed successfully!"
echo "   ‚úì Prerequisites bootstrapped"
echo "   ‚úì Existing resources destroyed"
echo "   ‚úì Secrets cleaned up"
echo "   ‚úì Fresh resources deployed"

# Get outputs using the dedicated script
echo "üìä Collecting deployment outputs..."
./scripts/get-outputs.sh
