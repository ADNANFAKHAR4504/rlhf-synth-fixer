You’re assisting me with building a multi-environment Terraform stack that must remain fully consistent across dev, staging, and prod, with only capacity-related values varying through tfvars. I need you to generate a single Terraform file named tap_stack.tf that defines the complete topology for a fleet telematics monitoring and compliance pipeline, while also satisfying enterprise requirements for high availability, encryption, compliance, and secure PHI handling. The environments will all share an identical resource graph — VPC layout, Kinesis ingestion streams, DynamoDB tables, Lambda processors, Redis clusters, Aurora databases, SNS/SQS messaging topology, S3 archival/data lake layers, Glue + Athena analytics stack, Step Functions reporting workflows, and full CloudWatch observability. Only scaling knobs like Lambda memory, Kinesis shard counts, DynamoDB RCU/WCU, Redis cluster size, and Firehose buffering may differ across tfvars.

The Terraform you generate should feel like something a senior cloud engineer would write for a production telematics & prescription-fulfillment ecosystem. Networking must be modeled with a dedicated VPC, public/private subnets, optional NAT, and appropriate security groups so that all Lambdas interacting with Aurora PostgreSQL and Redis run inside private subnets. Redis must be configured with TLS + auth, and Aurora connections must use Secrets Manager and connection pooling. All DynamoDB operations should be strictly atomic to prevent race-conditions (important for both vehicle diagnostics and real-time pharmacy inventory logic). Every data store — DynamoDB, S3, Redis, Aurora, Kinesis, Firehose, SNS/SQS, CloudWatch Logs — must use SSE-KMS encryption, and all inter-service communication must enforce TLS in transit. Include VPC endpoints for services like DynamoDB, Kinesis, S3, SNS, SQS, and SageMaker Runtime.

Model the full telemetry pipeline end-to-end: Kinesis streams ingest diagnostics, HOS updates, and GPS location data, feeding Lambda processors that write to DynamoDB tables with appropriate keys, TTL, and GSIs. DynamoDB streams must trigger additional Lambda functions for anomaly detection, geofence violations, fuel-efficiency analysis, and compliance evaluation. Redis clusters must store sorted-set time-series metrics, geospatial indexes, rolling-window samples, and real-time dashboard counters. SNS topics should fan out to multiple SQS queues, each consumed by Lambdas that integrate with Aurora for maintenance scheduling, update Redis counters for dashboards, or call external telematics APIs via API Gateway endpoints (with CORS + request validation requirements). Firehose should deliver raw diagnostic data into an S3 data lake (Parquet via transformation Lambda), followed by Glue Crawler cataloging and Athena queries executed within an encrypted workgroup. Step Functions should orchestrate daily DOT compliance reporting, pulling data from DynamoDB and Aurora, generating reports, uploading them to S3 with lifecycle policies, and publishing summaries via SNS.

Inside tap_stack.tf, include all variable declarations, locals for naming/tagging, all resources, all data sources, IAM roles, event source mappings, Kinesis/Lambda/DynamoDB/SNS/SQS/Redis/Aurora/S3/Glue/Firehose/Athena/Step Functions/CloudWatch resources, and all outputs for IDs/ARNs/endpoints. No placeholder code — always use concrete AWS resources. The provider configuration already exists elsewhere, but you must still declare variable "aws_region" in this file. At the end of the file, append three fully valid example tfvars files: dev.tfvars, staging.tfvars, and prod.tfvars, each showing how capacity parameters vary while keeping topology intact. Ensure the entire Terraform file is compatible with Terraform ≥1.5 and AWS provider ~>5.0.
