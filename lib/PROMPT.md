# **S3 Cross-Region Replication with Monitoring**

I need S3 cross-region replication between us-east-1 and eu-west-1 for a retail platform serving 100,000 daily users. The system needs to replicate 10TB of data automatically with comprehensive monitoring and audit logging to ensure consistency.

Create a source bucket in us-east-1 and a replica bucket in eu-west-1 using the naming pattern retail-data-source-{account-id}-{region}. Both buckets need versioning enabled since that's required for replication. Use customer-managed KMS keys for encryption with separate keys in each region. Block all public access on both buckets with all four settings enabled. Configure Intelligent-Tiering for automatic cost optimization and add lifecycle policies to delete incomplete multipart uploads after 7 days and expire non-current versions after 90 days.

Set up cross-region replication to replicate all objects including existing ones, delete markers, metadata, and tags. Since we're using KMS encryption, include the source_selection_criteria block with sse_kms_encrypted_objects status set to Enabled in the replication configuration. Enable Replication Time Control to guarantee objects replicate within 15 minutes. Enable replication metrics so we can monitor replication progress in CloudWatch. Create an IAM replication role that has permission to read from the source bucket, decrypt using the source region KMS key, write to the destination bucket, and encrypt using the destination region KMS key. Follow least privilege with no wildcard resource ARNs except where AWS requires it.

For the KMS keys, create one in each region with aliases alias/retail-data-source-key and alias/retail-data-replica-key. Enable automatic annual key rotation on both. The key policies need to allow the S3 service and the replication role to use the keys, and also allow CloudTrail to encrypt logs.

Set up CloudWatch monitoring with metric alarms for critical scenarios. Create alarms that alert if replication latency exceeds 15 minutes or if there's more than 100GB of data pending replication for more than 15 minutes. Also create alarms for high error rates like more than 100 4xx errors in 5 minutes or more than 10 5xx errors in 5 minutes. Create a CloudWatch dashboard called retail-s3-replication-dashboard that shows all key replication metrics from both regions including replication latency, pending bytes, pending operations, bucket sizes, and error rates with auto-refresh enabled.

Configure EventBridge rules to capture important S3 events and send them to SNS topics for alert notifications. Capture object events like PutObject, DeleteObject, and CompleteMultipartUpload. Capture replication-specific events like replication failures and Replication Time Control violations. Also capture security events like bucket policy changes, encryption changes, and versioning changes. Route these events to SNS topics based on severity - use retail-s3-critical-alerts for replication failures and security events, retail-s3-warning-alerts for high latency and backlog warnings, and retail-s3-info-alerts for routine notifications.

Enable CloudTrail logging for comprehensive audit trails. Create a trail called retail-s3-audit-trail that applies to all regions with log file validation enabled. Enable S3 data events for both buckets to capture all object-level API calls. Send CloudTrail logs to a dedicated S3 bucket called retail-cloudtrail-logs-{account-id} with proper bucket policy allowing CloudTrail write access, and also send them to CloudWatch Logs with 90-day retention for easier searching and analysis.

Organize the code into exactly four Terraform files following this structure: **provider.tf** should contain AWS provider configuration with required_providers block specifying Terraform version 1.5.0 or higher and AWS provider version 5.0 or higher, include two provider aliases "us_east_1" for us-east-1 region and "eu_west_1" for eu-west-1 region (use underscores in alias names not hyphens), define a locals block with common tags and use data source aws_caller_identity to dynamically fetch the account ID, and configure default_tags in both provider blocks for automatic resource tagging. **variables.tf** should contain all variable definitions including region with default us-east-1, environment with validation allowing only dev/staging/prod, project_name with default retail, alarm threshold variables for replication_latency_threshold defaulting to 900 seconds and pending_replication_threshold defaulting to 107374182400 bytes for 100GB, log_retention_days defaulting to 90, lifecycle_noncurrent_expiration_days defaulting to 90, lifecycle_multipart_expiration_days defaulting to 7, and any other configurable parameters with proper descriptions, types, defaults, and validation rules where appropriate. **tap_stack.tf** should contain all resource blocks organized with clear section comments in this order: data sources for AWS account ID and current region at the top, KMS customer-managed keys for both us-east-1 and eu-west-1 with key aliases and comprehensive key policies allowing S3 service, replication role, and CloudTrail service access, IAM replication role with trust policy allowing S3 service to assume it and IAM policy with least privilege permissions for source bucket read/KMS decrypt and destination bucket write/KMS encrypt using specific resource ARNs, source S3 bucket in us-east-1 with all configurations including versioning, server-side encryption with source KMS key, public access block with all four settings, intelligent-tiering configuration, and lifecycle policies, replica S3 bucket in eu-west-1 with matching configurations using replica KMS key, S3 replication configuration attached to source bucket including source_selection_criteria block for KMS encrypted objects, replication time control with 15-minute SLA, replication metrics, and delete marker replication enabled, CloudWatch metric alarms for replication latency, pending bytes, 4xx errors, and 5xx errors with appropriate thresholds and SNS topic actions, CloudWatch dashboard with widgets showing multi-region replication metrics, SNS topics for critical/warning/info alert severities, EventBridge rules for capturing S3 object events, replication events, and security configuration change events with SNS topics as targets, dedicated S3 bucket for CloudTrail logs with bucket policy allowing CloudTrail service to write logs, CloudTrail trail configuration with S3 data events enabled for both buckets, multi-region trail enabled, log file validation enabled, CloudWatch Logs integration configured, and IAM role for CloudTrail with permissions to write to CloudWatch Logs. **outputs.tf** should contain output blocks for source_bucket_name, source_bucket_arn, replica_bucket_name, replica_bucket_arn, replication_role_arn, source_kms_key_arn, replica_kms_key_arn, cloudtrail_trail_arn, sns_critical_topic_arn, sns_warning_topic_arn, and sns_info_topic_arn, all with descriptive labels explaining what each output represents.

Tag all resources with Project=retail, Environment=production, ManagedBy=Terraform, and DataClassification=Confidential. Don't output any sensitive information like KMS key IDs or secret access keys in the outputs. For testing during development, use small datasets like 10GB instead of the full 10TB to keep costs low. The code should work with terraform init, terraform plan, and terraform apply commands using a terraform.tfvars file for variable values.