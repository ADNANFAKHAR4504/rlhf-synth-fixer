# lib/ci-cd.yml
# Enterprise GitLab CI/CD — Multi-account AWS (dev/staging/prod) with OIDC, 15 stages, canary→blue/green→prod
# Images: pulled from private registry using $CI_REGISTRY/<category>/<tool>:<version>
# No hardcoded credentials or account IDs.

workflow:
  rules:
    - if: $CI_PIPELINE_SOURCE == "push"
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_PIPELINE_SOURCE == "web"
    - when: always

stages:
  - validation
  - build
  - test
  - security
  - compliance
  - dev_deploy
  - integration
  - canary
  - smoke
  - staging_deploy
  - e2e
  - prod_approval
  - prod_deploy
  - monitoring
  - rollback

default:
  interruptible: true
  retry:
    max: 1
    when:
      - runner_system_failure
      - api_failure
      - scheduler_failure
  artifacts:
    expire_in: 1 week
  cache:
    key: "node-${CI_PROJECT_NAME}-${CI_COMMIT_REF_SLUG}"
    paths:
      - node_modules/
    policy: pull-push
  before_script:
    - echo "Starting $CI_JOB_NAME in stage $CI_JOB_STAGE"

# -------------------------
# Anchors (reusable blocks)
# -------------------------
.aws_oidc_setup: &aws_oidc_setup
  image: $CI_REGISTRY/cli/awscli:2
  before_script:
    - mkdir -p .aws
    - echo "$CI_JOB_JWT_V2" > .aws/web_identity_token
    - export AWS_WEB_IDENTITY_TOKEN_FILE="$CI_PROJECT_DIR/.aws/web_identity_token"
    - export AWS_REGION="${AWS_REGION:-us-east-1}"
    - export ACCOUNT_ID="${ACCOUNT_ID:?ACCOUNT_ID missing}"
    - export AWS_ROLE_ARN="arn:aws:iam::${ACCOUNT_ID}:role/GitLabCIRole"
    - aws sts assume-role-with-web-identity --role-arn "$AWS_ROLE_ARN" --role-session-name "gitlab-${CI_PROJECT_NAME}-${CI_PIPELINE_ID}" --web-identity-token file://$AWS_WEB_IDENTITY_TOKEN_FILE --duration-seconds 3600 > .aws/creds.json
    - export AWS_ACCESS_KEY_ID=$(jq -r '.Credentials.AccessKeyId' .aws/creds.json)
    - export AWS_SECRET_ACCESS_KEY=$(jq -r '.Credentials.SecretAccessKey' .aws/creds.json)
    - export AWS_SESSION_TOKEN=$(jq -r '.Credentials.SessionToken' .aws/creds.json)

.kubectl_setup: &kubectl_setup
  image: $CI_REGISTRY/cli/kubectl:1.30
  before_script:
    - export AWS_REGION="${AWS_REGION:-us-east-1}"
    - export EKS_CLUSTER="${EKS_CLUSTER:?EKS_CLUSTER is required}"
    - aws eks update-kubeconfig --region "$AWS_REGION" --name "$EKS_CLUSTER"

.node_runtime: &node_runtime
  image: $CI_REGISTRY/ci/node:22
  before_script:
    - node --version
    - npm --version

# -------------------------
# Validation
# -------------------------
lint:
  stage: validation
  <<: *node_runtime
  rules:
    - changes:
        - "**/*.ts"
        - "**/*.js"
        - package.json
        - eslint.config.*
        - .eslintrc.*
  script:
    - npm ci
    - npm run lint
  artifacts:
    reports:
      junit: reports/junit/lint.xml
  coverage: '/Lines\s*:\s*(\d+\.\d+)%/'

license_check:
  stage: validation
  <<: *node_runtime
  rules:
    - changes:
        - package-lock.json
        - package.json
  script:
    - npm ci
    - npx license-checker --production --json > reports/license.json
    - test -s reports/license.json
  artifacts:
    paths:
      - reports/license.json

npm_audit:
  stage: validation
  <<: *node_runtime
  rules:
    - changes:
        - package-lock.json
        - package.json
  script:
    - npm ci
    - npm audit --audit-level=high --omit=dev
  artifacts:
    reports:
      junit: reports/junit/npm-audit.xml

# -------------------------
# Build
# -------------------------
cdk_synth:
  stage: build
  <<: *node_runtime
  rules:
    - changes:
        - "cdk/**"
        - "bin/**"
        - "lib/**"
        - "package*.json"
        - "tsconfig.json"
  script:
    - npm ci
    - npx cdk synth
    - tar -czf cdk.out.tgz cdk.out
  artifacts:
    paths:
      - cdk.out.tgz

container_build:
  stage: build
  image: $CI_REGISTRY/ci/docker:24
  services:
    - name: $CI_REGISTRY/ci/docker:24-dind
      alias: docker
  variables:
    DOCKER_DRIVER: overlay2
    DOCKER_TLS_CERTDIR: ""
  rules:
    - changes:
        - "Dockerfile"
        - "docker/**"
        - ".dockerignore"
  script:
    - echo "$CI_REGISTRY_PASSWORD" | docker login "$CI_REGISTRY" -u "$CI_REGISTRY_USER" --password-stdin
    - docker build -t "$CI_REGISTRY_IMAGE/app:${CI_COMMIT_SHORT_SHA}" -f Dockerfile .
    - docker push "$CI_REGISTRY_IMAGE/app:${CI_COMMIT_SHORT_SHA}"
    - docker tag "$CI_REGISTRY_IMAGE/app:${CI_COMMIT_SHORT_SHA}" "$CI_REGISTRY_IMAGE/app:latest"
    - docker push "$CI_REGISTRY_IMAGE/app:latest"
  artifacts:
    reports:
      junit: reports/junit/docker-build.xml

# -------------------------
# Testing
# -------------------------
unit_tests:
  stage: test
  <<: *node_runtime
  rules:
    - changes:
        - "**/*.ts"
        - "**/*.js"
        - "jest.*"
        - "test/**"
  script:
    - npm ci
    - npm run test:unit -- --ci --reporters=default --reporters=jest-junit --coverage --coverageReporters=cobertura --outputFile=reports/junit/unit.xml
  artifacts:
    reports:
      junit: reports/junit/unit.xml
      cobertura: coverage/cobertura-coverage.xml
    paths:
      - coverage/
  coverage: '/Lines\s*:\s*(\d+\.\d+)%/'

mutation_tests:
  stage: test
  <<: *node_runtime
  rules:
    - changes:
        - "**/*.ts"
        - "**/*.js"
        - "stryker.conf.*"
  script:
    - npm ci
    - npx stryker run
  artifacts:
    paths:
      - reports/mutation/

k6_load_tests:
  stage: test
  image: $CI_REGISTRY/ci/k6:0.51
  rules:
    - changes:
        - "load/**"
        - "k6/**"
  script:
    - k6 run load/main.js
  artifacts:
    paths:
      - reports/k6/

# -------------------------
# Security
# -------------------------
semgrep_sast:
  stage: security
  image: $CI_REGISTRY/security/semgrep:1.80
  rules:
    - changes:
        - "**/*.ts"
        - "**/*.js"
        - "semgrep/*.yml"
  script:
    - semgrep ci --config "p/ci" --config semgrep/ --json --output reports/semgrep.json
  artifacts:
    reports:
      sast: reports/semgrep.json
  allow_failure: false

trufflehog_secrets:
  stage: security
  image: $CI_REGISTRY/security/trufflehog:3
  rules:
    - changes:
        - "**/*"
  script:
    - trufflehog git --only-verified --fail --since-commit ${CI_MERGE_REQUEST_DIFF_BASE_SHA:-HEAD~50} --repo . --json > reports/trufflehog.json
  artifacts:
    paths:
      - reports/trufflehog.json
  allow_failure: false

snyk_sca:
  stage: security
  image: $CI_REGISTRY/security/snyk:latest
  rules:
    - changes:
        - "package*.json"
        - "snyk.*"
  script:
    - snyk test --severity-threshold=high --json-file-output=reports/snyk.json
  artifacts:
    paths:
      - reports/snyk.json

cdk_nag:
  stage: security
  <<: *node_runtime
  rules:
    - changes:
        - "cdk/**"
        - "lib/**"
        - "bin/**"
  script:
    - npm ci
    - npx cdk synth
  artifacts:
    reports:
      junit: reports/junit/cdk-nag.xml

trivy_grype_container_scans:
  stage: security
  image: $CI_REGISTRY/security/trivy:0.54
  rules:
    - changes:
        - "Dockerfile"
        - "docker/**"
  script:
    - trivy image --exit-code 1 --severity HIGH,CRITICAL --format json --output reports/trivy.json "$CI_REGISTRY_IMAGE/app:${CI_COMMIT_SHORT_SHA}"
    - grype "$CI_REGISTRY_IMAGE/app:${CI_COMMIT_SHORT_SHA}" -o json > reports/grype.json
    - syft "$CI_REGISTRY_IMAGE/app:${CI_COMMIT_SHORT_SHA}" -o spdx-json > reports/sbom.spdx.json
  artifacts:
    paths:
      - reports/trivy.json
      - reports/grype.json
      - reports/sbom.spdx.json
  allow_failure: false

# -------------------------
# Compliance
# -------------------------
checkov_cis:
  stage: compliance
  image: $CI_REGISTRY/compliance/checkov:3
  rules:
    - changes:
        - "**/*.tf"
        - ".checkov.yml"
  script:
    - checkov -d . -o junitxml > reports/junit/checkov.xml
  artifacts:
    reports:
      junit: reports/junit/checkov.xml

prowler_pci:
  stage: compliance
  <<: *aws_oidc_setup
  variables:
    ACCOUNT_ID: $STAGING_ACCOUNT_ID
  rules:
    - changes:
        - "infra/**"
        - "**/*.tf"
        - ".prowler/**"
  script:
    - bash scripts/run-prowler.sh pci-dss reports/prowler
  artifacts:
    paths:
      - reports/prowler/

infracost_estimate:
  stage: compliance
  image: $CI_REGISTRY/compliance/infracost:0.10
  rules:
    - changes:
        - "**/*.tf"
        - "infracost*.yml"
  script:
    - infracost breakdown --path . --format json --out-file reports/infracost.json
  artifacts:
    paths:
      - reports/infracost.json

# -------------------------
# Dev Deploy
# -------------------------
deploy_dev:
  stage: dev_deploy
  <<: *aws_oidc_setup
  variables:
    ACCOUNT_ID: $DEV_ACCOUNT_ID
    AWS_REGION: ${AWS_REGION:-us-east-1}
    ENV_NAME: dev
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      changes: ["cdk/**", "lib/**", "bin/**", "Dockerfile", "docker/**"]
    - if: $CI_COMMIT_BRANCH == "develop"
      changes: ["cdk/**", "lib/**", "bin/**", "Dockerfile", "docker/**"]
  script:
    - bash scripts/deploy.sh "$ENV_NAME"
  environment:
    name: dev/$CI_COMMIT_REF_SLUG
    url: https://dev.example.com
    on_stop: stop_dev
    auto_stop_in: 1 week
    tier: development
  artifacts:
    reports:
      junit: reports/junit/dev-deploy.xml
  needs:
    - cdk_synth
    - container_build

stop_dev:
  stage: dev_deploy
  <<: *aws_oidc_setup
  variables:
    ACCOUNT_ID: $DEV_ACCOUNT_ID
    ENV_NAME: dev
  when: manual
  script:
    - bash scripts/deploy.sh "$ENV_NAME" --destroy
  environment:
    name: dev/$CI_COMMIT_REF_SLUG
    action: stop
    tier: development

# -------------------------
# Integration Testing
# -------------------------
integration_tests:
  stage: integration
  <<: *node_runtime
  rules:
    - changes:
        - "test/integration/**"
        - "pact/**"
  script:
    - npm ci
    - npm run test:api:integration -- --reporters=jest-junit --outputFile=reports/junit/integration.xml
    - npm run pact:verify
  artifacts:
    reports:
      junit: reports/junit/integration.xml

# -------------------------
# Canary
# -------------------------
deploy_canary:
  stage: canary
  <<: *aws_oidc_setup
  variables:
    ACCOUNT_ID: $STAGING_ACCOUNT_ID
    AWS_REGION: ${AWS_REGION:-us-east-1}
    EKS_CLUSTER: staging-cluster
    CANARY_WEIGHT: "10"
  when: manual
  script:
    - bash scripts/deploy-canary.sh "$CANARY_WEIGHT"
  environment:
    name: staging/canary
    url: https://staging.example.com
    tier: staging
  needs:
    - deploy_dev
    - integration_tests

# -------------------------
# Smoke
# -------------------------
smoke_canary:
  stage: smoke
  image: $CI_REGISTRY/qa/newman:6
  rules:
    - changes:
        - "postman/**"
        - "scripts/monitor-canary.sh"
  script:
    - bash scripts/monitor-canary.sh
    - newman run postman/collection.json --environment postman/staging.json --reporters cli,junit --reporter-junit-export reports/junit/newman.xml
  artifacts:
    reports:
      junit: reports/junit/newman.xml
  needs:
    - deploy_canary

# -------------------------
# Staging Deploy
# -------------------------
deploy_staging:
  stage: staging_deploy
  <<: *aws_oidc_setup
  variables:
    ACCOUNT_ID: $STAGING_ACCOUNT_ID
    AWS_REGION: ${AWS_REGION:-us-east-1}
    EKS_CLUSTER: staging-cluster
  when: manual
  script:
    - bash scripts/deploy-blue-green.sh
  environment:
    name: staging
    url: https://staging.example.com
    tier: staging
  needs:
    - smoke_canary

promote_canary:
  stage: staging_deploy
  <<: *aws_oidc_setup
  variables:
    ACCOUNT_ID: $STAGING_ACCOUNT_ID
    AWS_REGION: ${AWS_REGION:-us-east-1}
    EKS_CLUSTER: staging-cluster
  when: manual
  script:
    - bash scripts/promote-canary.sh
  environment:
    name: staging
    url: https://staging.example.com
    tier: staging
  needs:
    - deploy_staging

# -------------------------
# E2E Testing
# -------------------------
e2e_cypress:
  stage: e2e
  image: $CI_REGISTRY/qa/cypress:13
  rules:
    - changes:
        - "cypress/**"
  script:
    - npm ci
    - npx cypress run --reporter junit --reporter-options "mochaFile=reports/junit/cypress-[hash].xml"
  artifacts:
    reports:
      junit: reports/junit/cypress-*.xml

e2e_playwright_accessibility:
  stage: e2e
  image: $CI_REGISTRY/qa/playwright:1.47
  rules:
    - changes:
        - "playwright/**"
        - "scripts/run-a11y-tests.sh"
  script:
    - bash scripts/run-a11y-tests.sh
  allow_failure: true

# -------------------------
# Production Approvals
# -------------------------
prod_security_approval:
  stage: prod_approval
  when: manual
  script:
    - echo "Security team approval granted"
  needs:
    - e2e_cypress
    - e2e_playwright_accessibility

prod_product_approval:
  stage: prod_approval
  when: manual
  script:
    - echo "Product team approval granted"
  needs:
    - e2e_cypress
    - e2e_playwright_accessibility

# -------------------------
# Production Deploy
# -------------------------
deploy_prod:
  stage: prod_deploy
  <<: *aws_oidc_setup
  variables:
    ACCOUNT_ID: $PROD_ACCOUNT_ID
    AWS_REGION: ${AWS_REGION:-us-east-1}
    EKS_CLUSTER: production-cluster
  when: manual
  script:
    - bash scripts/deploy.sh prod
  environment:
    name: production
    url: https://app.example.com
    tier: production
  needs:
    - prod_security_approval
    - prod_product_approval
  retry:
    max: 2
    when:
      - script_failure

# -------------------------
# Monitoring
# -------------------------
monitor_release:
  stage: monitoring
  image: $CI_REGISTRY/ops/node-tools:22
  rules:
    - changes:
        - "scripts/create-datadog-event.sh"
        - "scripts/run-synthetic-tests.sh"
        - "scripts/run-lighthouse.sh"
  script:
    - export SENTRY_RELEASE="${CI_COMMIT_TAG:-$CI_COMMIT_SHORT_SHA}"
    - npm ci && npx @sentry/cli releases new "$SENTRY_RELEASE"
    - bash scripts/create-datadog-event.sh "Deployed ${CI_COMMIT_SHORT_SHA} to ${CI_ENVIRONMENT_NAME}"
    - bash scripts/run-synthetic-tests.sh && bash scripts/run-lighthouse.sh
  needs:
    - deploy_prod

# -------------------------
# Rollback
# -------------------------
rollback_prod:
  stage: rollback
  <<: *aws_oidc_setup
  variables:
    ACCOUNT_ID: $PROD_ACCOUNT_ID
    AWS_REGION: ${AWS_REGION:-us-east-1}
    EKS_CLUSTER: production-cluster
  when: manual
  script:
    - bash scripts/rollback.sh
  environment:
    name: production
    action: rollback
    tier: production
  needs:
    - deploy_prod

# -------------------------
# Notifications
# -------------------------
notify_slack_main_develop:
  stage: monitoring
  image: $CI_REGISTRY/ops/curl:8
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      changes: ["**/*"]
    - if: $CI_COMMIT_BRANCH == "develop"
      changes: ["**/*"]
  script:
    - bash scripts/create-datadog-event.sh "Pipeline success ${CI_COMMIT_BRANCH} ${CI_COMMIT_SHORT_SHA}"
    - 'curl -sS -X POST "$SLACK_WEBHOOK_URL" -H "Content-type: application/json" -d "{\"text\":\"✅ Pipeline completed for *${CI_COMMIT_BRANCH}* @ *${CI_COMMIT_SHORT_SHA}* (<${CI_PROJECT_URL}/-/pipelines/${CI_PIPELINE_ID}|view>)\"}"'

notify_pagerduty_on_main_fail:
  stage: monitoring
  image: $CI_REGISTRY/ops/curl:8
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      changes: ["**/*"]
  when: on_failure
  script:
    - bash scripts/notify-pagerduty.sh "Main pipeline failed for ${CI_COMMIT_SHORT_SHA} — ${CI_PROJECT_URL}/-/pipelines/${CI_PIPELINE_ID}"
