AWSTemplateFormatVersion: "2010-09-09"
Description: "Serverless Image Processing Pipeline with S3, Lambda, DynamoDB, and CloudWatch"

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: "Environment Configuration"
        Parameters:
          - Environment
          - SourceBucketName
          - ThumbnailBucketName
          - ThumbnailSize

Parameters:
  Environment:
    Type: String
    Default: "production"
    AllowedValues:
      - development
      - staging
      - production
    Description: "Environment name for resource tagging"

  SourceBucketName:
    Type: String
    Default: "tap-source-images-bucket"
    Description: "Name for the source image bucket (must be globally unique)"
    MinLength: 3
    MaxLength: 63
    AllowedPattern: "^[a-z0-9][a-z0-9-]*[a-z0-9]$"
    ConstraintDescription: "Must be a valid S3 bucket name"

  ThumbnailBucketName:
    Type: String
    Default: "tap-thumbnail-images-bucket"
    Description: "Name for the thumbnail bucket (must be globally unique)"
    MinLength: 3
    MaxLength: 63
    AllowedPattern: "^[a-z0-9][a-z0-9-]*[a-z0-9]$"
    ConstraintDescription: "Must be a valid S3 bucket name"

  ThumbnailSize:
    Type: Number
    Default: 128
    MinValue: 64
    MaxValue: 512
    Description: "Thumbnail size in pixels (width and height)"

Resources:
  # S3 Bucket for Source Images
  SourceBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref SourceBucketName
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldImages
            Status: Enabled
            ExpirationInDays: 90
            NoncurrentVersionExpirationInDays: 30
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: Purpose
          Value: SourceImageStorage
        - Key: iac-rlhf-amazon
          Value: "true"

  # S3 Bucket for Thumbnails
  ThumbnailBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref ThumbnailBucketName
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: TransitionToIA
            Status: Enabled
            Transitions:
              - TransitionInDays: 30
                StorageClass: STANDARD_IA
          - Id: TransitionToGlacier
            Status: Enabled
            Transitions:
              - TransitionInDays: 90
                StorageClass: GLACIER
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: Purpose
          Value: ThumbnailStorage
        - Key: iac-rlhf-amazon
          Value: "true"

  # DynamoDB Table for Image Metadata
  ImageMetadataTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Sub "${AWS::StackName}-ImageMetadata"
      BillingMode: PAY_PER_REQUEST
      PointInTimeRecoverySpecification:
        PointInTimeRecoveryEnabled: true
      SSESpecification:
        SSEEnabled: true
      AttributeDefinitions:
        - AttributeName: ImageID
          AttributeType: S
        - AttributeName: UploadTimestamp
          AttributeType: S
      KeySchema:
        - AttributeName: ImageID
          KeyType: HASH
      GlobalSecondaryIndexes:
        - IndexName: TimestampIndex
          KeySchema:
            - AttributeName: UploadTimestamp
              KeyType: HASH
            - AttributeName: ImageID
              KeyType: RANGE
          Projection:
            ProjectionType: ALL
      StreamSpecification:
        StreamViewType: NEW_AND_OLD_IMAGES
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: Purpose
          Value: ImageMetadataStorage
        - Key: iac-rlhf-amazon
          Value: "true"

  # IAM Role for Lambda Function
  ImageProcessorRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${AWS::StackName}-ImageProcessorRole"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: "sts:AssumeRole"
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
      Policies:
        - PolicyName: ImageProcessorPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "s3:GetObject"
                  - "s3:GetObjectVersion"
                Resource: !Sub "${SourceBucket.Arn}/*"
              - Effect: Allow
                Action:
                  - "s3:PutObject"
                  - "s3:PutObjectAcl"
                Resource: !Sub "${ThumbnailBucket.Arn}/*"
              - Effect: Allow
                Action:
                  - "dynamodb:PutItem"
                  - "dynamodb:UpdateItem"
                  - "dynamodb:GetItem"
                Resource: !GetAtt ImageMetadataTable.Arn
              - Effect: Allow
                Action:
                  - "cloudwatch:PutMetricData"
                Resource: "*"
                Condition:
                  StringEquals:
                    "cloudwatch:namespace": "ImageProcessing"
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: iac-rlhf-amazon
          Value: "true"

  # Lambda Permission for S3 to Invoke
  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ImageProcessorFunction
      Action: "lambda:InvokeFunction"
      Principal: s3.amazonaws.com
      SourceAccount: !Ref "AWS::AccountId"
      SourceArn: !GetAtt SourceBucket.Arn

  # Custom Resource to Configure S3 Bucket Notifications (avoids circular dependency)
  S3BucketNotificationCustomResource:
    Type: AWS::CloudFormation::CustomResource
    DependsOn:
      - LambdaInvokePermission
    Properties:
      ServiceToken: !GetAtt ConfigureS3NotificationFunction.Arn
      BucketName: !Ref SourceBucket
      LambdaFunctionArn: !GetAtt ImageProcessorFunction.Arn

  # Lambda Function to Configure S3 Notifications
  ConfigureS3NotificationFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub "${AWS::StackName}-ConfigureS3Notification"
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt ConfigureS3NotificationRole.Arn
      Timeout: 60
      Code:
        ZipFile: |
          import boto3
          import json
          import urllib3
          import logging

          # Configure logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          def send_response(event, context, response_status, response_data=None, physical_resource_id=None, reason=None):
              """Send response to CloudFormation"""
              if response_data is None:
                  response_data = {}
                  
              if physical_resource_id is None:
                  physical_resource_id = context.log_stream_name
                  
              if reason is None:
                  reason = f"See CloudWatch Log Stream: {context.log_stream_name}"

              response_url = event['ResponseURL']
              
              response_body = {
                  'Status': response_status,
                  'Reason': reason,
                  'PhysicalResourceId': physical_resource_id,
                  'StackId': event['StackId'],
                  'RequestId': event['RequestId'],
                  'LogicalResourceId': event['LogicalResourceId'],
                  'Data': response_data
              }

              json_response_body = json.dumps(response_body)
              logger.info(f"Response body: {json_response_body}")

              headers = {
                  'content-type': '',
                  'content-length': str(len(json_response_body))
              }

              try:
                  http = urllib3.PoolManager()
                  response = http.request('PUT', response_url, body=json_response_body, headers=headers)
                  logger.info(f"Status code: {response.status}")
              except Exception as e:
                  logger.error(f"Failed to send response: {str(e)}")

          def lambda_handler(event, context):
              logger.info(f"Received event: {json.dumps(event, default=str)}")
              
              try:
                  request_type = event['RequestType']
                  properties = event['ResourceProperties']
                  
                  bucket_name = properties.get('BucketName')
                  lambda_arn = properties.get('LambdaFunctionArn')
                  
                  if not bucket_name:
                      raise ValueError("BucketName is required in ResourceProperties")
                  if not lambda_arn and request_type != 'Delete':
                      raise ValueError("LambdaFunctionArn is required in ResourceProperties for Create/Update")
                  
                  logger.info(f"Processing {request_type} request for bucket: {bucket_name}")
                  
                  s3_client = boto3.client('s3')
                  
                  if request_type in ['Create', 'Update']:
                      # Configure bucket notifications
                      notification_config = {
                          'LambdaFunctionConfigurations': [
                              {
                                  'Id': 'ImageProcessingJPG',
                                  'LambdaFunctionArn': lambda_arn,
                                  'Events': ['s3:ObjectCreated:*'],
                                  'Filter': {
                                      'Key': {
                                          'FilterRules': [
                                              {'Name': 'suffix', 'Value': '.jpg'}
                                          ]
                                      }
                                  }
                              },
                              {
                                  'Id': 'ImageProcessingJPEG',
                                  'LambdaFunctionArn': lambda_arn,
                                  'Events': ['s3:ObjectCreated:*'],
                                  'Filter': {
                                      'Key': {
                                          'FilterRules': [
                                              {'Name': 'suffix', 'Value': '.jpeg'}
                                          ]
                                      }
                                  }
                              },
                              {
                                  'Id': 'ImageProcessingPNG',
                                  'LambdaFunctionArn': lambda_arn,
                                  'Events': ['s3:ObjectCreated:*'],
                                  'Filter': {
                                      'Key': {
                                          'FilterRules': [
                                              {'Name': 'suffix', 'Value': '.png'}
                                          ]
                                      }
                                  }
                              }
                          ]
                      }
                      
                      logger.info(f"Setting notification configuration: {json.dumps(notification_config, default=str)}")
                      
                      s3_client.put_bucket_notification_configuration(
                          Bucket=bucket_name,
                          NotificationConfiguration=notification_config
                      )
                      
                      logger.info("Successfully configured bucket notifications")
                      
                  elif request_type == 'Delete':
                      # Remove bucket notifications
                      logger.info("Removing bucket notifications")
                      s3_client.put_bucket_notification_configuration(
                          Bucket=bucket_name,
                          NotificationConfiguration={}
                      )
                      logger.info("Successfully removed bucket notifications")
                  
                  # Send success response
                  send_response(event, context, 'SUCCESS', {
                      'BucketName': bucket_name,
                      'Status': f'{request_type} completed successfully'
                  }, f"{bucket_name}-notification-config")
                  
              except Exception as e:
                  error_message = f"Error processing {event.get('RequestType', 'Unknown')} request: {str(e)}"
                  logger.error(error_message)
                  logger.exception("Full exception details:")
                  
                  # Send failure response
                  send_response(event, context, 'FAILED', {}, None, error_message)

  # IAM Role for the S3 Notification Configuration Lambda
  ConfigureS3NotificationRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${AWS::StackName}-ConfigureS3NotificationRole"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: "sts:AssumeRole"
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
      Policies:
        - PolicyName: S3NotificationPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "s3:PutBucketNotification"
                  - "s3:GetBucketNotification"
                Resource: !GetAtt SourceBucket.Arn

  # CloudWatch Log Group for Lambda
  ImageProcessorLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/lambda/${AWS::StackName}-ImageProcessor"
      RetentionInDays: 7
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: iac-rlhf-amazon
          Value: "true"

  # Lambda Function for Image Processing (without external layer dependency)
  ImageProcessorFunction:
    Type: AWS::Lambda::Function
    DependsOn: ImageProcessorLogGroup
    Properties:
      FunctionName: !Sub "${AWS::StackName}-ImageProcessor"
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt ImageProcessorRole.Arn
      Timeout: 60
      MemorySize: 512
      ReservedConcurrentExecutions: 10
      Environment:
        Variables:
          THUMBNAIL_BUCKET: !Ref ThumbnailBucket
          METADATA_TABLE: !Ref ImageMetadataTable
          THUMBNAIL_SIZE: !Ref ThumbnailSize
          ENVIRONMENT: !Ref Environment
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          import uuid
          from datetime import datetime
          from urllib.parse import unquote_plus
          try:
              from PIL import Image
          except ImportError:
              print("PIL not available, using basic image processing")
              Image = None
          import io

          # Initialize AWS clients
          s3_client = boto3.client('s3')
          dynamodb = boto3.resource('dynamodb')
          cloudwatch = boto3.client('cloudwatch')

          # Environment variables
          THUMBNAIL_BUCKET = os.environ['THUMBNAIL_BUCKET']
          METADATA_TABLE = os.environ['METADATA_TABLE']
          THUMBNAIL_SIZE = int(os.environ.get('THUMBNAIL_SIZE', '128'))
          ENVIRONMENT = os.environ.get('ENVIRONMENT', 'production')

          def lambda_handler(event, context):
              """Process uploaded images and generate thumbnails"""
              
              # Get DynamoDB table
              table = dynamodb.Table(METADATA_TABLE)
              
              for record in event['Records']:
                  # Get bucket and key from the S3 event
                  bucket = record['s3']['bucket']['name']
                  key = unquote_plus(record['s3']['object']['key'])
                  
                  print(f"Processing image: {bucket}/{key}")
                  
                  try:
                      # Generate unique image ID
                      image_id = str(uuid.uuid4())
                      
                      # Get the image from S3
                      response = s3_client.get_object(Bucket=bucket, Key=key)
                      image_content = response['Body'].read()
                      
                      if Image:
                          # Process with PIL
                          image = Image.open(io.BytesIO(image_content))
                          image.thumbnail((THUMBNAIL_SIZE, THUMBNAIL_SIZE), Image.Resampling.LANCZOS)
                          
                          thumbnail_buffer = io.BytesIO()
                          format = 'JPEG' if image.mode == 'RGB' else 'PNG'
                          image.save(thumbnail_buffer, format=format)
                          thumbnail_buffer.seek(0)
                          thumbnail_content = thumbnail_buffer.getvalue()
                          dimensions = f"{image.width}x{image.height}"
                      else:
                          # Basic processing without PIL
                          thumbnail_content = image_content  # Use original for now
                          dimensions = "unknown"
                      
                      # Create thumbnail key
                      file_name = os.path.basename(key)
                      thumbnail_key = f"thumbnails/{image_id}/{file_name}"
                      
                      # Upload thumbnail to S3
                      s3_client.put_object(
                          Bucket=THUMBNAIL_BUCKET,
                          Key=thumbnail_key,
                          Body=thumbnail_content,
                          ContentType='image/jpeg',
                          Metadata={
                              'original-image': f"{bucket}/{key}",
                              'processed-date': datetime.utcnow().isoformat()
                          }
                      )
                      
                      # Store metadata in DynamoDB
                      metadata_item = {
                          'ImageID': image_id,
                          'OriginalBucket': bucket,
                          'OriginalKey': key,
                          'OriginalSize': len(image_content),
                          'ThumbnailBucket': THUMBNAIL_BUCKET,
                          'ThumbnailKey': thumbnail_key,
                          'ThumbnailSize': len(thumbnail_content),
                          'UploadTimestamp': datetime.utcnow().isoformat(),
                          'ProcessedTimestamp': datetime.utcnow().isoformat(),
                          'ImageDimensions': dimensions,
                          'ThumbnailDimensions': f"{THUMBNAIL_SIZE}x{THUMBNAIL_SIZE}",
                          'Environment': ENVIRONMENT,
                          'ProcessingStatus': 'SUCCESS'
                      }
                      
                      table.put_item(Item=metadata_item)
                      
                      # Send success metric to CloudWatch
                      send_metric('ProcessedImages', 1, 'Count')
                      
                      print(f"Successfully processed image: {image_id}")
                      
                  except Exception as e:
                      print(f"Error processing image {key}: {str(e)}")
                      
                      # Log error to DynamoDB
                      error_item = {
                          'ImageID': str(uuid.uuid4()),
                          'OriginalBucket': bucket,
                          'OriginalKey': key,
                          'UploadTimestamp': datetime.utcnow().isoformat(),
                          'ProcessedTimestamp': datetime.utcnow().isoformat(),
                          'ProcessingStatus': 'FAILED',
                          'ErrorMessage': str(e),
                          'Environment': ENVIRONMENT
                      }
                      
                      table.put_item(Item=error_item)
                      
                      # Send error metric to CloudWatch
                      send_metric('ProcessingErrors', 1, 'Count')
                      
                      # Re-raise the exception
                      raise

          def send_metric(metric_name, value, unit):
              """Send custom metric to CloudWatch"""
              try:
                  cloudwatch.put_metric_data(
                      Namespace='ImageProcessing',
                      MetricData=[
                          {
                              'MetricName': metric_name,
                              'Value': value,
                              'Unit': unit,
                              'Dimensions': [
                                  {
                                      'Name': 'Environment',
                                      'Value': ENVIRONMENT
                                  }
                              ]
                          }
                      ]
                  )
              except Exception as e:
                  print(f"Error sending metric: {str(e)}")

      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: Purpose
          Value: ImageProcessing
        - Key: iac-rlhf-amazon
          Value: "true"

  # CloudWatch Alarms
  ProcessingErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub "${AWS::StackName}-ProcessingErrors"
      AlarmDescription: "Alarm when image processing errors occur"
      MetricName: ProcessingErrors
      Namespace: ImageProcessing
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 5
      ComparisonOperator: GreaterThanThreshold
      TreatMissingData: notBreaching
      Dimensions:
        - Name: Environment
          Value: !Ref Environment
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: iac-rlhf-amazon
          Value: "true"

  LambdaDurationAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub "${AWS::StackName}-LambdaDuration"
      AlarmDescription: "Alarm when Lambda execution duration is high"
      MetricName: Duration
      Namespace: AWS/Lambda
      Statistic: Average
      Period: 300
      EvaluationPeriods: 2
      Threshold: 30000
      ComparisonOperator: GreaterThanThreshold
      TreatMissingData: notBreaching
      Dimensions:
        - Name: FunctionName
          Value: !Ref ImageProcessorFunction
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: iac-rlhf-amazon
          Value: "true"

  LambdaThrottleAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub "${AWS::StackName}-LambdaThrottles"
      AlarmDescription: "Alarm when Lambda throttling occurs"
      MetricName: Throttles
      Namespace: AWS/Lambda
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      TreatMissingData: notBreaching
      Dimensions:
        - Name: FunctionName
          Value: !Ref ImageProcessorFunction
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: iac-rlhf-amazon
          Value: "true"

Outputs:
  SourceBucketName:
    Description: "Name of the source image bucket"
    Value: !Ref SourceBucket
    Export:
      Name: !Sub "${AWS::StackName}-SourceBucket"

  ThumbnailBucketName:
    Description: "Name of the thumbnail bucket"
    Value: !Ref ThumbnailBucket
    Export:
      Name: !Sub "${AWS::StackName}-ThumbnailBucket"

  ImageMetadataTableName:
    Description: "Name of the DynamoDB table for image metadata"
    Value: !Ref ImageMetadataTable
    Export:
      Name: !Sub "${AWS::StackName}-MetadataTable"

  ImageProcessorFunctionArn:
    Description: "ARN of the image processor Lambda function"
    Value: !GetAtt ImageProcessorFunction.Arn
    Export:
      Name: !Sub "${AWS::StackName}-ProcessorFunctionArn"

  ImageProcessorFunctionName:
    Description: "Name of the image processor Lambda function"
    Value: !Ref ImageProcessorFunction
    Export:
      Name: !Sub "${AWS::StackName}-ProcessorFunctionName"

  CloudWatchDashboardURL:
    Description: "URL to CloudWatch dashboard for monitoring"
    Value: !Sub "https://console.aws.amazon.com/cloudwatch/home?region=${AWS::Region}#dashboards:name=${AWS::StackName}"
    Export:
      Name: !Sub "${AWS::StackName}-CloudWatchDashboardURL"
