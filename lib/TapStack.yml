AWSTemplateFormatVersion: '2010-09-09'
Description: 'Enterprise Log Analytics System for 500 servers (Fully Automated Deployment)'

Resources:
  # ==========================================
  # S3 Storage Resources
  # ==========================================
  LogBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'enterprise-log-analytics-${AWS::AccountId}'
      VersioningConfiguration:
        Status: Enabled
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: TransitionToStandardIA
            Status: Enabled
            Transitions:
              - TransitionInDays: 30
                StorageClass: STANDARD_IA
          - Id: TransitionToGlacier
            Status: Enabled
            Transitions:
              - TransitionInDays: 90
                StorageClass: GLACIER
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  AthenaQueryResultsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'enterprise-log-analytics-athena-results-${AWS::AccountId}'
      VersioningConfiguration:
        Status: Enabled
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: ExpireResults
            Status: Enabled
            ExpirationInDays: 30
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  # ==========================================
  # IAM Roles (LEAST PRIVILEGE)
  # ==========================================
  FirehoseDeliveryRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: firehose.amazonaws.com
            Action: 'sts:AssumeRole'
            Condition:
              StringEquals:
                'sts:ExternalId': !Ref 'AWS::AccountId'
      Policies:
        - PolicyName: FirehoseS3DeliveryPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:AbortMultipartUpload'
                  - 's3:GetBucketLocation'
                  - 's3:GetObject'
                  - 's3:ListBucket'
                  - 's3:ListBucketMultipartUploads'
                  - 's3:PutObject'
                Resource:
                  - !GetAtt LogBucket.Arn
                  - !Sub '${LogBucket.Arn}/*'
        - PolicyName: FirehoseCloudWatchLogsPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'logs:PutLogEvents'
                Resource: !GetAtt FirehoseLogGroup.Arn

  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'

  GlueServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole'
      Policies:
        - PolicyName: GlueS3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:GetObject'
                  - 's3:PutObject'
                  - 's3:DeleteObject'
                  - 's3:ListBucket'
                Resource:
                  - !GetAtt LogBucket.Arn
                  - !Sub '${LogBucket.Arn}/*'
              - Effect: Allow
                Action:
                  - 's3:ListBucket'
                  - 's3:PutObject'
                Resource:
                  - !GetAtt AthenaQueryResultsBucket.Arn
                  - !Sub '${AthenaQueryResultsBucket.Arn}/*'

  CloudWatchLogsRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: logs.amazonaws.com
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: PutRecordToFirehose
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'firehose:PutRecord'
                  - 'firehose:PutRecordBatch'
                Resource: !GetAtt LogDeliveryStream.Arn

  # ==========================================
  # Log Ingestion Pipeline
  # ==========================================
  FirehoseLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: /aws/kinesisfirehose/EnterpriseLogDeliveryStream
      RetentionInDays: 30

  LogDeliveryStream:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      DeliveryStreamName: EnterpriseLogDeliveryStream
      DeliveryStreamType: DirectPut
      ExtendedS3DestinationConfiguration:
        BucketARN: !GetAtt LogBucket.Arn
        RoleARN: !GetAtt FirehoseDeliveryRole.Arn
        Prefix: 'logs/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/'
        ErrorOutputPrefix: 'errors/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/!{firehose:error-output-type}'
        BufferingHints:
          IntervalInSeconds: 60
          SizeInMBs: 50
        CompressionFormat: GZIP
        EncryptionConfiguration:
          NoEncryptionConfig: NoEncryption
        CloudWatchLoggingOptions:
          Enabled: true
          LogGroupName: !Ref FirehoseLogGroup
          LogStreamName: S3Delivery
        ProcessingConfiguration:
          Enabled: true
          Processors:
            - Type: Lambda
              Parameters:
                - ParameterName: LambdaArn
                  ParameterValue: !GetAtt LogProcessorLambda.Arn 
                - ParameterName: NumberOfRetries
                  ParameterValue: '3' 

  # ==========================================
  # Real-time Processing (RUNTIME nodejs20.x)
  # ==========================================
  LogProcessorLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: LogProcessorFunction
      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        ZipFile: |
          exports.handler = async (event, context) => {
            const output = [];
            event.records.forEach(record => {
              // Base64 decode the payload
              const payload = Buffer.from(record.data, 'base64').toString('utf8');
              
              // Parse and process log data
              let processedPayload;
              try {
                const jsonData = JSON.parse(payload);
                
                // Add processing timestamp
                jsonData.processingTimestamp = new Date().toISOString();
                
                // Add log level classification if not present
                if (!jsonData.logLevel) {
                  if (payload.toLowerCase().includes("error") || payload.toLowerCase().includes("exception")) {
                    jsonData.logLevel = "ERROR";
                  } else if (payload.toLowerCase().includes("warn")) {
                    jsonData.logLevel = "WARN";
                  } else {
                    jsonData.logLevel = "INFO";
                  }
                }
                
                processedPayload = JSON.stringify(jsonData) + '\n';
              } catch (e) {
                // If not JSON, just add a timestamp
                processedPayload = payload.trim() + ' | processed_at=' + new Date().toISOString() + '\n';
              }
              
              // Return processed record
              output.push({
                recordId: record.recordId,
                result: 'Ok',
                data: Buffer.from(processedPayload).toString('base64')
              });
            });
            return { records: output };
          };
      Runtime: nodejs20.x 
      Timeout: 60
      MemorySize: 256

  # ==========================================
  # Glue Script Uploader Automation ðŸ’¾
  # ==========================================
  S3UploaderLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
      Policies:
        - PolicyName: S3PutPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:PutObject'
                  - 's3:DeleteObject'
                Resource: !Sub '${LogBucket.Arn}/scripts/log_etl_job.py'

  S3UploaderLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: GlueScriptS3Uploader
      Handler: index.handler
      Role: !GetAtt S3UploaderLambdaRole.Arn
      Runtime: python3.11
      Timeout: 60
      MemorySize: 128
      Code:
        # Python handler code to use Boto3 for uploading the script
        ZipFile: |
          import json
          import boto3
          import cfnresponse
          import traceback
          
          s3_client = boto3.client('s3')
          
          def handler(event, context):
              response_data = {}
              status = cfnresponse.SUCCESS
              try:
                  bucket_name = event['ResourceProperties']['BucketName']
                  script_key = event['ResourceProperties']['ScriptKey']
                  script_content = event['ResourceProperties']['ScriptContent']
                  
                  if event['RequestType'] in ['Create', 'Update']:
                      s3_client.put_object(
                          Bucket=bucket_name,
                          Key=script_key,
                          Body=script_content,
                          ContentType='text/x-python'
                      )
                      print(f"Uploaded script to s3://{bucket_name}/{script_key}")
                  
                  elif event['RequestType'] == 'Delete':
                      s3_client.delete_object(
                          Bucket=bucket_name,
                          Key=script_key
                      )
                      print(f"Deleted script from s3://{bucket_name}/{script_key}")

              except Exception as e:
                  print(f"Failed to upload/delete script: {e}")
                  traceback.print_exc()
                  status = cfnresponse.FAILED
                  response_data['Error'] = str(e)
              
              cfnresponse.send(event, context, status, response_data, "CustomResourceForScriptUpload")

  GlueScriptUploaderCustomResource:
    Type: Custom::S3ScriptUploader
    Properties:
      ServiceToken: !GetAtt S3UploaderLambda.Arn
      BucketName: !Ref LogBucket
      ScriptKey: 'scripts/log_etl_job.py'
      ScriptContent: |
        import sys
        from awsglue.transforms import *
        from awsglue.utils import getResolvedOptions
        from pyspark.context import SparkContext
        from awsglue.context import GlueContext
        from awsglue.job import Job
        from pyspark.sql.functions import col, date_format
        from pyspark.sql.types import TimestampType, StringType, StructType, StructField
        
        args = getResolvedOptions(sys.argv, ['JOB_NAME'])
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
        
        log_schema = StructType([
            StructField("timestamp", TimestampType(), True),
            StructField("message", StringType(), True),
            StructField("server_ip", StringType(), True),
            StructField("host_name", StringType(), True),
            StructField("logLevel", StringType(), True),
            StructField("processingTimestamp", TimestampType(), True),
            StructField("correlation_id", StringType(), True)
        ])
        
        datasource = glueContext.create_dynamic_frame.from_options(
            connection_type="s3",
            connection_options={
                "paths": [f"s3://{LogBucket.GetAtt('Name')}/logs/"],
                "recurse": True
            },
            format="json",
            transformation_ctx="datasource"
        )
        
        df = datasource.toDF()
        
        # Casting and selecting columns
        processed_df = df.select(
            col("timestamp").cast(TimestampType()).alias("log_timestamp"),
            col("message").alias("message_raw"),
            col("server_ip").alias("server_ip"),
            col("host_name").alias("host_name"),
            col("logLevel").alias("log_level"),
            col("processingTimestamp").cast(TimestampType()).alias("processing_ts")
        )
        
        # Adding dynamic partitioning columns
        final_df = processed_df.withColumn("year", date_format(col("log_timestamp"), "yyyy")) \
                               .withColumn("month", date_format(col("log_timestamp"), "MM")) \
                               .withColumn("day", date_format(col("log_timestamp"), "dd"))
        
        datasink = DynamicFrame.fromDF(final_df, glueContext, "datasink")
        
        # Writing optimized data in Parquet format
        glueContext.write_dynamic_frame.from_options(
            frame=datasink,
            connection_type="s3",
            connection_options={
                "path": f"s3://{LogBucket.GetAtt('Name')}/processed_logs/",
                "partitionKeys": ["year", "month", "day"]
            },
            format="parquet",
            transformation_ctx="datasink"
        )
        
        job.commit()

  # ==========================================
  # Schema Discovery and ETL
  # ==========================================
  LogAnalyticsDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: enterprise_log_analytics
        Description: Database for enterprise log analytics

  LogCrawler:
    Type: AWS::Glue::Crawler
    DependsOn: GlueScriptUploaderCustomResource # Ensure script is uploaded before job is defined
    Properties:
      Name: EnterpriseLogCrawler
      Role: !GetAtt GlueServiceRole.Arn
      DatabaseName: !Ref LogAnalyticsDatabase
      Targets:
        S3Targets:
          - Path: !Sub 's3://${LogBucket}/logs/'
      Schedule:
        ScheduleExpression: 'cron(0 */3 * * ? *)'
      SchemaChangePolicy:
        UpdateBehavior: UPDATE_IN_DATABASE
        DeleteBehavior: LOG
      Configuration: '{"Version":1.0,"CrawlerOutput":{"Partitions":{"AddOrUpdateBehavior":"InheritFromTable"},"Tables":{"AddOrUpdateBehavior":"MergeNewColumns"}}}'

  LogETLJob:
    Type: AWS::Glue::Job
    DependsOn: GlueScriptUploaderCustomResource # Ensure script is uploaded before job is created
    Properties:
      Command:
        Name: glueetl
        ScriptLocation: !Sub 's3://${LogBucket}/scripts/log_etl_job.py'
      DefaultArguments:
        '--job-bookmark-option': 'job-bookmark-enable'
        '--enable-metrics': 'true'
      ExecutionProperty:
        MaxConcurrentRuns: 2
      MaxRetries: 2
      Name: EnterpriseLogETLJob
      Role: !GetAtt GlueServiceRole.Arn
      GlueVersion: '3.0'
      NumberOfWorkers: 5
      WorkerType: 'G.1X'

  # ==========================================
  # Ad-hoc Query Capability 
  # ==========================================
  LogAnalyticsWorkgroup:
    Type: AWS::Athena::WorkGroup
    Properties:
      Name: EnterpriseLogAnalytics
      Description: Workgroup for enterprise log analytics
      State: ENABLED
      WorkGroupConfiguration:
        EnforceWorkGroupConfiguration: true
        PublishCloudWatchMetricsEnabled: true
        ResultConfiguration:
          OutputLocation: !Sub 's3://${AthenaQueryResultsBucket}/athena-results/'
          EncryptionConfiguration:
            EncryptionOption: SSE_S3

  # ==========================================
  # Monitoring and Alerts 
  # ==========================================
  LogAnalyticsDashboard:
    Type: AWS::CloudWatch::Dashboard
    Properties:
      DashboardName: EnterpriseLogAnalyticsDashboard
      DashboardBody: !Sub |
        {
          "widgets": [
            {
              "type": "metric",
              "x": 0,
              "y": 0,
              "width": 12,
              "height": 6,
              "properties": {
                "metrics": [
                  ["AWS/Firehose", "DeliveryToS3.Records", "DeliveryStreamName", "EnterpriseLogDeliveryStream"],
                  ["AWS/Firehose", "DeliveryToS3.Success", "DeliveryStreamName", "EnterpriseLogDeliveryStream"]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "${AWS::Region}",
                "title": "Firehose Delivery Metrics",
                "period": 300
              }
            },
            {
              "type": "metric",
              "x": 12,
              "y": 0,
              "width": 12,
              "height": 6,
              "properties": {
                "metrics": [
                  ["AWS/Lambda", "Invocations", "FunctionName", "LogProcessorFunction"],
                  ["AWS/Lambda", "Errors", "FunctionName", "LogProcessorFunction"],
                  ["AWS/Lambda", "Duration", "FunctionName", "LogProcessorFunction"]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "${AWS::Region}",
                "title": "Lambda Processor Metrics",
                "period": 300
              }
            },
            {
              "type": "metric",
              "x": 0,
              "y": 6,
              "width": 12,
              "height": 6,
              "properties": {
                "metrics": [
                  ["AWS/S3", "BucketSizeBytes", "BucketName", "${LogBucket}", "StorageType", "StandardStorage"],
                  ["AWS/S3", "NumberOfObjects", "BucketName", "${LogBucket}", "StorageType", "AllStorageTypes"]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "${AWS::Region}",
                "title": "S3 Storage Metrics",
                "period": 86400
              }
            },
            {
              "type": "metric",
              "x": 12,
              "y": 6,
              "width": 12,
              "height": 6,
              "properties": {
                "metrics": [
                  ["AWS/Athena", "TotalExecutionTime", "QueryType", "DML", "WorkGroup", "EnterpriseLogAnalytics"],
                  ["AWS/Athena", "ProcessedBytes", "QueryType", "DML", "WorkGroup", "EnterpriseLogAnalytics"]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "${AWS::Region}",
                "title": "Athena Query Metrics",
                "period": 300
              }
            }
          ]
        }

  LogAnalyticsAlarmTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: LogAnalyticsAlarmTopic
      DisplayName: Log Analytics Alarms

  FirehoseErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: FirehoseDeliveryError
      AlarmDescription: Alarm for Firehose delivery errors
      MetricName: DeliveryToS3.DataFreshness
      Namespace: AWS/Firehose
      Statistic: Maximum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 900
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: DeliveryStreamName
          Value: EnterpriseLogDeliveryStream
      AlarmActions:
        - !Ref LogAnalyticsAlarmTopic

  LambdaErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: LambdaProcessorError
      AlarmDescription: Alarm for Lambda processor errors
      MetricName: Errors
      Namespace: AWS/Lambda
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 5
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: FunctionName
          Value: LogProcessorFunction
      AlarmActions:
        - !Ref LogAnalyticsAlarmTopic

  # ==========================================
  # Log Collection from Servers 
  # ==========================================
  SyslogLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: /enterprise/servers/syslog
      RetentionInDays: 7
      
  AuthLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: /enterprise/servers/auth
      RetentionInDays: 7
      
  ApplicationLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: /enterprise/servers/application
      RetentionInDays: 7

  CloudWatchAgentConfigParameter:
    Type: AWS::SSM::Parameter
    Properties:
      Name: /log-analytics/cloudwatch-agent-config
      Type: String
      Value: !Sub |
        {
          "agent": {
            "metrics_collection_interval": 60,
            "run_as_user": "cwagent"
          },
          "logs": {
            "logs_collected": {
              "files": {
                "collect_list": [
                  {
                    "file_path": "/var/log/syslog",
                    "log_group_name": "/enterprise/servers/syslog",
                    "log_stream_name": "{instance_id}",
                    "timezone": "UTC"
                  },
                  {
                    "file_path": "/var/log/auth.log",
                    "log_group_name": "/enterprise/servers/auth",
                    "log_stream_name": "{instance_id}",
                    "timezone": "UTC"
                  },
                  {
                    "file_path": "/var/log/application.log",
                    "log_group_name": "/enterprise/servers/application",
                    "log_stream_name": "{instance_id}",
                    "timezone": "UTC"
                  }
                ]
              }
            }
          }
        }
      Description: Configuration for CloudWatch Agent on servers

  SyslogSubscriptionFilter:
    Type: AWS::Logs::SubscriptionFilter
    Properties:
      LogGroupName: !Ref SyslogLogGroup
      FilterPattern: ""
      DestinationArn: !GetAtt LogDeliveryStream.Arn
      RoleArn: !GetAtt CloudWatchLogsRole.Arn

  AuthLogSubscriptionFilter:
    Type: AWS::Logs::SubscriptionFilter
    Properties:
      LogGroupName: !Ref AuthLogGroup
      FilterPattern: ""
      DestinationArn: !GetAtt LogDeliveryStream.Arn
      RoleArn: !GetAtt CloudWatchLogsRole.Arn

  ApplicationLogSubscriptionFilter:
    Type: AWS::Logs::SubscriptionFilter
    Properties:
      LogGroupName: !Ref ApplicationLogGroup
      FilterPattern: ""
      DestinationArn: !GetAtt LogDeliveryStream.Arn
      RoleArn: !GetAtt CloudWatchLogsRole.Arn

  # ==========================================
  # Audit Compliance 
  # ==========================================
  AuditLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: /enterprise/log-analytics/audit
      RetentionInDays: 365

  # ==========================================
  # QuickSight Access Policy
  # ==========================================
  QuickSightAccessPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      Description: Policy for QuickSight to access log analytics resources
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Action:
              - 'athena:*'
              - 's3:GetBucketLocation'
              - 's3:GetObject'
              - 's3:ListBucket'
              - 's3:ListBucketMultipartUploads'
              - 's3:ListMultipartUploadParts'
              - 's3:AbortMultipartUpload'
              - 's3:CreateBucket'
              - 's3:PutObject'
              - 'glue:GetDatabase'
              - 'glue:GetTable'
              - 'glue:GetPartition'
              - 'glue:GetPartitions'
              - 'glue:GetDatabases'
              - 'glue:GetTables'
            Resource:
              - !GetAtt LogBucket.Arn
              - !Sub '${LogBucket.Arn}/*'
              - !GetAtt AthenaQueryResultsBucket.Arn
              - !Sub '${AthenaQueryResultsBucket.Arn}/*'
              - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:catalog'
              - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:database/enterprise_log_analytics'
              - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:table/enterprise_log_analytics/*'
              - !Sub 'arn:aws:athena:${AWS::Region}:${AWS::AccountId}:workgroup/EnterpriseLogAnalytics'

  # ==========================================
  # QuickSight Permissions Automation (CUSTOM RESOURCE)
  # ==========================================
  QuickSightUpdaterLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
      Policies:
        - PolicyName: QuickSightPermissionsPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'iam:AttachRolePolicy'
                  - 'iam:DetachRolePolicy'
                  - 'iam:GetRole'
                Resource: '*'

  QuickSightUpdaterLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: QuickSightPermissionUpdater
      Handler: index.handler
      Role: !GetAtt QuickSightUpdaterLambdaRole.Arn
      Runtime: python3.11
      Timeout: 300
      MemorySize: 256
      Code:
        # Python handler code with improved error handling for deletion
        ZipFile: |
          import json
          import boto3
          import cfnresponse
          import traceback
          from botocore.exceptions import ClientError
          
          iam_client = boto3.client('iam')
          
          def handler(event, context):
              response_data = {}
              status = cfnresponse.SUCCESS
              try:
                  policy_arn = event['ResourceProperties']['QuickSightAccessPolicyArn']
                  qs_role_name = 'aws-quicksight-service-role-v0'
                  
                  if event['RequestType'] in ['Create', 'Update']:
                      iam_client.attach_role_policy(
                          RoleName=qs_role_name,
                          PolicyArn=policy_arn
                      )
                      print(f"Attached policy {policy_arn} to QuickSight role.")
                  
                  elif event['RequestType'] == 'Delete':
                      try:
                          iam_client.detach_role_policy(
                              RoleName=qs_role_name,
                              PolicyArn=policy_arn
                          )
                          print(f"Detached policy {policy_arn} from QuickSight role.")
                      except ClientError as e:
                          # Ignore the error if the policy is already detached
                          if e.response['Error']['Code'] == 'NoSuchEntity':
                              print(f"Policy {policy_arn} not attached or role not found. Continuing stack deletion.")
                              pass
                          else:
                              raise # Re-raise other unexpected errors

              except Exception as e:
                  print(f"Failed to update QuickSight permissions: {e}")
                  traceback.print_exc()
                  status = cfnresponse.FAILED
                  response_data['Error'] = str(e)
              
              cfnresponse.send(event, context, status, response_data, "CustomResourceForQuickSight")

              
  QuickSightPermissionCustomResource:
    Type: Custom::QuickSightPermissionUpdater
    Properties:
      ServiceToken: !GetAtt QuickSightUpdaterLambda.Arn
      QuickSightAccessPolicyArn: !Ref QuickSightAccessPolicy
      AccountId: !Ref AWS::AccountId
      Region: !Ref AWS::Region

Outputs:
  LogBucketName:
    Description: Name of the S3 bucket for log storage
    Value: !Ref LogBucket
  
  AthenaQueryResultsBucketName:
    Description: Name of the S3 bucket for Athena query results
    Value: !Ref AthenaQueryResultsBucket

  DeliveryStreamName:
    Description: Name of the Kinesis Firehose Delivery Stream
    Value: !Ref LogDeliveryStream

  GlueDatabaseName:
    Description: Name of the Glue Database
    Value: !Ref LogAnalyticsDatabase

  AthenaWorkgroup:
    Description: Name of the Athena Workgroup
    Value: !Ref LogAnalyticsWorkgroup

  CloudWatchDashboard:
    Description: CloudWatch Dashboard URL
    Value: !Sub 'https://console.aws.amazon.com/cloudwatch/home?region=${AWS::Region}#dashboards:name=EnterpriseLogAnalyticsDashboard'

  CloudWatchAgentConfig:
    Description: SSM Parameter for CloudWatch Agent configuration
    Value: !Ref CloudWatchAgentConfigParameter
  
  AuditLogGroupName:
    Description: CloudWatch Log Group for audit logs.
    Value: !Ref AuditLogGroup