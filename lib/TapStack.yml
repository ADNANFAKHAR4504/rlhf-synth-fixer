AWSTemplateFormatVersion: '2010-09-09'
Description: 'Enhanced SaaS staging environment mirroring production with advanced monitoring, security, and performance optimizations'

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: 'Environment Configuration'
        Parameters:
          - EnvironmentSuffix
      - Label:
          default: 'Database Configuration'
        Parameters:
          - ProductionDBSnapshotIdentifier
      - Label:
          default: 'Network Configuration'
        Parameters:
          - VPNCidr
      - Label:
          default: 'Cost Control'
        Parameters:
          - NotificationEmail
          - MonthlyCostThreshold
      - Label:
          default: 'Performance Configuration'
        Parameters:
          - ElastiCacheNodeType
      - Label:
          default: 'AWS Config Configuration'
        Parameters:
          - CreateConfigDeliveryChannel
          - CreateConfigRecorder

Parameters:
  EnvironmentSuffix:
    Type: String
    Default: 'dev'
    Description: 'Environment suffix for resource naming (e.g., dev, staging, prod)'
    AllowedPattern: '^[a-zA-Z0-9]+$'
    ConstraintDescription: 'Must contain only alphanumeric characters'

  ProductionDBSnapshotIdentifier:
    Type: String
    Description: The ARN of the production database snapshot to clone
    Default: 'arn:aws:rds:us-west-2:123456789012:cluster-snapshot:production-snapshot'

  VPNCidr:
    Type: String
    Description: CIDR block for VPN access
    Default: '172.16.0.0/16'

  NotificationEmail:
    Type: String
    Description: Email address for cost control and monitoring alarm notifications
    Default: 'admin@example.com'

  MonthlyCostThreshold:
    Type: Number
    Description: Threshold for monthly cost alarm in USD
    Default: 1000

  ElastiCacheNodeType:
    Type: String
    Description: ElastiCache node type for performance optimization
    Default: 'cache.t3.micro'
    AllowedValues: ['cache.t3.micro', 'cache.t3.small', 'cache.t3.medium']

  CreateConfigDeliveryChannel:
    Type: String
    Description: |
      Whether to create AWS Config delivery channel. 
      IMPORTANT: AWS allows only 1 delivery channel per account per region.
      Set to 'false' if a delivery channel already exists in your account to avoid deployment errors.
      Check existing channels with: aws configservice describe-delivery-channels
    Default: 'false'
    AllowedValues: ['true', 'false']

  CreateConfigRecorder:
    Type: String
    Description: |
      Whether to create AWS Config configuration recorder.
      IMPORTANT: AWS allows only 1 configuration recorder per account per region.
      Set to 'false' if a configuration recorder already exists in your account to avoid deployment errors.
      Check existing recorders with: aws configservice describe-configuration-recorders
    Default: 'false'
    AllowedValues: ['true', 'false']

Conditions:
  ShouldCreateConfigDeliveryChannel:
    Fn::Equals:
      - Ref: CreateConfigDeliveryChannel
      - 'true'

  ShouldCreateConfigRecorder:
    Fn::Equals:
      - Ref: CreateConfigRecorder
      - 'true'

Resources:
  # ========================================================================
  # NETWORK CONFIGURATION
  # ========================================================================
  StagingVPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.25.0.0/16
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
        - Key: Name
          Value: !Sub 'staging-vpc-${EnvironmentSuffix}'
        - Key: Environment
          Value: staging
        - Key: EnvironmentSuffix
          Value: !Ref EnvironmentSuffix

  PrivateSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref StagingVPC
      AvailabilityZone: !Select [0, !GetAZs '']
      CidrBlock: 10.25.10.0/24
      MapPublicIpOnLaunch: false
      Tags:
        - Key: Name
          Value: !Sub 'staging-private-subnet-1-${EnvironmentSuffix}'
        - Key: Environment
          Value: staging
        - Key: EnvironmentSuffix
          Value: !Ref EnvironmentSuffix

  PrivateSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref StagingVPC
      AvailabilityZone: !Select [1, !GetAZs '']
      CidrBlock: 10.25.20.0/24
      MapPublicIpOnLaunch: false
      Tags:
        - Key: Name
          Value: !Sub 'staging-private-subnet-2-${EnvironmentSuffix}'
        - Key: Environment
          Value: staging
        - Key: EnvironmentSuffix
          Value: !Ref EnvironmentSuffix

  PrivateRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref StagingVPC
      Tags:
        - Key: Name
          Value: !Sub 'staging-private-route-table-${EnvironmentSuffix}'
        - Key: Environment
          Value: staging
        - Key: EnvironmentSuffix
          Value: !Ref EnvironmentSuffix

  PrivateSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      RouteTableId: !Ref PrivateRouteTable
      SubnetId: !Ref PrivateSubnet1

  PrivateSubnet2RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      RouteTableId: !Ref PrivateRouteTable
      SubnetId: !Ref PrivateSubnet2

  S3VPCEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.s3'
      VpcId: !Ref StagingVPC
      RouteTableIds:
        - !Ref PrivateRouteTable
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal: '*'
            Action: ['s3:GetObject', 's3:PutObject', 's3:ListBucket']
            Resource: '*'

  # ========================================================================
  # SECURITY GROUPS
  # ========================================================================
  DBSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for RDS Aurora MySQL
      VpcId: !Ref StagingVPC
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 3306
          ToPort: 3306
          CidrIp: !Ref VPNCidr
          Description: 'VPN access to MySQL'
        - IpProtocol: tcp
          FromPort: 3306
          ToPort: 3306
          SourceSecurityGroupId: !Ref LambdaSecurityGroup
          Description: 'Lambda access to MySQL'
        - IpProtocol: tcp
          FromPort: 3306
          ToPort: 3306
          SourceSecurityGroupId: !Ref ElastiCacheSecurityGroup
          Description: 'ElastiCache access to MySQL'
      Tags:
        - Key: Name
          Value: !Sub 'staging-db-sg-${EnvironmentSuffix}'
        - Key: Environment
          Value: staging
        - Key: EnvironmentSuffix
          Value: !Ref EnvironmentSuffix

  LambdaSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for Lambda functions
      VpcId: !Ref StagingVPC
      SecurityGroupEgress:
        - IpProtocol: -1
          CidrIp: 0.0.0.0/0
          Description: 'All outbound traffic'
      Tags:
        - Key: Name
          Value: !Sub 'staging-lambda-sg-${EnvironmentSuffix}'
        - Key: Environment
          Value: staging
        - Key: EnvironmentSuffix
          Value: !Ref EnvironmentSuffix

  ElastiCacheSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for ElastiCache Redis cluster
      VpcId: !Ref StagingVPC
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 6379
          ToPort: 6379
          SourceSecurityGroupId: !Ref LambdaSecurityGroup
          Description: 'Lambda access to Redis'
        - IpProtocol: tcp
          FromPort: 6379
          ToPort: 6379
          CidrIp: !Ref VPNCidr
          Description: 'VPN access to Redis'
      Tags:
        - Key: Name
          Value: !Sub 'staging-elasticache-sg-${EnvironmentSuffix}'
        - Key: Environment
          Value: staging
        - Key: EnvironmentSuffix
          Value: !Ref EnvironmentSuffix

  # ========================================================================
  # PERFORMANCE OPTIMIZATION - ELASTICACHE
  # ========================================================================
  ElastiCacheSubnetGroup:
    Type: AWS::ElastiCache::SubnetGroup
    Properties:
      Description: Subnet group for ElastiCache Redis cluster
      SubnetIds:
        - !Ref PrivateSubnet1
        - !Ref PrivateSubnet2

  ElastiCacheCluster:
    Type: AWS::ElastiCache::ReplicationGroup
    Properties:
      ReplicationGroupId: !Sub 'staging-redis-${EnvironmentSuffix}'
      ReplicationGroupDescription: 'Redis cluster for application performance caching'
      Engine: redis
      EngineVersion: 7.0
      CacheNodeType: !Ref ElastiCacheNodeType
      NumCacheClusters: 2
      Port: 6379
      SecurityGroupIds:
        - !Ref ElastiCacheSecurityGroup
      CacheSubnetGroupName: !Ref ElastiCacheSubnetGroup
      AtRestEncryptionEnabled: true
      TransitEncryptionEnabled: true
      MultiAZEnabled: true
      AutomaticFailoverEnabled: true
      Tags:
        - Key: Name
          Value: !Sub 'staging-redis-${EnvironmentSuffix}'
        - Key: Environment
          Value: staging
        - Key: EnvironmentSuffix
          Value: !Ref EnvironmentSuffix

  # ========================================================================
  # DATABASE CONFIGURATION
  # ========================================================================
  DBSubnetGroup:
    Type: AWS::RDS::DBSubnetGroup
    Properties:
      DBSubnetGroupDescription: Subnet group for Aurora MySQL
      DBSubnetGroupName: !Sub 'staging-db-subnet-group-${EnvironmentSuffix}'
      SubnetIds:
        - !Ref PrivateSubnet1
        - !Ref PrivateSubnet2
      Tags:
        - Key: Name
          Value: !Sub 'staging-db-subnet-group-${EnvironmentSuffix}'
        - Key: Environment
          Value: staging
        - Key: EnvironmentSuffix
          Value: !Ref EnvironmentSuffix

  DBSecret:
    Type: AWS::SecretsManager::Secret
    Properties:
      Name: !Sub 'staging/db/credentials-${EnvironmentSuffix}'
      Description: RDS Aurora MySQL credentials for staging
      GenerateSecretString:
        SecretStringTemplate: '{"username": "admin"}'
        GenerateStringKey: 'password'
        PasswordLength: 16
        ExcludeCharacters: '"@/\'
      Tags:
        - Key: Environment
          Value: staging
        - Key: EnvironmentSuffix
          Value: !Ref EnvironmentSuffix

  AuroraDBCluster:
    Type: AWS::RDS::DBCluster
    Properties:
      Engine: aurora-mysql
      EngineMode: provisioned
      EngineVersion: 8.0.mysql_aurora.3.04.4
      DBClusterIdentifier: !Sub 'staging-aurora-cluster-${EnvironmentSuffix}'
      MasterUsername:
        !Join [
          '',
          [
            '{{resolve:secretsmanager:',
            !Ref DBSecret,
            ':SecretString:username}}',
          ],
        ]
      MasterUserPassword:
        !Join [
          '',
          [
            '{{resolve:secretsmanager:',
            !Ref DBSecret,
            ':SecretString:password}}',
          ],
        ]
      DatabaseName: saasapp
      BackupRetentionPeriod: 7
      PreferredBackupWindow: '02:00-03:00'
      PreferredMaintenanceWindow: 'sun:05:00-sun:06:00'
      DBSubnetGroupName: !Ref DBSubnetGroup
      VpcSecurityGroupIds:
        - !Ref DBSecurityGroup
      StorageEncrypted: true
      DeletionProtection: false
      EnableCloudwatchLogsExports: ['audit', 'error', 'general', 'slowquery']
      PerformanceInsightsEnabled: true
      PerformanceInsightsRetentionPeriod: 7
      Tags:
        - Key: Name
          Value: !Sub 'staging-aurora-cluster-${EnvironmentSuffix}'
        - Key: Environment
          Value: staging
        - Key: EnvironmentSuffix
          Value: !Ref EnvironmentSuffix

  AuroraDBInstance:
    Type: AWS::RDS::DBInstance
    Properties:
      Engine: aurora-mysql
      DBInstanceIdentifier: !Sub 'staging-aurora-instance1-${EnvironmentSuffix}'
      DBClusterIdentifier: !Ref AuroraDBCluster
      DBInstanceClass: db.r5.large
      PubliclyAccessible: false
      MonitoringInterval: 60
      MonitoringRoleArn: !GetAtt RDSEnhancedMonitoringRole.Arn
      Tags:
        - Key: Name
          Value: !Sub 'staging-aurora-instance1-${EnvironmentSuffix}'
        - Key: Environment
          Value: staging
        - Key: EnvironmentSuffix
          Value: !Ref EnvironmentSuffix

  # ========================================================================
  # BACKUP AUTOMATION WITH CROSS-REGION REPLICATION
  # ========================================================================
  BackupVault:
    Type: AWS::Backup::BackupVault
    Properties:
      BackupVaultName: !Sub 'staging-backup-vault-${EnvironmentSuffix}'
      EncryptionKeyArn: !GetAtt BackupKMSKey.Arn
      BackupVaultTags:
        Environment: staging
        EnvironmentSuffix: !Ref EnvironmentSuffix

  BackupKMSKey:
    Type: AWS::KMS::Key
    Properties:
      Description: 'KMS Key for backup encryption'
      KeyPolicy:
        Version: '2012-10-17'
        Statement:
          - Sid: Enable IAM User Permissions
            Effect: Allow
            Principal:
              AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'
            Action: 'kms:*'
            Resource: '*'
          - Sid: Allow use of the key for backup
            Effect: Allow
            Principal:
              Service: backup.amazonaws.com
            Action:
              - 'kms:Encrypt'
              - 'kms:Decrypt'
              - 'kms:ReEncrypt*'
              - 'kms:GenerateDataKey*'
              - 'kms:DescribeKey'
            Resource: '*'

  BackupKMSKeyAlias:
    Type: AWS::KMS::Alias
    Properties:
      AliasName: !Sub 'alias/staging-backup-key-${EnvironmentSuffix}'
      TargetKeyId: !Ref BackupKMSKey

  BackupPlan:
    Type: AWS::Backup::BackupPlan
    Properties:
      BackupPlan:
        BackupPlanName: !Sub 'staging-backup-plan-${EnvironmentSuffix}'
        BackupPlanRule:
          - RuleName: DailyBackups
            TargetBackupVault: !Ref BackupVault
            ScheduleExpression: cron(0 4 ? * * *)
            StartWindowMinutes: 60
            CompletionWindowMinutes: 120
            Lifecycle:
              DeleteAfterDays: 30
            CopyActions:
              - DestinationBackupVaultArn: !Sub 'arn:aws:backup:us-east-1:${AWS::AccountId}:backup-vault:staging-backup-vault-${EnvironmentSuffix}-cross-region'
                Lifecycle:
                  DeleteAfterDays: 30
            RecoveryPointTags:
              Environment: staging
              EnvironmentSuffix: !Ref EnvironmentSuffix

  BackupRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 'staging-backup-role-${EnvironmentSuffix}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: backup.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSBackupServiceRolePolicyForBackup
        - arn:aws:iam::aws:policy/service-role/AWSBackupServiceRolePolicyForRestores

  BackupSelection:
    Type: AWS::Backup::BackupSelection
    Properties:
      BackupPlanId: !Ref BackupPlan
      BackupSelection:
        SelectionName: StagingDatabaseBackup
        IamRoleArn: !GetAtt BackupRole.Arn
        Resources:
          - !Sub 'arn:aws:rds:${AWS::Region}:${AWS::AccountId}:cluster:${AuroraDBCluster}'
        ListOfTags:
          - ConditionType: STRINGEQUALS
            ConditionKey: Environment
            ConditionValue: staging

  # ========================================================================
  # STORAGE CONFIGURATION
  # ========================================================================
  TestDataBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'staging-test-data-${AWS::AccountId}-${EnvironmentSuffix}'
      VersioningConfiguration:
        Status: Enabled
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldTestData
            Status: Enabled
            ExpirationInDays: 90
          - Id: IntelligentTiering
            Status: Enabled
            Transitions:
              - TransitionInDays: 30
                StorageClass: INTELLIGENT_TIERING
      # CloudWatch configurations removed as they're not supported in NotificationConfiguration
      Tags:
        - Key: Name
          Value: !Sub 'staging-test-data-${EnvironmentSuffix}'
        - Key: Environment
          Value: staging
        - Key: EnvironmentSuffix
          Value: !Ref EnvironmentSuffix

  TestDataBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref TestDataBucket
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Deny
            Principal: '*'
            Action: 's3:*'
            Resource:
              - !GetAtt TestDataBucket.Arn
              - !Sub '${TestDataBucket.Arn}/*'
            Condition:
              Bool:
                aws:SecureTransport: false

  # ========================================================================
  # ENHANCED LAMBDA FUNCTION WITH IMPROVED ERROR HANDLING
  # ========================================================================
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 'staging-lambda-role-${EnvironmentSuffix}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole
      Policies:
        - PolicyName: LambdaRDSAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - rds:DescribeDBSnapshots
                  - rds:RestoreDBClusterFromSnapshot
                  - rds:ModifyDBCluster
                  - rds:CreateDBInstance
                  - rds:DescribeDBInstances
                  - rds:DescribeDBClusters
                  - rds:DeleteDBInstance
                  - rds:DeleteDBCluster
                  - rds:CreateDBClusterSnapshot
                  - rds:DescribeDBClusterSnapshots
                Resource: '*'
              - Effect: Allow
                Action:
                  - secretsmanager:GetSecretValue
                Resource: !Ref DBSecret
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:ListBucket
                Resource:
                  - !GetAtt TestDataBucket.Arn
                  - !Sub '${TestDataBucket.Arn}/*'
              - Effect: Allow
                Action:
                  - elasticache:DescribeReplicationGroups
                  - elasticache:ModifyReplicationGroup
                Resource: !Sub 'arn:aws:elasticache:${AWS::Region}:${AWS::AccountId}:replicationgroup:${ElastiCacheCluster}'
              - Effect: Allow
                Action:
                  - sns:Publish
                Resource: !Ref AlertTopic
              - Effect: Allow
                Action:
                  - cloudwatch:PutMetricData
                Resource: '*'
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: 'arn:aws:logs:*:*:*'

  DataMaskingFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'staging-data-masking-${EnvironmentSuffix}'
      Description: 'Enhanced data masking function with comprehensive error handling and monitoring'
      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        ZipFile: |
          import boto3
          import json
          import logging
          import os
          import time
          import pymysql
          from botocore.exceptions import ClientError, BotoCoreError
          import traceback

          # Enhanced logging configuration
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          # CloudWatch custom metrics
          cloudwatch = boto3.client('cloudwatch')

          def put_custom_metric(metric_name, value, unit='Count'):
              """Send custom metrics to CloudWatch"""
              try:
                  cloudwatch.put_metric_data(
                      Namespace='SaaS/DataMasking',
                      MetricData=[{
                          'MetricName': metric_name,
                          'Value': value,
                          'Unit': unit
                      }]
                  )
              except Exception as e:
                  logger.error(f"Failed to send custom metric {metric_name}: {str(e)}")

          def send_alert(message, subject="Data Masking Alert"):
              """Send alert via SNS"""
              try:
                  sns = boto3.client('sns')
                  sns.publish(
                      TopicArn=os.environ['SNS_TOPIC_ARN'],
                      Subject=subject,
                      Message=message
                  )
              except Exception as e:
                  logger.error(f"Failed to send SNS alert: {str(e)}")

          def handler(event, context):
              start_time = time.time()
              logger.info("Enhanced data masking process started")
              
              try:
                  # Input validation
                  snapshot_identifier = event.get('snapshot_identifier') or os.environ.get('PRODUCTION_SNAPSHOT_ID')
                  if not snapshot_identifier:
                      error_msg = "No snapshot identifier provided in event or environment"
                      logger.error(error_msg)
                      put_custom_metric('MaskingErrors', 1)
                      send_alert(error_msg, "Data Masking Error")
                      raise ValueError(error_msg)
                  
                  # Initialize AWS clients with retry configuration
                  retry_config = {'retries': {'max_attempts': 3, 'mode': 'adaptive'}}
                  secrets_client = boto3.client('secretsmanager', config=boto3.session.Config(**retry_config))
                  rds_client = boto3.client('rds', config=boto3.session.Config(**retry_config))
                  
                  # Get database credentials with error handling
                  try:
                      secret_name = os.environ['DB_SECRET_NAME']
                      secret_response = secrets_client.get_secret_value(SecretId=secret_name)
                      credentials = json.loads(secret_response['SecretString'])
                  except (ClientError, KeyError, json.JSONDecodeError) as e:
                      error_msg = f"Failed to retrieve database credentials: {str(e)}"
                      logger.error(error_msg)
                      put_custom_metric('MaskingErrors', 1)
                      send_alert(error_msg, "Data Masking Credential Error")
                      raise
                  
                  temp_cluster_id = f"staging-temp-{int(time.time())}-{context.aws_request_id[:8]}"
                  
                  try:
                      # Restore snapshot with enhanced error handling
                      logger.info(f"Restoring snapshot {snapshot_identifier} to {temp_cluster_id}")
                      response = rds_client.restore_db_cluster_from_snapshot(
                          DBClusterIdentifier=temp_cluster_id,
                          SnapshotIdentifier=snapshot_identifier,
                          Engine='aurora-mysql',
                          VpcSecurityGroupIds=[os.environ['SECURITY_GROUP_ID']],
                          DBSubnetGroupName=os.environ['DB_SUBNET_GROUP_NAME']
                      )
                      put_custom_metric('ClusterRestores', 1)
                      
                      temp_instance_id = f"{temp_cluster_id}-instance"
                      rds_client.create_db_instance(
                          DBInstanceIdentifier=temp_instance_id,
                          DBClusterIdentifier=temp_cluster_id,
                          Engine='aurora-mysql',
                          DBInstanceClass='db.r5.large'
                      )
                      
                      # Enhanced waiting with timeout
                      logger.info(f"Waiting for DB instance {temp_instance_id} to be available")
                      waiter = rds_client.get_waiter('db_instance_available')
                      waiter.wait(
                          DBInstanceIdentifier=temp_instance_id,
                          WaiterConfig={'Delay': 30, 'MaxAttempts': 40}  # 20 minute timeout
                      )
                      
                      cluster_info = rds_client.describe_db_clusters(DBClusterIdentifier=temp_cluster_id)
                      endpoint = cluster_info['DBClusters'][0]['Endpoint']
                      
                      # Enhanced database connection with retry logic
                      max_retries = 5
                      for attempt in range(max_retries):
                          try:
                              conn = pymysql.connect(
                                  host=endpoint,
                                  user=credentials['username'],
                                  password=credentials['password'],
                                  database='saasapp',
                                  connect_timeout=30,
                                  read_timeout=60,
                                  write_timeout=60
                              )
                              break
                          except pymysql.Error as e:
                              if attempt == max_retries - 1:
                                  error_msg = f"Failed to connect to database after {max_retries} attempts: {str(e)}"
                                  logger.error(error_msg)
                                  put_custom_metric('ConnectionErrors', 1)
                                  send_alert(error_msg, "Data Masking Connection Error")
                                  raise
                              logger.warning(f"Database connection attempt {attempt + 1} failed, retrying...")
                              time.sleep(10 * (attempt + 1))  # Exponential backoff
                      
                      # Enhanced data masking with transaction management
                      try:
                          logger.info("Connected to DB, performing enhanced data masking")
                          conn.autocommit(False)  # Use transactions
                          
                          with conn.cursor() as cur:
                              # Count records before masking for metrics
                              cur.execute("SELECT COUNT(*) FROM users")
                              user_count = cur.fetchone()[0]
                              put_custom_metric('RecordsToMask', user_count)
                              
                              # Enhanced masking operations with better error handling
                              masking_operations = [
                                  ("UPDATE users SET email = CONCAT('masked_', MD5(email), '@example.com')", "Email masking"),
                                  ("UPDATE users SET phone = CONCAT('+1555', FLOOR(RAND() * 1000000))", "Phone masking"),
                                  ("UPDATE users SET address = '123 Test Street, Test City, TS 12345'", "Address masking"),
                                  ("UPDATE payment_info SET card_number = CONCAT('XXXX-XXXX-XXXX-', RIGHT(card_number, 4))", "Credit card masking"),
                                  ("UPDATE users SET ssn = MD5(ssn)", "SSN masking"),
                                  ("UPDATE users SET date_of_birth = '1990-01-01'", "DOB masking")
                              ]
                              
                              masked_operations = 0
                              for operation, description in masking_operations:
                                  try:
                                      logger.info(f"Performing {description}")
                                      cur.execute(operation)
                                      affected_rows = cur.rowcount
                                      logger.info(f"{description} completed: {affected_rows} rows affected")
                                      masked_operations += 1
                                  except pymysql.Error as e:
                                      logger.warning(f"{description} failed: {str(e)} - continuing with other operations")
                              
                              conn.commit()
                              put_custom_metric('MaskingOperationsCompleted', masked_operations)
                              logger.info(f"Data masking completed successfully: {masked_operations}/{len(masking_operations)} operations")
                              
                      except Exception as e:
                          conn.rollback()
                          error_msg = f"Error during data masking operations: {str(e)}"
                          logger.error(error_msg)
                          put_custom_metric('MaskingErrors', 1)
                          send_alert(error_msg, "Data Masking Operation Error")
                          raise
                      finally:
                          if 'conn' in locals() and conn:
                              conn.close()
                      
                      # Create masked snapshot with enhanced naming
                      masked_snapshot_id = f"staging-masked-{int(time.time())}-{os.environ.get('ENVIRONMENT_SUFFIX', 'dev')}"
                      logger.info(f"Creating snapshot {masked_snapshot_id} of masked database")
                      rds_client.create_db_cluster_snapshot(
                          DBClusterSnapshotIdentifier=masked_snapshot_id,
                          DBClusterIdentifier=temp_cluster_id,
                          Tags=[
                              {'Key': 'Environment', 'Value': 'staging'},
                              {'Key': 'MaskingDate', 'Value': str(int(time.time()))},
                              {'Key': 'SourceSnapshot', 'Value': snapshot_identifier}
                          ]
                      )
                      
                      # Wait for snapshot with timeout
                      waiter = rds_client.get_waiter('db_cluster_snapshot_available')
                      waiter.wait(
                          DBClusterSnapshotIdentifier=masked_snapshot_id,
                          WaiterConfig={'Delay': 30, 'MaxAttempts': 60}  # 30 minute timeout
                      )
                      put_custom_metric('SnapshotsCreated', 1)
                      
                  finally:
                      # Enhanced cleanup with better error handling
                      cleanup_errors = []
                      try:
                          logger.info("Cleaning up temporary database resources")
                          if 'temp_instance_id' in locals():
                              try:
                                  rds_client.delete_db_instance(
                                      DBInstanceIdentifier=temp_instance_id,
                                      SkipFinalSnapshot=True
                                  )
                                  waiter = rds_client.get_waiter('db_instance_deleted')
                                  waiter.wait(
                                      DBInstanceIdentifier=temp_instance_id,
                                      WaiterConfig={'Delay': 30, 'MaxAttempts': 40}
                                  )
                              except Exception as e:
                                  cleanup_errors.append(f"Instance cleanup: {str(e)}")
                          
                          if 'temp_cluster_id' in locals():
                              try:
                                  rds_client.delete_db_cluster(
                                      DBClusterIdentifier=temp_cluster_id,
                                      SkipFinalSnapshot=True
                                  )
                              except Exception as e:
                                  cleanup_errors.append(f"Cluster cleanup: {str(e)}")
                          
                          if cleanup_errors:
                              error_msg = f"Cleanup warnings: {'; '.join(cleanup_errors)}"
                              logger.warning(error_msg)
                              send_alert(error_msg, "Data Masking Cleanup Warning")
                          
                      except Exception as e:
                          logger.error(f"Critical cleanup error: {str(e)}")
                  
                  # Calculate execution time and send success metrics
                  execution_time = time.time() - start_time
                  put_custom_metric('ExecutionTime', execution_time, 'Seconds')
                  put_custom_metric('MaskingSuccess', 1)
                  
                  success_msg = f"Data masking completed successfully in {execution_time:.2f} seconds"
                  logger.info(success_msg)
                  send_alert(success_msg, "Data Masking Success")
                  
                  return {
                      'statusCode': 200,
                      'body': json.dumps({
                          'status': 'success',
                          'masked_snapshot_id': masked_snapshot_id,
                          'execution_time': execution_time,
                          'records_processed': user_count if 'user_count' in locals() else 0
                      })
                  }
                      
              except Exception as e:
                  error_msg = f"Critical error during data masking: {str(e)}"
                  logger.error(error_msg)
                  logger.error(traceback.format_exc())
                  put_custom_metric('MaskingErrors', 1)
                  send_alert(f"{error_msg}\n\nStackTrace:\n{traceback.format_exc()}", "Data Masking Critical Error")
                  
                  return {
                      'statusCode': 500,
                      'body': json.dumps({
                          'status': 'error',
                          'message': error_msg
                      })
                  }
      Runtime: python3.10
      Timeout: 900
      MemorySize: 1024
      VpcConfig:
        SecurityGroupIds:
          - !Ref LambdaSecurityGroup
        SubnetIds:
          - !Ref PrivateSubnet1
          - !Ref PrivateSubnet2
      Environment:
        Variables:
          DB_SECRET_NAME: !Ref DBSecret
          PRODUCTION_SNAPSHOT_ID: !Ref ProductionDBSnapshotIdentifier
          SECURITY_GROUP_ID: !Ref DBSecurityGroup
          DB_SUBNET_GROUP_NAME: !Ref DBSubnetGroup
          SNS_TOPIC_ARN: !Ref AlertTopic
          ENVIRONMENT_SUFFIX: !Ref EnvironmentSuffix
      Tags:
        - Key: Name
          Value: !Sub 'staging-data-masking-function-${EnvironmentSuffix}'
        - Key: Environment
          Value: staging
        - Key: EnvironmentSuffix
          Value: !Ref EnvironmentSuffix

  # ========================================================================
  # ENHANCED ACCESS CONTROL WITH GRANULAR IAM POLICIES
  # ========================================================================
  # Developer Role - Read-only access
  DeveloperRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 'StagingDeveloperRole-${EnvironmentSuffix}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'
            Action: sts:AssumeRole
      Policies:
        - PolicyName: DeveloperReadOnlyPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - rds:DescribeDBInstances
                  - rds:DescribeDBClusters
                  - rds:DescribeDBSnapshots
                  - s3:GetObject
                  - s3:ListBucket
                  - cloudwatch:GetMetricStatistics
                  - cloudwatch:ListMetrics
                  - logs:DescribeLogGroups
                  - logs:DescribeLogStreams
                  - logs:GetLogEvents
                Resource: '*'
                Condition:
                  StringEquals:
                    'aws:ResourceTag/Environment': 'staging'

  # DevOps Role - Limited administrative access
  DevOpsRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 'StagingDevOpsRole-${EnvironmentSuffix}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'
            Action: sts:AssumeRole
            Condition:
              Bool:
                aws:MultiFactorAuthPresent: true
      Policies:
        - PolicyName: DevOpsLimitedAdminPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - rds:ModifyDBCluster
                  - rds:ModifyDBInstance
                  - rds:RebootDBInstance
                  - lambda:InvokeFunction
                  - lambda:UpdateFunctionCode
                  - lambda:UpdateFunctionConfiguration
                  - s3:*
                  - cloudwatch:*
                  - elasticache:ModifyReplicationGroup
                  - elasticache:RebootCacheCluster
                Resource: '*'
                Condition:
                  StringEquals:
                    'aws:ResourceTag/Environment': 'staging'

  # Admin Role - Full access with MFA
  StagingAdminRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 'StagingAdminRole-${EnvironmentSuffix}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'
            Action: sts:AssumeRole
            Condition:
              Bool:
                aws:MultiFactorAuthPresent: true
              NumericLessThan:
                aws:MultiFactorAuthAge: 3600  # MFA must be within 1 hour
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/ReadOnlyAccess
      Policies:
        - PolicyName: StagingFullAdminPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: '*'
                Resource: '*'
                Condition:
                  StringEquals:
                    'aws:ResourceTag/Environment': 'staging'

  # Enhanced monitoring role
  RDSEnhancedMonitoringRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: monitoring.rds.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonRDSEnhancedMonitoringRole

  # ========================================================================
  # AWS CONFIG RULES FOR SECURITY SCANNING
  # ========================================================================
  ConfigurationRecorder:
    Type: AWS::Config::ConfigurationRecorder
    Condition: ShouldCreateConfigRecorder
    Properties:
      Name: !Sub 'staging-config-recorder-${EnvironmentSuffix}'
      RoleARN: !GetAtt ConfigRole.Arn
      RecordingGroup:
        AllSupported: true
        IncludeGlobalResourceTypes: true

  ConfigRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: config.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWS_ConfigRole

  ConfigDeliveryChannel:
    Type: AWS::Config::DeliveryChannel
    Condition: ShouldCreateConfigDeliveryChannel
    Properties:
      Name: !Sub 'staging-config-delivery-${EnvironmentSuffix}'
      S3BucketName: !Ref ConfigBucket

  ConfigBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'staging-config-${AWS::AccountId}-${EnvironmentSuffix}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256

  ConfigBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref ConfigBucket
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: config.amazonaws.com
            Action: s3:GetBucketAcl
            Resource: !GetAtt ConfigBucket.Arn
          - Effect: Allow
            Principal:
              Service: config.amazonaws.com
            Action: s3:PutObject
            Resource: !Sub '${ConfigBucket.Arn}/*'
            Condition:
              StringEquals:
                s3:x-amz-acl: bucket-owner-full-control

  # Security Config Rules
  RDSEncryptionRule:
    Type: AWS::Config::ConfigRule
    Condition: ShouldCreateConfigRecorder
    DependsOn: ConfigurationRecorder
    Properties:
      ConfigRuleName: !Sub 'rds-storage-encrypted-${EnvironmentSuffix}'
      Description: 'Checks if storage encryption is enabled for RDS DB instances'
      Source:
        Owner: AWS
        SourceIdentifier: RDS_STORAGE_ENCRYPTED

  S3EncryptionRule:
    Type: AWS::Config::ConfigRule
    Condition: ShouldCreateConfigRecorder
    DependsOn: ConfigurationRecorder
    Properties:
      ConfigRuleName: !Sub 's3-bucket-server-side-encryption-enabled-${EnvironmentSuffix}'
      Description: 'Checks if S3 bucket has server-side encryption enabled'
      Source:
        Owner: AWS
        SourceIdentifier: S3_BUCKET_SERVER_SIDE_ENCRYPTION_ENABLED

  SecurityGroupRestrictiveRule:
    Type: AWS::Config::ConfigRule
    Condition: ShouldCreateConfigRecorder
    DependsOn: ConfigurationRecorder
    Properties:
      ConfigRuleName: !Sub 'ec2-security-group-attached-to-eni-${EnvironmentSuffix}'
      Description: 'Checks if security groups are attached to ENIs'
      Source:
        Owner: AWS
        SourceIdentifier: EC2_SECURITY_GROUP_ATTACHED_TO_ENI

  # ========================================================================
  # ENHANCED MONITORING AND ALERTING
  # ========================================================================
  AlertTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Sub 'staging-alert-topic-${EnvironmentSuffix}'
      DisplayName: 'Staging Environment Alerts'
      Subscription:
        - Protocol: email
          Endpoint: !Ref NotificationEmail

  # Custom CloudWatch Log Groups
  S3AccessLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/s3/staging-access-logs-${EnvironmentSuffix}'
      RetentionInDays: 14

  ApplicationLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/application/staging-${EnvironmentSuffix}'
      RetentionInDays: 30

  # Custom CloudWatch Dashboard
  MonitoringDashboard:
    Type: AWS::CloudWatch::Dashboard
    Properties:
      DashboardName: !Sub 'Staging-Environment-${EnvironmentSuffix}'
      DashboardBody: !Sub |
        {
          "widgets": [
            {
              "type": "metric",
              "x": 0,
              "y": 0,
              "width": 12,
              "height": 6,
              "properties": {
                "metrics": [
                  [ "AWS/RDS", "CPUUtilization", "DBInstanceIdentifier", "${AuroraDBInstance}" ],
                  [ ".", "DatabaseConnections", ".", "." ],
                  [ ".", "ReadLatency", ".", "." ],
                  [ ".", "WriteLatency", ".", "." ]
                ],
                "period": 300,
                "stat": "Average",
                "region": "${AWS::Region}",
                "title": "RDS Performance Metrics"
              }
            },
            {
              "type": "metric",
              "x": 12,
              "y": 0,
              "width": 12,
              "height": 6,
              "properties": {
                "metrics": [
                  [ "AWS/ElastiCache", "CPUUtilization", "CacheClusterId", "${ElastiCacheCluster}" ],
                  [ ".", "CacheHits", ".", "." ],
                  [ ".", "CacheMisses", ".", "." ],
                  [ ".", "NetworkBytesIn", ".", "." ],
                  [ ".", "NetworkBytesOut", ".", "." ]
                ],
                "period": 300,
                "stat": "Average",
                "region": "${AWS::Region}",
                "title": "ElastiCache Performance Metrics"
              }
            },
            {
              "type": "metric",
              "x": 0,
              "y": 6,
              "width": 12,
              "height": 6,
              "properties": {
                "metrics": [
                  [ "SaaS/DataMasking", "MaskingSuccess" ],
                  [ ".", "MaskingErrors" ],
                  [ ".", "ExecutionTime" ],
                  [ ".", "RecordsToMask" ]
                ],
                "period": 300,
                "stat": "Sum",
                "region": "${AWS::Region}",
                "title": "Data Masking Metrics"
              }
            },
            {
              "type": "metric",
              "x": 12,
              "y": 6,
              "width": 12,
              "height": 6,
              "properties": {
                "metrics": [
                  [ "AWS/S3", "BucketSizeBytes", "BucketName", "${TestDataBucket}", "StorageType", "StandardStorage" ],
                  [ ".", "NumberOfObjects", ".", ".", ".", "." ]
                ],
                "period": 86400,
                "stat": "Average",
                "region": "${AWS::Region}",
                "title": "S3 Storage Metrics"
              }
            }
          ]
        }

  # Enhanced CloudWatch Alarms
  DatabaseCPUAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub 'staging-db-high-cpu-${EnvironmentSuffix}'
      AlarmDescription: Alarm for high CPU utilization in staging database
      MetricName: CPUUtilization
      Namespace: AWS/RDS
      Statistic: Average
      Period: 300
      EvaluationPeriods: 2
      Threshold: 80
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: DBInstanceIdentifier
          Value: !Ref AuroraDBInstance
      AlarmActions:
        - !Ref AlertTopic
      TreatMissingData: notBreaching

  DatabaseConnectionsAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub 'staging-db-high-connections-${EnvironmentSuffix}'
      AlarmDescription: Alarm for high database connections
      MetricName: DatabaseConnections
      Namespace: AWS/RDS
      Statistic: Average
      Period: 300
      EvaluationPeriods: 2
      Threshold: 50
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: DBInstanceIdentifier
          Value: !Ref AuroraDBInstance
      AlarmActions:
        - !Ref AlertTopic

  ElastiCacheCPUAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub 'staging-elasticache-high-cpu-${EnvironmentSuffix}'
      AlarmDescription: Alarm for high ElastiCache CPU utilization
      MetricName: CPUUtilization
      Namespace: AWS/ElastiCache
      Statistic: Average
      Period: 300
      EvaluationPeriods: 2
      Threshold: 75
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: CacheClusterId
          Value: !Ref ElastiCacheCluster
      AlarmActions:
        - !Ref AlertTopic

  DataMaskingErrorsAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub 'staging-data-masking-errors-${EnvironmentSuffix}'
      AlarmDescription: Alarm for data masking errors
      MetricName: MaskingErrors
      Namespace: SaaS/DataMasking
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      AlarmActions:
        - !Ref AlertTopic
      TreatMissingData: notBreaching

  MonthlySpendingAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub 'staging-monthly-cost-alarm-${EnvironmentSuffix}'
      AlarmDescription: Alarm if monthly costs exceed threshold
      MetricName: EstimatedCharges
      Namespace: AWS/Billing
      Statistic: Maximum
      Period: 21600
      EvaluationPeriods: 1
      Threshold: !Ref MonthlyCostThreshold
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: Currency
          Value: USD
      AlarmActions:
        - !Ref AlertTopic
      TreatMissingData: notBreaching

  DailyTransactionsMetricAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub 'staging-daily-transactions-alarm-${EnvironmentSuffix}'
      AlarmDescription: Alarm if daily transactions exceed threshold
      MetricName: Transactions
      Namespace: SaaS/Application
      Statistic: Sum
      Period: 86400
      EvaluationPeriods: 1
      Threshold: 6000
      ComparisonOperator: GreaterThanThreshold
      AlarmActions:
        - !Ref AlertTopic
      TreatMissingData: notBreaching

Outputs:
  # Network Outputs
  VpcId:
    Description: VPC ID of the staging environment
    Value: !Ref StagingVPC
    Export:
      Name: !Sub '${AWS::StackName}-VpcId'

  PrivateSubnet1Id:
    Description: Private Subnet 1 ID
    Value: !Ref PrivateSubnet1
    Export:
      Name: !Sub '${AWS::StackName}-PrivateSubnet1Id'

  PrivateSubnet2Id:
    Description: Private Subnet 2 ID
    Value: !Ref PrivateSubnet2
    Export:
      Name: !Sub '${AWS::StackName}-PrivateSubnet2Id'

  # Database Outputs
  AuroraClusterId:
    Description: Aurora MySQL Cluster Identifier
    Value: !Ref AuroraDBCluster
    Export:
      Name: !Sub '${AWS::StackName}-AuroraClusterId'

  AuroraClusterEndpoint:
    Description: Aurora MySQL Cluster Endpoint
    Value: !GetAtt AuroraDBCluster.Endpoint.Address
    Export:
      Name: !Sub '${AWS::StackName}-AuroraClusterEndpoint'

  # Cache Outputs
  ElastiCacheEndpoint:
    Description: ElastiCache Redis Endpoint
    Value: !GetAtt ElastiCacheCluster.PrimaryEndPoint.Address
    Export:
      Name: !Sub '${AWS::StackName}-ElastiCacheEndpoint'

  ElastiCachePort:
    Description: ElastiCache Redis Port
    Value: !GetAtt ElastiCacheCluster.PrimaryEndPoint.Port
    Export:
      Name: !Sub '${AWS::StackName}-ElastiCachePort'

  # Storage Outputs
  TestDataBucketName:
    Description: S3 Bucket for test data
    Value: !Ref TestDataBucket
    Export:
      Name: !Sub '${AWS::StackName}-TestDataBucketName'

  ConfigBucketName:
    Description: S3 Bucket for AWS Config
    Value: !Ref ConfigBucket
    Export:
      Name: !Sub '${AWS::StackName}-ConfigBucketName'

  # Function Outputs
  DataMaskingFunctionName:
    Description: Lambda function for data masking
    Value: !Ref DataMaskingFunction
    Export:
      Name: !Sub '${AWS::StackName}-DataMaskingFunctionName'

  DataMaskingFunctionArn:
    Description: ARN of the data masking Lambda function
    Value: !GetAtt DataMaskingFunction.Arn
    Export:
      Name: !Sub '${AWS::StackName}-DataMaskingFunctionArn'

  # Security Outputs
  DBSecretARN:
    Description: ARN of the secret containing database credentials
    Value: !Ref DBSecret
    Export:
      Name: !Sub '${AWS::StackName}-DBSecretARN'

  DeveloperRoleARN:
    Description: ARN of the developer role for read-only access
    Value: !GetAtt DeveloperRole.Arn
    Export:
      Name: !Sub '${AWS::StackName}-DeveloperRoleARN'

  DevOpsRoleARN:
    Description: ARN of the DevOps role for limited admin access
    Value: !GetAtt DevOpsRole.Arn
    Export:
      Name: !Sub '${AWS::StackName}-DevOpsRoleARN'

  StagingAdminRoleARN:
    Description: ARN of the staging admin role requiring MFA
    Value: !GetAtt StagingAdminRole.Arn
    Export:
      Name: !Sub '${AWS::StackName}-StagingAdminRoleARN'

  # Monitoring Outputs
  AlertTopicArn:
    Description: SNS Topic ARN for alerts
    Value: !Ref AlertTopic
    Export:
      Name: !Sub '${AWS::StackName}-AlertTopicArn'

  MonitoringDashboardURL:
    Description: CloudWatch Dashboard URL
    Value: !Sub 'https://console.aws.amazon.com/cloudwatch/home?region=${AWS::Region}#dashboards:name=${MonitoringDashboard}'
    Export:
      Name: !Sub '${AWS::StackName}-MonitoringDashboardURL'

  # Backup Outputs
  BackupVaultName:
    Description: AWS Backup Vault Name
    Value: !Ref BackupVault
    Export:
      Name: !Sub '${AWS::StackName}-BackupVaultName'

  # Environment Info
  StackName:
    Description: Name of this CloudFormation stack
    Value: !Ref AWS::StackName
    Export:
      Name: !Sub '${AWS::StackName}-StackName'

  EnvironmentSuffix:
    Description: Environment suffix used for this deployment
    Value: !Ref EnvironmentSuffix
    Export:
      Name: !Sub '${AWS::StackName}-EnvironmentSuffix'