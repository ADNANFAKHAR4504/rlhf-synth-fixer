Instructions:This is a HCL Terraform project.
Ignore any files inside the archive/ folder.
Main infrastructure code is in lib/tap_stack.tf.
And lib/provider.tf only for providers. No provider should be in tap_stack.tf file.
Test code is in the test folder. terraform.int.test.ts and terraform.unit.test.ts
All the requirementsare in lib/PROMPT.md.
Your job is to check weather all the requirements are covers in tap_stack.tf file or not, 
The language and platform are also confirmed in metadata.json as Terraform, 
Ask for Help: Use GitHub PR comments to request feedback if needed.

 What this is

- A Terraform project where **all infrastructure code lives in one file**: `lib/tap_stack.tf` (you may later rename it to `tap_stack.tf`).
- `provider.tf` already exists and holds your **AWS provider + S3 backend**. You usually **don’t touch it** unless there’s a real change to providers/backends.
- If you need **multiple environments/regions**, you add **provider aliases in `provider.tf`** and reference those aliases from `lib/tap_stack.tf`.

2 Files at a glance

```
iac-test-automation/
├─ 
├─ lib/
|-- provider.tf               # AWS provider + S3 backend (stable; edit only if truly needed)
│  └─ tap_stack.tf                # (Single file) ALL variables, locals, resources, and outputs
├─ cfn-outputs/
│  └─ all-outputs.json       # Written by CI/CD “Get Deployment Outputs from Deploy stage” step (used by integration tests)
└─ tests/                    # Your own test folders/files (naming is up to you)
├─ └─ terraform.unit.test.ts
   └─ terraform.int.test.ts

   > **Non‑negotiables**
> - Keep **all Terraform logic in `lib/tap_stack.tf`** (variables, locals, resources, outputs).
> - **Do not** put a `provider` block in `tap_stack.tf`. That stays in `provider.tf`.
> - **No external modules**: build resources directly (this is a brand‑new stack).
> - The variable **`aws_region`** must be declared in `tap_stack.tf` and is consumed by `provider.tf`.
> - Integration tests **must read** from `cfn-outputs/all-outputs.json` (CI/CD will create it).
> - Tests must **not** run `terraform init/plan/apply` during test cases stage.

---

Task1: Ensure that the infrastructure in lib/tap_stack.tf fully covers all the requirements in lib/PROMPT.md. If the current code does not satisfy the requirements, fix it before proceeding. Wrte a code for complete requirements:
Consideration: make sure code follow least permissions in roles/policies. do not use * or wildcard. Make sure code follows proper depends_on and relationship between each resources. Make sure we follow security and compliance in all resources creation, like in s3 bucket enalble versioning, encryption, block public access and so on...
Ensure that the code is well-structured, follows best practices, and is easy to understand. 
Ensure that the code is modular, with clear separation of concerns.

Task2: Build JOb: Ensure the build step completes successfully 

Task3: Write and build unit tests that are 100 percentage coverage and passing fully.
Consideration: code is in tests folder for terraform. (no Terraform commands; static validation only).  
```
I need comprehensive Unit tests based on below information
1. I have tap_stack.tf file available inside lib directory and directory name should be read as ../lib/tap_stack.tf
2. It should validate components based on user requirements.
3. avoid running terraform init or apply commands etc in unit tests
4. evaluate full stack with validations/inputs/standards and outputs etc
5. I need test cases in typescript
```

Task4: Write and build integration tests that are 100 percentage coverage and passing fully. integration tests using mock outputs if needed (e.g., before deployment or if cfn-outputs are missing).
Consideration code is in tests folder for terraform.ive checks work without applying Terraform.  
```
Integration tests should not apply any terraform apply or init or deploy commands.
They must run using live AWS read-only checks, driven by the CI/CD outputs file
Our CI/CD pipeline generates a JSON file for resource outputs under the deploy stage called "Get Deployment Outputs".
That JSON file is saved at cfn-outputs/all-outputs.json — use this path in tests.

I need integration tests based on below information:
1. avoid running terraform init or apply commands
2. I need test cases in typescript
3. evaluate full stack with validations/inpurts/standards and outputs etc [positive + edge cases]
4. I have json file available at const p = path.resolve(process.cwd(), "cfn-outputs/all-outputs.json"); so manage full logic and integration parts accordingly
5. Below is my tap_stack.tf file code
```

Task5: Make sure Lint test is passing.

Task6: Generate the Ideal Response MD file content at ./lib/IDEAL_RESPONSE.nd,  Rules for IDEAL_RESPONSE.md: Must provide a complete solution to lib/PROMPT.md.Must align with the actual code in lib/tap_stack.tf. After the solution is ready, and changes are made in tap_stack.tf code, copy it into Ideal response file. And then generate MODEL_FAILURES.md file by compare ideal response with model response files. 
Document all differences in lib/MODEL_FAILURES.md.Task7: Markdown Linting verification: Run markdownlint on all .md files in the lib/ folder. Rules: Must be self-contained (do not reference other .md files).



Task7: FINAL REVIEW for PR CHECKLIST:
Code Quality & Testing:

1. Does the code include appropriate test coverage?

2. Are integration tests provided and passing?

3. Is code style consistent with project standards?

4. Did you review and correct any typos or unclear code?

5. Have you commented any complex sections?

Documentation & Formatting:

1. Is PROMPT.md properly formatted?

2. Is IDEAL_RESPONSE.md properly formatted?

3. Is MODEL_RESPONSE.md properly formatted?

Consistency & Structure:

1. Is the code in IDEAL_RESPONSE.md consistent with tap_stack.py?

2. Have you avoided adding new files or folders unless necessary and documen