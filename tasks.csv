Task_ID,Trainer_Name,Trainer_GH_User,Status,Platform,Language,Difficulty,Subtask,Background,Problem,Environment,Constraints,Subject_Labels
101000782,Claude AI,ayushj1-turing,done,Terraform,HCL,medium,Multi-Environment Consistency & Replication,Your organization is expanding operations across multiple AWS regions to improve latency for global customers. The network team requires identical VPC configurations in each region to maintain consistent security policies and simplify operations management.,"{""background"": ""Your organization is expanding operations across multiple AWS regions to improve latency for global customers. The network team requires identical VPC configurations in each region to maintain consistent security policies and simplify operations management."", ""constraints"": {""count"": 9, ""items"": [""Use Terraform workspaces to manage multiple environments"", ""VPC CIDR blocks must not overlap between regions"", ""Each VPC must have exactly 3 availability zones configured"", ""Private subnets must use /24 CIDR blocks"", ""Public subnets must use /26 CIDR blocks"", ""NAT Gateways must be deployed only in public subnets"", ""All resources must be tagged with Environment and Region keys"", ""Use Terraform modules to avoid code duplication"", ""Route tables must be explicitly associated with each subnet""]}, ""environment"": ""Multi-region AWS deployment targeting us-east-1, eu-west-1, and ap-southeast-1 regions. Each region requires identical VPC architecture with 3 availability zones, public and private subnets, Internet Gateway, and NAT Gateways for high availability. Infrastructure managed through Terraform 1.5+ with HCL configuration. AWS provider version 5.x required. State files stored in S3 with DynamoDB locking. VPCs use non-overlapping CIDR ranges: 10.1.0.0/16 for us-east-1, 10.2.0.0/16 for eu-west-1, and 10.3.0.0/16 for ap-southeast-1."", ""problem"": ""Create a Terraform configuration to deploy identical VPC infrastructure across three AWS regions. The configuration must: 1. Define a reusable VPC module that accepts region-specific parameters. 2. Configure VPCs with non-overlapping CIDR blocks (10.1.0.0/16, 10.2.0.0/16, 10.3.0.0/16). 3. Create 3 public subnets (one per AZ) using /26 CIDR blocks. 4. Create 3 private subnets (one per AZ) using /24 CIDR blocks. 5. Deploy one Internet Gateway per VPC. 6. Place one NAT Gateway in each public subnet for redundancy. 7. Configure route tables with appropriate routes for public and private subnets. 8. Use Terraform workspaces to manage deployments (dev, staging, prod). 9. Apply consistent tags including Environment and Region to all resources. 10. Use data sources to dynamically fetch availability zones for each region. Expected output: A modular Terraform configuration with a VPC module and root configuration that can deploy identical network infrastructure across multiple regions using workspaces, ensuring consistency while accommodating region-specific requirements like CIDR ranges and availability zones."", ""input_file"": null}","Multi-region AWS deployment targeting us-east-1, eu-west-1, and ap-southeast-1 regions. Each region requires identical VPC architecture with 3 availability zones, public and private subnets, Internet Gateway, and NAT Gateways for high availability. Infrastructure managed through Terraform 1.5+ with HCL configuration. AWS provider version 5.x required. State files stored in S3 with DynamoDB locking. VPCs use non-overlapping CIDR ranges: 10.1.0.0/16 for us-east-1, 10.2.0.0/16 for eu-west-1, and 10.3.0.0/16 for ap-southeast-1.","1. Use Terraform workspaces to manage multiple environments
2. VPC CIDR blocks must not overlap between regions
3. Each VPC must have exactly 3 availability zones configured
4. Private subnets must use /24 CIDR blocks
5. Public subnets must use /26 CIDR blocks
6. NAT Gateways must be deployed only in public subnets
7. All resources must be tagged with Environment and Region keys
8. Use Terraform modules to avoid code duplication
9. Route tables must be explicitly associated with each subnet",
101000783,error,anthropic-claude,in_progress,Terraform,HCL,medium,Multi-Environment Consistency & Replication,"A fintech startup needs to ensure their payment processing infrastructure is identical across development, staging, and production environments. They've experienced configuration drift between environments causing production incidents when features work in staging but fail in production.","{""background"": ""A fintech startup needs to ensure their payment processing infrastructure is identical across development, staging, and production environments. They've experienced configuration drift between environments causing production incidents when features work in staging but fail in production."", ""constraints"": {""count"": 10, ""items"": [""Use CDK context values to manage environment-specific configurations"", ""All Lambda functions must use the same runtime version across environments"", ""Database schemas must be versioned and consistent using AWS RDS proxy"", ""API Gateway stages must have identical throttling settings except for rate limits"", ""Environment variables must be stored in AWS Systems Manager Parameter Store"", ""Each environment must have its own VPC with identical CIDR block structures"", ""Security groups must follow the same rule patterns with environment-specific prefixes"", ""CloudWatch alarm thresholds must scale proportionally based on environment size"", ""S3 bucket naming must follow pattern: {company}-{service}-{environment}-{region}"", ""IAM roles must use least-privilege policies with environment-scoped resource ARNs""]}, ""environment"": ""Multi-environment AWS infrastructure spanning three accounts (dev: 111111111111, staging: 222222222222, prod: 333333333333) deployed in us-east-1. Each environment requires VPC with 3 availability zones, public and private subnets with CIDR blocks following 10.{env}.0.0/16 pattern where env is 1 for dev, 2 for staging, 3 for prod. Uses Lambda for compute, RDS PostgreSQL 14.7 for database, API Gateway for REST APIs, S3 for static assets, CloudWatch for monitoring. CDK 2.x with TypeScript, Node.js 18.x required. Deployment uses CodePipeline for CI/CD across environments."", ""problem"": ""Create a CDK TypeScript program to deploy a payment processing API infrastructure that maintains exact parity across development, staging, and production environments. The configuration must: 1. Define a reusable CDK Stack class that accepts environment context (dev/staging/prod) as a parameter. 2. Create VPCs with consistent subnet layouts where CIDR blocks follow 10.X.0.0/16 pattern (X=1 for dev, 2 for staging, 3 for prod). 3. Deploy RDS PostgreSQL instances with environment-appropriate instance sizes (db.t3.micro for dev, db.t3.small for staging, db.r6g.large for prod) while maintaining identical parameter groups. 4. Configure Lambda functions for payment validation with consistent memory allocations and timeout settings across environments. 5. Set up API Gateway REST APIs with environment-specific rate limits (10 req/s for dev, 100 req/s for staging, 1000 req/s for prod) while keeping all other configurations identical. 6. Implement CloudWatch dashboards that automatically adjust metric periods based on environment (5-min for dev, 1-min for staging/prod). 7. Create S3 buckets following the naming convention with lifecycle policies that retain logs for 7 days in dev, 30 days in staging, and 90 days in production. 8. Configure CloudWatch alarms with proportional thresholds where production thresholds are baseline, staging is 50% of production, and dev is 10% of production values. 9. Use CDK context to manage environment-specific values while ensuring type safety. 10. Output the API endpoint URL, RDS endpoint, and S3 bucket names for each deployed environment. Expected output: A CDK application that can be deployed to any environment using 'cdk deploy --context environment=dev|staging|prod' producing identical infrastructure topology with appropriately scaled resources, consistent security policies, and environment-specific operational parameters while preventing configuration drift between environments."", ""input_file"": null}","Multi-environment AWS infrastructure spanning three accounts (dev: 111111111111, staging: 222222222222, prod: 333333333333) deployed in us-east-1. Each environment requires VPC with 3 availability zones, public and private subnets with CIDR blocks following 10.{env}.0.0/16 pattern where env is 1 for dev, 2 for staging, 3 for prod. Uses Lambda for compute, RDS PostgreSQL 14.7 for database, API Gateway for REST APIs, S3 for static assets, CloudWatch for monitoring. CDK 2.x with TypeScript, Node.js 18.x required. Deployment uses CodePipeline for CI/CD across environments.","1. Use CDK context values to manage environment-specific configurations
2. All Lambda functions must use the same runtime version across environments
3. Database schemas must be versioned and consistent using AWS RDS proxy
4. API Gateway stages must have identical throttling settings except for rate limits
5. Environment variables must be stored in AWS Systems Manager Parameter Store
6. Each environment must have its own VPC with identical CIDR block structures
7. Security groups must follow the same rule patterns with environment-specific prefixes
8. CloudWatch alarm thresholds must scale proportionally based on environment size
9. S3 bucket naming must follow pattern: {company}-{service}-{environment}-{region}
10. IAM roles must use least-privilege policies with environment-scoped resource ARNs",
101000784,Claude AI,anthropic-claude,error,Terraform,HCL,hard,Environment Migration,A financial services company is migrating their monolithic application from on-premises to AWS. They need to establish a staging environment that mirrors their production setup before the final cutover. The existing application uses a PostgreSQL database and runs on VMs with specific networking requirements.,"{""background"": ""A financial services company is migrating their monolithic application from on-premises to AWS. They need to establish a staging environment that mirrors their production setup before the final cutover. The existing application uses a PostgreSQL database and runs on VMs with specific networking requirements."", ""constraints"": {""count"": 7, ""items"": [""Database migration must use AWS DMS with continuous replication enabled"", ""All resources must be tagged with Environment, MigrationPhase, and CostCenter tags"", ""RDS instances must use encrypted storage with customer-managed KMS keys"", ""Application servers must run in private subnets with no direct internet access"", ""Database credentials must be stored in AWS Secrets Manager and rotated automatically"", ""VPC peering must be established between staging and existing production VPCs"", ""CloudWatch alarms must monitor DMS replication lag and alert if lag exceeds 5 minutes""]}, ""environment"": ""AWS staging environment in us-east-2 for migration testing. Uses RDS PostgreSQL 14.7 for database tier, EC2 instances in Auto Scaling Groups for application tier, and Application Load Balancer for traffic distribution. Requires CDK 2.x with TypeScript, Node.js 18+, and AWS CLI configured. VPC spans 3 availability zones with private subnets for compute/database and public subnets for ALB. NAT Gateways provide outbound internet access. DMS replication instance connects to on-premises database via VPN connection."", ""problem"": ""Create a CDK TypeScript program to deploy a staging environment for migrating an on-premises application to AWS. The configuration must: 1. Create a VPC with 3 availability zones, each with public and private subnets 2. Deploy an RDS PostgreSQL instance (db.r6g.xlarge) in a Multi-AZ configuration with automated backups 3. Set up AWS DMS with a replication instance (dms.r5.large) to migrate data from on-premises PostgreSQL 4. Create an Auto Scaling Group with EC2 instances (t3.large) running Amazon Linux 2 in private subnets 5. Configure an Application Load Balancer in public subnets with target group health checks 6. Implement VPC peering between the staging VPC and an existing production VPC (vpc-prod-12345) 7. Store database credentials in Secrets Manager with automatic rotation every 30 days 8. Create CloudWatch dashboards showing DMS replication metrics and application health 9. Set up SNS topic for alerts when DMS replication lag exceeds threshold 10. Apply consistent tagging strategy across all resources for cost tracking 11. Configure security groups to allow traffic only between application and database tiers 12. Enable VPC Flow Logs for security compliance. Expected output: A complete CDK application that can be deployed using 'cdk deploy' to create the staging environment. The stack should output the ALB DNS name, DMS endpoint, and VPC peering connection ID. All resources should be properly configured for the migration phase with appropriate monitoring and security controls in place."", ""input_file"": null}","AWS staging environment in us-east-2 for migration testing. Uses RDS PostgreSQL 14.7 for database tier, EC2 instances in Auto Scaling Groups for application tier, and Application Load Balancer for traffic distribution. Requires CDK 2.x with TypeScript, Node.js 18+, and AWS CLI configured. VPC spans 3 availability zones with private subnets for compute/database and public subnets for ALB. NAT Gateways provide outbound internet access. DMS replication instance connects to on-premises database via VPN connection.","1. Database migration must use AWS DMS with continuous replication enabled
2. All resources must be tagged with Environment, MigrationPhase, and CostCenter tags
3. RDS instances must use encrypted storage with customer-managed KMS keys
4. Application servers must run in private subnets with no direct internet access
5. Database credentials must be stored in AWS Secrets Manager and rotated automatically
6. VPC peering must be established between staging and existing production VPCs
7. CloudWatch alarms must monitor DMS replication lag and alert if lag exceeds 5 minutes",
101000785,Claude AI,anthropic-claude,in_progress,Terraform,HCL,medium,Multi-Environment Consistency & Replication,"Your company maintains separate AWS accounts for development, staging, and production environments. The engineering team needs to ensure that critical infrastructure components remain synchronized across all environments while allowing for environment-specific configurations like instance sizes and scaling parameters.","{""background"": ""Your company maintains separate AWS accounts for development, staging, and production environments. The engineering team needs to ensure that critical infrastructure components remain synchronized across all environments while allowing for environment-specific configurations like instance sizes and scaling parameters."", ""constraints"": {""count"": 5, ""items"": [""Use AWS CDK v2 with Go bindings for all infrastructure definitions"", ""Implement a single CDK app that can deploy to multiple AWS accounts using cross-account IAM roles"", ""Environment-specific configurations must be stored in separate JSON configuration files"", ""All S3 buckets must have versioning enabled and use AES256 encryption"", ""RDS instances must use encrypted storage and have automated backups configured""]}, ""environment"": ""Multi-account AWS setup spanning three environments (dev, staging, prod) across us-east-1 region. Each environment has its own AWS account with cross-account IAM roles configured for deployment. Infrastructure includes VPC with public/private subnets, Application Load Balancer, ECS Fargate service, RDS Aurora PostgreSQL cluster, and S3 buckets for static assets. Requires AWS CDK 2.x with Go 1.19+, AWS CLI configured with appropriate credentials. Each environment uses isolated VPCs with CIDR blocks: dev (10.0.0.0/16), staging (10.1.0.0/16), prod (10.2.0.0/16)."", ""problem"": ""Create a CDK Go program to deploy a containerized web application consistently across development, staging, and production AWS accounts. The configuration must: 1. Define a VPC with 2 public and 2 private subnets in each environment. 2. Deploy an ECS Fargate service running a containerized application with environment-specific CPU/memory allocations. 3. Create an Application Load Balancer that routes traffic to the ECS service. 4. Set up an RDS Aurora PostgreSQL cluster with environment-specific instance types. 5. Configure S3 buckets for static assets with CloudFront distributions. 6. Implement IAM roles and policies for ECS task execution and S3 access. 7. Use AWS Systems Manager Parameter Store to manage database connection strings. 8. Apply environment-specific tags to all resources for cost tracking. 9. Configure CloudWatch alarms for ECS service health and RDS CPU utilization. 10. Enable VPC Flow Logs for all environments with different retention periods. Expected output: A CDK Go application with separate stacks for each environment that can be deployed using 'cdk deploy --all' with appropriate AWS profiles. The solution should read environment configurations from JSON files (dev.json, staging.json, prod.json) and apply them during synthesis."", ""input_file"": null}","Multi-account AWS setup spanning three environments (dev, staging, prod) across us-east-1 region. Each environment has its own AWS account with cross-account IAM roles configured for deployment. Infrastructure includes VPC with public/private subnets, Application Load Balancer, ECS Fargate service, RDS Aurora PostgreSQL cluster, and S3 buckets for static assets. Requires AWS CDK 2.x with Go 1.19+, AWS CLI configured with appropriate credentials. Each environment uses isolated VPCs with CIDR blocks: dev (10.0.0.0/16), staging (10.1.0.0/16), prod (10.2.0.0/16).","1. Use AWS CDK v2 with Go bindings for all infrastructure definitions
2. Implement a single CDK app that can deploy to multiple AWS accounts using cross-account IAM roles
3. Environment-specific configurations must be stored in separate JSON configuration files
4. All S3 buckets must have versioning enabled and use AES256 encryption
5. RDS instances must use encrypted storage and have automated backups configured",
101000786,Claude AI,ayushj1-turing,done,Terraform,HCL,hard,Multi-Environment Consistency & Replication,"A fintech startup needs to maintain identical infrastructure across development, staging, and production environments. They've experienced configuration drift between environments leading to deployment failures and require a standardized approach to ensure consistency.","{""background"": ""A fintech startup needs to maintain identical infrastructure across development, staging, and production environments. They've experienced configuration drift between environments leading to deployment failures and require a standardized approach to ensure consistency."", ""constraints"": {""count"": 10, ""items"": [""Use Terraform workspaces to manage multiple environments"", ""Implement remote state storage with environment-specific state files"", ""Use data sources to reference existing VPCs in each environment"", ""Environment-specific variables must be loaded from .tfvars files"", ""RDS instance classes must vary by environment (t3.micro for dev, t3.small for staging, t3.medium for prod)"", ""All resources must be tagged with Environment and ManagedBy tags"", ""Use count or for_each to create environment-specific security group rules"", ""Implement lifecycle rules to prevent accidental deletion of production resources"", ""Use locals to compute environment-specific naming conventions"", ""Output values must include environment-specific endpoints and connection strings""]}, ""environment"": ""Multi-environment AWS deployment across us-east-1 region with separate VPCs per environment (dev: 10.0.0.0/16, staging: 10.1.0.0/16, prod: 10.2.0.0/16). Each environment has existing VPCs with public and private subnets across 2 AZs. Requires Terraform 1.5+ with AWS provider 5.x. State files stored in S3 with DynamoDB locking. IAM roles pre-configured for cross-environment access. Application requires RDS PostgreSQL, ALB, and Auto Scaling Groups."", ""problem"": ""Create a Terraform configuration to deploy a consistent web application infrastructure across dev, staging, and production environments. The configuration must: 1. Define a module structure that accepts environment-specific parameters 2. Create an RDS PostgreSQL instance with environment-appropriate sizing 3. Set up an Application Load Balancer with target group and health checks 4. Configure an Auto Scaling Group with launch template for EC2 instances 5. Implement security groups with environment-specific ingress rules (dev allows wider access) 6. Use Terraform workspaces to manage state separation between environments 7. Create parameter groups for RDS with environment-specific settings 8. Configure CloudWatch alarms with environment-appropriate thresholds 9. Set up S3 buckets for application logs with environment-specific retention policies 10. Implement IAM roles and policies for EC2 instances to access S3 and CloudWatch 11. Use consistent naming patterns that include the environment name 12. Configure backup retention periods that vary by environment (1 day for dev, 7 for staging, 30 for prod). Expected output: A modular Terraform configuration with a main module and environment-specific .tfvars files that can be applied using workspace commands like 'terraform workspace select dev' followed by 'terraform apply -var-file=dev.tfvars', ensuring identical architecture with environment-appropriate resource sizing and security settings."", ""input_file"": null}","Multi-environment AWS deployment across us-east-1 region with separate VPCs per environment (dev: 10.0.0.0/16, staging: 10.1.0.0/16, prod: 10.2.0.0/16). Each environment has existing VPCs with public and private subnets across 2 AZs. Requires Terraform 1.5+ with AWS provider 5.x. State files stored in S3 with DynamoDB locking. IAM roles pre-configured for cross-environment access. Application requires RDS PostgreSQL, ALB, and Auto Scaling Groups.","1. Use Terraform workspaces to manage multiple environments
2. Implement remote state storage with environment-specific state files
3. Use data sources to reference existing VPCs in each environment
4. Environment-specific variables must be loaded from .tfvars files
5. RDS instance classes must vary by environment (t3.micro for dev, t3.small for staging, t3.medium for prod)
6. All resources must be tagged with Environment and ManagedBy tags
7. Use count or for_each to create environment-specific security group rules
8. Implement lifecycle rules to prevent accidental deletion of production resources
9. Use locals to compute environment-specific naming conventions
10. Output values must include environment-specific endpoints and connection strings",
